{"prompt": "def is_json_serializable(val):", "metadata": {"task_id": "Text-Processing/python-benedict/0", "ground_truth": "    json_types = (type(None), bool, dict, float, int, list, str, tuple)\n    return isinstance(val, json_types)", "fpath_tuple": ["Text-Processing", "python-benedict", "benedict", "utils", "type_util.py"], "context_start_lineno": 52, "line_no": 53, "id": "benedict.utils.type_util.is_json_serializable", "target_function_prompt": "def is_json_serializable(val):", "function_signature": "def is_json_serializable(val):"}}
{"prompt": "def convert_to_idn(url):", "metadata": {"task_id": "Text-Processing/feedparser/0", "ground_truth": "    parts = list(urllib.parse.urlsplit(url))\n    try:\n        parts[1].encode('ascii')\n    except UnicodeEncodeError:\n        # the url needs to be converted to idn notation\n        host = parts[1].rsplit(':', 1)\n        newhost = []\n        port = ''\n        if len(host) == 2:\n            port = host.pop()\n        for h in host[0].split('.'):\n            newhost.append(h.encode('idna').decode('utf-8'))\n        parts[1] = '.'.join(newhost)\n        if port:\n            parts[1] += ':' + port\n        return urllib.parse.urlunsplit(parts)\n    else:\n        return url", "fpath_tuple": ["Text-Processing", "feedparser", "feedparser", "urls.py"], "context_start_lineno": 60, "line_no": 65, "id": "feedparser.urls.convert_to_idn", "target_function_prompt": "def convert_to_idn(url):", "function_signature": "def convert_to_idn(url):"}}
{"prompt": "def add_toc_hook(md, min_level=1, max_level=3, heading_id=None):", "metadata": {"task_id": "Text-Processing/mistune/0", "ground_truth": "    if heading_id is None:\n        def heading_id(token, index):\n            return 'toc_' + str(index + 1)\n\n    def toc_hook(md, state):\n        headings = []\n\n        for tok in state.tokens:\n            if tok['type'] == 'heading':\n                level = tok['attrs']['level']\n                if min_level <= level <= max_level:\n                    headings.append(tok)\n\n        toc_items = []\n        for i, tok in enumerate(headings):\n            tok['attrs']['id'] = heading_id(tok, i)\n            toc_items.append(normalize_toc_item(md, tok))\n\n        # save items into state\n        state.env['toc_items'] = toc_items\n\n    md.before_render_hooks.append(toc_hook)", "fpath_tuple": ["Text-Processing", "mistune", "src", "mistune", "toc.py"], "context_start_lineno": 3, "line_no": 22, "id": "mistune.toc.add_toc_hook", "target_function_prompt": "def add_toc_hook(md, min_level=1, max_level=3, heading_id=None):", "function_signature": "def add_toc_hook(md, min_level=1, max_level=3, heading_id=None):"}}
{"prompt": "def table_in_quote(md):", "metadata": {"task_id": "Text-Processing/mistune/1", "ground_truth": "    md.block.insert_rule(md.block.block_quote_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.block_quote_rules, 'nptable', before='paragraph')", "fpath_tuple": ["Text-Processing", "mistune", "src", "mistune", "plugins", "table.py"], "context_start_lineno": 169, "line_no": 171, "id": "mistune.plugins.table.table_in_quote", "target_function_prompt": "def table_in_quote(md):", "function_signature": "def table_in_quote(md):"}}
{"prompt": "def table_in_list(md):", "metadata": {"task_id": "Text-Processing/mistune/2", "ground_truth": "    md.block.insert_rule(md.block.list_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.list_rules, 'nptable', before='paragraph')", "fpath_tuple": ["Text-Processing", "mistune", "src", "mistune", "plugins", "table.py"], "context_start_lineno": 175, "line_no": 177, "id": "mistune.plugins.table.table_in_list", "target_function_prompt": "def table_in_list(md):", "function_signature": "def table_in_list(md):"}}
{"prompt": "def parallel_handler(callback: Callable, texts: List[str], n_jobs: int = 2, **kwargs) -> Generator[\n    List[Any], None, None\n]:", "metadata": {"task_id": "Text-Processing/xmnlp/0", "ground_truth": "    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n    if kwargs:\n        callback = partial(callback, **kwargs)\n    with futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n        for ret in executor.map(callback, texts):\n            yield ret", "fpath_tuple": ["Text-Processing", "xmnlp", "xmnlp", "utils", "__init__.py"], "context_start_lineno": 89, "line_no": 100, "id": "xmnlp.utils.parallel_handler", "target_function_prompt": "def parallel_handler(callback: Callable, texts: List[str], n_jobs: int = 2, **kwargs) -> Generator[\n    List[Any], None, None\n]:", "function_signature": "def parallel_handler(callback: Callable, texts: List[str], n_jobs: int = 2, **kwargs) -> Generator[\n    List[Any], None, None\n]:"}}
{"prompt": "def shorten(text: str, width: int, suffix: str = \"...\") -> str:", "metadata": {"task_id": "Text-Processing/parsel/0", "ground_truth": "    if len(text) <= width:\n        return text\n    if width > len(suffix):\n        return text[: width - len(suffix)] + suffix\n    if width >= 0:\n        return suffix[len(suffix) - width :]\n    raise ValueError(\"width must be equal or greater than 0\")", "fpath_tuple": ["Text-Processing", "parsel", "parsel", "utils.py"], "context_start_lineno": 86, "line_no": 88, "id": "parsel.utils.shorten", "target_function_prompt": "def shorten(text: str, width: int, suffix: str = \"...\") -> str:", "function_signature": "def shorten(text: str, width: int, suffix: str = \"...\") -> str:"}}
{"prompt": "def set_xpathfunc(fname: str, func: Optional[Callable]) -> None:  # type: ignore[type-arg]", "metadata": {"task_id": "Text-Processing/parsel/1", "ground_truth": "    ns_fns = etree.FunctionNamespace(None)  # type: ignore[attr-defined]\n    if func is not None:\n        ns_fns[fname] = func\n    else:\n        del ns_fns[fname]", "fpath_tuple": ["Text-Processing", "parsel", "parsel", "xpathfuncs.py"], "context_start_lineno": 12, "line_no": 26, "id": "parsel.xpathfuncs.set_xpathfunc", "target_function_prompt": "def set_xpathfunc(fname: str, func: Optional[Callable]) -> None:  # type: ignore[type-arg]", "function_signature": "def set_xpathfunc(fname: str, func: Optional[Callable]) -> None:  # type: ignore[type-arg]"}}
{"prompt": "def _get_thread_context():", "metadata": {"task_id": "Text-Processing/dominate/0", "ground_truth": "  context = [threading.current_thread()]\n  if greenlet:\n    context.append(greenlet.getcurrent())\n  return hash(tuple(context))", "fpath_tuple": ["Text-Processing", "dominate", "dominate", "dom_tag.py"], "context_start_lineno": 46, "line_no": 47, "id": "dominate.dom_tag._get_thread_context", "target_function_prompt": "def _get_thread_context():", "function_signature": "def _get_thread_context():"}}
{"prompt": "def system(cmd, data=None):", "metadata": {"task_id": "Text-Processing/dominate/1", "ground_truth": "  import subprocess\n  s = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stdin=subprocess.PIPE)\n  out, err = s.communicate(data)\n  return out.decode('utf8')", "fpath_tuple": ["Text-Processing", "dominate", "dominate", "util.py"], "context_start_lineno": 44, "line_no": 48, "id": "dominate.util.system", "target_function_prompt": "def system(cmd, data=None):", "function_signature": "def system(cmd, data=None):"}}
{"prompt": "def url_unescape(data):", "metadata": {"task_id": "Text-Processing/dominate/2", "ground_truth": "  return re.sub('%([0-9a-fA-F]{2})',\n    lambda m: unichr(int(m.group(1), 16)), data)", "fpath_tuple": ["Text-Processing", "dominate", "dominate", "util.py"], "context_start_lineno": 117, "line_no": 118, "id": "dominate.util.url_unescape", "target_function_prompt": "def url_unescape(data):", "function_signature": "def url_unescape(data):"}}
{"prompt": "    def serialize(cls, value, *args, **kwargs):", "metadata": {"task_id": "Text-Processing/rows/0", "ground_truth": "        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())", "fpath_tuple": ["Text-Processing", "rows", "rows", "fields.py"], "context_start_lineno": 389, "line_no": 390, "id": "rows.fields.DatetimeField.serialize", "target_function_prompt": "    def serialize(cls, value, *args, **kwargs):", "function_signature": "    def serialize(cls, value, *args, **kwargs):"}}
{"prompt": "    def serialize(cls, value, *args, **kwargs):", "metadata": {"task_id": "Text-Processing/rows/1", "ground_truth": "        if value is None:\n            value = \"\"\n        return value", "fpath_tuple": ["Text-Processing", "rows", "rows", "fields.py"], "context_start_lineno": 76, "line_no": 83, "id": "rows.fields.Field.serialize", "target_function_prompt": "    def serialize(cls, value, *args, **kwargs):", "function_signature": "    def serialize(cls, value, *args, **kwargs):"}}
{"prompt": "    def serialize(cls, value, *args, **kwargs):", "metadata": {"task_id": "Text-Processing/rows/2", "ground_truth": "        if value is None:\n            return \"\"\n\n        return six.text_type(value)", "fpath_tuple": ["Text-Processing", "rows", "rows", "fields.py"], "context_start_lineno": 437, "line_no": 438, "id": "rows.fields.EmailField.serialize", "target_function_prompt": "    def serialize(cls, value, *args, **kwargs):", "function_signature": "    def serialize(cls, value, *args, **kwargs):"}}
{"prompt": "def as_string(value):", "metadata": {"task_id": "Text-Processing/rows/3", "ground_truth": "    if isinstance(value, six.binary_type):\n        raise ValueError(\"Binary is not supported\")\n    elif isinstance(value, six.text_type):\n        return value\n    else:\n        return six.text_type(value)", "fpath_tuple": ["Text-Processing", "rows", "rows", "fields.py"], "context_start_lineno": 477, "line_no": 478, "id": "rows.fields.as_string", "target_function_prompt": "def as_string(value):", "function_signature": "def as_string(value):"}}
{"prompt": "def get_items(*indexes):", "metadata": {"task_id": "Text-Processing/rows/4", "ground_truth": "    return lambda obj: tuple(\n        obj[index] if len(obj) > index else None for index in indexes\n    )", "fpath_tuple": ["Text-Processing", "rows", "rows", "fields.py"], "context_start_lineno": 505, "line_no": 512, "id": "rows.fields.get_items", "target_function_prompt": "def get_items(*indexes):", "function_signature": "def get_items(*indexes):"}}
{"prompt": "def load_dict_file(path):", "metadata": {"task_id": "Text-Processing/pycorrector/0", "ground_truth": "    result = {}\n    if path:\n        if not os.path.exists(path):\n            logger.warning('file not found.%s' % path)\n            return result\n        else:\n            with open(path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('#'):\n                        continue\n                    terms = line.split()\n                    if len(terms) < 2:\n                        continue\n                    result[terms[0]] = terms[1]\n    return result", "fpath_tuple": ["Text-Processing", "pycorrector", "pycorrector", "proper_corrector.py"], "context_start_lineno": 30, "line_no": 36, "id": "pycorrector.proper_corrector.load_dict_file", "target_function_prompt": "def load_dict_file(path):", "function_signature": "def load_dict_file(path):"}}
{"prompt": "def envelop_spans(spans, envelopes):", "metadata": {"task_id": "Text-Processing/natasha/0", "ground_truth": "    index = 0\n    for envelope in envelopes:\n        chunk = []\n        while index < len(spans):\n            span = spans[index]\n            index += 1\n            if span.start < envelope.start:\n                continue\n            elif span.stop <= envelope.stop:\n                chunk.append(span)\n            else:\n                index -= 1\n                break\n        yield chunk", "fpath_tuple": ["Text-Processing", "natasha", "natasha", "span.py"], "context_start_lineno": 22, "line_no": 23, "id": "natasha.span.envelop_spans", "target_function_prompt": "def envelop_spans(spans, envelopes):", "function_signature": "def envelop_spans(spans, envelopes):"}}
{"prompt": "def parse_unique_urlencoded(content):", "metadata": {"task_id": "Internet/google-api-python-client/0", "ground_truth": "    urlencoded_params = urllib.parse.parse_qs(content)\n    params = {}\n    for key, value in urlencoded_params.items():\n        if len(value) != 1:\n            msg = \"URL-encoded content contains a repeated value:\" \"%s -> %s\" % (\n                key,\n                \", \".join(value),\n            )\n            raise ValueError(msg)\n        params[key] = value[0]\n    return params", "fpath_tuple": ["Internet", "google-api-python-client", "googleapiclient", "_helpers.py"], "context_start_lineno": 140, "line_no": 152, "id": "googleapiclient._helpers.parse_unique_urlencoded", "target_function_prompt": "def parse_unique_urlencoded(content):", "function_signature": "def parse_unique_urlencoded(content):"}}
{"prompt": "async def auto_aiter(\n    iterable: \"t.Union[t.AsyncIterable[V], t.Iterable[V]]\",\n) -> \"t.AsyncIterator[V]\":", "metadata": {"task_id": "Internet/Jinja2/0", "ground_truth": "    if hasattr(iterable, \"__aiter__\"):\n        async for item in t.cast(\"t.AsyncIterable[V]\", iterable):\n            yield item\n    else:\n        for item in t.cast(\"t.Iterable[V]\", iterable):\n            yield item", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "async_utils.py"], "context_start_lineno": 69, "line_no": 72, "id": "jinja2.async_utils.auto_aiter", "target_function_prompt": "async def auto_aiter(\n    iterable: \"t.Union[t.AsyncIterable[V], t.Iterable[V]]\",\n) -> \"t.AsyncIterator[V]\":", "function_signature": "async def auto_aiter(\n    iterable: \"t.Union[t.AsyncIterable[V], t.Iterable[V]]\",\n) -> \"t.AsyncIterator[V]\":"}}
{"prompt": "def consume(iterable: t.Iterable[t.Any]) -> None:", "metadata": {"task_id": "Internet/Jinja2/1", "ground_truth": "    for _ in iterable:\n        pass", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "utils.py"], "context_start_lineno": 111, "line_no": 113, "id": "jinja2.utils.consume", "target_function_prompt": "def consume(iterable: t.Iterable[t.Any]) -> None:", "function_signature": "def consume(iterable: t.Iterable[t.Any]) -> None:"}}
{"prompt": "def segment(sentence, cut_type='word', pos=False):", "metadata": {"task_id": "Text-Processing/pycorrector/1", "ground_truth": "    if pos:\n        if cut_type == 'word':\n            word_pos_seq = posseg.lcut(sentence)\n            word_seq, pos_seq = [], []\n            for w, p in word_pos_seq:\n                word_seq.append(w)\n                pos_seq.append(p)\n            return word_seq, pos_seq\n        elif cut_type == 'char':\n            word_seq = list(sentence)\n            pos_seq = []\n            for w in word_seq:\n                w_p = posseg.lcut(w)\n                pos_seq.append(w_p[0].flag)\n            return word_seq, pos_seq\n    else:\n        if cut_type == 'word':\n            return jieba.lcut(sentence)\n        elif cut_type == 'char':\n            return list(sentence)", "fpath_tuple": ["Text-Processing", "pycorrector", "pycorrector", "utils", "tokenizer.py"], "context_start_lineno": 100, "line_no": 108, "id": "pycorrector.utils.tokenizer.segment", "target_function_prompt": "def segment(sentence, cut_type='word', pos=False):", "function_signature": "def segment(sentence, cut_type='word', pos=False):"}}
{"prompt": "def object_type_repr(obj: t.Any) -> str:", "metadata": {"task_id": "Internet/Jinja2/2", "ground_truth": "    if obj is None:\n        return \"None\"\n    elif obj is Ellipsis:\n        return \"Ellipsis\"\n\n    cls = type(obj)\n\n    if cls.__module__ == \"builtins\":\n        return f\"{cls.__name__} object\"\n\n    return f\"{cls.__module__}.{cls.__name__} object\"", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "utils.py"], "context_start_lineno": 164, "line_no": 169, "id": "jinja2.utils.object_type_repr", "target_function_prompt": "def object_type_repr(obj: t.Any) -> str:", "function_signature": "def object_type_repr(obj: t.Any) -> str:"}}
{"prompt": "    def setdefault(self, key: t.Any, default: t.Any = None) -> t.Any:", "metadata": {"task_id": "Internet/Jinja2/3", "ground_truth": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "utils.py"], "context_start_lineno": 469, "line_no": 473, "id": "jinja2.utils.LRUCache.setdefault", "target_function_prompt": "    def setdefault(self, key: t.Any, default: t.Any = None) -> t.Any:", "function_signature": "    def setdefault(self, key: t.Any, default: t.Any = None) -> t.Any:"}}
{"prompt": "    def _compute_word_freq(list_of_words):", "metadata": {"task_id": "Internet/sumy/0", "ground_truth": "        word_freq = {}\n        for w in list_of_words:\n            word_freq[w] = word_freq.get(w, 0) + 1\n        return word_freq", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "sum_basic.py"], "context_start_lineno": 48, "line_no": 49, "id": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "target_function_prompt": "    def _compute_word_freq(list_of_words):", "function_signature": "    def _compute_word_freq(list_of_words):"}}
{"prompt": "    def _compute_average_probability_of_words(word_freq_in_doc, content_words_in_sentence):", "metadata": {"task_id": "Internet/sumy/1", "ground_truth": "        content_words_count = len(content_words_in_sentence)\n        if content_words_count > 0:\n            word_freq_sum = sum([word_freq_in_doc[w] for w in content_words_in_sentence])\n            word_freq_avg = word_freq_sum / content_words_count\n            return word_freq_avg\n        else:\n            return 0", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "sum_basic.py"], "context_start_lineno": 71, "line_no": 72, "id": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "target_function_prompt": "    def _compute_average_probability_of_words(word_freq_in_doc, content_words_in_sentence):", "function_signature": "    def _compute_average_probability_of_words(word_freq_in_doc, content_words_in_sentence):"}}
{"prompt": "    def _compute_idf(sentences):", "metadata": {"task_id": "Internet/sumy/2", "ground_truth": "        idf_metrics = {}\n        sentences_count = len(sentences)\n\n        for sentence in sentences:\n            for term in sentence:\n                if term not in idf_metrics:\n                    n_j = sum(1 for s in sentences if term in s)\n                    idf_metrics[term] = math.log(sentences_count / (1 + n_j))\n\n        return idf_metrics", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "lex_rank.py"], "context_start_lineno": 77, "line_no": 78, "id": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "target_function_prompt": "    def _compute_idf(sentences):", "function_signature": "    def _compute_idf(sentences):"}}
{"prompt": "    def cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics):", "metadata": {"task_id": "Internet/sumy/3", "ground_truth": "        unique_words1 = frozenset(sentence1)\n        unique_words2 = frozenset(sentence2)\n        common_words = unique_words1 & unique_words2\n\n        numerator = 0.0\n        for term in common_words:\n            numerator += tf1[term]*tf2[term] * idf_metrics[term]**2\n\n        denominator1 = sum((tf1[t]*idf_metrics[t])**2 for t in unique_words1)\n        denominator2 = sum((tf2[t]*idf_metrics[t])**2 for t in unique_words2)\n\n        if denominator1 > 0 and denominator2 > 0:\n            return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))\n        else:\n            return 0.0", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "lex_rank.py"], "context_start_lineno": 118, "line_no": 141, "id": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "target_function_prompt": "    def cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics):", "function_signature": "    def cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics):"}}
{"prompt": "def _get_ngrams(n, text):", "metadata": {"task_id": "Internet/sumy/4", "ground_truth": "    ngram_set = set()\n    text_length = len(text)\n    max_index_ngram_start = text_length - n\n    for i in range(max_index_ngram_start + 1):\n        ngram_set.add(tuple(text[i:i + n]))\n    return ngram_set", "fpath_tuple": ["Internet", "sumy", "sumy", "evaluation", "rouge.py"], "context_start_lineno": 8, "line_no": 9, "id": "sumy.evaluation.rouge._get_ngrams", "target_function_prompt": "def _get_ngrams(n, text):", "function_signature": "def _get_ngrams(n, text):"}}
{"prompt": "def _split_into_words(sentences):", "metadata": {"task_id": "Internet/sumy/5", "ground_truth": "    full_text_words = []\n    for s in sentences:\n        if not isinstance(s, Sentence):\n            raise (ValueError(\"Object in collection must be of type Sentence\"))\n        full_text_words.extend(s.words)\n    return full_text_words", "fpath_tuple": ["Internet", "sumy", "sumy", "evaluation", "rouge.py"], "context_start_lineno": 17, "line_no": 18, "id": "sumy.evaluation.rouge._split_into_words", "target_function_prompt": "def _split_into_words(sentences):", "function_signature": "def _split_into_words(sentences):"}}
{"prompt": "def register_router(router_class):", "metadata": {"task_id": "Internet/falcon/0", "ground_truth": "    def wraps(fn):\n        if router_class in _supported_routers:\n            raise ValueError(\n                'Another function is already registered'\n                ' for the router {}'.format(router_class)\n            )\n        _supported_routers[router_class] = fn\n        return fn\n\n    return wraps", "fpath_tuple": ["Internet", "falcon", "falcon", "inspect.py"], "context_start_lineno": 70, "line_no": 88, "id": "falcon.inspect.register_router", "target_function_prompt": "def register_router(router_class):", "function_signature": "def register_router(router_class):"}}
{"prompt": "def inspect_compiled_router(router: CompiledRouter) -> 'List[RouteInfo]':", "metadata": {"task_id": "Internet/falcon/1", "ground_truth": "    def _traverse(roots, parent):\n        for root in roots:\n            path = parent + '/' + root.raw_segment\n            if root.resource is not None:\n                methods = []\n                if root.method_map:\n                    for method, func in root.method_map.items():\n                        if isinstance(func, partial):\n                            real_func = func.func\n                        else:\n                            real_func = func\n\n                        source_info = _get_source_info(real_func)\n                        internal = _is_internal(real_func)\n\n                        method_info = RouteMethodInfo(\n                            method, source_info, real_func.__name__, internal\n                        )\n                        methods.append(method_info)\n                source_info, class_name = _get_source_info_and_name(root.resource)\n\n                route_info = RouteInfo(path, class_name, source_info, methods)\n                routes.append(route_info)\n\n            if root.children:\n                _traverse(root.children, path)\n\n    routes = []  # type: List[RouteInfo]\n    _traverse(router._roots, '')\n    return routes", "fpath_tuple": ["Internet", "falcon", "falcon", "inspect.py"], "context_start_lineno": 203, "line_no": 215, "id": "falcon.inspect.inspect_compiled_router", "target_function_prompt": "def inspect_compiled_router(router: CompiledRouter) -> 'List[RouteInfo]':", "function_signature": "def inspect_compiled_router(router: CompiledRouter) -> 'List[RouteInfo]':"}}
{"prompt": "def _is_internal(obj):", "metadata": {"task_id": "Internet/falcon/2", "ground_truth": "    module = inspect.getmodule(obj)\n    if module:\n        return module.__name__.startswith('falcon.')\n    return False", "fpath_tuple": ["Internet", "falcon", "falcon", "inspect.py"], "context_start_lineno": 786, "line_no": 788, "id": "falcon.inspect._is_internal", "target_function_prompt": "def _is_internal(obj):", "function_signature": "def _is_internal(obj):"}}
{"prompt": "def load_app(parser, args):\n", "metadata": {"task_id": "Internet/falcon/3", "ground_truth": "    try:\n        module, instance = args.app_module.split(':', 1)\n    except ValueError:\n        parser.error(\n            'The app_module must include a colon between the module and instance'\n        )\n    try:\n        app = getattr(importlib.import_module(module), instance)\n    except AttributeError:\n        parser.error('{!r} not found in module {!r}'.format(instance, module))\n\n    if not isinstance(app, falcon.App):\n        if callable(app):\n            app = app()\n            if not isinstance(app, falcon.App):\n                parser.error(\n                    '{} did not return a falcon.App instance'.format(args.app_module)\n                )\n        else:\n            parser.error(\n                'The instance must be of falcon.App or be '\n                'a callable without args that returns falcon.App'\n            )\n    return app", "fpath_tuple": ["Internet", "falcon", "falcon", "cmd", "inspect_app.py"], "context_start_lineno": 60, "line_no": 62, "id": "falcon.cmd.inspect_app.load_app", "target_function_prompt": "def load_app(parser, args):\n", "function_signature": "def load_app(parser, args):\n"}}
{"prompt": "def make_parser():", "metadata": {"task_id": "Internet/falcon/4", "ground_truth": "    parser = argparse.ArgumentParser(\n        description='Example: falcon-inspect-app myprogram:app'\n    )\n    parser.add_argument(\n        '-r',\n        '--route_only',\n        action='store_true',\n        help='Prints only the information regarding the routes',\n    )\n    parser.add_argument(\n        '-v',\n        '--verbose',\n        action='store_true',\n        help='More verbose output',\n    )\n    parser.add_argument(\n        '-i',\n        '--internal',\n        action='store_true',\n        help='Print also internal falcon route methods and error handlers',\n    )\n    parser.add_argument(\n        'app_module',\n        help='The module and app to inspect. Example: myapp.somemodule:api',\n    )\n    return parser", "fpath_tuple": ["Internet", "falcon", "falcon", "cmd", "inspect_app.py"], "context_start_lineno": 30, "line_no": 32, "id": "falcon.cmd.inspect_app.make_parser", "target_function_prompt": "def make_parser():", "function_signature": "def make_parser():"}}
{"prompt": "def unquote_string(quoted):", "metadata": {"task_id": "Internet/falcon/5", "ground_truth": "    if len(quoted) < 2:\n        return quoted\n    elif quoted[0] != '\"' or quoted[-1] != '\"':\n        # return original one, prevent side-effect\n        return quoted\n\n    tmp_quoted = quoted[1:-1]\n\n    # PERF(philiptzou): Most header strings don't contain \"quoted-pair\" which\n    # defined by RFC 7320. We use this little trick (quick string search) to\n    # speed up string parsing by preventing unnecessary processes if possible.\n    if '\\\\' not in tmp_quoted:\n        return tmp_quoted\n    elif r'\\\\' not in tmp_quoted:\n        return tmp_quoted.replace('\\\\', '')\n    else:\n        return '\\\\'.join([q.replace('\\\\', '') for q in tmp_quoted.split(r'\\\\')])", "fpath_tuple": ["Internet", "falcon", "falcon", "util", "uri.py"], "context_start_lineno": 504, "line_no": 517, "id": "falcon.util.uri.unquote_string", "target_function_prompt": "def unquote_string(quoted):", "function_signature": "def unquote_string(quoted):"}}
{"prompt": "def get_argnames(func):", "metadata": {"task_id": "Internet/falcon/6", "ground_truth": "    sig = inspect.signature(func)\n\n    args = [\n        param.name\n        for param in sig.parameters.values()\n        if param.kind\n        not in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD)\n    ]\n\n    # NOTE(kgriffs): Depending on the version of Python, 'self' may or may not\n    # be present, so we normalize the results by removing 'self' as needed.\n    # Note that this behavior varies between 3.x versions.\n    if args and args[0] == 'self':\n        args = args[1:]\n\n    return args", "fpath_tuple": ["Internet", "falcon", "falcon", "util", "misc.py"], "context_start_lineno": 286, "line_no": 297, "id": "falcon.util.misc.get_argnames", "target_function_prompt": "def get_argnames(func):", "function_signature": "def get_argnames(func):"}}
{"prompt": "def _is_asgi_app(app):", "metadata": {"task_id": "Internet/falcon/7", "ground_truth": "    app_args = inspect.getfullargspec(app).args\n    num_app_args = len(app_args)\n\n    # NOTE(kgriffs): Technically someone could name the \"self\" or \"cls\"\n    #   arg something else, but we will make the simplifying\n    #   assumption that this is rare enough to not worry about.\n    if app_args[0] in {'cls', 'self'}:\n        num_app_args -= 1\n\n    is_asgi = num_app_args == 3\n\n    return is_asgi", "fpath_tuple": ["Internet", "falcon", "falcon", "testing", "client.py"], "context_start_lineno": 2160, "line_no": 2161, "id": "falcon.testing.client._is_asgi_app", "target_function_prompt": "def _is_asgi_app(app):", "function_signature": "def _is_asgi_app(app):"}}
{"prompt": "    def convert(self, value):", "metadata": {"task_id": "Internet/falcon/8", "ground_truth": "        try:\n            return uuid.UUID(value)\n        except ValueError:\n            return None", "fpath_tuple": ["Internet", "falcon", "falcon", "routing", "converters.py"], "context_start_lineno": 127, "line_no": 128, "id": "falcon.routing.converters.UUIDConverter.convert", "target_function_prompt": "    def convert(self, value):", "function_signature": "    def convert(self, value):"}}
{"prompt": "def make_utc(dt: datetime) -> datetime:", "metadata": {"task_id": "Internet/djangorestframework-simplejwt/0", "ground_truth": "    if settings.USE_TZ and is_naive(dt):\n        return make_aware(dt, timezone=timezone.utc)\n\n    return dt", "fpath_tuple": ["Internet", "djangorestframework-simplejwt", "rest_framework_simplejwt", "utils.py"], "context_start_lineno": 17, "line_no": 18, "id": "rest_framework_simplejwt.utils.make_utc", "target_function_prompt": "def make_utc(dt: datetime) -> datetime:", "function_signature": "def make_utc(dt: datetime) -> datetime:"}}
{"prompt": "def fib(cv=1, lv=0):", "metadata": {"task_id": "Internet/boto/0", "ground_truth": "    if cv is None:\n        cv = 1\n    if lv is None:\n        lv = 0\n    return cv + lv", "fpath_tuple": ["Internet", "boto", "boto", "sdb", "db", "sequence.py"], "context_start_lineno": 90, "line_no": 93, "id": "boto.sdb.db.sequence.fib", "target_function_prompt": "def fib(cv=1, lv=0):", "function_signature": "def fib(cv=1, lv=0):"}}
{"prompt": "    def add_rule(self, rule):", "metadata": {"task_id": "Internet/boto/1", "ground_truth": "        self.append(rule)\n        return self", "fpath_tuple": ["Internet", "boto", "boto", "s3", "website.py"], "context_start_lineno": 141, "line_no": 151, "id": "boto.s3.website.RoutingRules.add_rule", "target_function_prompt": "    def add_rule(self, rule):", "function_signature": "    def add_rule(self, rule):"}}
{"prompt": "    def _canned_policy(resource, expires):", "metadata": {"task_id": "Internet/boto/2", "ground_truth": "        policy = ('{\"Statement\":[{\"Resource\":\"%(resource)s\",'\n                  '\"Condition\":{\"DateLessThan\":{\"AWS:EpochTime\":'\n                  '%(expires)s}}}]}' % locals())\n        return policy", "fpath_tuple": ["Internet", "boto", "boto", "cloudfront", "distribution.py"], "context_start_lineno": 616, "line_no": 620, "id": "boto.cloudfront.distribution.Distribution._canned_policy", "target_function_prompt": "    def _canned_policy(resource, expires):", "function_signature": "    def _canned_policy(resource, expires):"}}
{"prompt": "    def escape(self, p):", "metadata": {"task_id": "Internet/boto/3", "ground_truth": "        if not p[0] == \"/\":\n            p = \"/%s\" % p\n        return urllib.parse.quote(p, safe = \"/*\")", "fpath_tuple": ["Internet", "boto", "boto", "cloudfront", "invalidation.py"], "context_start_lineno": 69, "line_no": 71, "id": "boto.cloudfront.invalidation.InvalidationBatch.escape", "target_function_prompt": "    def escape(self, p):", "function_signature": "    def escape(self, p):"}}
{"prompt": "def get_status_code(resp, start=9, stop=12):", "metadata": {"task_id": "Internet/proxybroker/0", "ground_truth": "    try:\n        code = int(resp[start:stop])\n    except ValueError:\n        return 400  # Bad Request\n    else:\n        return code", "fpath_tuple": ["Internet", "proxybroker", "proxybroker", "utils.py"], "context_start_lineno": 58, "line_no": 59, "id": "proxybroker.utils.get_status_code", "target_function_prompt": "def get_status_code(resp, start=9, stop=12):", "function_signature": "def get_status_code(resp, start=9, stop=12):"}}
{"prompt": "def scope_to_list(scope):", "metadata": {"task_id": "Internet/Authlib/0", "ground_truth": "    if isinstance(scope, (tuple, list, set)):\n        return [to_unicode(s) for s in scope]\n    elif scope is None:\n        return None\n    return scope.strip().split()", "fpath_tuple": ["Internet", "Authlib", "authlib", "oauth2", "rfc6749", "util.py"], "context_start_lineno": 14, "line_no": 16, "id": "authlib.oauth2.rfc6749.util.scope_to_list", "target_function_prompt": "def scope_to_list(scope):", "function_signature": "def scope_to_list(scope):"}}
{"prompt": "def to_unicode(x, charset='utf-8', errors='strict'):", "metadata": {"task_id": "Internet/Authlib/1", "ground_truth": "    if x is None or isinstance(x, str):\n        return x\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    return str(x)", "fpath_tuple": ["Internet", "Authlib", "authlib", "common", "encoding.py"], "context_start_lineno": 17, "line_no": 18, "id": "authlib.common.encoding.to_unicode", "target_function_prompt": "def to_unicode(x, charset='utf-8', errors='strict'):", "function_signature": "def to_unicode(x, charset='utf-8', errors='strict'):"}}
{"prompt": "def to_bytes(x, charset='utf-8', errors='strict'):", "metadata": {"task_id": "Internet/Authlib/2", "ground_truth": "    if x is None:\n        return None\n    if isinstance(x, bytes):\n        return x\n    if isinstance(x, str):\n        return x.encode(charset, errors)\n    if isinstance(x, (int, float)):\n        return str(x).encode(charset, errors)\n    return bytes(x)", "fpath_tuple": ["Internet", "Authlib", "authlib", "common", "encoding.py"], "context_start_lineno": 5, "line_no": 6, "id": "authlib.common.encoding.to_bytes", "target_function_prompt": "def to_bytes(x, charset='utf-8', errors='strict'):", "function_signature": "def to_bytes(x, charset='utf-8', errors='strict'):"}}
{"prompt": "def urlsafe_b64decode(s):", "metadata": {"task_id": "Internet/Authlib/3", "ground_truth": "    s += b'=' * (-len(s) % 4)\n    return base64.urlsafe_b64decode(s)", "fpath_tuple": ["Internet", "Authlib", "authlib", "common", "encoding.py"], "context_start_lineno": 39, "line_no": 40, "id": "authlib.common.encoding.urlsafe_b64decode", "target_function_prompt": "def urlsafe_b64decode(s):", "function_signature": "def urlsafe_b64decode(s):"}}
{"prompt": "def table_exists(conn, table):", "metadata": {"task_id": "Database/csvs-to-sqlite/0", "ground_truth": "    return conn.execute(\n        \"\"\"\n        select count(*) from sqlite_master\n        where type=\"table\" and name=?\n    \"\"\",\n        [table],\n    ).fetchone()[0]", "fpath_tuple": ["Database", "csvs-to-sqlite", "csvs_to_sqlite", "utils.py"], "context_start_lineno": 256, "line_no": 257, "id": "csvs_to_sqlite.utils.table_exists", "target_function_prompt": "def table_exists(conn, table):", "function_signature": "def table_exists(conn, table):"}}
{"prompt": "    def get_tablenames(filename):", "metadata": {"task_id": "Database/sqlitedict/0", "ground_truth": "        if not os.path.isfile(filename):\n            raise IOError('file %s does not exist' % (filename))\n        GET_TABLENAMES = 'SELECT name FROM sqlite_master WHERE type=\"table\"'\n        with sqlite3.connect(filename) as conn:\n            cursor = conn.execute(GET_TABLENAMES)\n            res = cursor.fetchall()\n\n        return [name[0] for name in res]", "fpath_tuple": ["Database", "sqlitedict", "sqlitedict.py"], "context_start_lineno": 358, "line_no": 360, "id": "sqlitedict.SqliteDict.get_tablenames", "target_function_prompt": "    def get_tablenames(filename):", "function_signature": "    def get_tablenames(filename):"}}
{"prompt": "def query_starts_with(query, prefixes):", "metadata": {"task_id": "Database/litecli/0", "ground_truth": "    prefixes = [prefix.lower() for prefix in prefixes]\n    formatted_sql = sqlparse.format(query.lower(), strip_comments=True)\n    return bool(formatted_sql) and formatted_sql.split()[0] in prefixes", "fpath_tuple": ["Database", "litecli", "litecli", "packages", "parseutils.py"], "context_start_lineno": 203, "line_no": 205, "id": "litecli.packages.parseutils.query_starts_with", "target_function_prompt": "def query_starts_with(query, prefixes):", "function_signature": "def query_starts_with(query, prefixes):"}}
{"prompt": "    def filter_renderers(self, renderers, format):", "metadata": {"task_id": "Internet/djangorestframework/0", "ground_truth": "        renderers = [renderer for renderer in renderers\n                     if renderer.format == format]\n        if not renderers:\n            raise Http404\n        return renderers", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "negotiation.py"], "context_start_lineno": 79, "line_no": 84, "id": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "target_function_prompt": "    def filter_renderers(self, renderers, format):", "function_signature": "    def filter_renderers(self, renderers, format):"}}
{"prompt": "def as_string(value):", "metadata": {"task_id": "Internet/djangorestframework/1", "ground_truth": "    if value is None:\n        return ''\n    return '%s' % value", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "templatetags", "rest_framework.py"], "context_start_lineno": 158, "line_no": 159, "id": "rest_framework.templatetags.rest_framework.as_string", "target_function_prompt": "def as_string(value):", "function_signature": "def as_string(value):"}}
{"prompt": "def add_nested_class(value):", "metadata": {"task_id": "Internet/djangorestframework/2", "ground_truth": "    if isinstance(value, dict):\n        return 'class=nested'\n    if isinstance(value, list) and any(isinstance(item, (list, dict)) for item in value):\n        return 'class=nested'\n    return ''", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "templatetags", "rest_framework.py"], "context_start_lineno": 285, "line_no": 286, "id": "rest_framework.templatetags.rest_framework.add_nested_class", "target_function_prompt": "def add_nested_class(value):", "function_signature": "def add_nested_class(value):"}}
{"prompt": "    def loads(self, bstruct):", "metadata": {"task_id": "Internet/pyramid/0", "ground_truth": "        try:\n            return pickle.loads(bstruct)\n        except Exception:\n            # this block should catch at least:\n            # ValueError, AttributeError, ImportError; but more to be safe\n            raise ValueError", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "session.py"], "context_start_lineno": 66, "line_no": 68, "id": "pyramid.session.PickleSerializer.loads", "target_function_prompt": "    def loads(self, bstruct):", "function_signature": "    def loads(self, bstruct):"}}
{"prompt": "    def flash(self, msg, queue='', allow_duplicate=True):", "metadata": {"task_id": "Internet/pyramid/1", "ground_truth": "        storage = self.setdefault('_f_' + queue, [])\n        if allow_duplicate or (msg not in storage):\n            storage.append(msg)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "testing.py"], "context_start_lineno": 246, "line_no": 247, "id": "pyramid.testing.DummySession.flash", "target_function_prompt": "    def flash(self, msg, queue='', allow_duplicate=True):", "function_signature": "    def flash(self, msg, queue='', allow_duplicate=True):"}}
{"prompt": "    def pop_flash(self, queue=''):", "metadata": {"task_id": "Internet/pyramid/2", "ground_truth": "        storage = self.pop('_f_' + queue, [])\n        return storage", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "testing.py"], "context_start_lineno": 251, "line_no": 252, "id": "pyramid.testing.DummySession.pop_flash", "target_function_prompt": "    def pop_flash(self, queue=''):", "function_signature": "    def pop_flash(self, queue=''):"}}
{"prompt": "    def peek_flash(self, queue=''):", "metadata": {"task_id": "Internet/pyramid/3", "ground_truth": "        storage = self.get('_f_' + queue, [])\n        return storage", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "testing.py"], "context_start_lineno": 255, "line_no": 256, "id": "pyramid.testing.DummySession.peek_flash", "target_function_prompt": "    def peek_flash(self, queue=''):", "function_signature": "    def peek_flash(self, queue=''):"}}
{"prompt": "    def new_csrf_token(self):", "metadata": {"task_id": "Internet/pyramid/4", "ground_truth": "        token = '0123456789012345678901234567890123456789'\n        self['_csrft_'] = token\n        return token", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "testing.py"], "context_start_lineno": 259, "line_no": 260, "id": "pyramid.testing.DummySession.new_csrf_token", "target_function_prompt": "    def new_csrf_token(self):", "function_signature": "    def new_csrf_token(self):"}}
{"prompt": "def view_defaults(**settings):", "metadata": {"task_id": "Internet/pyramid/5", "ground_truth": "    def wrap(wrapped):\n        wrapped.__view_defaults__ = settings\n        return wrapped\n\n    return wrap", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "view.py"], "context_start_lineno": 263, "line_no": 272, "id": "pyramid.view.view_defaults", "target_function_prompt": "def view_defaults(**settings):", "function_signature": "def view_defaults(**settings):"}}
{"prompt": "def bytes_(s, encoding='latin-1', errors='strict'):", "metadata": {"task_id": "Internet/pyramid/6", "ground_truth": "    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "util.py"], "context_start_lineno": 37, "line_no": 40, "id": "pyramid.util.bytes_", "target_function_prompt": "def bytes_(s, encoding='latin-1', errors='strict'):", "function_signature": "def bytes_(s, encoding='latin-1', errors='strict'):"}}
{"prompt": "def parse_vars(args):", "metadata": {"task_id": "Internet/pyramid/7", "ground_truth": "    result = {}\n    for arg in args:\n        if '=' not in arg:\n            raise ValueError('Variable assignment %r invalid (no \"=\")' % arg)\n        name, value = arg.split('=', 1)\n        result[name] = value\n    return result", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "scripts", "common.py"], "context_start_lineno": 3, "line_no": 8, "id": "pyramid.scripts.common.parse_vars", "target_function_prompt": "def parse_vars(args):", "function_signature": "def parse_vars(args):"}}
{"prompt": "    def _find_multi_routes(self, mapper, request):", "metadata": {"task_id": "Internet/pyramid/8", "ground_truth": "        infos = []\n        path = request.path_info\n        # find all routes that match path, regardless of predicates\n        for route in mapper.get_routes():\n            match = route.match(path)\n            if match is not None:\n                info = {'match': match, 'route': route}\n                infos.append(info)\n        return infos", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "scripts", "pviews.py"], "context_start_lineno": 69, "line_no": 70, "id": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "target_function_prompt": "    def _find_multi_routes(self, mapper, request):", "function_signature": "    def _find_multi_routes(self, mapper, request):"}}
{"prompt": "    def guess_server_url(self, loader, server_name, global_conf=None):", "metadata": {"task_id": "Internet/pyramid/9", "ground_truth": "        server_name = server_name or 'main'\n        settings = loader.get_settings('server:' + server_name, global_conf)\n        if 'port' in settings:\n            return 'http://127.0.0.1:{port}'.format(**settings)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "scripts", "pserve.py"], "context_start_lineno": 173, "line_no": 174, "id": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "target_function_prompt": "    def guess_server_url(self, loader, server_name, global_conf=None):", "function_signature": "    def guess_server_url(self, loader, server_name, global_conf=None):"}}
{"prompt": "def pep8_to_camel_case(name: str, initial: bool = False) -> str:", "metadata": {"task_id": "Database/happybase/0", "ground_truth": "    chunks = name.split('_')\n    converted = [s.capitalize() for s in chunks]\n    if initial:\n        return ''.join(converted)\n    else:\n        return chunks[0].lower() + ''.join(converted[1:])", "fpath_tuple": ["Database", "happybase", "aiohappybase", "_util.py"], "context_start_lineno": 26, "line_no": 28, "id": "aiohappybase._util.pep8_to_camel_case", "target_function_prompt": "def pep8_to_camel_case(name: str, initial: bool = False) -> str:", "function_signature": "def pep8_to_camel_case(name: str, initial: bool = False) -> str:"}}
{"prompt": "def bytes_increment(b: bytes) -> Optional[bytes]:", "metadata": {"task_id": "Database/happybase/1", "ground_truth": "    assert isinstance(b, bytes)\n    b = bytearray(b)  # Used subset of its API is the same on Python 2 and 3.\n    for i in range(len(b) - 1, -1, -1):\n        if b[i] != 0xff:\n            b[i] += 1\n            return bytes(b[:i+1])\n    return None", "fpath_tuple": ["Database", "happybase", "aiohappybase", "_util.py"], "context_start_lineno": 60, "line_no": 71, "id": "aiohappybase._util.bytes_increment", "target_function_prompt": "def bytes_increment(b: bytes) -> Optional[bytes]:", "function_signature": "def bytes_increment(b: bytes) -> Optional[bytes]:"}}
{"prompt": "def ensure_dir_exists(path):", "metadata": {"task_id": "Database/mssql-cli/0", "ground_truth": "    parent_dir = expanduser(dirname(path))\n    if not os.path.exists(parent_dir):\n        os.makedirs(parent_dir)", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "config.py"], "context_start_lineno": 28, "line_no": 29, "id": "mssqlcli.config.ensure_dir_exists", "target_function_prompt": "def ensure_dir_exists(path):", "function_signature": "def ensure_dir_exists(path):"}}
{"prompt": "def _user_id_file_is_old(id_file_path):", "metadata": {"task_id": "Database/mssql-cli/1", "ground_truth": "    if os.path.exists(id_file_path):\n        last_24_hours = datetime.now() - timedelta(hours=24)\n        id_file_modified_time = datetime.fromtimestamp(os.path.getmtime(id_file_path))\n\n        return id_file_modified_time < last_24_hours\n    return False", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "telemetry.py"], "context_start_lineno": 187, "line_no": 188, "id": "mssqlcli.telemetry._user_id_file_is_old", "target_function_prompt": "def _user_id_file_is_old(id_file_path):", "function_signature": "def _user_id_file_is_old(id_file_path):"}}
{"prompt": "def is_command_valid(command):", "metadata": {"task_id": "Database/mssql-cli/2", "ground_truth": "    if not command:\n        return False\n\n    try:\n        # call command silentyly\n        with open(devnull, 'wb') as no_out:\n            subprocess.call(command, stdout=no_out, stderr=no_out)\n    except OSError:\n        return False\n    else:\n        return True", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "util.py"], "context_start_lineno": 20, "line_no": 25, "id": "mssqlcli.util.is_command_valid", "target_function_prompt": "def is_command_valid(command):", "function_signature": "def is_command_valid(command):"}}
{"prompt": "def find_prev_keyword(sql, n_skip=0):", "metadata": {"task_id": "Database/mssql-cli/3", "ground_truth": "    if not sql.strip():\n        return None, ''\n\n    parsed = sqlparse.parse(sql)[0]\n    flattened = list(parsed.flatten())\n    flattened = flattened[:len(flattened) - n_skip]\n\n    logical_operators = ('AND', 'OR', 'NOT', 'BETWEEN')\n\n    for t in reversed(flattened):\n        if t.value == '(' or (t.is_keyword and\n                              (t.value.upper() not in logical_operators)\n                             ):\n            # Find the location of token t in the original parsed statement\n            # We can't use parsed.token_index(t) because t may be a child token\n            # inside a TokenList, in which case token_index thows an error\n            # Minimal example:\n            #   p = sqlparse.parse('select * from foo where bar')\n            #   t = list(p.flatten())[-3]  # The \"Where\" token\n            #   p.token_index(t)  # Throws ValueError: not in list\n            idx = flattened.index(t)\n\n            # Combine the string values of all tokens in the original list\n            # up to and including the target keyword token t, to produce a\n            # query string with everything after the keyword token removed\n            text = ''.join(tok.value for tok in flattened[:idx + 1])\n            return t, text\n\n    return None, ''", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "packages", "parseutils", "utils.py"], "context_start_lineno": 64, "line_no": 70, "id": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "target_function_prompt": "def find_prev_keyword(sql, n_skip=0):", "function_signature": "def find_prev_keyword(sql, n_skip=0):"}}
{"prompt": "def text_(s, encoding='latin-1', errors='strict'):", "metadata": {"task_id": "Internet/pyramid/10", "ground_truth": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "util.py"], "context_start_lineno": 29, "line_no": 32, "id": "pyramid.util.text_", "target_function_prompt": "def text_(s, encoding='latin-1', errors='strict'):", "function_signature": "def text_(s, encoding='latin-1', errors='strict'):"}}
{"prompt": "def where_filters(request, database, datasette):\n    # This one deals with ?_where=", "metadata": {"task_id": "Database/datasette/0", "ground_truth": "    async def inner():\n        where_clauses = []\n        extra_wheres_for_ui = []\n        if \"_where\" in request.args:\n            if not await datasette.permission_allowed(\n                request.actor,\n                \"execute-sql\",\n                resource=database,\n                default=True,\n            ):\n                raise DatasetteError(\"_where= is not allowed\", status=403)\n            else:\n                where_clauses.extend(request.args.getlist(\"_where\"))\n                extra_wheres_for_ui = [\n                    {\n                        \"text\": text,\n                        \"remove_url\": path_with_removed_args(request, {\"_where\": text}),\n                    }\n                    for text in request.args.getlist(\"_where\")\n                ]\n\n        return FilterArguments(\n            where_clauses,\n            extra_context={\n                \"extra_wheres_for_ui\": extra_wheres_for_ui,\n            },\n        )\n\n    return inner", "fpath_tuple": ["Database", "datasette", "datasette", "filters.py"], "context_start_lineno": 9, "line_no": 11, "id": "datasette.filters.where_filters", "target_function_prompt": "def where_filters(request, database, datasette):\n    # This one deals with ?_where=", "function_signature": "def where_filters(request, database, datasette):\n    # This one deals with ?_where="}}
{"prompt": "def path_with_added_args(request, args, path=None):", "metadata": {"task_id": "Database/datasette/1", "ground_truth": "    path = path or request.path\n    if isinstance(args, dict):\n        args = args.items()\n    args_to_remove = {k for k, v in args if v is None}\n    current = []\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if key not in args_to_remove:\n            current.append((key, value))\n    current.extend([(key, value) for key, value in args if value is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 272, "line_no": 273, "id": "datasette.utils.path_with_added_args", "target_function_prompt": "def path_with_added_args(request, args, path=None):", "function_signature": "def path_with_added_args(request, args, path=None):"}}
{"prompt": "def path_with_replaced_args(request, args, path=None):", "metadata": {"task_id": "Database/datasette/2", "ground_truth": "    path = path or request.path\n    if isinstance(args, dict):\n        args = args.items()\n    keys_to_replace = {p[0] for p in args}\n    current = []\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if key not in keys_to_replace:\n            current.append((key, value))\n    current.extend([p for p in args if p[1] is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 317, "line_no": 318, "id": "datasette.utils.path_with_replaced_args", "target_function_prompt": "def path_with_replaced_args(request, args, path=None):", "function_signature": "def path_with_replaced_args(request, args, path=None):"}}
{"prompt": "def format_bytes(bytes):", "metadata": {"task_id": "Database/datasette/3", "ground_truth": "    current = float(bytes)\n    for unit in (\"bytes\", \"KB\", \"MB\", \"GB\", \"TB\"):\n        if current < 1024:\n            break\n        current = current / 1024\n    if unit == \"bytes\":\n        return f\"{int(current)} {unit}\"\n    else:\n        return f\"{current:.1f} {unit}\"", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 869, "line_no": 870, "id": "datasette.utils.format_bytes", "target_function_prompt": "def format_bytes(bytes):", "function_signature": "def format_bytes(bytes):"}}
{"prompt": "def actor_matches_allow(actor, allow):", "metadata": {"task_id": "Database/datasette/4", "ground_truth": "    if allow is True:\n        return True\n    if allow is False:\n        return False\n    if actor is None and allow and allow.get(\"unauthenticated\") is True:\n        return True\n    if allow is None:\n        return True\n    actor = actor or {}\n    for key, values in allow.items():\n        if values == \"*\" and key in actor:\n            return True\n        if not isinstance(values, list):\n            values = [values]\n        actor_values = actor.get(key)\n        if actor_values is None:\n            continue\n        if not isinstance(actor_values, list):\n            actor_values = [actor_values]\n        actor_values = set(actor_values)\n        if actor_values.intersection(values):\n            return True\n    return False", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 1012, "line_no": 1013, "id": "datasette.utils.actor_matches_allow", "target_function_prompt": "def actor_matches_allow(actor, allow):", "function_signature": "def actor_matches_allow(actor, allow):"}}
{"prompt": "def resolve_env_secrets(config, environ):", "metadata": {"task_id": "Database/datasette/5", "ground_truth": "    if isinstance(config, dict):\n        if list(config.keys()) == [\"$env\"]:\n            return environ.get(list(config.values())[0])\n        elif list(config.keys()) == [\"$file\"]:\n            return open(list(config.values())[0]).read()\n        else:\n            return {\n                key: resolve_env_secrets(value, environ)\n                for key, value in config.items()\n            }\n    elif isinstance(config, list):\n        return [resolve_env_secrets(value, environ) for value in config]\n    else:\n        return config", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 1038, "line_no": 1040, "id": "datasette.utils.resolve_env_secrets", "target_function_prompt": "def resolve_env_secrets(config, environ):", "function_signature": "def resolve_env_secrets(config, environ):"}}
{"prompt": "def display_actor(actor):", "metadata": {"task_id": "Database/datasette/6", "ground_truth": "    for key in (\"display\", \"name\", \"username\", \"login\", \"id\"):\n        if actor.get(key):\n            return actor[key]\n    return str(actor)", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 1056, "line_no": 1057, "id": "datasette.utils.display_actor", "target_function_prompt": "def display_actor(actor):", "function_signature": "def display_actor(actor):"}}
{"prompt": "async def initial_path_for_datasette(datasette):", "metadata": {"task_id": "Database/datasette/7", "ground_truth": "    databases = dict([p for p in datasette.databases.items() if p[0] != \"_internal\"])\n    if len(databases) == 1:\n        db_name = next(iter(databases.keys()))\n        path = datasette.urls.database(db_name)\n        # Does this DB only have one table?\n        db = next(iter(databases.values()))\n        tables = await db.table_names()\n        if len(tables) == 1:\n            path = datasette.urls.table(db_name, tables[0])\n    else:\n        path = datasette.urls.instance()\n    return path", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 1075, "line_no": 1077, "id": "datasette.utils.initial_path_for_datasette", "target_function_prompt": "async def initial_path_for_datasette(datasette):", "function_signature": "async def initial_path_for_datasette(datasette):"}}
{"prompt": "def tilde_decode(s: str) -> str:", "metadata": {"task_id": "Database/datasette/8", "ground_truth": "    temp = secrets.token_hex(16)\n    s = s.replace(\"%\", temp)\n    decoded = urllib.parse.unquote_plus(s.replace(\"~\", \"%\"))\n    return decoded.replace(temp, \"%\")", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 1175, "line_no": 1178, "id": "datasette.utils.tilde_decode", "target_function_prompt": "def tilde_decode(s: str) -> str:", "function_signature": "def tilde_decode(s: str) -> str:"}}
{"prompt": "def resolve_routes(routes, path):", "metadata": {"task_id": "Database/datasette/9", "ground_truth": "    for regex, view in routes:\n        match = regex.match(path)\n        if match is not None:\n            return match, view\n    return None, None", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 1184, "line_no": 1185, "id": "datasette.utils.resolve_routes", "target_function_prompt": "def resolve_routes(routes, path):", "function_signature": "def resolve_routes(routes, path):"}}
{"prompt": "def truncate_url(url, length):", "metadata": {"task_id": "Database/datasette/10", "ground_truth": "    if (not length) or (len(url) <= length):\n        return url\n    bits = url.rsplit(\".\", 1)\n    if len(bits) == 2 and 1 <= len(bits[1]) <= 4 and \"/\" not in bits[1]:\n        rest, ext = bits\n        return rest[: length - 1 - len(ext)] + \"\u2026.\" + ext\n    return url[: length - 1] + \"\u2026\"", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 1192, "line_no": 1193, "id": "datasette.utils.truncate_url", "target_function_prompt": "def truncate_url(url, length):", "function_signature": "def truncate_url(url, length):"}}
{"prompt": "def groupfinder(userid, request):", "metadata": {"task_id": "Internet/kinto/0", "ground_truth": "    backend = getattr(request.registry, \"permission\", None)\n    # Permission backend not configured. Ignore.\n    if not backend:\n        return []\n\n    # Safety check when Kinto-Core is used without pyramid_multiauth.\n    if request.prefixed_userid:\n        userid = request.prefixed_userid\n\n    # Query the permission backend only once per request (e.g. batch).\n    reify_key = userid + \"_principals\"\n    if reify_key not in request.bound_data:\n        principals = backend.get_user_principals(userid)\n        request.bound_data[reify_key] = principals\n\n    return request.bound_data[reify_key]", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "authorization.py"], "context_start_lineno": 20, "line_no": 25, "id": "kinto.core.authorization.groupfinder", "target_function_prompt": "def groupfinder(userid, request):", "function_signature": "def groupfinder(userid, request):"}}
{"prompt": "    def dumps(v, **kw):", "metadata": {"task_id": "Internet/kinto/1", "ground_truth": "        kw.setdefault(\"bytes_mode\", rapidjson.BM_NONE)\n        return rapidjson.dumps(v, **kw)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "utils.py"], "context_start_lineno": 34, "line_no": 35, "id": "kinto.core.utils.json.dumps", "target_function_prompt": "    def dumps(v, **kw):", "function_signature": "    def dumps(v, **kw):"}}
{"prompt": "    def loads(v, **kw):", "metadata": {"task_id": "Internet/kinto/2", "ground_truth": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "utils.py"], "context_start_lineno": 42, "line_no": 43, "id": "kinto.core.utils.json.loads", "target_function_prompt": "    def loads(v, **kw):", "function_signature": "    def loads(v, **kw):"}}
{"prompt": "def hmac_digest(secret, message, encoding=\"utf-8\"):", "metadata": {"task_id": "Internet/kinto/3", "ground_truth": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    return hmac.new(secret, message.encode(encoding), hashlib.sha256).hexdigest()", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "utils.py"], "context_start_lineno": 160, "line_no": 162, "id": "kinto.core.utils.hmac_digest", "target_function_prompt": "def hmac_digest(secret, message, encoding=\"utf-8\"):", "function_signature": "def hmac_digest(secret, message, encoding=\"utf-8\"):"}}
{"prompt": "def current_service(request):", "metadata": {"task_id": "Internet/kinto/4", "ground_truth": "    if request.matched_route:\n        services = request.registry.cornice_services\n        pattern = request.matched_route.pattern\n        try:\n            service = services[pattern]\n        except KeyError:\n            return None\n        else:\n            return service", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "utils.py"], "context_start_lineno": 284, "line_no": 290, "id": "kinto.core.utils.current_service", "target_function_prompt": "def current_service(request):", "function_signature": "def current_service(request):"}}
{"prompt": "def prefixed_principals(request):", "metadata": {"task_id": "Internet/kinto/5", "ground_truth": "    principals = request.effective_principals\n    if Authenticated not in principals:\n        return principals\n\n    # Remove unprefixed user id on effective_principals to avoid conflicts.\n    # (it is added via Pyramid Authn policy effective principals)\n    prefix, userid = request.prefixed_userid.split(\":\", 1)\n    principals = [p for p in principals if p != userid]\n\n    if request.prefixed_userid not in principals:\n        principals = [request.prefixed_userid] + principals\n\n    return principals", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "utils.py"], "context_start_lineno": 327, "line_no": 331, "id": "kinto.core.utils.prefixed_principals", "target_function_prompt": "def prefixed_principals(request):", "function_signature": "def prefixed_principals(request):"}}
{"prompt": "def on_account_created(event):", "metadata": {"task_id": "Internet/kinto/6", "ground_truth": "    request = event.request\n    settings = request.registry.settings\n    if not settings.get(\"account_validation\", False):\n        return\n\n    for impacted_object in event.impacted_objects:\n        account = impacted_object[\"new\"]\n        user_email = account[\"id\"]\n        activation_key = get_cached_validation_key(user_email, request.registry)\n        if activation_key is None:\n            continue\n\n        # Send an email to the user with the link to activate their account.\n        Emailer(request, account).send_activation(activation_key)", "fpath_tuple": ["Internet", "kinto", "kinto", "plugins", "accounts", "views", "__init__.py"], "context_start_lineno": 175, "line_no": 176, "id": "kinto.plugins.accounts.views.on_account_created", "target_function_prompt": "def on_account_created(event):", "function_signature": "def on_account_created(event):"}}
{"prompt": "def hash_password(password):\n    # Store password safely in database as str\n    # (bcrypt.hashpw returns base64 bytes).", "metadata": {"task_id": "Internet/kinto/7", "ground_truth": "    pwd_str = password.encode(encoding=\"utf-8\")\n    hashed = bcrypt.hashpw(pwd_str, bcrypt.gensalt())\n    return hashed.decode(encoding=\"utf-8\")", "fpath_tuple": ["Internet", "kinto", "kinto", "plugins", "accounts", "utils.py"], "context_start_lineno": 12, "line_no": 15, "id": "kinto.plugins.accounts.utils.hash_password", "target_function_prompt": "def hash_password(password):\n    # Store password safely in database as str\n    # (bcrypt.hashpw returns base64 bytes).", "function_signature": "def hash_password(password):\n    # Store password safely in database as str\n    # (bcrypt.hashpw returns base64 bytes)."}}
{"prompt": "def get_parent_uri(object_uri):", "metadata": {"task_id": "Internet/kinto/8", "ground_truth": "    path = object_uri.rsplit(\"/\", 2)\n    # len(path) == 1: no '/', probably a broken URL?\n    # len(path) == 2: one '/', doesn't conform to our URL scheme\n    if len(path) < 3:\n        return \"\"\n\n    return path[0]", "fpath_tuple": ["Internet", "kinto", "kinto", "views", "admin.py"], "context_start_lineno": 43, "line_no": 51, "id": "kinto.views.admin.get_parent_uri", "target_function_prompt": "def get_parent_uri(object_uri):", "function_signature": "def get_parent_uri(object_uri):"}}
{"prompt": "def register(name: str) -> Callable:", "metadata": {"task_id": "Database/alembic/0", "ground_truth": "    def decorate(fn):\n        _registry[name] = fn\n        return fn\n\n    return decorate", "fpath_tuple": ["Database", "alembic", "alembic", "script", "write_hooks.py"], "context_start_lineno": 22, "line_no": 34, "id": "alembic.script.write_hooks.register", "target_function_prompt": "def register(name: str) -> Callable:", "function_signature": "def register(name: str) -> Callable:"}}
{"prompt": "def match_replace_regex(regex, src_namespace, dest_namespace):", "metadata": {"task_id": "Database/mongo-doc-manager/0", "ground_truth": "    match = regex.match(src_namespace)\n    if match:\n        return dest_namespace.replace(\"*\", match.group(1))\n    return None", "fpath_tuple": ["Database", "mongo-doc-manager", "mongo_connector", "namespace_config.py"], "context_start_lineno": 545, "line_no": 548, "id": "mongo_connector.namespace_config.match_replace_regex", "target_function_prompt": "def match_replace_regex(regex, src_namespace, dest_namespace):", "function_signature": "def match_replace_regex(regex, src_namespace, dest_namespace):"}}
{"prompt": "def namespace_to_regex(namespace):", "metadata": {"task_id": "Database/mongo-doc-manager/1", "ground_truth": "    db_name, coll_name = namespace.split(\".\", 1)\n    # A database name cannot contain a '.' character\n    db_regex = re.escape(db_name).replace(r\"\\*\", \"([^.]*)\")\n    # But a collection name can.\n    coll_regex = re.escape(coll_name).replace(r\"\\*\", \"(.*)\")\n    return re.compile(r\"\\A\" + db_regex + r\"\\.\" + coll_regex + r\"\\Z\")", "fpath_tuple": ["Database", "mongo-doc-manager", "mongo_connector", "namespace_config.py"], "context_start_lineno": 559, "line_no": 561, "id": "mongo_connector.namespace_config.namespace_to_regex", "target_function_prompt": "def namespace_to_regex(namespace):", "function_signature": "def namespace_to_regex(namespace):"}}
{"prompt": "def long_to_bson_ts(val):", "metadata": {"task_id": "Database/mongo-doc-manager/2", "ground_truth": "    seconds = val >> 32\n    increment = val & 0xFFFFFFFF\n\n    return Timestamp(seconds, increment)", "fpath_tuple": ["Database", "mongo-doc-manager", "mongo_connector", "util.py"], "context_start_lineno": 59, "line_no": 62, "id": "mongo_connector.util.long_to_bson_ts", "target_function_prompt": "def long_to_bson_ts(val):", "function_signature": "def long_to_bson_ts(val):"}}
{"prompt": "    def format_document(self, document):", "metadata": {"task_id": "Database/mongo-doc-manager/3", "ground_truth": "        def flatten(doc, path):\n            top_level = len(path) == 0\n            if not top_level:\n                path_string = \".\".join(path)\n            for k in doc:\n                v = doc[k]\n                if isinstance(v, dict):\n                    path.append(k)\n                    for inner_k, inner_v in flatten(v, path):\n                        yield inner_k, inner_v\n                    path.pop()\n                else:\n                    transformed = self.transform_element(k, v)\n                    for new_k, new_v in transformed:\n                        if top_level:\n                            yield new_k, new_v\n                        else:\n                            yield \"%s.%s\" % (path_string, new_k), new_v\n\n        return dict(flatten(document, []))", "fpath_tuple": ["Database", "mongo-doc-manager", "mongo_connector", "doc_managers", "formatters.py"], "context_start_lineno": 149, "line_no": 150, "id": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "target_function_prompt": "    def format_document(self, document):", "function_signature": "    def format_document(self, document):"}}
{"prompt": "def open_file_in_dir(path: str) -> Tuple[io.FileIO, Optional[int]]:", "metadata": {"task_id": "Database/bplustree/0", "ground_truth": "    directory = os.path.dirname(path)\n    if not os.path.isdir(directory):\n        raise ValueError('No directory {}'.format(directory))\n\n    if not os.path.exists(path):\n        file_fd = open(path, mode='x+b', buffering=0)\n    else:\n        file_fd = open(path, mode='r+b', buffering=0)\n\n    if platform.system() == 'Windows':\n        # Opening a directory is not possible on Windows, but that is not\n        # a problem since Windows does not need to fsync the directory in\n        # order to persist metadata\n        dir_fd = None\n    else:\n        dir_fd = os.open(directory, os.O_RDONLY)\n\n    return file_fd, dir_fd", "fpath_tuple": ["Database", "bplustree", "bplustree", "memory.py"], "context_start_lineno": 22, "line_no": 31, "id": "bplustree.memory.open_file_in_dir", "target_function_prompt": "def open_file_in_dir(path: str) -> Tuple[io.FileIO, Optional[int]]:", "function_signature": "def open_file_in_dir(path: str) -> Tuple[io.FileIO, Optional[int]]:"}}
{"prompt": "    def read_transaction(self):\n", "metadata": {"task_id": "Database/bplustree/1", "ground_truth": "        class ReadTransaction:\n\n            def __enter__(self2):\n                self._lock.reader_lock.acquire()\n\n            def __exit__(self2, exc_type, exc_val, exc_tb):\n                self._lock.reader_lock.release()\n\n        return ReadTransaction()", "fpath_tuple": ["Database", "bplustree", "bplustree", "memory.py"], "context_start_lineno": 166, "line_no": 168, "id": "bplustree.memory.FileMemory.read_transaction", "target_function_prompt": "    def read_transaction(self):\n", "function_signature": "    def read_transaction(self):\n"}}
{"prompt": "def pairwise(iterable: Iterable):", "metadata": {"task_id": "Database/bplustree/2", "ground_truth": "    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)", "fpath_tuple": ["Database", "bplustree", "bplustree", "utils.py"], "context_start_lineno": 4, "line_no": 9, "id": "bplustree.utils.pairwise", "target_function_prompt": "def pairwise(iterable: Iterable):", "function_signature": "def pairwise(iterable: Iterable):"}}
{"prompt": "def iter_slice(iterable: bytes, n: int):", "metadata": {"task_id": "Database/bplustree/3", "ground_truth": "    start = 0\n    stop = start + n\n    final_offset = len(iterable)\n\n    while True:\n        if start >= final_offset:\n            break\n\n        rv = iterable[start:stop]\n        start = stop\n        stop = start + n\n        yield rv, start >= final_offset", "fpath_tuple": ["Database", "bplustree", "bplustree", "utils.py"], "context_start_lineno": 14, "line_no": 19, "id": "bplustree.utils.iter_slice", "target_function_prompt": "def iter_slice(iterable: bytes, n: int):", "function_signature": "def iter_slice(iterable: bytes, n: int):"}}
{"prompt": "    def serialize(self, obj: str, key_size: int) -> bytes:", "metadata": {"task_id": "Database/bplustree/4", "ground_truth": "        rv = obj.encode(encoding='utf-8')\n        assert len(rv) <= key_size\n        return rv", "fpath_tuple": ["Database", "bplustree", "bplustree", "serializer.py"], "context_start_lineno": 43, "line_no": 44, "id": "bplustree.serializer.StrSerializer.serialize", "target_function_prompt": "    def serialize(self, obj: str, key_size: int) -> bytes:", "function_signature": "    def serialize(self, obj: str, key_size: int) -> bytes:"}}
{"prompt": "def pack(fmt, *args):", "metadata": {"task_id": "Multimedia/psd-tools/0", "ground_truth": "    fmt = str(\">\" + fmt)\n    return struct.pack(fmt, *args)", "fpath_tuple": ["Multimedia", "psd-tools", "src", "psd_tools", "utils.py"], "context_start_lineno": 17, "line_no": 18, "id": "psd_tools.utils.pack", "target_function_prompt": "def pack(fmt, *args):", "function_signature": "def pack(fmt, *args):"}}
{"prompt": "def unpack(fmt, data):", "metadata": {"task_id": "Multimedia/psd-tools/1", "ground_truth": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)", "fpath_tuple": ["Multimedia", "psd-tools", "src", "psd_tools", "utils.py"], "context_start_lineno": 22, "line_no": 23, "id": "psd_tools.utils.unpack", "target_function_prompt": "def unpack(fmt, data):", "function_signature": "def unpack(fmt, data):"}}
{"prompt": "def get_pattern(pattern):", "metadata": {"task_id": "Multimedia/psd-tools/2", "ground_truth": "    height, width = pattern.data.rectangle[2], pattern.data.rectangle[3]\n    return np.stack([\n        _parse_array(c.get_data(), c.pixel_depth)\n        for c in pattern.data.channels if c.is_written\n    ],\n                    axis=1).reshape((height, width, -1))", "fpath_tuple": ["Multimedia", "psd-tools", "src", "psd_tools", "api", "numpy_io.py"], "context_start_lineno": 104, "line_no": 106, "id": "psd_tools.api.numpy_io.get_pattern", "target_function_prompt": "def get_pattern(pattern):", "function_signature": "def get_pattern(pattern):"}}
{"prompt": "def maximize_csv_field_size_limit():", "metadata": {"task_id": "Database/sqlite-utils/0", "ground_truth": "    field_size_limit = sys.maxsize\n\n    while True:\n        try:\n            csv.field_size_limit(field_size_limit)\n            break\n        except OverflowError:\n            field_size_limit = int(field_size_limit / 10)", "fpath_tuple": ["Database", "sqlite-utils", "sqlite_utils", "utils.py"], "context_start_lineno": 44, "line_no": 49, "id": "sqlite_utils.utils.maximize_csv_field_size_limit", "target_function_prompt": "def maximize_csv_field_size_limit():", "function_signature": "def maximize_csv_field_size_limit():"}}
{"prompt": "def column_affinity(column_type):\n    # Implementation of SQLite affinity rules from\n    # https://www.sqlite.org/datatype3.html#determination_of_column_affinity", "metadata": {"task_id": "Database/sqlite-utils/1", "ground_truth": "    assert isinstance(column_type, str)\n    column_type = column_type.upper().strip()\n    if column_type == \"\":\n        return str  # We differ from spec, which says it should be BLOB\n    if \"INT\" in column_type:\n        return int\n    if \"CHAR\" in column_type or \"CLOB\" in column_type or \"TEXT\" in column_type:\n        return str\n    if \"BLOB\" in column_type:\n        return bytes\n    if \"REAL\" in column_type or \"FLOA\" in column_type or \"DOUB\" in column_type:\n        return float\n    # Default is 'NUMERIC', which we currently also treat as float\n    return float", "fpath_tuple": ["Database", "sqlite-utils", "sqlite_utils", "utils.py"], "context_start_lineno": 122, "line_no": 125, "id": "sqlite_utils.utils.column_affinity", "target_function_prompt": "def column_affinity(column_type):\n    # Implementation of SQLite affinity rules from\n    # https://www.sqlite.org/datatype3.html#determination_of_column_affinity", "function_signature": "def column_affinity(column_type):\n    # Implementation of SQLite affinity rules from\n    # https://www.sqlite.org/datatype3.html#determination_of_column_affinity"}}
{"prompt": "def decode_base64_values(doc):\n    # Looks for '{\"$base64\": true..., \"encoded\": ...}' values and decodes them", "metadata": {"task_id": "Database/sqlite-utils/2", "ground_truth": "    to_fix = [\n        k\n        for k in doc\n        if isinstance(doc[k], dict)\n        and doc[k].get(\"$base64\") is True\n        and \"encoded\" in doc[k]\n    ]\n    if not to_fix:\n        return doc\n    return dict(doc, **{k: base64.b64decode(doc[k][\"encoded\"]) for k in to_fix})", "fpath_tuple": ["Database", "sqlite-utils", "sqlite_utils", "utils.py"], "context_start_lineno": 141, "line_no": 143, "id": "sqlite_utils.utils.decode_base64_values", "target_function_prompt": "def decode_base64_values(doc):\n    # Looks for '{\"$base64\": true..., \"encoded\": ...}' values and decodes them", "function_signature": "def decode_base64_values(doc):\n    # Looks for '{\"$base64\": true..., \"encoded\": ...}' values and decodes them"}}
{"prompt": "def chunks(sequence: Iterable, size: int) -> Iterable[Iterable]:", "metadata": {"task_id": "Database/sqlite-utils/3", "ground_truth": "    iterator = iter(sequence)\n    for item in iterator:\n        yield itertools.chain([item], itertools.islice(iterator, size - 1))", "fpath_tuple": ["Database", "sqlite-utils", "sqlite_utils", "utils.py"], "context_start_lineno": 488, "line_no": 495, "id": "sqlite_utils.utils.chunks", "target_function_prompt": "def chunks(sequence: Iterable, size: int) -> Iterable[Iterable]:", "function_signature": "def chunks(sequence: Iterable, size: int) -> Iterable[Iterable]:"}}
{"prompt": "def hash_record(record: Dict, keys: Optional[Iterable[str]] = None):", "metadata": {"task_id": "Database/sqlite-utils/4", "ground_truth": "    to_hash = record\n    if keys is not None:\n        to_hash = {key: record[key] for key in keys}\n    return hashlib.sha1(\n        json.dumps(to_hash, separators=(\",\", \":\"), sort_keys=True, default=repr).encode(\n            \"utf8\"\n        )\n    ).hexdigest()", "fpath_tuple": ["Database", "sqlite-utils", "sqlite_utils", "utils.py"], "context_start_lineno": 500, "line_no": 521, "id": "sqlite_utils.utils.hash_record", "target_function_prompt": "def hash_record(record: Dict, keys: Optional[Iterable[str]] = None):", "function_signature": "def hash_record(record: Dict, keys: Optional[Iterable[str]] = None):"}}
{"prompt": "def _get_host(store):", "metadata": {"task_id": "Database/arctic-latest/0", "ground_truth": "    ret = {}\n    if store:\n        try:\n            if isinstance(store, (list, tuple)):\n                store = store[0]\n            ret['l'] = store._arctic_lib.get_name()\n            ret['mnodes'] = [\"{}:{}\".format(h, p) for h, p in store._collection.database.client.nodes]\n            ret['mhost'] = \"{}\".format(store._arctic_lib.arctic.mongo_host)\n        except Exception:\n            # Sometimes get_name(), for example, fails if we're not connected to MongoDB.\n            pass\n    return ret", "fpath_tuple": ["Database", "arctic-latest", "arctic", "decorators.py"], "context_start_lineno": 15, "line_no": 16, "id": "arctic.decorators._get_host", "target_function_prompt": "def _get_host(store):", "function_signature": "def _get_host(store):"}}
{"prompt": "def mongo_retry(f):", "metadata": {"task_id": "Database/arctic-latest/1", "ground_truth": "    log_all_exceptions = 'arctic' in f.__module__ if f.__module__ else False\n\n    @wraps(f)\n    def f_retry(*args, **kwargs):\n        global _retry_count, _in_retry\n        top_level = not _in_retry\n        _in_retry = True\n        try:\n            while True:\n                try:\n                    return f(*args, **kwargs)\n                except (DuplicateKeyError, ServerSelectionTimeoutError, BulkWriteError) as e:\n                    # Re-raise errors that won't go away.\n                    _handle_error(f, e, _retry_count, **_get_host(args))\n                    raise\n                except (OperationFailure, AutoReconnect) as e:\n                    _retry_count += 1\n                    _handle_error(f, e, _retry_count, **_get_host(args))\n                except Exception as e:\n                    if log_all_exceptions:\n                        _log_exception(f.__name__, e, _retry_count, **_get_host(args))\n                    raise\n        finally:\n            if top_level:\n                _in_retry = False\n                _retry_count = 0\n    return f_retry", "fpath_tuple": ["Database", "arctic-latest", "arctic", "decorators.py"], "context_start_lineno": 34, "line_no": 39, "id": "arctic.decorators.mongo_retry", "target_function_prompt": "def mongo_retry(f):", "function_signature": "def mongo_retry(f):"}}
{"prompt": "def are_equals(o1, o2, **kwargs):", "metadata": {"task_id": "Database/arctic-latest/2", "ground_truth": "    try:\n        if isinstance(o1, DataFrame):\n            assert_frame_equal(o1, o2, kwargs)\n            return True\n        return o1 == o2\n    except Exception:\n        return False", "fpath_tuple": ["Database", "arctic-latest", "arctic", "_util.py"], "context_start_lineno": 36, "line_no": 37, "id": "arctic._util.are_equals", "target_function_prompt": "def are_equals(o1, o2, **kwargs):", "function_signature": "def are_equals(o1, o2, **kwargs):"}}
{"prompt": "def register_resolve_mongodb_hook(hook):", "metadata": {"task_id": "Database/arctic-latest/3", "ground_truth": "    global _resolve_mongodb_hook\n    _resolve_mongodb_hook = hook", "fpath_tuple": ["Database", "arctic-latest", "arctic", "hooks.py"], "context_start_lineno": 17, "line_no": 18, "id": "arctic.hooks.register_resolve_mongodb_hook", "target_function_prompt": "def register_resolve_mongodb_hook(hook):", "function_signature": "def register_resolve_mongodb_hook(hook):"}}
{"prompt": "def register_log_exception_hook(hook):", "metadata": {"task_id": "Database/arctic-latest/4", "ground_truth": "    global _log_exception_hook\n    _log_exception_hook = hook", "fpath_tuple": ["Database", "arctic-latest", "arctic", "hooks.py"], "context_start_lineno": 29, "line_no": 30, "id": "arctic.hooks.register_log_exception_hook", "target_function_prompt": "def register_log_exception_hook(hook):", "function_signature": "def register_log_exception_hook(hook):"}}
{"prompt": "def register_get_auth_hook(hook):", "metadata": {"task_id": "Database/arctic-latest/5", "ground_truth": "    global _get_auth_hook\n    _get_auth_hook = hook", "fpath_tuple": ["Database", "arctic-latest", "arctic", "hooks.py"], "context_start_lineno": 34, "line_no": 35, "id": "arctic.hooks.register_get_auth_hook", "target_function_prompt": "def register_get_auth_hook(hook):", "function_signature": "def register_get_auth_hook(hook):"}}
{"prompt": "def _split_arrs(array_2d, slices):", "metadata": {"task_id": "Database/arctic-latest/6", "ground_truth": "    if len(array_2d) == 0:\n        return np.empty(0, dtype=object)\n\n    rtn = np.empty(len(slices) + 1, dtype=object)\n    start = 0\n    for i, s in enumerate(slices):\n        rtn[i] = array_2d[start:s]\n        start = s\n    rtn[-1] = array_2d[start:]\n    return rtn", "fpath_tuple": ["Database", "arctic-latest", "arctic", "store", "_version_store_utils.py"], "context_start_lineno": 16, "line_no": 21, "id": "arctic.store._version_store_utils._split_arrs", "target_function_prompt": "def _split_arrs(array_2d, slices):", "function_signature": "def _split_arrs(array_2d, slices):"}}
{"prompt": "def checksum(symbol, doc):", "metadata": {"task_id": "Database/arctic-latest/7", "ground_truth": "    sha = hashlib.sha1()\n    sha.update(symbol.encode('ascii'))\n    for k in sorted(iter(doc.keys()), reverse=True):\n        v = doc[k]\n        if isinstance(v, bytes):\n            sha.update(doc[k])\n        else:\n            sha.update(str(doc[k]).encode('ascii'))\n    return Binary(sha.digest())", "fpath_tuple": ["Database", "arctic-latest", "arctic", "store", "_version_store_utils.py"], "context_start_lineno": 33, "line_no": 37, "id": "arctic.store._version_store_utils.checksum", "target_function_prompt": "def checksum(symbol, doc):", "function_signature": "def checksum(symbol, doc):"}}
{"prompt": "    def __str__(self):", "metadata": {"task_id": "Database/arctic-latest/8", "ground_truth": "        return \"VersionedItem(symbol=%s,library=%s,data=%s,version=%s,metadata=%s,host=%s)\" % \\\n            (self.symbol, self.library, type(self.data), self.version, self.metadata, self.host)", "fpath_tuple": ["Database", "arctic-latest", "arctic", "store", "versioned_item.py"], "context_start_lineno": 17, "line_no": 18, "id": "arctic.store.versioned_item.VersionedItem.__str__", "target_function_prompt": "    def __str__(self):", "function_signature": "    def __str__(self):"}}
{"prompt": "    def _dtype(self, string, metadata=None):", "metadata": {"task_id": "Database/arctic-latest/9", "ground_truth": "        if metadata is None:\n            metadata = {}\n        if string.startswith('['):\n            return np.dtype(eval(string), metadata=metadata)\n        return np.dtype(string, metadata=metadata)", "fpath_tuple": ["Database", "arctic-latest", "arctic", "store", "_ndarray_store.py"], "context_start_lineno": 312, "line_no": 313, "id": "arctic.store._ndarray_store.NdarrayStore._dtype", "target_function_prompt": "    def _dtype(self, string, metadata=None):", "function_signature": "    def _dtype(self, string, metadata=None):"}}
{"prompt": "def _promote_struct_dtypes(dtype1, dtype2):", "metadata": {"task_id": "Database/arctic-latest/10", "ground_truth": "    if not set(dtype1.names).issuperset(set(dtype2.names)):\n        raise Exception(\"Removing columns from dtype not handled\")\n\n    def _promote(type1, type2):\n        if type2 is None:\n            return type1\n        if type1.shape is not None:\n            if not type1.shape == type2.shape:\n                raise Exception(\"We do not handle changes to dtypes that have shape\")\n            return np.promote_types(type1.base, type2.base), type1.shape\n        return np.promote_types(type1, type2)\n    return np.dtype([(n, _promote(dtype1.fields[n][0], dtype2.fields.get(n, (None,))[0])) for n in dtype1.names])", "fpath_tuple": ["Database", "arctic-latest", "arctic", "store", "_ndarray_store.py"], "context_start_lineno": 25, "line_no": 26, "id": "arctic.store._ndarray_store._promote_struct_dtypes", "target_function_prompt": "def _promote_struct_dtypes(dtype1, dtype2):", "function_signature": "def _promote_struct_dtypes(dtype1, dtype2):"}}
{"prompt": "    def exclude(self, data, range_obj):", "metadata": {"task_id": "Database/arctic-latest/11", "ground_truth": "        if isinstance(data, DataFrame):\n            return DataFrame()\n        else:\n            return Series()", "fpath_tuple": ["Database", "arctic-latest", "arctic", "chunkstore", "passthrough_chunker.py"], "context_start_lineno": 61, "line_no": 71, "id": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "target_function_prompt": "    def exclude(self, data, range_obj):", "function_signature": "    def exclude(self, data, range_obj):"}}
{"prompt": "    def to_chunks(self, df, chunk_size='D', func=None, **kwargs):", "metadata": {"task_id": "Database/arctic-latest/12", "ground_truth": "        if 'date' in df.index.names:\n            dates = df.index.get_level_values('date')\n            if not df.index.is_monotonic_increasing:\n                df = df.sort_index()\n        elif 'date' in df.columns:\n            dates = pd.DatetimeIndex(df.date)\n            if not dates.is_monotonic_increasing:\n                # providing support for pandas 0.16.2 to 0.20.x\n                # neither sort method exists in both\n                try:\n                    df = df.sort_values('date')\n                except AttributeError:\n                    df = df.sort(columns='date')\n                dates = pd.DatetimeIndex(df.date)\n        else:\n            raise Exception(\"Data must be datetime indexed or have a column named 'date'\")\n\n        period_obj = dates.to_period(chunk_size)\n        period_obj_reduced = period_obj.drop_duplicates()\n        count = 0\n        for _, g in df.groupby(period_obj._data):\n            start = period_obj_reduced[count].start_time.to_pydatetime(warn=False)\n            end = period_obj_reduced[count].end_time.to_pydatetime(warn=False)\n            count += 1\n            if func:\n                yield start, end, chunk_size, func(g)\n            else:\n                yield start, end, chunk_size, g", "fpath_tuple": ["Database", "arctic-latest", "arctic", "chunkstore", "date_chunker.py"], "context_start_lineno": 9, "line_no": 27, "id": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "target_function_prompt": "    def to_chunks(self, df, chunk_size='D', func=None, **kwargs):", "function_signature": "    def to_chunks(self, df, chunk_size='D', func=None, **kwargs):"}}
{"prompt": "    def exclude(self, data, range_obj):", "metadata": {"task_id": "Database/arctic-latest/13", "ground_truth": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        if 'date' in data.index.names:\n            return data[(data.index.get_level_values('date') < range_obj.start) | (data.index.get_level_values('date') > range_obj.end)]\n        elif 'date' in data.columns:\n            return data[(data.date < range_obj.start) | (data.date > range_obj.end)]\n        else:\n            return data", "fpath_tuple": ["Database", "arctic-latest", "arctic", "chunkstore", "date_chunker.py"], "context_start_lineno": 134, "line_no": 142, "id": "arctic.chunkstore.date_chunker.DateChunker.exclude", "target_function_prompt": "    def exclude(self, data, range_obj):", "function_signature": "    def exclude(self, data, range_obj):"}}
{"prompt": "def format_proxy(proxy_config, auth=True):", "metadata": {"task_id": "Multimedia/Mopidy/0", "ground_truth": "    if not proxy_config.get(\"hostname\"):\n        return None\n\n    scheme = proxy_config.get(\"scheme\") or \"http\"\n    username = proxy_config.get(\"username\")\n    password = proxy_config.get(\"password\")\n    hostname = proxy_config[\"hostname\"]\n    port = proxy_config.get(\"port\")\n    if not port or port < 0:\n        port = 80\n\n    if username and password and auth:\n        return f\"{scheme}://{username}:{password}@{hostname}:{port}\"\n    else:\n        return f\"{scheme}://{hostname}:{port}\"", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "httpclient.py"], "context_start_lineno": 7, "line_no": 18, "id": "mopidy.httpclient.format_proxy", "target_function_prompt": "def format_proxy(proxy_config, auth=True):", "function_signature": "def format_proxy(proxy_config, auth=True):"}}
{"prompt": "    def filter(self, data, range_obj):", "metadata": {"task_id": "Database/arctic-latest/14", "ground_truth": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[start:end]\n        elif 'date' in data.columns:\n            if start and end:\n                return data[(data.date >= start) & (data.date <= end)]\n            elif start:\n                return data[(data.date >= start)]\n            elif end:\n                return data[(data.date <= end)]\n            else:\n                return data\n        else:\n            return data", "fpath_tuple": ["Database", "arctic-latest", "arctic", "chunkstore", "date_chunker.py"], "context_start_lineno": 100, "line_no": 113, "id": "arctic.chunkstore.date_chunker.DateChunker.filter", "target_function_prompt": "    def filter(self, data, range_obj):", "function_signature": "    def filter(self, data, range_obj):"}}
{"prompt": "def validate_required(value, required):", "metadata": {"task_id": "Multimedia/Mopidy/1", "ground_truth": "    if required and not value:\n        raise ValueError(\"must be set.\")", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "validators.py"], "context_start_lineno": 3, "line_no": 9, "id": "mopidy.config.validators.validate_required", "target_function_prompt": "def validate_required(value, required):", "function_signature": "def validate_required(value, required):"}}
{"prompt": "def validate_choice(value, choices):", "metadata": {"task_id": "Multimedia/Mopidy/2", "ground_truth": "    if choices is not None and value not in choices:\n        names = \", \".join(repr(c) for c in choices)\n        raise ValueError(f\"must be one of {names}, not {value}.\")", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "validators.py"], "context_start_lineno": 13, "line_no": 18, "id": "mopidy.config.validators.validate_choice", "target_function_prompt": "def validate_choice(value, choices):", "function_signature": "def validate_choice(value, choices):"}}
{"prompt": "def validate_minimum(value, minimum):", "metadata": {"task_id": "Multimedia/Mopidy/3", "ground_truth": "    if minimum is not None and value < minimum:\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "validators.py"], "context_start_lineno": 23, "line_no": 28, "id": "mopidy.config.validators.validate_minimum", "target_function_prompt": "def validate_minimum(value, minimum):", "function_signature": "def validate_minimum(value, minimum):"}}
{"prompt": "def validate_maximum(value, maximum):", "metadata": {"task_id": "Multimedia/Mopidy/4", "ground_truth": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "validators.py"], "context_start_lineno": 32, "line_no": 37, "id": "mopidy.config.validators.validate_maximum", "target_function_prompt": "def validate_maximum(value, maximum):", "function_signature": "def validate_maximum(value, maximum):"}}
{"prompt": "def _did_you_mean(name, choices):", "metadata": {"task_id": "Multimedia/Mopidy/5", "ground_truth": "    if not choices:\n        return None\n\n    name = name.lower()\n    candidates = [(_levenshtein(name, c), c) for c in choices]\n    candidates.sort()\n\n    if candidates[0][0] <= 3:\n        return candidates[0][1]\n    return None", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "schemas.py"], "context_start_lineno": 5, "line_no": 7, "id": "mopidy.config.schemas._did_you_mean", "target_function_prompt": "def _did_you_mean(name, choices):", "function_signature": "def _did_you_mean(name, choices):"}}
{"prompt": "def encode(value):", "metadata": {"task_id": "Multimedia/Mopidy/6", "ground_truth": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 20, "line_no": 21, "id": "mopidy.config.types.encode", "target_function_prompt": "def encode(value):", "function_signature": "def encode(value):"}}
{"prompt": "def decode(value):", "metadata": {"task_id": "Multimedia/Mopidy/7", "ground_truth": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 8, "line_no": 9, "id": "mopidy.config.types.decode", "target_function_prompt": "def decode(value):", "function_signature": "def decode(value):"}}
{"prompt": "    def serialize(self, value, display=False):", "metadata": {"task_id": "Multimedia/Mopidy/8", "ground_truth": "        if value is None:\n            return \"\"\n        return str(value)", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 65, "line_no": 67, "id": "mopidy.config.types.ConfigValue.serialize", "target_function_prompt": "    def serialize(self, value, display=False):", "function_signature": "    def serialize(self, value, display=False):"}}
{"prompt": "    def serialize(self, value, display=False):", "metadata": {"task_id": "Multimedia/Mopidy/9", "ground_truth": "        if value is True:\n            return \"true\"\n        elif value in (False, None):\n            return \"false\"\n        else:\n            raise ValueError(f\"{value!r} is not a boolean\")", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 212, "line_no": 213, "id": "mopidy.config.types.Boolean.serialize", "target_function_prompt": "    def serialize(self, value, display=False):", "function_signature": "    def serialize(self, value, display=False):"}}
{"prompt": "def df2mat(data, return_labels=False):", "metadata": {"task_id": "Multimedia/hypertools/0", "ground_truth": "    df_str = data.select_dtypes(include=['object'])\n    df_num = data.select_dtypes(exclude=['object'])\n\n    for colname in df_str.columns:\n        df_num = df_num.join(pd.get_dummies(data[colname], prefix=colname))\n\n    plot_data = df_num.values\n\n    labels=list(df_num.columns.values)\n\n    if return_labels:\n        return plot_data,labels\n    else:\n        return plot_data", "fpath_tuple": ["Multimedia", "hypertools", "hypertools", "tools", "df2mat.py"], "context_start_lineno": 5, "line_no": 31, "id": "hypertools.tools.df2mat.df2mat", "target_function_prompt": "def df2mat(data, return_labels=False):", "function_signature": "def df2mat(data, return_labels=False):"}}
{"prompt": "def center(x):", "metadata": {"task_id": "Multimedia/hypertools/1", "ground_truth": "    assert type(x) is list, \"Input data to center must be list\"\n    x_stacked = np.vstack(x)\n    return [i - np.mean(x_stacked, 0) for i in x]", "fpath_tuple": ["Multimedia", "hypertools", "hypertools", "_shared", "helpers.py"], "context_start_lineno": 19, "line_no": 20, "id": "hypertools._shared.helpers.center", "target_function_prompt": "def center(x):", "function_signature": "def center(x):"}}
{"prompt": "def group_by_category(vals):", "metadata": {"task_id": "Multimedia/hypertools/2", "ground_truth": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    val_set = list(sorted(set(vals), key=list(vals).index))\n    return [val_set.index(val) for val in vals]", "fpath_tuple": ["Multimedia", "hypertools", "hypertools", "_shared", "helpers.py"], "context_start_lineno": 34, "line_no": 35, "id": "hypertools._shared.helpers.group_by_category", "target_function_prompt": "def group_by_category(vals):", "function_signature": "def group_by_category(vals):"}}
{"prompt": "def vals2colors(vals, cmap='GnBu',res=100):", "metadata": {"task_id": "Multimedia/hypertools/3", "ground_truth": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # get palette from seaborn\n    palette = np.array(sns.color_palette(cmap, res))\n    ranks = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return [tuple(i) for i in palette[ranks, :]]", "fpath_tuple": ["Multimedia", "hypertools", "hypertools", "_shared", "helpers.py"], "context_start_lineno": 41, "line_no": 51, "id": "hypertools._shared.helpers.vals2colors", "target_function_prompt": "def vals2colors(vals, cmap='GnBu',res=100):", "function_signature": "def vals2colors(vals, cmap='GnBu',res=100):"}}
{"prompt": "def vals2bins(vals,res=100):", "metadata": {"task_id": "Multimedia/hypertools/4", "ground_truth": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    return list(np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1)", "fpath_tuple": ["Multimedia", "hypertools", "hypertools", "_shared", "helpers.py"], "context_start_lineno": 60, "line_no": 69, "id": "hypertools._shared.helpers.vals2bins", "target_function_prompt": "def vals2bins(vals,res=100):", "function_signature": "def vals2bins(vals,res=100):"}}
{"prompt": "def interp_array(arr,interp_val=10):", "metadata": {"task_id": "Multimedia/hypertools/5", "ground_truth": "    x=np.arange(0, len(arr), 1)\n    xx=np.arange(0, len(arr)-1, 1/interp_val)\n    q=pchip(x,arr)\n    return q(xx)", "fpath_tuple": ["Multimedia", "hypertools", "hypertools", "_shared", "helpers.py"], "context_start_lineno": 74, "line_no": 75, "id": "hypertools._shared.helpers.interp_array", "target_function_prompt": "def interp_array(arr,interp_val=10):", "function_signature": "def interp_array(arr,interp_val=10):"}}
{"prompt": "def parse_args(x,args):", "metadata": {"task_id": "Multimedia/hypertools/6", "ground_truth": "    args_list = []\n    for i,item in enumerate(x):\n        tmp = []\n        for ii, arg in enumerate(args):\n            if isinstance(arg, (tuple, list)):\n                if len(arg) == len(x):\n                    tmp.append(arg[i])\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp.append(arg)\n        args_list.append(tuple(tmp))\n    return args_list", "fpath_tuple": ["Multimedia", "hypertools", "hypertools", "_shared", "helpers.py"], "context_start_lineno": 88, "line_no": 89, "id": "hypertools._shared.helpers.parse_args", "target_function_prompt": "def parse_args(x,args):", "function_signature": "def parse_args(x,args):"}}
{"prompt": "def parse_kwargs(x, kwargs):", "metadata": {"task_id": "Multimedia/hypertools/7", "ground_truth": "    kwargs_list = []\n    for i,item in enumerate(x):\n        tmp = {}\n        for kwarg in kwargs:\n            if isinstance(kwargs[kwarg], (tuple, list)):\n                if len(kwargs[kwarg]) == len(x):\n                    tmp[kwarg]=kwargs[kwarg][i]\n                else:\n                    tmp[kwarg] = None\n            else:\n                tmp[kwarg]=kwargs[kwarg]\n        kwargs_list.append(tmp)\n    return kwargs_list", "fpath_tuple": ["Multimedia", "hypertools", "hypertools", "_shared", "helpers.py"], "context_start_lineno": 105, "line_no": 106, "id": "hypertools._shared.helpers.parse_kwargs", "target_function_prompt": "def parse_kwargs(x, kwargs):", "function_signature": "def parse_kwargs(x, kwargs):"}}
{"prompt": "def _get_default_display_mode(environ):", "metadata": {"task_id": "Multimedia/gif-for-cli/0", "ground_truth": "    TERM = environ.get('TERM', '').lower()\n    # FIXME: COLORTERM may not be accepted by sshd\n    COLORTERM = environ.get('COLORTERM', '').lower()\n    if 'truecolor' in TERM or 'truecolor' in COLORTERM:\n        return 'truecolor'\n    elif '256' in TERM or '256' in COLORTERM:\n        return '256fgbg'\n    return 'nocolor'", "fpath_tuple": ["Multimedia", "gif-for-cli", "gif_for_cli", "utils.py"], "context_start_lineno": 38, "line_no": 39, "id": "gif_for_cli.utils._get_default_display_mode", "target_function_prompt": "def _get_default_display_mode(environ):", "function_signature": "def _get_default_display_mode(environ):"}}
{"prompt": "def _pool_type(val):", "metadata": {"task_id": "Multimedia/gif-for-cli/1", "ground_truth": "    if val is None:\n        return val\n    val = int(val)\n    if val <= 0:\n        raise argparse.ArgumentTypeError('Minimum cpu_pool_size is 1')\n    return val", "fpath_tuple": ["Multimedia", "gif-for-cli", "gif_for_cli", "utils.py"], "context_start_lineno": 49, "line_no": 50, "id": "gif_for_cli.utils._pool_type", "target_function_prompt": "def _pool_type(val):", "function_signature": "def _pool_type(val):"}}
{"prompt": "def get_avg_for_em(px, x, y, cell_height, cell_width):", "metadata": {"task_id": "Multimedia/gif-for-cli/2", "ground_truth": "    pixels = [\n        px[sx, sy]\n        for sy in range(y, y + cell_height)\n        for sx in range(x, x + cell_width)\n    ]\n    return [round(n) for n in map(mean, zip(*pixels))]", "fpath_tuple": ["Multimedia", "gif-for-cli", "gif_for_cli", "generate", "utils.py"], "context_start_lineno": 64, "line_no": 65, "id": "gif_for_cli.generate.utils.get_avg_for_em", "target_function_prompt": "def get_avg_for_em(px, x, y, cell_height, cell_width):", "function_signature": "def get_avg_for_em(px, x, y, cell_height, cell_width):"}}
{"prompt": "def process_input_source(input_source, api_key):", "metadata": {"task_id": "Multimedia/gif-for-cli/3", "ground_truth": "    if input_source.strip().startswith('https://tenor.com/view/'):\n        gif_id = input_source.rsplit('-', 1)[-1]\n        if gif_id.isdigit():\n            input_source = gif_id\n        else:\n            raise Exception('Bad GIF URL.')\n\n    is_url = input_source.startswith(('http://', 'https://'))\n\n    if not os.path.exists(input_source) and not is_url:\n        # get from Tenor GIF API\n        params = {'key': api_key}\n        if input_source.isdigit():\n            endpoint = 'gifs'\n            params.update({'ids': input_source})\n        elif input_source == '':\n            endpoint = 'trending'\n            params.update({'limit': 1})\n        else:\n            endpoint = 'search'\n            params.update({'limit': 1, 'q': input_source})\n\n        resp = requests.get(\n            'https://api.tenor.com/v1/{}'.format(endpoint),\n            params=params\n        )\n\n        try:\n            resp_json = resp.json()\n        except JSONDecodeError:\n            raise Exception('A server error occurred.')\n\n        if 'error' in resp_json:\n            raise Exception('An error occurred: {}'.format(resp_json['error']))\n\n        results = resp_json.get('results')\n\n        if not results:\n            raise Exception('Could not find GIF.')\n\n        input_source = results[0]['media'][0]['mp4']['url']\n    return input_source", "fpath_tuple": ["Multimedia", "gif-for-cli", "gif_for_cli", "generate", "utils.py"], "context_start_lineno": 73, "line_no": 74, "id": "gif_for_cli.generate.utils.process_input_source", "target_function_prompt": "def process_input_source(input_source, api_key):", "function_signature": "def process_input_source(input_source, api_key):"}}
{"prompt": "def reshape_data(x, hue, labels):", "metadata": {"task_id": "Multimedia/hypertools/8", "ground_truth": "    categories = list(sorted(set(hue), key=list(hue).index))\n    x_stacked = np.vstack(x)\n    x_reshaped = [[] for _ in categories]\n    labels_reshaped = [[] for _ in categories]\n    if labels is None:\n        labels = [None]*len(hue)\n    for idx, (point, label) in enumerate(zip(hue, labels)):\n        x_reshaped[categories.index(point)].append(x_stacked[idx])\n        labels_reshaped[categories.index(point)].append(labels[idx])\n    return [np.vstack(i) for i in x_reshaped], labels_reshaped", "fpath_tuple": ["Multimedia", "hypertools", "hypertools", "_shared", "helpers.py"], "context_start_lineno": 121, "line_no": 122, "id": "hypertools._shared.helpers.reshape_data", "target_function_prompt": "def reshape_data(x, hue, labels):", "function_signature": "def reshape_data(x, hue, labels):"}}
{"prompt": "def from_Note(note, process_octaves=True, standalone=True):", "metadata": {"task_id": "Multimedia/mingus/0", "ground_truth": "    if not hasattr(note, \"name\"):\n        return False\n\n    # Lower the case of the name\n    result = note.name[0].lower()\n\n    # Convert #'s and b's to 'is' and 'es' suffixes\n    for accidental in note.name[1:]:\n        if accidental == \"#\":\n            result += \"is\"\n        elif accidental == \"b\":\n            result += \"es\"\n\n    # Place ' and , for octaves\n    if process_octaves:\n        oct = note.octave\n        if oct >= 4:\n            while oct > 3:\n                result += \"'\"\n                oct -= 1\n        elif oct < 3:\n            while oct < 3:\n                result += \",\"\n                oct += 1\n    if standalone:\n        return \"{ %s }\" % result\n    else:\n        return result", "fpath_tuple": ["Multimedia", "mingus", "mingus", "extra", "lilypond.py"], "context_start_lineno": 36, "line_no": 45, "id": "mingus.extra.lilypond.from_Note", "target_function_prompt": "def from_Note(note, process_octaves=True, standalone=True):", "function_signature": "def from_Note(note, process_octaves=True, standalone=True):"}}
{"prompt": "def _get_qsize(tuning, width):", "metadata": {"task_id": "Multimedia/mingus/1", "ground_truth": "    names = [x.to_shorthand() for x in tuning.tuning]\n    basesize = len(max(names)) + 3\n    barsize = ((width - basesize) - 2) - 1\n\n    # x * 4 + 0.5x - barsize = 0 4.5x = barsize x = barsize / 4.5\n    return max(0, int(barsize / 4.5))", "fpath_tuple": ["Multimedia", "mingus", "mingus", "extra", "tablature.py"], "context_start_lineno": 451, "line_no": 453, "id": "mingus.extra.tablature._get_qsize", "target_function_prompt": "def _get_qsize(tuning, width):", "function_signature": "def _get_qsize(tuning, width):"}}
{"prompt": "def augment(note):", "metadata": {"task_id": "Multimedia/mingus/2", "ground_truth": "    if note[-1] != \"b\":\n        return note + \"#\"\n    else:\n        return note[:-1]", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "notes.py"], "context_start_lineno": 146, "line_no": 155, "id": "mingus.core.notes.augment", "target_function_prompt": "def augment(note):", "function_signature": "def augment(note):"}}
{"prompt": "def valid_beat_duration(duration):", "metadata": {"task_id": "Multimedia/mingus/3", "ground_truth": "    if duration == 0:\n        return False\n    elif duration == 1:\n        return True\n    else:\n        r = duration\n        while r != 1:\n            if r % 2 == 1:\n                return False\n            r /= 2\n        return True", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "meter.py"], "context_start_lineno": 29, "line_no": 31, "id": "mingus.core.meter.valid_beat_duration", "target_function_prompt": "def valid_beat_duration(duration):", "function_signature": "def valid_beat_duration(duration):"}}
{"prompt": "def diminish(note):", "metadata": {"task_id": "Multimedia/mingus/4", "ground_truth": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "notes.py"], "context_start_lineno": 161, "line_no": 170, "id": "mingus.core.notes.diminish", "target_function_prompt": "def diminish(note):", "function_signature": "def diminish(note):"}}
{"prompt": "def invert(interval):", "metadata": {"task_id": "Multimedia/mingus/5", "ground_truth": "    interval.reverse()\n    res = list(interval)\n    interval.reverse()\n    return res", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "intervals.py"], "context_start_lineno": 311, "line_no": 318, "id": "mingus.core.intervals.invert", "target_function_prompt": "def invert(interval):", "function_signature": "def invert(interval):"}}
{"prompt": "def parse_string(progression):", "metadata": {"task_id": "Multimedia/mingus/6", "ground_truth": "    acc = 0\n    roman_numeral = \"\"\n    suffix = \"\"\n    i = 0\n    for c in progression:\n        if c == \"#\":\n            acc += 1\n        elif c == \"b\":\n            acc -= 1\n        elif c.upper() == \"I\" or c.upper() == \"V\":\n            roman_numeral += c.upper()\n        else:\n            break\n        i += 1\n    suffix = progression[i:]\n    return (roman_numeral, acc, suffix)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "progressions.py"], "context_start_lineno": 214, "line_no": 223, "id": "mingus.core.progressions.parse_string", "target_function_prompt": "def parse_string(progression):", "function_signature": "def parse_string(progression):"}}
{"prompt": "def bytes_to_int(bytes, byteorder='big'):", "metadata": {"task_id": "System/exodus-bundler/0", "ground_truth": "    endian = {'big': '>', 'little': '<'}[byteorder]\n    chars = struct.unpack(endian + ('B' * len(bytes)), bytes)\n    if byteorder == 'big':\n        chars = chars[::-1]\n    return sum(int(char) * 256 ** i for (i, char) in enumerate(chars))", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "bundling.py"], "context_start_lineno": 34, "line_no": 36, "id": "exodus_bundler.bundling.bytes_to_int", "target_function_prompt": "def bytes_to_int(bytes, byteorder='big'):", "function_signature": "def bytes_to_int(bytes, byteorder='big'):"}}
{"prompt": "def render_template(string, **context):", "metadata": {"task_id": "System/exodus-bundler/1", "ground_truth": "    for key, value in context.items():\n        string = string.replace('{{%s}}' % key, value)\n    return string", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "templating.py"], "context_start_lineno": 12, "line_no": 13, "id": "exodus_bundler.templating.render_template", "target_function_prompt": "def render_template(string, **context):", "function_signature": "def render_template(string, **context):"}}
{"prompt": "def strip_pid_prefix(line):", "metadata": {"task_id": "System/exodus-bundler/2", "ground_truth": "    match = re.match(r'\\[pid\\s+\\d+\\]\\s*', line)\n    if match:\n        return line[len(match.group()):]\n    return line", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "input_parsing.py"], "context_start_lineno": 104, "line_no": 106, "id": "exodus_bundler.input_parsing.strip_pid_prefix", "target_function_prompt": "def strip_pid_prefix(line):", "function_signature": "def strip_pid_prefix(line):"}}
{"prompt": "def abspath(path):\n    # type: (Text) -> Text", "metadata": {"task_id": "System/fs/0", "ground_truth": "    if not path.startswith(\"/\"):\n        return \"/\" + path\n    return path", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 168, "line_no": 183, "id": "fs.path.abspath", "target_function_prompt": "def abspath(path):\n    # type: (Text) -> Text", "function_signature": "def abspath(path):\n    # type: (Text) -> Text"}}
{"prompt": "def combine(path1, path2):\n    # type: (Text, Text) -> Text", "metadata": {"task_id": "System/fs/1", "ground_truth": "    if not path1:\n        return path2.lstrip()\n    return \"{}/{}\".format(path1.rstrip(\"/\"), path2.lstrip(\"/\"))", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 243, "line_no": 263, "id": "fs.path.combine", "target_function_prompt": "def combine(path1, path2):\n    # type: (Text, Text) -> Text", "function_signature": "def combine(path1, path2):\n    # type: (Text, Text) -> Text"}}
{"prompt": "def split(path):\n    # type: (Text) -> Tuple[Text, Text]", "metadata": {"task_id": "System/fs/2", "ground_truth": "    if \"/\" not in path:\n        return (\"\", path)\n    split = path.rsplit(\"/\", 1)\n    return (split[0] or \"/\", split[1])", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 292, "line_no": 314, "id": "fs.path.split", "target_function_prompt": "def split(path):\n    # type: (Text) -> Tuple[Text, Text]", "function_signature": "def split(path):\n    # type: (Text) -> Tuple[Text, Text]"}}
{"prompt": "def isparent(path1, path2):\n    # type: (Text, Text) -> bool", "metadata": {"task_id": "System/fs/3", "ground_truth": "    bits1 = path1.split(\"/\")\n    bits2 = path2.split(\"/\")\n    while bits1 and bits1[-1] == \"\":\n        bits1.pop()\n    if len(bits1) > len(bits2):\n        return False\n    for (bit1, bit2) in zip(bits1, bits2):\n        if bit1 != bit2:\n            return False\n    return True", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 463, "line_no": 485, "id": "fs.path.isparent", "target_function_prompt": "def isparent(path1, path2):\n    # type: (Text, Text) -> bool", "function_signature": "def isparent(path1, path2):\n    # type: (Text, Text) -> bool"}}
{"prompt": "def forcedir(path):\n    # type: (Text) -> Text", "metadata": {"task_id": "System/fs/4", "ground_truth": "    if not path.endswith(\"/\"):\n        return path + \"/\"\n    return path", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 497, "line_no": 516, "id": "fs.path.forcedir", "target_function_prompt": "def forcedir(path):\n    # type: (Text) -> Text", "function_signature": "def forcedir(path):\n    # type: (Text) -> Text"}}
{"prompt": "def match_any(patterns, name):\n    # type: (Iterable[Text], Text) -> bool", "metadata": {"task_id": "System/fs/5", "ground_truth": "    if not patterns:\n        return True\n    return any(match(pattern, name) for pattern in patterns)", "fpath_tuple": ["System", "fs", "fs", "wildcard.py"], "context_start_lineno": 60, "line_no": 75, "id": "fs.wildcard.match_any", "target_function_prompt": "def match_any(patterns, name):\n    # type: (Iterable[Text], Text) -> bool", "function_signature": "def match_any(patterns, name):\n    # type: (Iterable[Text], Text) -> bool"}}
{"prompt": "def imatch_any(patterns, name):\n    # type: (Iterable[Text], Text) -> bool", "metadata": {"task_id": "System/fs/6", "ground_truth": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)", "fpath_tuple": ["System", "fs", "fs", "wildcard.py"], "context_start_lineno": 80, "line_no": 95, "id": "fs.wildcard.imatch_any", "target_function_prompt": "def imatch_any(patterns, name):\n    # type: (Iterable[Text], Text) -> bool", "function_signature": "def imatch_any(patterns, name):\n    # type: (Iterable[Text], Text) -> bool"}}
{"prompt": "def parse_boolean_envvar(val):", "metadata": {"task_id": "System/wal-e/0", "ground_truth": "    if not val or val.lower() in {'false', '0'}:\n        return False\n    elif val.lower() in {'true', '1'}:\n        return True\n    else:\n        raise ValueError('Invalid boolean environment variable: %s' % val)", "fpath_tuple": ["System", "wal-e", "wal_e", "cmd.py"], "context_start_lineno": 160, "line_no": 162, "id": "wal_e.cmd.parse_boolean_envvar", "target_function_prompt": "def parse_boolean_envvar(val):", "function_signature": "def parse_boolean_envvar(val):"}}
{"prompt": "def get_log_destinations():", "metadata": {"task_id": "System/wal-e/1", "ground_truth": "    env = os.getenv('WALE_LOG_DESTINATION', 'stderr,syslog')\n    return env.split(\",\")", "fpath_tuple": ["System", "wal-e", "wal_e", "log_help.py"], "context_start_lineno": 110, "line_no": 113, "id": "wal_e.log_help.get_log_destinations", "target_function_prompt": "def get_log_destinations():", "function_signature": "def get_log_destinations():"}}
{"prompt": "    def _fmt_structured(d):", "metadata": {"task_id": "System/wal-e/2", "ground_truth": "        timeEntry = datetime.datetime.utcnow().strftime(\n            \"time=%Y-%m-%dT%H:%M:%S.%f-00\")\n        pidEntry = \"pid=\" + str(os.getpid())\n\n        rest = sorted('='.join([str(k), str(v)])\n                      for (k, v) in list(d.items()))\n\n        return ' '.join([timeEntry, pidEntry] + rest)", "fpath_tuple": ["System", "wal-e", "wal_e", "log_help.py"], "context_start_lineno": 144, "line_no": 150, "id": "wal_e.log_help.WalELogger._fmt_structured", "target_function_prompt": "    def _fmt_structured(d):", "function_signature": "    def _fmt_structured(d):"}}
{"prompt": "def _fsync_files(filenames):", "metadata": {"task_id": "System/wal-e/3", "ground_truth": "    touched_directories = set()\n\n    mode = os.O_RDONLY\n\n    # Windows\n    if hasattr(os, 'O_BINARY'):\n        mode |= os.O_BINARY\n\n    for filename in filenames:\n        fd = os.open(filename, mode)\n        os.fsync(fd)\n        os.close(fd)\n        touched_directories.add(os.path.dirname(filename))\n\n    # Some OSes also require us to fsync the directory where we've\n    # created files or subdirectories.\n    if hasattr(os, 'O_DIRECTORY'):\n        for dirname in touched_directories:\n            fd = os.open(dirname, os.O_RDONLY | os.O_DIRECTORY)\n            os.fsync(fd)\n            os.close(fd)", "fpath_tuple": ["System", "wal-e", "wal_e", "tar_partition.py"], "context_start_lineno": 157, "line_no": 163, "id": "wal_e.tar_partition._fsync_files", "target_function_prompt": "def _fsync_files(filenames):", "function_signature": "def _fsync_files(filenames):"}}
{"prompt": "    def list(self, prefix):", "metadata": {"task_id": "System/wal-e/4", "ground_truth": "        path = \"/\" + prefix\n        file_paths = [os.path.join(root, f)\n                      for root, dirs, files in os.walk(path) for f in files]\n        # convert to an array of Keys\n        return [FileKey(bucket=self, name=f) for f in file_paths]", "fpath_tuple": ["System", "wal-e", "wal_e", "blobstore", "file", "calling_format.py"], "context_start_lineno": 71, "line_no": 72, "id": "wal_e.blobstore.file.calling_format.Bucket.list", "target_function_prompt": "    def list(self, prefix):", "function_signature": "    def list(self, prefix):"}}
{"prompt": "def unix_path_join(*path_parts):", "metadata": {"task_id": "System/pyinfra/0", "ground_truth": "    parts = list(path_parts)\n    parts[0:-1] = [part.rstrip(\"/\") for part in parts[0:-1]]\n    return \"/\".join(parts)", "fpath_tuple": ["System", "pyinfra", "pyinfra", "operations", "util", "files.py"], "context_start_lineno": 6, "line_no": 7, "id": "pyinfra.operations.util.files.unix_path_join", "target_function_prompt": "def unix_path_join(*path_parts):", "function_signature": "def unix_path_join(*path_parts):"}}
{"prompt": "def shell(commands):", "metadata": {"task_id": "System/pyinfra/1", "ground_truth": "    if isinstance(commands, str):\n        commands = [commands]\n\n    for command in commands:\n        yield command", "fpath_tuple": ["System", "pyinfra", "pyinfra", "operations", "server.py"], "context_start_lineno": 132, "line_no": 151, "id": "pyinfra.operations.server.shell", "target_function_prompt": "def shell(commands):", "function_signature": "def shell(commands):"}}
{"prompt": "def try_int(value):", "metadata": {"task_id": "System/pyinfra/2", "ground_truth": "    try:\n        return int(value)\n    except (TypeError, ValueError):\n        return value", "fpath_tuple": ["System", "pyinfra", "pyinfra", "api", "util.py"], "context_start_lineno": 54, "line_no": 55, "id": "pyinfra.api.util.try_int", "target_function_prompt": "def try_int(value):", "function_signature": "def try_int(value):"}}
{"prompt": "    def mr_job_script(cls):", "metadata": {"task_id": "System/mrjob/0", "ground_truth": "        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 926, "line_no": 930, "id": "mrjob.job.MRJob.mr_job_script", "target_function_prompt": "    def mr_job_script(cls):", "function_signature": "    def mr_job_script(cls):"}}
{"prompt": "def map_version(version, version_map):", "metadata": {"task_id": "System/mrjob/1", "ground_truth": "    if version is None:\n        raise TypeError\n\n    if not version_map:\n        raise ValueError\n\n    if isinstance(version_map, dict):\n        version_map = sorted((LooseVersion(k), v)\n                             for k, v in version_map.items())\n\n    req_version = LooseVersion(version)\n\n    for min_version, value in reversed(version_map):\n        if req_version >= min_version:\n            return value\n    else:\n        return version_map[0][1]", "fpath_tuple": ["System", "mrjob", "mrjob", "compat.py"], "context_start_lineno": 622, "line_no": 637, "id": "mrjob.compat.map_version", "target_function_prompt": "def map_version(version, version_map):", "function_signature": "def map_version(version, version_map):"}}
{"prompt": "def combine_values(*values):", "metadata": {"task_id": "System/mrjob/2", "ground_truth": "    for v in reversed(values):\n        if v is not None:\n            return v\n    else:\n        return None", "fpath_tuple": ["System", "mrjob", "mrjob", "conf.py"], "context_start_lineno": 377, "line_no": 382, "id": "mrjob.conf.combine_values", "target_function_prompt": "def combine_values(*values):", "function_signature": "def combine_values(*values):"}}
{"prompt": "    def read(self, line):", "metadata": {"task_id": "System/mrjob/3", "ground_truth": "        key_value = line.split(b'\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)", "fpath_tuple": ["System", "mrjob", "mrjob", "protocol.py"], "context_start_lineno": 360, "line_no": 361, "id": "mrjob.protocol.BytesProtocol.read", "target_function_prompt": "    def read(self, line):", "function_signature": "    def read(self, line):"}}
{"prompt": "    def write(self, key, value):", "metadata": {"task_id": "System/mrjob/4", "ground_truth": "        return b'\\t'.join(\n            x.encode('utf_8') for x in (key, value) if x is not None)", "fpath_tuple": ["System", "mrjob", "mrjob", "protocol.py"], "context_start_lineno": 417, "line_no": 418, "id": "mrjob.protocol.TextProtocol.write", "target_function_prompt": "    def write(self, key, value):", "function_signature": "    def write(self, key, value):"}}
{"prompt": "    def read(self, line):", "metadata": {"task_id": "System/mrjob/5", "ground_truth": "        try:\n            line = line.decode('utf_8')\n        except UnicodeDecodeError:\n            line = line.decode('latin_1')\n\n        key_value = line.split(u'\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)", "fpath_tuple": ["System", "mrjob", "mrjob", "protocol.py"], "context_start_lineno": 405, "line_no": 406, "id": "mrjob.protocol.TextProtocol.read", "target_function_prompt": "    def read(self, line):", "function_signature": "    def read(self, line):"}}
{"prompt": "    def read(self, line):", "metadata": {"task_id": "System/mrjob/6", "ground_truth": "        try:\n            return (None, line.decode('utf_8'))\n        except UnicodeDecodeError:\n            return (None, line.decode('latin_1'))", "fpath_tuple": ["System", "mrjob", "mrjob", "protocol.py"], "context_start_lineno": 443, "line_no": 444, "id": "mrjob.protocol.TextValueProtocol.read", "target_function_prompt": "    def read(self, line):", "function_signature": "    def read(self, line):"}}
{"prompt": "def file_ext(filename):", "metadata": {"task_id": "System/mrjob/7", "ground_truth": "    stripped_name = filename.lstrip('.')\n    dot_index = stripped_name.find('.')\n\n    if dot_index == -1:\n        return ''\n    return stripped_name[dot_index:]", "fpath_tuple": ["System", "mrjob", "mrjob", "util.py"], "context_start_lineno": 66, "line_no": 78, "id": "mrjob.util.file_ext", "target_function_prompt": "def file_ext(filename):", "function_signature": "def file_ext(filename):"}}
{"prompt": "def cmd_line(args):", "metadata": {"task_id": "System/mrjob/8", "ground_truth": "    args = [str(x) for x in args]\n    return ' '.join(pipes.quote(x) for x in args)", "fpath_tuple": ["System", "mrjob", "mrjob", "util.py"], "context_start_lineno": 48, "line_no": 51, "id": "mrjob.util.cmd_line", "target_function_prompt": "def cmd_line(args):", "function_signature": "def cmd_line(args):"}}
{"prompt": "def save_cwd():", "metadata": {"task_id": "System/mrjob/9", "ground_truth": "    original_cwd = os.getcwd()\n\n    try:\n        yield\n\n    finally:\n        os.chdir(original_cwd)", "fpath_tuple": ["System", "mrjob", "mrjob", "util.py"], "context_start_lineno": 185, "line_no": 188, "id": "mrjob.util.save_cwd", "target_function_prompt": "def save_cwd():", "function_signature": "def save_cwd():"}}
{"prompt": "def save_sys_std():", "metadata": {"task_id": "System/mrjob/10", "ground_truth": "    stdin, stdout, stderr = sys.stdin, sys.stdout, sys.stderr\n\n    try:\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        yield\n\n        # at this point, sys.stdout/stderr may have been patched. Don't\n        # raise an exception if flush() fails\n        try:\n            sys.stdout.flush()\n        except:\n            pass\n\n        try:\n            sys.stderr.flush()\n        except:\n            pass\n    finally:\n        sys.stdin, sys.stdout, sys.stderr = stdin, stdout, stderr", "fpath_tuple": ["System", "mrjob", "mrjob", "util.py"], "context_start_lineno": 198, "line_no": 203, "id": "mrjob.util.save_sys_std", "target_function_prompt": "def save_sys_std():", "function_signature": "def save_sys_std():"}}
{"prompt": "def unarchive(archive_path, dest):", "metadata": {"task_id": "System/mrjob/11", "ground_truth": "    if tarfile.is_tarfile(archive_path):\n        with tarfile.open(archive_path, 'r') as archive:\n            archive.extractall(dest)\n    elif is_zipfile(archive_path):\n        with ZipFile(archive_path, 'r') as archive:\n            for name in archive.namelist():\n                # the zip spec specifies that front slashes are always\n                # used as directory separators\n                dest_path = os.path.join(dest, *name.split('/'))\n\n                # now, split out any dirname and filename and create\n                # one and/or the other\n                dirname, filename = os.path.split(dest_path)\n                if dirname and not os.path.exists(dirname):\n                    os.makedirs(dirname)\n                if filename:\n                    with open(dest_path, 'wb') as dest_file:\n                        dest_file.write(archive.read(name))\n    else:\n        raise IOError('Unknown archive type: %s' % (archive_path,))", "fpath_tuple": ["System", "mrjob", "mrjob", "util.py"], "context_start_lineno": 333, "line_no": 347, "id": "mrjob.util.unarchive", "target_function_prompt": "def unarchive(archive_path, dest):", "function_signature": "def unarchive(archive_path, dest):"}}
{"prompt": "def unique(items):", "metadata": {"task_id": "System/mrjob/12", "ground_truth": "    seen = set()\n\n    for item in items:\n        if item in seen:\n            continue\n        else:\n            yield item\n            seen.add(item)", "fpath_tuple": ["System", "mrjob", "mrjob", "util.py"], "context_start_lineno": 321, "line_no": 323, "id": "mrjob.util.unique", "target_function_prompt": "def unique(items):", "function_signature": "def unique(items):"}}
{"prompt": "def urlparse(urlstring, scheme='', allow_fragments=True, *args, **kwargs):", "metadata": {"task_id": "System/mrjob/13", "ground_truth": "    (scheme, netloc, path, params, query, fragment) = (\n        urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs))\n\n    if allow_fragments and '#' in path and not fragment:\n        path, fragment = path.split('#', 1)\n\n    return ParseResult(scheme, netloc, path, params, query, fragment)", "fpath_tuple": ["System", "mrjob", "mrjob", "parse.py"], "context_start_lineno": 78, "line_no": 84, "id": "mrjob.parse.urlparse", "target_function_prompt": "def urlparse(urlstring, scheme='', allow_fragments=True, *args, **kwargs):", "function_signature": "def urlparse(urlstring, scheme='', allow_fragments=True, *args, **kwargs):"}}
{"prompt": "def which(cmd, path=None):", "metadata": {"task_id": "System/mrjob/14", "ground_truth": "    if hasattr(shutil, 'which'):\n        # added in Python 3.3\n        return shutil.which(cmd, path=path)\n    elif path is None and os.environ.get('PATH') is None:\n        # find_executable() errors if neither path nor $PATH is set\n        return None\n    else:\n        return find_executable(cmd, path=path)", "fpath_tuple": ["System", "mrjob", "mrjob", "util.py"], "context_start_lineno": 369, "line_no": 378, "id": "mrjob.util.which", "target_function_prompt": "def which(cmd, path=None):", "function_signature": "def which(cmd, path=None):"}}
{"prompt": "def parse_hostport(rhostport):", "metadata": {"task_id": "System/sshuttle/0", "ground_truth": "    if rhostport is None or len(rhostport) == 0:\n        return None, None, None, None\n    port = None\n    username = None\n    password = None\n    host = rhostport\n\n    if \"@\" in host:\n        # split username (and possible password) from the host[:port]\n        username, host = host.rsplit(\"@\", 1)\n        # Fix #410 bad username error detect\n        if \":\" in username:\n            # this will even allow for the username to be empty\n            username, password = username.split(\":\")\n\n    if \":\" in host:\n        # IPv6 address and/or got a port specified\n\n        # If it is an IPv6 address with port specification,\n        # then it will look like: [::1]:22\n\n        try:\n            # try to parse host as an IP address,\n            # if that works it is an IPv6 address\n            host = str(ipaddress.ip_address(host))\n        except ValueError:\n            # if that fails parse as URL to get the port\n            parsed = urlparse('//{}'.format(host))\n            try:\n                host = str(ipaddress.ip_address(parsed.hostname))\n            except ValueError:\n                # else if both fails, we have a hostname with port\n                host = parsed.hostname\n            port = parsed.port\n\n    if password is None or len(password) == 0:\n        password = None\n\n    return username, password, port, host", "fpath_tuple": ["System", "sshuttle", "sshuttle", "ssh.py"], "context_start_lineno": 32, "line_no": 45, "id": "sshuttle.ssh.parse_hostport", "target_function_prompt": "def parse_hostport(rhostport):", "function_signature": "def parse_hostport(rhostport):"}}
{"prompt": "def stringified_dict_contains_value(key, value, str_dict):", "metadata": {"task_id": "System/flower/0", "ground_truth": "    if not str_dict:\n        return False\n    value = str(value)\n    try:\n        # + 3 for key right quote, one for colon and one for space\n        key_index = str_dict.index(key) + len(key) + 3\n    except ValueError:\n        return False\n    try:\n        comma_index = str_dict.index(',', key_index)\n    except ValueError:\n        # last value in dict\n        comma_index = str_dict.index('}', key_index)\n    return str(value) == str_dict[key_index:comma_index].strip('\"\\'')", "fpath_tuple": ["System", "flower", "flower", "utils", "search.py"], "context_start_lineno": 61, "line_no": 66, "id": "flower.utils.search.stringified_dict_contains_value", "target_function_prompt": "def stringified_dict_contains_value(key, value, str_dict):", "function_signature": "def stringified_dict_contains_value(key, value, str_dict):"}}
{"prompt": "def abs_path(path):", "metadata": {"task_id": "System/flower/1", "ground_truth": "    path = os.path.expanduser(path)\n    if not os.path.isabs(path):\n        cwd = os.environ.get('PWD') or os.getcwd()\n        path = os.path.join(cwd, path)\n    return path", "fpath_tuple": ["System", "flower", "flower", "utils", "__init__.py"], "context_start_lineno": 31, "line_no": 32, "id": "flower.utils.abs_path", "target_function_prompt": "def abs_path(path):", "function_signature": "def abs_path(path):"}}
{"prompt": "def strtobool(val):", "metadata": {"task_id": "System/flower/2", "ground_truth": "    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    if val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    raise ValueError(f\"invalid truth value {val!r}\")", "fpath_tuple": ["System", "flower", "flower", "utils", "__init__.py"], "context_start_lineno": 43, "line_no": 50, "id": "flower.utils.strtobool", "target_function_prompt": "def strtobool(val):", "function_signature": "def strtobool(val):"}}
{"prompt": "def get_method(method_name):", "metadata": {"task_id": "System/sshuttle/1", "ground_truth": "    module = importlib.import_module(\"sshuttle.methods.%s\" % method_name)\n    return module.Method(method_name)", "fpath_tuple": ["System", "sshuttle", "sshuttle", "methods", "__init__.py"], "context_start_lineno": 102, "line_no": 103, "id": "sshuttle.methods.get_method", "target_function_prompt": "def get_method(method_name):", "function_signature": "def get_method(method_name):"}}
{"prompt": "def all_known_iam_permissions():", "metadata": {"task_id": "Security/trailscraper/0", "ground_truth": "    with open(os.path.join(os.path.dirname(__file__), 'known-iam-actions.txt'), encoding=\"UTF-8\") as iam_file:\n        return {line.rstrip('\\n') for line in iam_file.readlines()}", "fpath_tuple": ["Security", "trailscraper", "trailscraper", "iam.py"], "context_start_lineno": 172, "line_no": 174, "id": "trailscraper.iam.all_known_iam_permissions", "target_function_prompt": "def all_known_iam_permissions():", "function_signature": "def all_known_iam_permissions():"}}
{"prompt": "def parse_records(json_records):", "metadata": {"task_id": "Security/trailscraper/1", "ground_truth": "    parsed_records = [_parse_record(record) for record in json_records]\n    return [r for r in parsed_records if r is not None]", "fpath_tuple": ["Security", "trailscraper", "trailscraper", "cloudtrail.py"], "context_start_lineno": 239, "line_no": 241, "id": "trailscraper.cloudtrail.parse_records", "target_function_prompt": "def parse_records(json_records):", "function_signature": "def parse_records(json_records):"}}
{"prompt": "    def int_to_script_bytes(class_, v):", "metadata": {"task_id": "Security/pycoin/0", "ground_truth": "        if v == 0:\n            return b''\n        is_negative = (v < 0)\n        if is_negative:\n            v = -v\n        ba = bytearray()\n        while v >= 256:\n            ba.append(v & 0xff)\n            v >>= 8\n        ba.append(v & 0xff)\n        if ba[-1] >= 128:\n            ba.append(0x80 if is_negative else 0)\n        elif is_negative:\n            ba[-1] |= 0x80\n        return bytes(ba)", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "IntStreamer.py"], "context_start_lineno": 30, "line_no": 31, "id": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "target_function_prompt": "    def int_to_script_bytes(class_, v):", "function_signature": "    def int_to_script_bytes(class_, v):"}}
{"prompt": "def do_OP_2DROP(stack):", "metadata": {"task_id": "Security/pycoin/1", "ground_truth": "    stack.pop()\n    stack.pop()", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "stackops.py"], "context_start_lineno": 32, "line_no": 33, "id": "pycoin.satoshi.stackops.do_OP_2DROP", "target_function_prompt": "def do_OP_2DROP(stack):", "function_signature": "def do_OP_2DROP(stack):"}}
{"prompt": "def do_OP_2DUP(stack):\n    #  (x1 x2 -- x1 x2 x1 x2)", "metadata": {"task_id": "Security/pycoin/2", "ground_truth": "    stack.append(stack[-2])\n    stack.append(stack[-2])", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "stackops.py"], "context_start_lineno": 37, "line_no": 39, "id": "pycoin.satoshi.stackops.do_OP_2DUP", "target_function_prompt": "def do_OP_2DUP(stack):\n    #  (x1 x2 -- x1 x2 x1 x2)", "function_signature": "def do_OP_2DUP(stack):\n    #  (x1 x2 -- x1 x2 x1 x2)"}}
{"prompt": "def do_OP_3DUP(stack):\n    #  (x1 x2 x3 -- x1 x2 x3 x1 x2 x3)", "metadata": {"task_id": "Security/pycoin/3", "ground_truth": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "stackops.py"], "context_start_lineno": 43, "line_no": 45, "id": "pycoin.satoshi.stackops.do_OP_3DUP", "target_function_prompt": "def do_OP_3DUP(stack):\n    #  (x1 x2 x3 -- x1 x2 x3 x1 x2 x3)", "function_signature": "def do_OP_3DUP(stack):\n    #  (x1 x2 x3 -- x1 x2 x3 x1 x2 x3)"}}
{"prompt": "def _s3_key_prefixes(prefix, org_ids, account_ids, regions, from_date, to_date):", "metadata": {"task_id": "Security/trailscraper/2", "ground_truth": "    delta = to_date.astimezone(pytz.utc) - from_date.astimezone(pytz.utc)\n\n    days = [to_date - datetime.timedelta(days=delta_days) for delta_days in range(delta.days + 1)]\n    if org_ids:\n        return [_s3_key_prefix_for_org_trails(prefix, day, org_id, account_id, region)\n                for org_id in org_ids\n                for account_id in account_ids\n                for day in days\n                for region in regions]\n\n    return [_s3_key_prefix(prefix, day, account_id, region)\n            for account_id in account_ids\n            for day in days\n            for region in regions]", "fpath_tuple": ["Security", "trailscraper", "trailscraper", "s3_download.py"], "context_start_lineno": 21, "line_no": 22, "id": "trailscraper.s3_download._s3_key_prefixes", "target_function_prompt": "def _s3_key_prefixes(prefix, org_ids, account_ids, regions, from_date, to_date):", "function_signature": "def _s3_key_prefixes(prefix, org_ids, account_ids, regions, from_date, to_date):"}}
{"prompt": "def do_OP_2OVER(stack):\n    #  (x1 x2 x3 x4 -- x1 x2 x3 x4 x1 x2)", "metadata": {"task_id": "Security/pycoin/4", "ground_truth": "    stack.append(stack[-4])\n    stack.append(stack[-4])", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "stackops.py"], "context_start_lineno": 50, "line_no": 52, "id": "pycoin.satoshi.stackops.do_OP_2OVER", "target_function_prompt": "def do_OP_2OVER(stack):\n    #  (x1 x2 x3 x4 -- x1 x2 x3 x4 x1 x2)", "function_signature": "def do_OP_2OVER(stack):\n    #  (x1 x2 x3 x4 -- x1 x2 x3 x4 x1 x2)"}}
{"prompt": "def do_OP_2SWAP(stack):", "metadata": {"task_id": "Security/pycoin/5", "ground_truth": "    stack.append(stack.pop(-4))\n    stack.append(stack.pop(-4))", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "stackops.py"], "context_start_lineno": 62, "line_no": 63, "id": "pycoin.satoshi.stackops.do_OP_2SWAP", "target_function_prompt": "def do_OP_2SWAP(stack):", "function_signature": "def do_OP_2SWAP(stack):"}}
{"prompt": "def do_OP_IFDUP(stack):", "metadata": {"task_id": "Security/pycoin/6", "ground_truth": "    if stack[-1]:\n        stack.append(stack[-1])", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "stackops.py"], "context_start_lineno": 67, "line_no": 68, "id": "pycoin.satoshi.stackops.do_OP_IFDUP", "target_function_prompt": "def do_OP_IFDUP(stack):", "function_signature": "def do_OP_IFDUP(stack):"}}
{"prompt": "def do_OP_NIP(stack):", "metadata": {"task_id": "Security/pycoin/7", "ground_truth": "    v = stack.pop()\n    stack.pop()\n    stack.append(v)", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "stackops.py"], "context_start_lineno": 80, "line_no": 81, "id": "pycoin.satoshi.stackops.do_OP_NIP", "target_function_prompt": "def do_OP_NIP(stack):", "function_signature": "def do_OP_NIP(stack):"}}
{"prompt": "def do_OP_TUCK(stack):", "metadata": {"task_id": "Security/pycoin/8", "ground_truth": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1)\n    stack.append(v2)\n    stack.append(v1)", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "stackops.py"], "context_start_lineno": 98, "line_no": 99, "id": "pycoin.satoshi.stackops.do_OP_TUCK", "target_function_prompt": "def do_OP_TUCK(stack):", "function_signature": "def do_OP_TUCK(stack):"}}
{"prompt": "def do_OP_CAT(stack):", "metadata": {"task_id": "Security/pycoin/9", "ground_truth": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v2 + v1)", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "stackops.py"], "context_start_lineno": 106, "line_no": 107, "id": "pycoin.satoshi.stackops.do_OP_CAT", "target_function_prompt": "def do_OP_CAT(stack):", "function_signature": "def do_OP_CAT(stack):"}}
{"prompt": "def crack_secret_exponent_from_k(generator, signed_value, sig, k):", "metadata": {"task_id": "Security/pycoin/10", "ground_truth": "    r, s = sig\n    return ((s * k - signed_value) * generator.inverse(r)) % generator.order()", "fpath_tuple": ["Security", "pycoin", "pycoin", "crack", "ecdsa.py"], "context_start_lineno": 1, "line_no": 5, "id": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "target_function_prompt": "def crack_secret_exponent_from_k(generator, signed_value, sig, k):", "function_signature": "def crack_secret_exponent_from_k(generator, signed_value, sig, k):"}}
{"prompt": "def crack_k_from_sigs(generator, sig1, val1, sig2, val2):", "metadata": {"task_id": "Security/pycoin/11", "ground_truth": "    r1, s1 = sig1\n    r2, s2 = sig2\n    if r1 != r2:\n        raise ValueError(\"r values of signature do not match\")\n    k = (r2 * val1 - r1 * val2) * generator.inverse(r2 * s1 - r1 * s2)\n    return k % generator.order()", "fpath_tuple": ["Security", "pycoin", "pycoin", "crack", "ecdsa.py"], "context_start_lineno": 9, "line_no": 28, "id": "pycoin.crack.ecdsa.crack_k_from_sigs", "target_function_prompt": "def crack_k_from_sigs(generator, sig1, val1, sig2, val2):", "function_signature": "def crack_k_from_sigs(generator, sig1, val1, sig2, val2):"}}
{"prompt": "def standard_streamer(parsing_functions, parse_satoshi_int=parse_satoshi_int):", "metadata": {"task_id": "Security/pycoin/12", "ground_truth": "    streamer = Streamer()\n    streamer.register_array_count_parse(parse_satoshi_int)\n    streamer.register_functions(parsing_functions)\n    return streamer", "fpath_tuple": ["Security", "pycoin", "pycoin", "message", "make_parser_and_packer.py"], "context_start_lineno": 208, "line_no": 213, "id": "pycoin.message.make_parser_and_packer.standard_streamer", "target_function_prompt": "def standard_streamer(parsing_functions, parse_satoshi_int=parse_satoshi_int):", "function_signature": "def standard_streamer(parsing_functions, parse_satoshi_int=parse_satoshi_int):"}}
{"prompt": "def subpaths_for_path_range(path_range, hardening_chars=\"'pH\"):", "metadata": {"task_id": "Security/pycoin/13", "ground_truth": "    if path_range == '':\n        yield ''\n        return\n\n    def range_iterator(the_range):\n        for r in the_range.split(\",\"):\n            is_hardened = r[-1] in hardening_chars\n            hardened_char = hardening_chars[-1] if is_hardened else ''\n            if is_hardened:\n                r = r[:-1]\n            if '-' in r:\n                low, high = [int(x) for x in r.split(\"-\", 1)]\n                for t in range(low, high+1):\n                    yield \"%d%s\" % (t, hardened_char)\n            else:\n                yield \"%s%s\" % (r, hardened_char)\n\n    components = path_range.split(\"/\")\n    iterators = [range_iterator(c) for c in components]\n    for v in itertools.product(*iterators):\n        yield '/'.join(v)", "fpath_tuple": ["Security", "pycoin", "pycoin", "key", "subpaths.py"], "context_start_lineno": 3, "line_no": 15, "id": "pycoin.key.subpaths.subpaths_for_path_range", "target_function_prompt": "def subpaths_for_path_range(path_range, hardening_chars=\"'pH\"):", "function_signature": "def subpaths_for_path_range(path_range, hardening_chars=\"'pH\"):"}}
{"prompt": "def _is_python_file(path):", "metadata": {"task_id": "Security/python-taint/0", "ground_truth": "    if os.path.splitext(path)[1] == '.py':\n        return True\n    return False", "fpath_tuple": ["Security", "python-taint", "pyt", "core", "project_handler.py"], "context_start_lineno": 72, "line_no": 73, "id": "pyt.core.project_handler._is_python_file", "target_function_prompt": "def _is_python_file(path):", "function_signature": "def _is_python_file(path):"}}
{"prompt": "def h2b(h):", "metadata": {"task_id": "Security/pycoin/14", "ground_truth": "    try:\n        return binascii.unhexlify(h.encode(\"ascii\"))\n    except Exception:\n        raise ValueError(\"h2b failed on %s\" % h)", "fpath_tuple": ["Security", "pycoin", "pycoin", "encoding", "hexbytes.py"], "context_start_lineno": 3, "line_no": 11, "id": "pycoin.encoding.hexbytes.h2b", "target_function_prompt": "def h2b(h):", "function_signature": "def h2b(h):"}}
{"prompt": "def calc_average_degree(graph):", "metadata": {"task_id": "Security/zxcvbn-python/0", "ground_truth": "    average = 0\n\n    for key, neighbors in graph.items():\n        average += len([n for n in neighbors if n])\n    average /= float(len(graph.items()))\n\n    return average", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "scoring.py"], "context_start_lineno": 9, "line_no": 10, "id": "zxcvbn.scoring.calc_average_degree", "target_function_prompt": "def calc_average_degree(graph):", "function_signature": "def calc_average_degree(graph):"}}
{"prompt": "def nCk(n, k):", "metadata": {"task_id": "Security/zxcvbn-python/1", "ground_truth": "    if k > n:\n        return 0\n    if k == 0:\n        return 1\n\n    r = 1\n    for d in range(1, k + 1):\n        r *= n\n        r /= d\n        n -= 1\n\n    return r", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "scoring.py"], "context_start_lineno": 28, "line_no": 30, "id": "zxcvbn.scoring.nCk", "target_function_prompt": "def nCk(n, k):", "function_signature": "def nCk(n, k):"}}
{"prompt": "def relevant_l33t_subtable(password, table):", "metadata": {"task_id": "Security/zxcvbn-python/2", "ground_truth": "    password_chars = {}\n    for char in list(password):\n        password_chars[char] = True\n\n    subtable = {}\n    for letter, subs in table.items():\n        relevant_subs = [sub for sub in subs if sub in password_chars]\n        if len(relevant_subs) > 0:\n            subtable[letter] = relevant_subs\n\n    return subtable", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "matching.py"], "context_start_lineno": 133, "line_no": 134, "id": "zxcvbn.matching.relevant_l33t_subtable", "target_function_prompt": "def relevant_l33t_subtable(password, table):", "function_signature": "def relevant_l33t_subtable(password, table):"}}
{"prompt": "def translate(string, chr_map):", "metadata": {"task_id": "Security/zxcvbn-python/3", "ground_truth": "    chars = []\n    for char in list(string):\n        if chr_map.get(char, False):\n            chars.append(chr_map[char])\n        else:\n            chars.append(char)\n\n    return ''.join(chars)", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "matching.py"], "context_start_lineno": 203, "line_no": 204, "id": "zxcvbn.matching.translate", "target_function_prompt": "def translate(string, chr_map):", "function_signature": "def translate(string, chr_map):"}}
{"prompt": "def get_nets(objects, db):", "metadata": {"task_id": "Security/capirca/0", "ground_truth": "  results = []\n  for obj in objects:\n    net = db.GetNet(obj)\n    results.append((obj, net))\n  return results", "fpath_tuple": ["Security", "capirca", "tools", "cgrep.py"], "context_start_lineno": 379, "line_no": 389, "id": "tools.cgrep.get_nets", "target_function_prompt": "def get_nets(objects, db):", "function_signature": "def get_nets(objects, db):"}}
{"prompt": "def get_ports(svc_group, db):", "metadata": {"task_id": "Security/capirca/1", "ground_truth": "  results = []\n  for svc in svc_group:\n    port = db.GetService(svc)\n    results.append((svc, port))\n  return results", "fpath_tuple": ["Security", "capirca", "tools", "cgrep.py"], "context_start_lineno": 446, "line_no": 457, "id": "tools.cgrep.get_ports", "target_function_prompt": "def get_ports(svc_group, db):", "function_signature": "def get_ports(svc_group, db):"}}
{"prompt": "def compare_ip_token(options, db):", "metadata": {"task_id": "Security/capirca/2", "ground_truth": "  token = options.token\n  results = []\n  for ip in options.ip:\n    rval = db.GetIpParents(ip)\n    if token in rval:\n      results = '%s is in %s' % (ip, token)\n    else:\n      results = '%s is _not_ in %s' % (ip, token)\n  return results", "fpath_tuple": ["Security", "capirca", "tools", "cgrep.py"], "context_start_lineno": 425, "line_no": 435, "id": "tools.cgrep.compare_ip_token", "target_function_prompt": "def compare_ip_token(options, db):", "function_signature": "def compare_ip_token(options, db):"}}
{"prompt": "def get_services(options, db):", "metadata": {"task_id": "Security/capirca/3", "ground_truth": "  results = []\n  port, protocol = options.port\n  # swap values if they were passed in wrong order\n  if port.isalpha() and protocol.isdigit():\n    port, protocol = protocol, port\n  results = db.GetPortParents(port, protocol)\n  return port, protocol, results", "fpath_tuple": ["Security", "capirca", "tools", "cgrep.py"], "context_start_lineno": 464, "line_no": 475, "id": "tools.cgrep.get_services", "target_function_prompt": "def get_services(options, db):", "function_signature": "def get_services(options, db):"}}
{"prompt": "def String(value: Union[bytes, str]) -> bytes:", "metadata": {"task_id": "Security/asyncssh/0", "ground_truth": "    if isinstance(value, str):\n        value = value.encode('utf-8', errors='strict')\n\n    return len(value).to_bytes(4, 'big') + value", "fpath_tuple": ["Security", "asyncssh", "asyncssh", "packet.py"], "context_start_lineno": 66, "line_no": 69, "id": "asyncssh.packet.String", "target_function_prompt": "def String(value: Union[bytes, str]) -> bytes:", "function_signature": "def String(value: Union[bytes, str]) -> bytes:"}}
{"prompt": "def laplace_smooth_cmd_counts(\n    seq1_counts: DefaultDict[str, int],\n    seq2_counts: DefaultDict[str, DefaultDict[str, int]],\n    start_token: str,\n    end_token: str,\n    unk_token: str,\n) -> Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]:", "metadata": {"task_id": "Security/msticpy/0", "ground_truth": "    seq1_counts_ls = copy.deepcopy(seq1_counts)\n    seq2_counts_ls = copy.deepcopy(seq2_counts)\n\n    cmds: List[str] = list(seq1_counts_ls.keys()) + [unk_token]\n    for cmd1 in cmds:\n        for cmd2 in cmds:\n            if cmd1 != end_token and cmd2 != start_token:\n                seq1_counts_ls[cmd1] += 1\n                seq2_counts_ls[cmd1][cmd2] += 1\n                seq1_counts_ls[cmd2] += 1\n\n    return seq1_counts_ls, seq2_counts_ls", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "laplace_smooth.py"], "context_start_lineno": 11, "line_no": 44, "id": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "target_function_prompt": "def laplace_smooth_cmd_counts(\n    seq1_counts: DefaultDict[str, int],\n    seq2_counts: DefaultDict[str, DefaultDict[str, int]],\n    start_token: str,\n    end_token: str,\n    unk_token: str,\n) -> Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]:", "function_signature": "def laplace_smooth_cmd_counts(\n    seq1_counts: DefaultDict[str, int],\n    seq2_counts: DefaultDict[str, DefaultDict[str, int]],\n    start_token: str,\n    end_token: str,\n    unk_token: str,\n) -> Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]:"}}
{"prompt": "def laplace_smooth_param_counts(\n    cmds: List[str],\n    param_counts: DefaultDict[str, int],\n    cmd_param_counts: DefaultDict[str, DefaultDict[str, int]],\n    unk_token: str,\n) -> Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]:", "metadata": {"task_id": "Security/msticpy/1", "ground_truth": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    params: List[str] = list(param_counts.keys()) + [unk_token]\n    for cmd in cmds:\n        for param in params:\n            if param in cmd_param_counts_ls[cmd] or param == unk_token:\n                param_counts_ls[param] += 1\n                cmd_param_counts_ls[cmd][param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "laplace_smooth.py"], "context_start_lineno": 58, "line_no": 88, "id": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "target_function_prompt": "def laplace_smooth_param_counts(\n    cmds: List[str],\n    param_counts: DefaultDict[str, int],\n    cmd_param_counts: DefaultDict[str, DefaultDict[str, int]],\n    unk_token: str,\n) -> Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]:", "function_signature": "def laplace_smooth_param_counts(\n    cmds: List[str],\n    param_counts: DefaultDict[str, int],\n    cmd_param_counts: DefaultDict[str, DefaultDict[str, int]],\n    unk_token: str,\n) -> Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]:"}}
{"prompt": "def laplace_smooth_value_counts(\n    params: List[str],\n    value_counts: DefaultDict[str, int],\n    param_value_counts: DefaultDict[str, DefaultDict[str, int]],\n    unk_token: str,\n) -> Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]:", "metadata": {"task_id": "Security/msticpy/2", "ground_truth": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts_ls.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "laplace_smooth.py"], "context_start_lineno": 101, "line_no": 131, "id": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "target_function_prompt": "def laplace_smooth_value_counts(\n    params: List[str],\n    value_counts: DefaultDict[str, int],\n    param_value_counts: DefaultDict[str, DefaultDict[str, int]],\n    unk_token: str,\n) -> Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]:", "function_signature": "def laplace_smooth_value_counts(\n    params: List[str],\n    value_counts: DefaultDict[str, int],\n    param_value_counts: DefaultDict[str, DefaultDict[str, int]],\n    unk_token: str,\n) -> Tuple[DefaultDict[str, int], DefaultDict[str, DefaultDict[str, int]]]:"}}
{"prompt": "def check_epsilon_delta(epsilon, delta, allow_zero=False):", "metadata": {"task_id": "Security/diffprivlib/0", "ground_truth": "    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n\n    if not 0 <= delta <= 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n\n    if not allow_zero and epsilon + delta == 0:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "validation.py"], "context_start_lineno": 27, "line_no": 46, "id": "diffprivlib.validation.check_epsilon_delta", "target_function_prompt": "def check_epsilon_delta(epsilon, delta, allow_zero=False):", "function_signature": "def check_epsilon_delta(epsilon, delta, allow_zero=False):"}}
{"prompt": "def check_random_state(seed, secure=False):", "metadata": {"task_id": "Security/diffprivlib/1", "ground_truth": "    if secure:\n        if isinstance(seed, secrets.SystemRandom):\n            return seed\n\n        if seed is None or seed is np.random.mtrand._rand:  # pylint: disable=protected-access\n            return secrets.SystemRandom()\n    elif isinstance(seed, secrets.SystemRandom):\n        raise ValueError(\"secrets.SystemRandom instance cannot be passed when secure is False.\")\n\n    return skl_check_random_state(seed)", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "utils.py"], "context_start_lineno": 74, "line_no": 92, "id": "diffprivlib.utils.check_random_state", "target_function_prompt": "def check_random_state(seed, secure=False):", "function_signature": "def check_random_state(seed, secure=False):"}}
{"prompt": "def clip_to_norm(array, clip):", "metadata": {"task_id": "Security/diffprivlib/2", "ground_truth": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1) / clip\n    norms[norms < 1] = 1\n\n    return array / norms[:, np.newaxis]", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "validation.py"], "context_start_lineno": 134, "line_no": 151, "id": "diffprivlib.validation.clip_to_norm", "target_function_prompt": "def clip_to_norm(array, clip):", "function_signature": "def clip_to_norm(array, clip):"}}
{"prompt": "    def fit_transform(self, X, y=None):", "metadata": {"task_id": "Security/diffprivlib/3", "ground_truth": "        del y\n\n        self._fit(X)\n\n        return self.transform(X)", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "models", "pca.py"], "context_start_lineno": 282, "line_no": 283, "id": "diffprivlib.models.pca.PCA.fit_transform", "target_function_prompt": "    def fit_transform(self, X, y=None):", "function_signature": "    def fit_transform(self, X, y=None):"}}
{"prompt": "def get_slots(cls: Type[Any]) -> Iterator[str]:", "metadata": {"task_id": "Software-Development/discord-py/0", "ground_truth": "    for mro in reversed(cls.__mro__):\n        try:\n            yield from mro.__slots__  # type: ignore\n        except AttributeError:\n            continue", "fpath_tuple": ["Software-Development", "discord-py", "discord", "utils.py"], "context_start_lineno": 720, "line_no": 721, "id": "discord.utils.get_slots", "target_function_prompt": "def get_slots(cls: Type[Any]) -> Iterator[str]:", "function_signature": "def get_slots(cls: Type[Any]) -> Iterator[str]:"}}
{"prompt": "def is_inside_class(func: Callable[..., Any]) -> bool:\n    # For methods defined in a class, the qualname has a dotted path\n    # denoting which class it belongs to. So, e.g. for A.foo the qualname\n    # would be A.foo while a global foo() would just be foo.\n    #\n    # Unfortunately, for nested functions this breaks. So inside an outer\n    # function named outer, those two would end up having a qualname with\n    # outer.<locals>.A.foo and outer.<locals>.foo\n", "metadata": {"task_id": "Software-Development/discord-py/1", "ground_truth": "    if func.__qualname__ == func.__name__:\n        return False\n    (remaining, _, _) = func.__qualname__.rpartition('.')\n    return not remaining.endswith('<locals>')", "fpath_tuple": ["Software-Development", "discord-py", "discord", "utils.py"], "context_start_lineno": 1177, "line_no": 1186, "id": "discord.utils.is_inside_class", "target_function_prompt": "def is_inside_class(func: Callable[..., Any]) -> bool:\n    # For methods defined in a class, the qualname has a dotted path\n    # denoting which class it belongs to. So, e.g. for A.foo the qualname\n    # would be A.foo while a global foo() would just be foo.\n    #\n    # Unfortunately, for nested functions this breaks. So inside an outer\n    # function named outer, those two would end up having a qualname with\n    # outer.<locals>.A.foo and outer.<locals>.foo\n", "function_signature": "def is_inside_class(func: Callable[..., Any]) -> bool:\n    # For methods defined in a class, the qualname has a dotted path\n    # denoting which class it belongs to. So, e.g. for A.foo the qualname\n    # would be A.foo while a global foo() would just be foo.\n    #\n    # Unfortunately, for nested functions this breaks. So inside an outer\n    # function named outer, those two would end up having a qualname with\n    # outer.<locals>.A.foo and outer.<locals>.foo\n"}}
{"prompt": "def slugify(fn: Callable) -> Callable:\n    @wraps(fn)", "metadata": {"task_id": "Software-Development/Faker/0", "ground_truth": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs))\n\n    return wrapper", "fpath_tuple": ["Software-Development", "Faker", "faker", "utils", "decorators.py"], "context_start_lineno": 8, "line_no": 10, "id": "faker.utils.decorators.slugify", "target_function_prompt": "def slugify(fn: Callable) -> Callable:\n    @wraps(fn)", "function_signature": "def slugify(fn: Callable) -> Callable:\n    @wraps(fn)"}}
{"prompt": "def slugify_domain(fn: Callable) -> Callable:\n    @wraps(fn)", "metadata": {"task_id": "Software-Development/Faker/1", "ground_truth": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper", "fpath_tuple": ["Software-Development", "Faker", "faker", "utils", "decorators.py"], "context_start_lineno": 16, "line_no": 18, "id": "faker.utils.decorators.slugify_domain", "target_function_prompt": "def slugify_domain(fn: Callable) -> Callable:\n    @wraps(fn)", "function_signature": "def slugify_domain(fn: Callable) -> Callable:\n    @wraps(fn)"}}
{"prompt": "def slugify_unicode(fn: Callable) -> Callable:\n    @wraps(fn)", "metadata": {"task_id": "Software-Development/Faker/2", "ground_truth": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_unicode=True)\n\n    return wrapper", "fpath_tuple": ["Software-Development", "Faker", "faker", "utils", "decorators.py"], "context_start_lineno": 24, "line_no": 26, "id": "faker.utils.decorators.slugify_unicode", "target_function_prompt": "def slugify_unicode(fn: Callable) -> Callable:\n    @wraps(fn)", "function_signature": "def slugify_unicode(fn: Callable) -> Callable:\n    @wraps(fn)"}}
{"prompt": "def get_path(module: ModuleType) -> str:", "metadata": {"task_id": "Software-Development/Faker/3", "ground_truth": "    if getattr(sys, \"frozen\", False):\n        # frozen\n\n        if getattr(sys, \"_MEIPASS\", False):\n            # PyInstaller\n            lib_dir = Path(getattr(sys, \"_MEIPASS\"))\n        else:\n            # others\n            lib_dir = Path(sys.executable).parent / \"lib\"\n\n        path = lib_dir.joinpath(*module.__package__.split(\".\"))  # type: ignore\n    else:\n        # unfrozen\n        if module.__file__ is not None:\n            path = Path(module.__file__).parent\n        else:\n            raise RuntimeError(f\"Can't find path from module `{module}.\")\n    return str(path)", "fpath_tuple": ["Software-Development", "Faker", "faker", "utils", "loading.py"], "context_start_lineno": 9, "line_no": 10, "id": "faker.utils.loading.get_path", "target_function_prompt": "def get_path(module: ModuleType) -> str:", "function_signature": "def get_path(module: ModuleType) -> str:"}}
{"prompt": "def luhn_checksum(number: float) -> int:", "metadata": {"task_id": "Software-Development/Faker/4", "ground_truth": "    def digits_of(n: float) -> List[int]:\n        return [int(d) for d in str(n)]\n\n    digits = digits_of(number)\n    odd_digits = digits[-1::-2]\n    even_digits = digits[-2::-2]\n    checksum = 0\n    checksum += sum(odd_digits)\n    for d in even_digits:\n        checksum += sum(digits_of(d * 2))\n    return checksum % 10", "fpath_tuple": ["Software-Development", "Faker", "faker", "utils", "checksums.py"], "context_start_lineno": 3, "line_no": 4, "id": "faker.utils.checksums.luhn_checksum", "target_function_prompt": "def luhn_checksum(number: float) -> int:", "function_signature": "def luhn_checksum(number: float) -> int:"}}
{"prompt": "def add_ordereddicts(*odicts: OrderedDictType) -> OrderedDictType:", "metadata": {"task_id": "Software-Development/Faker/5", "ground_truth": "    items = [odict.items() for odict in odicts]\n    return OrderedDictType(chain(*items))", "fpath_tuple": ["Software-Development", "Faker", "faker", "utils", "datasets.py"], "context_start_lineno": 5, "line_no": 6, "id": "faker.utils.datasets.add_ordereddicts", "target_function_prompt": "def add_ordereddicts(*odicts: OrderedDictType) -> OrderedDictType:", "function_signature": "def add_ordereddicts(*odicts: OrderedDictType) -> OrderedDictType:"}}
{"prompt": "def checksum_identity_card_number(characters: Sequence[Union[str, int]]) -> int:", "metadata": {"task_id": "Software-Development/Faker/6", "ground_truth": "    weights_for_check_digit = [7, 3, 1, 0, 7, 3, 1, 7, 3]\n    integer_characters = [\n        (ord(character) - 55) if isinstance(character, str) else character for character in characters\n    ]\n    check_digit = sum(weight * ch for weight, ch in zip(weights_for_check_digit, integer_characters)) % 10\n    return check_digit", "fpath_tuple": ["Software-Development", "Faker", "faker", "providers", "person", "pl_PL", "__init__.py"], "context_start_lineno": 6, "line_no": 10, "id": "faker.providers.person.pl_PL.checksum_identity_card_number", "target_function_prompt": "def checksum_identity_card_number(characters: Sequence[Union[str, int]]) -> int:", "function_signature": "def checksum_identity_card_number(characters: Sequence[Union[str, int]]) -> int:"}}
{"prompt": "def regon_checksum(digits: List[int]) -> int:", "metadata": {"task_id": "Software-Development/Faker/7", "ground_truth": "    weights_for_check_digit = [8, 9, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 8):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "fpath_tuple": ["Software-Development", "Faker", "faker", "providers", "company", "pl_PL", "__init__.py"], "context_start_lineno": 5, "line_no": 9, "id": "faker.providers.company.pl_PL.regon_checksum", "target_function_prompt": "def regon_checksum(digits: List[int]) -> int:", "function_signature": "def regon_checksum(digits: List[int]) -> int:"}}
{"prompt": "def calculate_checksum(value: str) -> str:", "metadata": {"task_id": "Software-Development/Faker/8", "ground_truth": "    factors = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8][-len(value) :]\n    check_sum = 0\n    for number, factor in zip(value, factors):\n        check_sum += int(number) * factor\n\n    return str((check_sum % 11) % 10)", "fpath_tuple": ["Software-Development", "Faker", "faker", "providers", "company", "ru_RU", "__init__.py"], "context_start_lineno": 5, "line_no": 6, "id": "faker.providers.company.ru_RU.calculate_checksum", "target_function_prompt": "def calculate_checksum(value: str) -> str:", "function_signature": "def calculate_checksum(value: str) -> str:"}}
{"prompt": "def local_regon_checksum(digits: List[int]) -> int:", "metadata": {"task_id": "Software-Development/Faker/9", "ground_truth": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 13):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "fpath_tuple": ["Software-Development", "Faker", "faker", "providers", "company", "pl_PL", "__init__.py"], "context_start_lineno": 23, "line_no": 27, "id": "faker.providers.company.pl_PL.local_regon_checksum", "target_function_prompt": "def local_regon_checksum(digits: List[int]) -> int:", "function_signature": "def local_regon_checksum(digits: List[int]) -> int:"}}
{"prompt": "def company_vat_checksum(digits: List[int]) -> int:", "metadata": {"task_id": "Software-Development/Faker/10", "ground_truth": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    return check_digit", "fpath_tuple": ["Software-Development", "Faker", "faker", "providers", "company", "pl_PL", "__init__.py"], "context_start_lineno": 41, "line_no": 45, "id": "faker.providers.company.pl_PL.company_vat_checksum", "target_function_prompt": "def company_vat_checksum(digits: List[int]) -> int:", "function_signature": "def company_vat_checksum(digits: List[int]) -> int:"}}
{"prompt": "def company_id_checksum(digits: List[int]) -> List[int]:", "metadata": {"task_id": "Software-Development/Faker/11", "ground_truth": "    digits = list(digits)\n    weights = 6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2\n\n    dv = sum(w * d for w, d in zip(weights[1:], digits))\n    dv = (11 - dv) % 11\n    dv = 0 if dv >= 10 else dv\n    digits.append(dv)\n\n    dv2 = sum(w * d for w, d in zip(weights, digits))\n    dv2 = (11 - dv2) % 11\n    dv2 = 0 if dv2 >= 10 else dv2\n    digits.append(dv2)\n\n    return digits[-2:]", "fpath_tuple": ["Software-Development", "Faker", "faker", "providers", "company", "pt_BR", "__init__.py"], "context_start_lineno": 5, "line_no": 6, "id": "faker.providers.company.pt_BR.company_id_checksum", "target_function_prompt": "def company_id_checksum(digits: List[int]) -> List[int]:", "function_signature": "def company_id_checksum(digits: List[int]) -> List[int]:"}}
{"prompt": "    def binary(self, length: int = (1 * 1024 * 1024)) -> bytes:", "metadata": {"task_id": "Software-Development/Faker/12", "ground_truth": "        if self.generator._is_seeded:\n            blob = [self.generator.random.randrange(256) for _ in range(length)]\n            return bytes(blob)\n\n        # Generator is unseeded anyway, just use urandom\n        return os.urandom(length)", "fpath_tuple": ["Software-Development", "Faker", "faker", "providers", "misc", "__init__.py"], "context_start_lineno": 42, "line_no": 51, "id": "faker.providers.misc.Provider.binary", "target_function_prompt": "    def binary(self, length: int = (1 * 1024 * 1024)) -> bytes:", "function_signature": "    def binary(self, length: int = (1 * 1024 * 1024)) -> bytes:"}}
{"prompt": "    def pystr(\n        self,\n        min_chars: Optional[int] = None,\n        max_chars: int = 20,\n        prefix: str = \"\",\n        suffix: str = \"\",\n    ) -> str:", "metadata": {"task_id": "Software-Development/Faker/13", "ground_truth": "        if min_chars is None:\n            chars = \"\".join(self.random_letters(length=max_chars))\n        else:\n            assert max_chars >= min_chars, \"Maximum length must be greater than or equal to minimum length\"\n            chars = \"\".join(\n                self.random_letters(\n                    length=self.generator.random.randint(min_chars, max_chars),\n                ),\n            )\n\n        return prefix + chars + suffix", "fpath_tuple": ["Software-Development", "Faker", "faker", "providers", "python", "__init__.py"], "context_start_lineno": 104, "line_no": 120, "id": "faker.providers.python.Provider.pystr", "target_function_prompt": "    def pystr(\n        self,\n        min_chars: Optional[int] = None,\n        max_chars: int = 20,\n        prefix: str = \"\",\n        suffix: str = \"\",\n    ) -> str:", "function_signature": "    def pystr(\n        self,\n        min_chars: Optional[int] = None,\n        max_chars: int = 20,\n        prefix: str = \"\",\n        suffix: str = \"\",\n    ) -> str:"}}
{"prompt": "    def set_read_only(self, names, msg=\"Attribute is read-only\"):", "metadata": {"task_id": "Software-Development/dash/0", "ground_truth": "        new_read_only = {name: msg for name in names}\n        if getattr(self, \"_read_only\", False):\n            self._read_only.update(new_read_only)\n        else:\n            object.__setattr__(self, \"_read_only\", new_read_only)", "fpath_tuple": ["Software-Development", "dash", "dash", "_utils.py"], "context_start_lineno": 91, "line_no": 98, "id": "dash._utils.AttributeDict.set_read_only", "target_function_prompt": "    def set_read_only(self, names, msg=\"Attribute is read-only\"):", "function_signature": "    def set_read_only(self, names, msg=\"Attribute is read-only\"):"}}
{"prompt": "    def first(self, *names):", "metadata": {"task_id": "Software-Development/dash/1", "ground_truth": "        for name in names:\n            value = self.get(name)\n            if value:\n                return value\n        if not names:\n            return next(iter(self), {})", "fpath_tuple": ["Software-Development", "dash", "dash", "_utils.py"], "context_start_lineno": 124, "line_no": 125, "id": "dash._utils.AttributeDict.first", "target_function_prompt": "    def first(self, *names):", "function_signature": "    def first(self, *names):"}}
{"prompt": "def app_get_asset_url(config, path):", "metadata": {"task_id": "Software-Development/dash/2", "ground_truth": "    if config.assets_external_path:\n        prefix = config.assets_external_path\n    else:\n        prefix = config.requests_pathname_prefix\n    return \"/\".join(\n        [\n            # Only take the first part of the pathname\n            prefix.rstrip(\"/\"),\n            config.assets_url_path.lstrip(\"/\"),\n            path,\n        ]\n    )", "fpath_tuple": ["Software-Development", "dash", "dash", "_get_paths.py"], "context_start_lineno": 10, "line_no": 11, "id": "dash._get_paths.app_get_asset_url", "target_function_prompt": "def app_get_asset_url(config, path):", "function_signature": "def app_get_asset_url(config, path):"}}
{"prompt": "def sort_models(models):", "metadata": {"task_id": "Software-Development/peewee/0", "ground_truth": "    models = set(models)\n    seen = set()\n    ordering = []\n    def dfs(model):\n        if model in models and model not in seen:\n            seen.add(model)\n            for foreign_key, rel_model in model._meta.refs.items():\n                # Do not depth-first search deferred foreign-keys as this can\n                # cause tables to be created in the incorrect order.\n                if not foreign_key.deferred:\n                    dfs(rel_model)\n            if model._meta.depends_on:\n                for dependency in model._meta.depends_on:\n                    dfs(dependency)\n            ordering.append(model)\n\n    names = lambda m: (m._meta.name, m._meta.table_name)\n    for m in sorted(models, key=names):\n        dfs(m)\n    return ordering", "fpath_tuple": ["Software-Development", "peewee", "peewee.py"], "context_start_lineno": 7057, "line_no": 7058, "id": "peewee.sort_models", "target_function_prompt": "def sort_models(models):", "function_signature": "def sort_models(models):"}}
{"prompt": "def grouping_len(grouping):", "metadata": {"task_id": "Software-Development/dash/3", "ground_truth": "    if isinstance(grouping, (tuple, list)):\n        return sum([grouping_len(group_el) for group_el in grouping])\n\n    if isinstance(grouping, dict):\n        return sum([grouping_len(group_el) for group_el in grouping.values()])\n\n    return 1", "fpath_tuple": ["Software-Development", "dash", "dash", "_grouping.py"], "context_start_lineno": 49, "line_no": 58, "id": "dash._grouping.grouping_len", "target_function_prompt": "def grouping_len(grouping):", "function_signature": "def grouping_len(grouping):"}}
{"prompt": "    def get(self, key, default=None):", "metadata": {"task_id": "Software-Development/peewee/1", "ground_truth": "        try:\n            return self[key]\n        except KeyError:\n            return default", "fpath_tuple": ["Software-Development", "peewee", "playhouse", "kv.py"], "context_start_lineno": 149, "line_no": 150, "id": "playhouse.kv.KeyValue.get", "target_function_prompt": "    def get(self, key, default=None):", "function_signature": "    def get(self, key, default=None):"}}
{"prompt": "    def setdefault(self, key, default=None):", "metadata": {"task_id": "Software-Development/peewee/2", "ground_truth": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default", "fpath_tuple": ["Software-Development", "peewee", "playhouse", "kv.py"], "context_start_lineno": 155, "line_no": 156, "id": "playhouse.kv.KeyValue.setdefault", "target_function_prompt": "    def setdefault(self, key, default=None):", "function_signature": "    def setdefault(self, key, default=None):"}}
{"prompt": "def get_public_key_sha256(certificate: Certificate) -> bytes:", "metadata": {"task_id": "System/sslyze/0", "ground_truth": "    pub_bytes = certificate.public_key().public_bytes(encoding=Encoding.DER, format=PublicFormat.SubjectPublicKeyInfo)\n    digest = sha256(pub_bytes).digest()\n    return digest", "fpath_tuple": ["System", "sslyze", "sslyze", "plugins", "certificate_info", "_certificate_utils.py"], "context_start_lineno": 53, "line_no": 54, "id": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "target_function_prompt": "def get_public_key_sha256(certificate: Certificate) -> bytes:", "function_signature": "def get_public_key_sha256(certificate: Certificate) -> bytes:"}}
{"prompt": "def _compare_title(titles: List[str]) -> str:", "metadata": {"task_id": "Software-Development/ydata-profiling/0", "ground_truth": "    if all(titles[0] == title for title in titles[1:]):\n        return titles[0]\n    else:\n        title = \", \".join(titles[:-1])\n        return f\"<em>Comparing</em> {title} <em>and</em> {titles[-1]}\"", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "compare_reports.py"], "context_start_lineno": 111, "line_no": 112, "id": "ydata_profiling.compare_reports._compare_title", "target_function_prompt": "def _compare_title(titles: List[str]) -> str:", "function_signature": "def _compare_title(titles: List[str]) -> str:"}}
{"prompt": "def fmt_bytesize(num: float, suffix: str = \"B\") -> str:", "metadata": {"task_id": "Software-Development/ydata-profiling/1", "ground_truth": "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) < 1024.0:\n            return f\"{num:3.1f} {unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:.1f} Yi{suffix}\"", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "report", "formatters.py"], "context_start_lineno": 60, "line_no": 70, "id": "ydata_profiling.report.formatters.fmt_bytesize", "target_function_prompt": "def fmt_bytesize(num: float, suffix: str = \"B\") -> str:", "function_signature": "def fmt_bytesize(num: float, suffix: str = \"B\") -> str:"}}
{"prompt": "def fmt_percent(value: float, edge_cases: bool = True) -> str:", "metadata": {"task_id": "Software-Development/ydata-profiling/2", "ground_truth": "    if edge_cases and round(value, 3) == 0 and value > 0:\n        return \"< 0.1%\"\n    if edge_cases and round(value, 3) == 1 and value < 1:\n        return \"> 99.9%\"\n\n    return f\"{value*100:2.1f}%\"", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "report", "formatters.py"], "context_start_lineno": 78, "line_no": 88, "id": "ydata_profiling.report.formatters.fmt_percent", "target_function_prompt": "def fmt_percent(value: float, edge_cases: bool = True) -> str:", "function_signature": "def fmt_percent(value: float, edge_cases: bool = True) -> str:"}}
{"prompt": "def fmt_numeric(value: float, precision: int = 10) -> str:", "metadata": {"task_id": "Software-Development/ydata-profiling/3", "ground_truth": "    fmtted = f\"{{:.{precision}g}}\".format(value)\n    for v in [\"e+\", \"e-\"]:\n        if v in fmtted:\n            sign = \"-\" if v in \"e-\" else \"\"\n            fmtted = fmtted.replace(v, \" \u00d7 10<sup>\") + \"</sup>\"\n            fmtted = fmtted.replace(\"<sup>0\", \"<sup>\")\n            fmtted = fmtted.replace(\"<sup>\", f\"<sup>{sign}\")\n\n    return fmtted", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "report", "formatters.py"], "context_start_lineno": 236, "line_no": 246, "id": "ydata_profiling.report.formatters.fmt_numeric", "target_function_prompt": "def fmt_numeric(value: float, precision: int = 10) -> str:", "function_signature": "def fmt_numeric(value: float, precision: int = 10) -> str:"}}
{"prompt": "def fmt_array(value: np.ndarray, threshold: Any = np.nan) -> str:", "metadata": {"task_id": "Software-Development/ydata-profiling/4", "ground_truth": "    with np.printoptions(threshold=3, edgeitems=threshold):\n        return_value = str(value)\n\n    return return_value", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "report", "formatters.py"], "context_start_lineno": 271, "line_no": 281, "id": "ydata_profiling.report.formatters.fmt_array", "target_function_prompt": "def fmt_array(value: np.ndarray, threshold: Any = np.nan) -> str:", "function_signature": "def fmt_array(value: np.ndarray, threshold: Any = np.nan) -> str:"}}
{"prompt": "def fmt_monotonic(value: int) -> str:", "metadata": {"task_id": "Software-Development/ydata-profiling/5", "ground_truth": "    if value == 2:\n        return \"Strictly increasing\"\n    elif value == 1:\n        return \"Increasing\"\n    elif value == 0:\n        return \"Not monotonic\"\n    elif value == -1:\n        return \"Decreasing\"\n    elif value == -2:\n        return \"Strictly decreasing\"\n    else:\n        raise ValueError(\"Value should be integer ranging from -2 to 2.\")", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "report", "formatters.py"], "context_start_lineno": 304, "line_no": 305, "id": "ydata_profiling.report.formatters.fmt_monotonic", "target_function_prompt": "def fmt_monotonic(value: int) -> str:", "function_signature": "def fmt_monotonic(value: int) -> str:"}}
{"prompt": "def _plot_pie_chart(\n    data: pd.Series, colors: List, hide_legend: bool = False\n) -> Tuple[plt.Axes, matplotlib.legend.Legend]:", "metadata": {"task_id": "Software-Development/ydata-profiling/6", "ground_truth": "    def make_autopct(values: pd.Series) -> Callable:\n        def my_autopct(pct: float) -> str:\n            total = np.sum(values)\n            val = int(round(pct * total / 100.0))\n            return f\"{pct:.1f}%  ({val:d})\"\n\n        return my_autopct\n\n    _, ax = plt.subplots(figsize=(4, 4))\n    wedges, _, _ = plt.pie(\n        data,\n        autopct=make_autopct(data),\n        textprops={\"color\": \"w\"},\n        colors=colors,\n    )\n\n    legend = None\n    if not hide_legend:\n        legend = plt.legend(\n            wedges,\n            data.index.values,\n            fontsize=\"large\",\n            bbox_to_anchor=(0, 0),\n            loc=\"upper left\",\n        )\n\n    return ax, legend", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "visualisation", "plot.py"], "context_start_lineno": 428, "line_no": 444, "id": "ydata_profiling.visualisation.plot._plot_pie_chart", "target_function_prompt": "def _plot_pie_chart(\n    data: pd.Series, colors: List, hide_legend: bool = False\n) -> Tuple[plt.Axes, matplotlib.legend.Legend]:", "function_signature": "def _plot_pie_chart(\n    data: pd.Series, colors: List, hide_legend: bool = False\n) -> Tuple[plt.Axes, matplotlib.legend.Legend]:"}}
{"prompt": "def _prepare_heatmap_data(\n    dataframe: pd.DataFrame,\n    entity_column: str,\n    sortby: Optional[Union[str, list]] = None,\n    max_entities: int = 5,\n    selected_entities: Optional[List[str]] = None,\n) -> pd.DataFrame:", "metadata": {"task_id": "Software-Development/ydata-profiling/7", "ground_truth": "    if sortby is None:\n        sortbykey = \"_index\"\n        df = dataframe[entity_column].copy().reset_index()\n        df.columns = [sortbykey, entity_column]\n\n    else:\n        if isinstance(sortby, str):\n            sortby = [sortby]\n        cols = [entity_column, *sortby]\n        df = dataframe[cols].copy()\n        sortbykey = sortby[0]\n\n    if df[sortbykey].dtype == \"O\":\n        try:\n            df[sortbykey] = pd.to_datetime(df[sortbykey])\n        except Exception as ex:\n            raise ValueError(\n                f\"column {sortbykey} dtype {df[sortbykey].dtype} is not supported.\"\n            ) from ex\n    nbins = np.min([50, df[sortbykey].nunique()])\n\n    df[\"__bins\"] = pd.cut(\n        df[sortbykey], bins=nbins, include_lowest=True, labels=range(nbins)\n    )\n\n    df = df.groupby([entity_column, \"__bins\"])[sortbykey].count()\n    df = df.reset_index().pivot_table(entity_column, \"__bins\", sortbykey).T\n\n    if selected_entities:\n        df = df[selected_entities].T\n    else:\n        df = df.T[:max_entities]\n\n    return df", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "visualisation", "plot.py"], "context_start_lineno": 801, "line_no": 808, "id": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "target_function_prompt": "def _prepare_heatmap_data(\n    dataframe: pd.DataFrame,\n    entity_column: str,\n    sortby: Optional[Union[str, list]] = None,\n    max_entities: int = 5,\n    selected_entities: Optional[List[str]] = None,\n) -> pd.DataFrame:", "function_signature": "def _prepare_heatmap_data(\n    dataframe: pd.DataFrame,\n    entity_column: str,\n    sortby: Optional[Union[str, list]] = None,\n    max_entities: int = 5,\n    selected_entities: Optional[List[str]] = None,\n) -> pd.DataFrame:"}}
{"prompt": "def _create_timeseries_heatmap(\n    df: pd.DataFrame,\n    figsize: Tuple[int, int] = (12, 5),\n    color: str = \"#337ab7\",\n) -> plt.Axes:", "metadata": {"task_id": "Software-Development/ydata-profiling/8", "ground_truth": "    _, ax = plt.subplots(figsize=figsize)\n    cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\n        \"report\", [\"white\", color], N=64\n    )\n    pc = ax.pcolormesh(df, edgecolors=ax.get_facecolor(), linewidth=0.25, cmap=cmap)\n    pc.set_clim(0, np.nanmax(df))\n    ax.set_yticks([x + 0.5 for x in range(len(df))])\n    ax.set_yticklabels(df.index)\n    ax.set_xticks([])\n    ax.set_xlabel(\"Time\")\n    ax.invert_yaxis()\n    return ax", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "visualisation", "plot.py"], "context_start_lineno": 844, "line_no": 849, "id": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "target_function_prompt": "def _create_timeseries_heatmap(\n    df: pd.DataFrame,\n    figsize: Tuple[int, int] = (12, 5),\n    color: str = \"#337ab7\",\n) -> plt.Axes:", "function_signature": "def _create_timeseries_heatmap(\n    df: pd.DataFrame,\n    figsize: Tuple[int, int] = (12, 5),\n    color: str = \"#337ab7\",\n) -> plt.Axes:"}}
{"prompt": "def generic_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:", "metadata": {"task_id": "Software-Development/ydata-profiling/9", "ground_truth": "    batch.expect_column_to_exist(name)\n\n    if summary[\"n_missing\"] == 0:\n        batch.expect_column_values_to_not_be_null(name)\n\n    if summary[\"p_unique\"] == 1.0:\n        batch.expect_column_values_to_be_unique(name)\n\n    return name, summary, batch", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "model", "expectation_algorithms.py"], "context_start_lineno": 3, "line_no": 6, "id": "ydata_profiling.model.expectation_algorithms.generic_expectations", "target_function_prompt": "def generic_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:", "function_signature": "def generic_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:"}}
{"prompt": "def numeric_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:", "metadata": {"task_id": "Software-Development/ydata-profiling/10", "ground_truth": "    from great_expectations.profile.base import ProfilerTypeMapping\n\n    numeric_type_names = (\n        ProfilerTypeMapping.INT_TYPE_NAMES + ProfilerTypeMapping.FLOAT_TYPE_NAMES\n    )\n\n    batch.expect_column_values_to_be_in_type_list(\n        name,\n        numeric_type_names,\n        meta={\n            \"notes\": {\n                \"format\": \"markdown\",\n                \"content\": [\n                    \"The column values should be stored in one of these types.\"\n                ],\n            }\n        },\n    )\n\n    if summary[\"monotonic_increase\"]:\n        batch.expect_column_values_to_be_increasing(\n            name, strictly=summary[\"monotonic_increase_strict\"]\n        )\n\n    if summary[\"monotonic_decrease\"]:\n        batch.expect_column_values_to_be_decreasing(\n            name, strictly=summary[\"monotonic_decrease_strict\"]\n        )\n\n    if any(k in summary for k in [\"min\", \"max\"]):\n        batch.expect_column_values_to_be_between(\n            name, min_value=summary.get(\"min\"), max_value=summary.get(\"max\")\n        )\n\n    return name, summary, batch", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "model", "expectation_algorithms.py"], "context_start_lineno": 17, "line_no": 20, "id": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "target_function_prompt": "def numeric_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:", "function_signature": "def numeric_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:"}}
{"prompt": "def categorical_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:\n    # Use for both categorical and special case (boolean)", "metadata": {"task_id": "Software-Development/ydata-profiling/11", "ground_truth": "    absolute_threshold = 10\n    relative_threshold = 0.2\n    if (\n        summary[\"n_distinct\"] < absolute_threshold\n        or summary[\"p_distinct\"] < relative_threshold\n    ):\n        batch.expect_column_values_to_be_in_set(\n            name, set(summary[\"value_counts_without_nan\"].keys())\n        )\n    return name, summary, batch", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "model", "expectation_algorithms.py"], "context_start_lineno": 57, "line_no": 61, "id": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "target_function_prompt": "def categorical_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:\n    # Use for both categorical and special case (boolean)", "function_signature": "def categorical_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:\n    # Use for both categorical and special case (boolean)"}}
{"prompt": "def datetime_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:", "metadata": {"task_id": "Software-Development/ydata-profiling/12", "ground_truth": "    if any(k in summary for k in [\"min\", \"max\"]):\n        batch.expect_column_values_to_be_between(\n            name,\n            min_value=summary.get(\"min\"),\n            max_value=summary.get(\"max\"),\n            parse_strings_as_datetimes=True,\n        )\n\n    return name, summary, batch", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "model", "expectation_algorithms.py"], "context_start_lineno": 79, "line_no": 82, "id": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "target_function_prompt": "def datetime_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:", "function_signature": "def datetime_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:"}}
{"prompt": "def file_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:\n    # By definition within our type logic, a file exists (as it's a path that also exists)", "metadata": {"task_id": "Software-Development/ydata-profiling/13", "ground_truth": "    batch.expect_file_to_exist(name)\n\n    return name, summary, batch", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "model", "expectation_algorithms.py"], "context_start_lineno": 105, "line_no": 109, "id": "ydata_profiling.model.expectation_algorithms.file_expectations", "target_function_prompt": "def file_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:\n    # By definition within our type logic, a file exists (as it's a path that also exists)", "function_signature": "def file_expectations(\n    name: str, summary: dict, batch: Any, *args\n) -> Tuple[str, dict, Any]:\n    # By definition within our type logic, a file exists (as it's a path that also exists)"}}
{"prompt": "def word_summary_vc(vc: pd.Series, stop_words: List[str] = []) -> dict:", "metadata": {"task_id": "Software-Development/ydata-profiling/14", "ground_truth": "    series = pd.Series(vc.index, index=vc)\n    word_lists = series.str.lower().str.split()\n    words = word_lists.explode().str.strip(string.punctuation + string.whitespace)\n    word_counts = pd.Series(words.index, index=words)\n    # fix for pandas 1.0.5\n    word_counts = word_counts[word_counts.index.notnull()]\n    word_counts = word_counts.groupby(level=0, sort=False).sum()\n    word_counts = word_counts.sort_values(ascending=False)\n\n    # Remove stop words\n    if len(stop_words) > 0:\n        stop_words = [x.lower() for x in stop_words]\n        word_counts = word_counts.loc[~word_counts.index.isin(stop_words)]\n\n    return {\"word_counts\": word_counts} if not word_counts.empty else {}", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "model", "pandas", "describe_categorical_pandas.py"], "context_start_lineno": 153, "line_no": 171, "id": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "target_function_prompt": "def word_summary_vc(vc: pd.Series, stop_words: List[str] = []) -> dict:", "function_signature": "def word_summary_vc(vc: pd.Series, stop_words: List[str] = []) -> dict:"}}
{"prompt": "def column_imbalance_score(\n    value_counts: pd.Series, n_classes: int\n) -> Union[float, int]:", "metadata": {"task_id": "Software-Development/ydata-profiling/15", "ground_truth": "    if n_classes > 1:\n        # casting to numpy array to ensure correct dtype when a categorical integer\n        # variable is evaluated\n        value_counts = value_counts.to_numpy(dtype=float)\n        return 1 - (entropy(value_counts, base=2) / log2(n_classes))\n    return 0", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "model", "pandas", "imbalance_pandas.py"], "context_start_lineno": 7, "line_no": 29, "id": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "target_function_prompt": "def column_imbalance_score(\n    value_counts: pd.Series, n_classes: int\n) -> Union[float, int]:", "function_signature": "def column_imbalance_score(\n    value_counts: pd.Series, n_classes: int\n) -> Union[float, int]:"}}
{"prompt": "    def messages(self):", "metadata": {"task_id": "Software-Development/Django/0", "ground_truth": "        if hasattr(self, \"error_dict\"):\n            return sum(dict(self).values(), [])\n        return list(self)", "fpath_tuple": ["Software-Development", "Django", "django", "core", "exceptions.py"], "context_start_lineno": 187, "line_no": 188, "id": "django.core.exceptions.ValidationError.messages", "target_function_prompt": "    def messages(self):", "function_signature": "    def messages(self):"}}
{"prompt": "def module_has_submodule(package, module_name):", "metadata": {"task_id": "Software-Development/Django/1", "ground_truth": "    try:\n        package_name = package.__name__\n        package_path = package.__path__\n    except AttributeError:\n        # package isn't a package.\n        return False\n\n    full_module_name = package_name + \".\" + module_name\n    try:\n        return importlib_find(full_module_name, package_path) is not None\n    except ModuleNotFoundError:\n        # When module_name is an invalid dotted path, Python raises\n        # ModuleNotFoundError.\n        return False", "fpath_tuple": ["Software-Development", "Django", "django", "utils", "module_loading.py"], "context_start_lineno": 73, "line_no": 75, "id": "django.utils.module_loading.module_has_submodule", "target_function_prompt": "def module_has_submodule(package, module_name):", "function_signature": "def module_has_submodule(package, module_name):"}}
{"prompt": "def get_fixed_timezone(offset):", "metadata": {"task_id": "Software-Development/Django/2", "ground_truth": "    if isinstance(offset, timedelta):\n        offset = offset.total_seconds() // 60\n    sign = \"-\" if offset < 0 else \"+\"\n    hhmm = \"%02d%02d\" % divmod(abs(offset), 60)\n    name = sign + hhmm\n    return timezone(timedelta(minutes=offset), name)", "fpath_tuple": ["Software-Development", "Django", "django", "utils", "timezone.py"], "context_start_lineno": 58, "line_no": 60, "id": "django.utils.timezone.get_fixed_timezone", "target_function_prompt": "def get_fixed_timezone(offset):", "function_signature": "def get_fixed_timezone(offset):"}}
{"prompt": "def filepath_to_uri(path):", "metadata": {"task_id": "Software-Development/Django/3", "ground_truth": "    if path is None:\n        return path\n    # I know about `os.sep` and `os.altsep` but I want to leave\n    # some flexibility for hardcoding separators.\n    return quote(str(path).replace(\"\\\\\", \"/\"), safe=\"/~!*()'\")", "fpath_tuple": ["Software-Development", "Django", "django", "utils", "encoding.py"], "context_start_lineno": 235, "line_no": 243, "id": "django.utils.encoding.filepath_to_uri", "target_function_prompt": "def filepath_to_uri(path):", "function_signature": "def filepath_to_uri(path):"}}
{"prompt": "def to_path(value):", "metadata": {"task_id": "Software-Development/Django/4", "ground_truth": "    if isinstance(value, Path):\n        return value\n    elif not isinstance(value, str):\n        raise TypeError(\"Invalid path type: %s\" % type(value).__name__)\n    return Path(value)", "fpath_tuple": ["Software-Development", "Django", "django", "utils", "_os.py"], "context_start_lineno": 55, "line_no": 57, "id": "django.utils._os.to_path", "target_function_prompt": "def to_path(value):", "function_signature": "def to_path(value):"}}
{"prompt": "def sentence():", "metadata": {"task_id": "Software-Development/Django/5", "ground_truth": "    sections = [\n        \" \".join(random.sample(WORDS, random.randint(3, 12)))\n        for i in range(random.randint(1, 5))\n    ]\n    s = \", \".join(sections)\n    # Convert to sentence case and add end punctuation.\n    return \"%s%s%s\" % (s[0].upper(), s[1:], random.choice(\"?.\"))", "fpath_tuple": ["Software-Development", "Django", "django", "utils", "lorem_ipsum.py"], "context_start_lineno": 224, "line_no": 233, "id": "django.utils.lorem_ipsum.sentence", "target_function_prompt": "def sentence():", "function_signature": "def sentence():"}}
{"prompt": "def sort_column_names(dct: dict, sort: Optional[str]) -> dict:", "metadata": {"task_id": "Software-Development/ydata-profiling/16", "ground_truth": "    if sort is None:\n        return dct\n\n    sort = sort.lower()\n    if sort.startswith(\"asc\"):\n        dct = dict(sorted(dct.items(), key=lambda x: x[0].casefold()))\n    elif sort.startswith(\"desc\"):\n        dct = dict(sorted(dct.items(), key=lambda x: x[0].casefold(), reverse=True))\n    else:\n        raise ValueError('\"sort\" should be \"ascending\", \"descending\" or None.')\n    return dct", "fpath_tuple": ["Software-Development", "ydata-profiling", "src", "ydata_profiling", "utils", "dataframe.py"], "context_start_lineno": 226, "line_no": 227, "id": "ydata_profiling.utils.dataframe.sort_column_names", "target_function_prompt": "def sort_column_names(dct: dict, sort: Optional[str]) -> dict:", "function_signature": "def sort_column_names(dct: dict, sort: Optional[str]) -> dict:"}}
{"prompt": "def is_valid_ipv6_address(ip_str):", "metadata": {"task_id": "Software-Development/Django/6", "ground_truth": "    try:\n        ipaddress.IPv6Address(ip_str)\n    except ValueError:\n        return False\n    return True", "fpath_tuple": ["Software-Development", "Django", "django", "utils", "ipv6.py"], "context_start_lineno": 38, "line_no": 42, "id": "django.utils.ipv6.is_valid_ipv6_address", "target_function_prompt": "def is_valid_ipv6_address(ip_str):", "function_signature": "def is_valid_ipv6_address(ip_str):"}}
{"prompt": "def urlsafe_base64_decode(s):", "metadata": {"task_id": "Software-Development/Django/7", "ground_truth": "    s = s.encode()\n    try:\n        return base64.urlsafe_b64decode(s.ljust(len(s) + len(s) % 4, b\"=\"))\n    except (LookupError, BinasciiError) as e:\n        raise ValueError(e)", "fpath_tuple": ["Software-Development", "Django", "django", "utils", "http.py"], "context_start_lineno": 198, "line_no": 203, "id": "django.utils.http.urlsafe_base64_decode", "target_function_prompt": "def urlsafe_base64_decode(s):", "function_signature": "def urlsafe_base64_decode(s):"}}
{"prompt": "def parse_etags(etag_str):", "metadata": {"task_id": "Software-Development/Django/8", "ground_truth": "    if etag_str.strip() == \"*\":\n        return [\"*\"]\n    else:\n        # Parse each ETag individually, and return any that are valid.\n        etag_matches = (ETAG_MATCH.match(etag.strip()) for etag in etag_str.split(\",\"))\n        return [match[1] for match in etag_matches if match]", "fpath_tuple": ["Software-Development", "Django", "django", "utils", "http.py"], "context_start_lineno": 210, "line_no": 216, "id": "django.utils.http.parse_etags", "target_function_prompt": "def parse_etags(etag_str):", "function_signature": "def parse_etags(etag_str):"}}
{"prompt": "def is_same_domain(host, pattern):", "metadata": {"task_id": "Software-Development/Django/9", "ground_truth": "    if not pattern:\n        return False\n\n    pattern = pattern.lower()\n    return (\n        pattern[0] == \".\"\n        and (host.endswith(pattern) or host == pattern[1:])\n        or pattern == host\n    )", "fpath_tuple": ["Software-Development", "Django", "django", "utils", "http.py"], "context_start_lineno": 235, "line_no": 244, "id": "django.utils.http.is_same_domain", "target_function_prompt": "def is_same_domain(host, pattern):", "function_signature": "def is_same_domain(host, pattern):"}}
{"prompt": "def content_disposition_header(as_attachment, filename):", "metadata": {"task_id": "Software-Development/Django/10", "ground_truth": "    if filename:\n        disposition = \"attachment\" if as_attachment else \"inline\"\n        try:\n            filename.encode(\"ascii\")\n            file_expr = 'filename=\"{}\"'.format(\n                filename.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', r\"\\\"\")\n            )\n        except UnicodeEncodeError:\n            file_expr = \"filename*=utf-8''{}\".format(quote(filename))\n        return f\"{disposition}; {file_expr}\"\n    elif as_attachment:\n        return \"attachment\"\n    else:\n        return None", "fpath_tuple": ["Software-Development", "Django", "django", "utils", "http.py"], "context_start_lineno": 430, "line_no": 435, "id": "django.utils.http.content_disposition_header", "target_function_prompt": "def content_disposition_header(as_attachment, filename):", "function_signature": "def content_disposition_header(as_attachment, filename):"}}
{"prompt": "def truncate(string, max_length):", "metadata": {"task_id": "Software-Development/PySnooper/0", "ground_truth": "    if (max_length is None) or (len(string) <= max_length):\n        return string\n    else:\n        left = (max_length - 3) // 2\n        right = max_length - 3 - left\n        return u'{}...{}'.format(string[:left], string[-right:])", "fpath_tuple": ["Software-Development", "PySnooper", "pysnooper", "utils.py"], "context_start_lineno": 80, "line_no": 81, "id": "pysnooper.utils.truncate", "target_function_prompt": "def truncate(string, max_length):", "function_signature": "def truncate(string, max_length):"}}
{"prompt": "def needs_parentheses(source):", "metadata": {"task_id": "Software-Development/PySnooper/1", "ground_truth": "    def code(s):\n        return compile(s, '<variable>', 'eval').co_code\n\n    return code('{}.x'.format(source)) != code('({}).x'.format(source))", "fpath_tuple": ["Software-Development", "PySnooper", "pysnooper", "variables.py"], "context_start_lineno": 12, "line_no": 13, "id": "pysnooper.variables.needs_parentheses", "target_function_prompt": "def needs_parentheses(source):", "function_signature": "def needs_parentheses(source):"}}
{"prompt": "def extend_sys_path(*paths):", "metadata": {"task_id": "Software-Development/Django/11", "ground_truth": "    _orig_sys_path = sys.path[:]\n    sys.path.extend(paths)\n    try:\n        yield\n    finally:\n        sys.path = _orig_sys_path", "fpath_tuple": ["Software-Development", "Django", "django", "test", "utils.py"], "context_start_lineno": 778, "line_no": 780, "id": "django.test.utils.extend_sys_path", "target_function_prompt": "def extend_sys_path(*paths):", "function_signature": "def extend_sys_path(*paths):"}}
{"prompt": "def normalize_cv2(img, mean, denominator):", "metadata": {"task_id": "Software-Development/albumentations/0", "ground_truth": "    if mean.shape and len(mean) != 4 and mean.shape != img.shape:\n        mean = np.array(mean.tolist() + [0] * (4 - len(mean)), dtype=np.float64)\n    if not denominator.shape:\n        denominator = np.array([denominator.tolist()] * 4, dtype=np.float64)\n    elif len(denominator) != 4 and denominator.shape != img.shape:\n        denominator = np.array(denominator.tolist() + [1] * (4 - len(denominator)), dtype=np.float64)\n\n    img = np.ascontiguousarray(img.astype(\"float32\"))\n    cv2.subtract(img, mean.astype(np.float64), img)\n    cv2.multiply(img, denominator.astype(np.float64), img)\n    return img", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "augmentations", "functional.py"], "context_start_lineno": 66, "line_no": 67, "id": "albumentations.augmentations.functional.normalize_cv2", "target_function_prompt": "def normalize_cv2(img, mean, denominator):", "function_signature": "def normalize_cv2(img, mean, denominator):"}}
{"prompt": "def normalize_numpy(img, mean, denominator):", "metadata": {"task_id": "Software-Development/albumentations/1", "ground_truth": "    img = img.astype(np.float32)\n    img -= mean\n    img *= denominator\n    return img", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "augmentations", "functional.py"], "context_start_lineno": 80, "line_no": 81, "id": "albumentations.augmentations.functional.normalize_numpy", "target_function_prompt": "def normalize_numpy(img, mean, denominator):", "function_signature": "def normalize_numpy(img, mean, denominator):"}}
{"prompt": "def gamma_transform(img, gamma):", "metadata": {"task_id": "Software-Development/albumentations/2", "ground_truth": "    if img.dtype == np.uint8:\n        table = (np.arange(0, 256.0 / 255, 1.0 / 255) ** gamma) * 255\n        img = cv2.LUT(img, table.astype(np.uint8))\n    else:\n        img = np.power(img, gamma)\n\n    return img", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "augmentations", "functional.py"], "context_start_lineno": 819, "line_no": 820, "id": "albumentations.augmentations.functional.gamma_transform", "target_function_prompt": "def gamma_transform(img, gamma):", "function_signature": "def gamma_transform(img, gamma):"}}
{"prompt": "def swap_tiles_on_image(image, tiles):", "metadata": {"task_id": "Software-Development/albumentations/3", "ground_truth": "    new_image = image.copy()\n\n    for tile in tiles:\n        new_image[tile[0] : tile[0] + tile[4], tile[1] : tile[1] + tile[5]] = image[\n            tile[2] : tile[2] + tile[4], tile[3] : tile[3] + tile[5]\n        ]\n\n    return new_image", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "augmentations", "functional.py"], "context_start_lineno": 974, "line_no": 989, "id": "albumentations.augmentations.functional.swap_tiles_on_image", "target_function_prompt": "def swap_tiles_on_image(image, tiles):", "function_signature": "def swap_tiles_on_image(image, tiles):"}}
{"prompt": "def keypoint_rotate(keypoint, angle, rows, cols, **params):", "metadata": {"task_id": "Software-Development/albumentations/4", "ground_truth": "    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    x, y, a, s = keypoint[:4]\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    return x, y, a + math.radians(angle), s", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "augmentations", "geometric", "functional.py"], "context_start_lineno": 196, "line_no": 209, "id": "albumentations.augmentations.geometric.functional.keypoint_rotate", "target_function_prompt": "def keypoint_rotate(keypoint, angle, rows, cols, **params):", "function_signature": "def keypoint_rotate(keypoint, angle, rows, cols, **params):"}}
{"prompt": "def keypoint_shift_scale_rotate(keypoint, angle, scale, dx, dy, rows, cols, **params):", "metadata": {"task_id": "Software-Development/albumentations/5", "ground_truth": "    (\n        x,\n        y,\n        a,\n        s,\n    ) = keypoint[:4]\n    height, width = rows, cols\n    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    matrix[0, 2] += dx * width\n    matrix[1, 2] += dy * height\n\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    angle = a + math.radians(angle)\n    scale = s * scale\n\n    return x, y, angle, scale", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "augmentations", "geometric", "functional.py"], "context_start_lineno": 235, "line_no": 236, "id": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "target_function_prompt": "def keypoint_shift_scale_rotate(keypoint, angle, scale, dx, dy, rows, cols, **params):", "function_signature": "def keypoint_shift_scale_rotate(keypoint, angle, scale, dx, dy, rows, cols, **params):"}}
{"prompt": "def angle_to_2pi_range(angle: float) -> float:", "metadata": {"task_id": "Software-Development/albumentations/6", "ground_truth": "    two_pi = 2 * math.pi\n    return angle % two_pi", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "core", "keypoints_utils.py"], "context_start_lineno": 22, "line_no": 23, "id": "albumentations.core.keypoints_utils.angle_to_2pi_range", "target_function_prompt": "def angle_to_2pi_range(angle: float) -> float:", "function_signature": "def angle_to_2pi_range(angle: float) -> float:"}}
{"prompt": "def rot90(img: np.ndarray, factor: int) -> np.ndarray:", "metadata": {"task_id": "Software-Development/albumentations/7", "ground_truth": "    img = np.rot90(img, factor)\n    return np.ascontiguousarray(img)", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "augmentations", "geometric", "functional.py"], "context_start_lineno": 884, "line_no": 885, "id": "albumentations.augmentations.geometric.functional.rot90", "target_function_prompt": "def rot90(img: np.ndarray, factor: int) -> np.ndarray:", "function_signature": "def rot90(img: np.ndarray, factor: int) -> np.ndarray:"}}
{"prompt": "def convert_keypoints_to_albumentations(\n    keypoints: Sequence[Sequence],\n    source_format: str,\n    rows: int,\n    cols: int,\n    check_validity: bool = False,\n    angle_in_degrees: bool = True,\n) -> List[Tuple]:", "metadata": {"task_id": "Software-Development/albumentations/8", "ground_truth": "    return [\n        convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "core", "keypoints_utils.py"], "context_start_lineno": 260, "line_no": 268, "id": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "target_function_prompt": "def convert_keypoints_to_albumentations(\n    keypoints: Sequence[Sequence],\n    source_format: str,\n    rows: int,\n    cols: int,\n    check_validity: bool = False,\n    angle_in_degrees: bool = True,\n) -> List[Tuple]:", "function_signature": "def convert_keypoints_to_albumentations(\n    keypoints: Sequence[Sequence],\n    source_format: str,\n    rows: int,\n    cols: int,\n    check_validity: bool = False,\n    angle_in_degrees: bool = True,\n) -> List[Tuple]:"}}
{"prompt": "def convert_keypoints_from_albumentations(\n    keypoints: Sequence[Sequence],\n    target_format: str,\n    rows: int,\n    cols: int,\n    check_validity: bool = False,\n    angle_in_degrees: bool = True,\n) -> List[Tuple]:", "metadata": {"task_id": "Software-Development/albumentations/9", "ground_truth": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "core", "keypoints_utils.py"], "context_start_lineno": 274, "line_no": 282, "id": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "target_function_prompt": "def convert_keypoints_from_albumentations(\n    keypoints: Sequence[Sequence],\n    target_format: str,\n    rows: int,\n    cols: int,\n    check_validity: bool = False,\n    angle_in_degrees: bool = True,\n) -> List[Tuple]:", "function_signature": "def convert_keypoints_from_albumentations(\n    keypoints: Sequence[Sequence],\n    target_format: str,\n    rows: int,\n    cols: int,\n    check_validity: bool = False,\n    angle_in_degrees: bool = True,\n) -> List[Tuple]:"}}
{"prompt": "def to_tuple(param, low=None, bias=None):", "metadata": {"task_id": "Software-Development/albumentations/10", "ground_truth": "    if low is not None and bias is not None:\n        raise ValueError(\"Arguments low and bias are mutually exclusive\")\n\n    if param is None:\n        return param\n\n    if isinstance(param, (int, float)):\n        if low is None:\n            param = -param, +param\n        else:\n            param = (low, param) if low < param else (param, low)\n    elif isinstance(param, Sequence):\n        if len(param) != 2:\n            raise ValueError(\"to_tuple expects 1 or 2 values\")\n        param = tuple(param)\n    else:\n        raise ValueError(\"Argument param must be either scalar (int, float) or tuple\")\n\n    if bias is not None:\n        return tuple(bias + x for x in param)\n\n    return tuple(param)", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "core", "transforms_interface.py"], "context_start_lineno": 40, "line_no": 49, "id": "albumentations.core.transforms_interface.to_tuple", "target_function_prompt": "def to_tuple(param, low=None, bias=None):", "function_signature": "def to_tuple(param, low=None, bias=None):"}}
{"prompt": "    def replay(saved_augmentations: typing.Dict[str, typing.Any], **kwargs) -> typing.Dict[str, typing.Any]:", "metadata": {"task_id": "Software-Development/albumentations/11", "ground_truth": "        augs = ReplayCompose._restore_for_replay(saved_augmentations)\n        return augs(force_apply=True, **kwargs)", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "core", "composition.py"], "context_start_lineno": 461, "line_no": 462, "id": "albumentations.core.composition.ReplayCompose.replay", "target_function_prompt": "    def replay(saved_augmentations: typing.Dict[str, typing.Any], **kwargs) -> typing.Dict[str, typing.Any]:", "function_signature": "    def replay(saved_augmentations: typing.Dict[str, typing.Any], **kwargs) -> typing.Dict[str, typing.Any]:"}}
{"prompt": "def shorten_class_name(class_fullname: str) -> str:", "metadata": {"task_id": "Software-Development/albumentations/12", "ground_truth": "    splitted = class_fullname.split(\".\")\n    if len(splitted) == 1:\n        return class_fullname\n    top_module, *_, class_name = splitted\n    if top_module == \"albumentations\":\n        return class_name\n    return class_fullname", "fpath_tuple": ["Software-Development", "albumentations", "albumentations", "core", "serialization.py"], "context_start_lineno": 25, "line_no": 26, "id": "albumentations.core.serialization.shorten_class_name", "target_function_prompt": "def shorten_class_name(class_fullname: str) -> str:", "function_signature": "def shorten_class_name(class_fullname: str) -> str:"}}
{"prompt": "def to_forward_slash_path(path: str) -> str:", "metadata": {"task_id": "Scientific-Engineering/wandb/0", "ground_truth": "    if platform.system() == \"Windows\":\n        path = path.replace(\"\\\\\", \"/\")\n    return path", "fpath_tuple": ["Scientific-Engineering", "wandb", "wandb", "util.py"], "context_start_lineno": 1398, "line_no": 1399, "id": "wandb.util.to_forward_slash_path", "target_function_prompt": "def to_forward_slash_path(path: str) -> str:", "function_signature": "def to_forward_slash_path(path: str) -> str:"}}
{"prompt": "def make_artifact_name_safe(name: str) -> str:", "metadata": {"task_id": "Scientific-Engineering/wandb/1", "ground_truth": "    cleaned = re.sub(r\"[^a-zA-Z0-9_\\-.]\", \"_\", name)\n    if len(cleaned) <= 128:\n        return cleaned\n    # truncate with dots in the middle using regex\n    return re.sub(r\"(^.{63}).*(.{63}$)\", r\"\\g<1>..\\g<2>\", cleaned)", "fpath_tuple": ["Scientific-Engineering", "wandb", "wandb", "util.py"], "context_start_lineno": 1727, "line_no": 1730, "id": "wandb.util.make_artifact_name_safe", "target_function_prompt": "def make_artifact_name_safe(name: str) -> str:", "function_signature": "def make_artifact_name_safe(name: str) -> str:"}}
{"prompt": "def _redact_dict(\n    d: Dict[str, Any],\n    unsafe_keys: Union[Set[str], FrozenSet[str]] = frozenset({\"api_key\"}),\n    redact_str: str = \"***REDACTED***\",\n) -> Dict[str, Any]:", "metadata": {"task_id": "Scientific-Engineering/wandb/2", "ground_truth": "    if not d or unsafe_keys.isdisjoint(d):\n        return d\n    safe_dict = d.copy()\n    safe_dict.update({k: redact_str for k in unsafe_keys.intersection(d)})\n    return safe_dict", "fpath_tuple": ["Scientific-Engineering", "wandb", "wandb", "sdk", "wandb_settings.py"], "context_start_lineno": 144, "line_no": 150, "id": "wandb.sdk.wandb_settings._redact_dict", "target_function_prompt": "def _redact_dict(\n    d: Dict[str, Any],\n    unsafe_keys: Union[Set[str], FrozenSet[str]] = frozenset({\"api_key\"}),\n    redact_str: str = \"***REDACTED***\",\n) -> Dict[str, Any]:", "function_signature": "def _redact_dict(\n    d: Dict[str, Any],\n    unsafe_keys: Union[Set[str], FrozenSet[str]] = frozenset({\"api_key\"}),\n    redact_str: str = \"***REDACTED***\",\n) -> Dict[str, Any]:"}}
{"prompt": "def get_current_python_version() -> Tuple[str, str]:", "metadata": {"task_id": "Scientific-Engineering/wandb/3", "ground_truth": "    full_version = sys.version.split()[0].split(\".\")\n    major = full_version[0]\n    version = \".\".join(full_version[:2]) if len(full_version) >= 2 else major + \".0\"\n    return version, major", "fpath_tuple": ["Scientific-Engineering", "wandb", "wandb", "sdk", "launch", "builder", "build.py"], "context_start_lineno": 163, "line_no": 164, "id": "wandb.sdk.launch.builder.build.get_current_python_version", "target_function_prompt": "def get_current_python_version() -> Tuple[str, str]:", "function_signature": "def get_current_python_version() -> Tuple[str, str]:"}}
{"prompt": "    def lookup_by_name(cls, name: str) -> Type[\"StoragePolicy\"]:", "metadata": {"task_id": "Scientific-Engineering/wandb/4", "ground_truth": "        import wandb.sdk.artifacts.storage_policies  # noqa: F401\n\n        for sub in cls.__subclasses__():\n            if sub.name() == name:\n                return sub\n        raise NotImplementedError(f\"Failed to find storage policy '{name}'\")", "fpath_tuple": ["Scientific-Engineering", "wandb", "wandb", "sdk", "artifacts", "storage_policy.py"], "context_start_lineno": 14, "line_no": 15, "id": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "target_function_prompt": "    def lookup_by_name(cls, name: str) -> Type[\"StoragePolicy\"]:", "function_signature": "    def lookup_by_name(cls, name: str) -> Type[\"StoragePolicy\"]:"}}
{"prompt": "def generate_id(length: int = 8) -> str:", "metadata": {"task_id": "Scientific-Engineering/wandb/5", "ground_truth": "    alphabet = string.ascii_lowercase + string.digits\n    return \"\".join(secrets.choice(alphabet) for _ in range(length))", "fpath_tuple": ["Scientific-Engineering", "wandb", "wandb", "sdk", "lib", "runid.py"], "context_start_lineno": 6, "line_no": 10, "id": "wandb.sdk.lib.runid.generate_id", "target_function_prompt": "def generate_id(length: int = 8) -> str:", "function_signature": "def generate_id(length: int = 8) -> str:"}}
{"prompt": "    def get_consecutive_offsets(console: Dict[int, str]) -> List[List[int]]:", "metadata": {"task_id": "Scientific-Engineering/wandb/6", "ground_truth": "        offsets = sorted(list(console.keys()))\n        intervals: List = []\n        for i, num in enumerate(offsets):\n            if i == 0:\n                intervals.append([num, num])\n                continue\n            largest = intervals[-1][1]\n            if num == largest + 1:\n                intervals[-1][1] = num\n            else:\n                intervals.append([num, num])\n        return intervals", "fpath_tuple": ["Scientific-Engineering", "wandb", "wandb", "sdk", "internal", "file_stream.py"], "context_start_lineno": 150, "line_no": 165, "id": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "target_function_prompt": "    def get_consecutive_offsets(console: Dict[int, str]) -> List[List[int]]:", "function_signature": "    def get_consecutive_offsets(console: Dict[int, str]) -> List[List[int]]:"}}
{"prompt": "    def sample(self) -> None:", "metadata": {"task_id": "Scientific-Engineering/wandb/7", "ground_truth": "        try:\n            stats = {}\n            devices = self._gc_ipu_info.getDevices()\n\n            for device in devices:\n                device_metrics: Dict[str, str] = dict(device)\n\n                pid = device_metrics.get(\"user process id\")\n                if pid is None or int(pid) != self._pid:\n                    continue\n\n                device_id = device_metrics.get(\"id\")\n                initial_call = device_id not in self._devices_called\n                if device_id is not None:\n                    self._devices_called.add(device_id)\n\n                for key, value in device_metrics.items():\n                    log_metric = initial_call or key in self.variable_metric_keys\n                    if not log_metric:\n                        continue\n                    parsed = self.parse_metric(key, value)\n                    if parsed is None:\n                        continue\n                    parsed_key, parsed_value = parsed\n                    stats[self.name.format(device_id, parsed_key)] = parsed_value\n\n            self.samples.append(stats)\n\n        except Exception as e:\n            wandb.termwarn(f\"IPU stats error {e}\", repeat=False)", "fpath_tuple": ["Scientific-Engineering", "wandb", "wandb", "sdk", "internal", "system", "assets", "ipu.py"], "context_start_lineno": 80, "line_no": 81, "id": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "target_function_prompt": "    def sample(self) -> None:", "function_signature": "    def sample(self) -> None:"}}
{"prompt": "def join_rows(rows, joiner=' '):", "metadata": {"task_id": "Scientific-Engineering/csvkit/0", "ground_truth": "    rows = list(rows)\n    fixed_row = rows[0][:]\n\n    for row in rows[1:]:\n        if len(row) == 0:\n            row = ['']\n\n        fixed_row[-1] += f\"{joiner}{row[0]}\"\n        fixed_row.extend(row[1:])\n\n    return fixed_row", "fpath_tuple": ["Scientific-Engineering", "csvkit", "csvkit", "cleanup.py"], "context_start_lineno": 5, "line_no": 10, "id": "csvkit.cleanup.join_rows", "target_function_prompt": "def join_rows(rows, joiner=' '):", "function_signature": "def join_rows(rows, joiner=' '):"}}
{"prompt": "def guess_format(filename):", "metadata": {"task_id": "Scientific-Engineering/csvkit/1", "ground_truth": "    last_period = filename.rfind('.')\n\n    if last_period == -1:\n        # No extension: assume fixed-width\n        return 'fixed'\n\n    extension = filename[last_period + 1:].lower()\n\n    if extension in ('csv', 'dbf', 'fixed', 'xls', 'xlsx'):\n        return extension\n    if extension in ('json', 'js'):\n        return 'json'\n\n    return None", "fpath_tuple": ["Scientific-Engineering", "csvkit", "csvkit", "convert", "__init__.py"], "context_start_lineno": 3, "line_no": 7, "id": "csvkit.convert.guess_format", "target_function_prompt": "def guess_format(filename):", "function_signature": "def guess_format(filename):"}}
{"prompt": "def normalize(rendered):", "metadata": {"task_id": "Scientific-Engineering/folium/0", "ground_truth": "    out = \"\".join([line.strip() for line in rendered.splitlines() if line.strip()])\n    out = out.replace(\", \", \",\")\n    return out", "fpath_tuple": ["Scientific-Engineering", "folium", "folium", "utilities.py"], "context_start_lineno": 442, "line_no": 444, "id": "folium.utilities.normalize", "target_function_prompt": "def normalize(rendered):", "function_signature": "def normalize(rendered):"}}
{"prompt": "def initialize_stats_dict(individual):", "metadata": {"task_id": "Scientific-Engineering/TPOT/0", "ground_truth": "    individual.statistics['generation'] = 0\n    individual.statistics['mutation_count'] = 0\n    individual.statistics['crossover_count'] = 0\n    individual.statistics['predecessor'] = 'ROOT',", "fpath_tuple": ["Scientific-Engineering", "TPOT", "tpot", "gp_deap.py"], "context_start_lineno": 153, "line_no": 170, "id": "tpot.gp_deap.initialize_stats_dict", "target_function_prompt": "def initialize_stats_dict(individual):", "function_signature": "def initialize_stats_dict(individual):"}}
{"prompt": "def remove_env_arg(cmd_args: list[str]) -> list[str]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/0", "ground_truth": "    indices_to_remove: list[int] = []\n    for i, arg in enumerate(cmd_args):\n        # regex matching click.option\n        if re.match(r\"^--env=?.*\", arg):\n            indices_to_remove.append(i)\n            if \"=\" not in arg:\n                indices_to_remove.append(i + 1)\n\n    new_cmd_args: list[str] = []\n    for i, arg in enumerate(cmd_args):\n        if i not in indices_to_remove:\n            new_cmd_args.append(arg)\n    return new_cmd_args", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml_cli", "env_manager.py"], "context_start_lineno": 33, "line_no": 37, "id": "bentoml_cli.env_manager.remove_env_arg", "target_function_prompt": "def remove_env_arg(cmd_args: list[str]) -> list[str]:", "function_signature": "def remove_env_arg(cmd_args: list[str]) -> list[str]:"}}
{"prompt": "def path_to_uri(path: str) -> str:", "metadata": {"task_id": "Scientific-Engineering/bentoml/1", "ground_truth": "    path = os.path.abspath(path)\n    if psutil.WINDOWS:\n        return pathlib.PureWindowsPath(path).as_uri()\n    if psutil.POSIX:\n        return pathlib.PurePosixPath(path).as_uri()\n    raise ValueError(\"Unsupported OS\")", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "utils", "uri.py"], "context_start_lineno": 10, "line_no": 20, "id": "bentoml._internal.utils.uri.path_to_uri", "target_function_prompt": "def path_to_uri(path: str) -> str:", "function_signature": "def path_to_uri(path: str) -> str:"}}
{"prompt": "def uri_to_path(uri: str) -> str:", "metadata": {"task_id": "Scientific-Engineering/bentoml/2", "ground_truth": "    parsed = urlparse(uri)\n    if parsed.scheme not in (\"file\", \"filesystem\", \"unix\"):\n        raise ValueError(\"Unsupported URI scheme\")\n    host = \"{0}{0}{mnt}{0}\".format(os.path.sep, mnt=parsed.netloc)\n    return os.path.normpath(os.path.join(host, url2pathname(unquote(parsed.path))))", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "utils", "uri.py"], "context_start_lineno": 28, "line_no": 38, "id": "bentoml._internal.utils.uri.uri_to_path", "target_function_prompt": "def uri_to_path(uri: str) -> str:", "function_signature": "def uri_to_path(uri: str) -> str:"}}
{"prompt": "def validate_labels(labels: dict[str, str]):", "metadata": {"task_id": "Scientific-Engineering/bentoml/3", "ground_truth": "    if not isinstance(labels, dict):\n        raise ValueError(\"labels must be a dict!\")\n\n    for key, val in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(\"label keys must be strings\")\n\n        if not isinstance(val, str):\n            raise ValueError(\"label values must be strings\")", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "utils", "__init__.py"], "context_start_lineno": 302, "line_no": 303, "id": "bentoml._internal.utils.validate_labels", "target_function_prompt": "def validate_labels(labels: dict[str, str]):", "function_signature": "def validate_labels(labels: dict[str, str]):"}}
{"prompt": "def is_valid_ip_address(addr: str) -> bool:", "metadata": {"task_id": "Scientific-Engineering/bentoml/4", "ground_truth": "    try:\n        _ = ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "configuration", "helpers.py"], "context_start_lineno": 161, "line_no": 163, "id": "bentoml._internal.configuration.helpers.is_valid_ip_address", "target_function_prompt": "def is_valid_ip_address(addr: str) -> bool:", "function_signature": "def is_valid_ip_address(addr: str) -> bool:"}}
{"prompt": "    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.PdDataFrame],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/5", "ground_truth": "        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return pd.concat(batches, ignore_index=True), indices  # type: ignore (incomplete panadas types)", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 347, "line_no": 352, "id": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "target_function_prompt": "    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.PdDataFrame],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:", "function_signature": "    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.PdDataFrame],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:"}}
{"prompt": "    def batch_to_batches(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[ext.PdDataFrame]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/6", "ground_truth": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        return [\n            batch.iloc[indices[i] : indices[i + 1]].reset_index(drop=True)\n            for i in range(len(indices) - 1)\n        ]", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 364, "line_no": 370, "id": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "target_function_prompt": "    def batch_to_batches(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[ext.PdDataFrame]:", "function_signature": "    def batch_to_batches(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[ext.PdDataFrame]:"}}
{"prompt": "    def batches_to_batch(\n        cls, batches: t.Sequence[list[t.Any]], batch_dim: int = 0\n    ) -> tuple[list[t.Any], list[int]]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/7", "ground_truth": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        batch: list[t.Any] = []\n        for subbatch in batches:\n            batch.extend(subbatch)\n        indices = list(itertools.accumulate(len(subbatch) for subbatch in batches))\n        indices = [0] + indices\n        return batch, indices", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 496, "line_no": 499, "id": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "target_function_prompt": "    def batches_to_batch(\n        cls, batches: t.Sequence[list[t.Any]], batch_dim: int = 0\n    ) -> tuple[list[t.Any], list[int]]:", "function_signature": "    def batches_to_batch(\n        cls, batches: t.Sequence[list[t.Any]], batch_dim: int = 0\n    ) -> tuple[list[t.Any], list[int]]:"}}
{"prompt": "    def batch_to_batches(\n        cls, batch: list[t.Any], indices: t.Sequence[int], batch_dim: int = 0\n    ) -> list[list[t.Any]]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/8", "ground_truth": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        return [batch[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 510, "line_no": 513, "id": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "target_function_prompt": "    def batch_to_batches(\n        cls, batch: list[t.Any], indices: t.Sequence[int], batch_dim: int = 0\n    ) -> list[list[t.Any]]:", "function_signature": "    def batch_to_batches(\n        cls, batch: list[t.Any], indices: t.Sequence[int], batch_dim: int = 0\n    ) -> list[list[t.Any]]:"}}
{"prompt": "def force_bytes(value: Union[bytes, str]) -> bytes:", "metadata": {"task_id": "Utilities/PyJWT/0", "ground_truth": "    if isinstance(value, str):\n        return value.encode(\"utf-8\")\n    elif isinstance(value, bytes):\n        return value\n    else:\n        raise TypeError(\"Expected a string value\")", "fpath_tuple": ["Utilities", "PyJWT", "jwt", "utils.py"], "context_start_lineno": 15, "line_no": 16, "id": "jwt.utils.force_bytes", "target_function_prompt": "def force_bytes(value: Union[bytes, str]) -> bytes:", "function_signature": "def force_bytes(value: Union[bytes, str]) -> bytes:"}}
{"prompt": "def display_progress_bar(\n    bytes_received: int, filesize: int, ch: str = \"\u2588\", scale: float = 0.55\n) -> None:", "metadata": {"task_id": "Utilities/pytube/0", "ground_truth": "    columns = shutil.get_terminal_size().columns\n    max_width = int(columns * scale)\n\n    filled = int(round(max_width * bytes_received / float(filesize)))\n    remaining = max_width - filled\n    progress_bar = ch * filled + \" \" * remaining\n    percent = round(100.0 * bytes_received / float(filesize), 1)\n    text = f\" \u21b3 |{progress_bar}| {percent}%\\r\"\n    sys.stdout.write(text)\n    sys.stdout.flush()", "fpath_tuple": ["Utilities", "pytube", "pytube", "cli.py"], "context_start_lineno": 208, "line_no": 229, "id": "pytube.cli.display_progress_bar", "target_function_prompt": "def display_progress_bar(\n    bytes_received: int, filesize: int, ch: str = \"\u2588\", scale: float = 0.55\n) -> None:", "function_signature": "def display_progress_bar(\n    bytes_received: int, filesize: int, ch: str = \"\u2588\", scale: float = 0.55\n) -> None:"}}
{"prompt": "def _download(\n    stream: Stream,\n    target: Optional[str] = None,\n    filename: Optional[str] = None,\n) -> None:", "metadata": {"task_id": "Utilities/pytube/1", "ground_truth": "    filesize_megabytes = stream.filesize // 1048576\n    print(f\"{filename or stream.default_filename} | {filesize_megabytes} MB\")\n    file_path = stream.get_file_path(filename=filename, output_path=target)\n    if stream.exists_at_path(file_path):\n        print(f\"Already downloaded at:\\n{file_path}\")\n        return\n\n    stream.download(output_path=target, filename=filename)\n    sys.stdout.write(\"\\n\")", "fpath_tuple": ["Utilities", "pytube", "pytube", "cli.py"], "context_start_lineno": 250, "line_no": 255, "id": "pytube.cli._download", "target_function_prompt": "def _download(\n    stream: Stream,\n    target: Optional[str] = None,\n    filename: Optional[str] = None,\n) -> None:", "function_signature": "def _download(\n    stream: Stream,\n    target: Optional[str] = None,\n    filename: Optional[str] = None,\n) -> None:"}}
{"prompt": "def display_streams(youtube: YouTube) -> None:", "metadata": {"task_id": "Utilities/pytube/2", "ground_truth": "    for stream in youtube.streams:\n        print(stream)", "fpath_tuple": ["Utilities", "pytube", "pytube", "cli.py"], "context_start_lineno": 483, "line_no": 490, "id": "pytube.cli.display_streams", "target_function_prompt": "def display_streams(youtube: YouTube) -> None:", "function_signature": "def display_streams(youtube: YouTube) -> None:"}}
{"prompt": "def _unique_name(base: str, subtype: str, media_type: str, target: str) -> str:", "metadata": {"task_id": "Utilities/pytube/3", "ground_truth": "    counter = 0\n    while True:\n        file_name = f\"{base}_{media_type}_{counter}\"\n        file_path = os.path.join(target, f\"{file_name}.{subtype}\")\n        if not os.path.exists(file_path):\n            return file_name\n        counter += 1", "fpath_tuple": ["Utilities", "pytube", "pytube", "cli.py"], "context_start_lineno": 266, "line_no": 279, "id": "pytube.cli._unique_name", "target_function_prompt": "def _unique_name(base: str, subtype: str, media_type: str, target: str) -> str:", "function_signature": "def _unique_name(base: str, subtype: str, media_type: str, target: str) -> str:"}}
{"prompt": "def _print_available_captions(captions: CaptionQuery) -> None:", "metadata": {"task_id": "Utilities/pytube/4", "ground_truth": "    print(\n        f\"Available caption codes are: {', '.join(c.code for c in captions)}\"\n    )", "fpath_tuple": ["Utilities", "pytube", "pytube", "cli.py"], "context_start_lineno": 494, "line_no": 495, "id": "pytube.cli._print_available_captions", "target_function_prompt": "def _print_available_captions(captions: CaptionQuery) -> None:", "function_signature": "def _print_available_captions(captions: CaptionQuery) -> None:"}}
{"prompt": "def throttling_reverse(arr: list):", "metadata": {"task_id": "Utilities/pytube/5", "ground_truth": "    reverse_copy = arr.copy()[::-1]\n    for i in range(len(reverse_copy)):\n        arr[i] = reverse_copy[i]", "fpath_tuple": ["Utilities", "pytube", "pytube", "cipher.py"], "context_start_lineno": 481, "line_no": 488, "id": "pytube.cipher.throttling_reverse", "target_function_prompt": "def throttling_reverse(arr: list):", "function_signature": "def throttling_reverse(arr: list):"}}
{"prompt": "def setup_logger(level: int = logging.ERROR, log_filename: Optional[str] = None) -> None:", "metadata": {"task_id": "Utilities/pytube/6", "ground_truth": "    fmt = \"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\"\n    date_fmt = \"%H:%M:%S\"\n    formatter = logging.Formatter(fmt, datefmt=date_fmt)\n\n    # https://github.com/pytube/pytube/issues/163\n    logger = logging.getLogger(\"pytube\")\n    logger.setLevel(level)\n\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n\n    if log_filename is not None:\n        file_handler = logging.FileHandler(log_filename)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)", "fpath_tuple": ["Utilities", "pytube", "pytube", "helpers.py"], "context_start_lineno": 179, "line_no": 185, "id": "pytube.helpers.setup_logger", "target_function_prompt": "def setup_logger(level: int = logging.ERROR, log_filename: Optional[str] = None) -> None:", "function_signature": "def setup_logger(level: int = logging.ERROR, log_filename: Optional[str] = None) -> None:"}}
{"prompt": "def deprecated(reason: str) -> Callable:", "metadata": {"task_id": "Utilities/pytube/7", "ground_truth": "    def decorator(func1):\n        message = \"Call to deprecated function {name} ({reason}).\"\n\n        @functools.wraps(func1)\n        def new_func1(*args, **kwargs):\n            warnings.simplefilter(\"always\", DeprecationWarning)\n            warnings.warn(\n                message.format(name=func1.__name__, reason=reason),\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n            warnings.simplefilter(\"default\", DeprecationWarning)\n            return func1(*args, **kwargs)\n\n        return new_func1\n\n    return decorator", "fpath_tuple": ["Utilities", "pytube", "pytube", "helpers.py"], "context_start_lineno": 211, "line_no": 218, "id": "pytube.helpers.deprecated", "target_function_prompt": "def deprecated(reason: str) -> Callable:", "function_signature": "def deprecated(reason: str) -> Callable:"}}
{"prompt": "def uniqueify(duped_list: List) -> List:", "metadata": {"task_id": "Utilities/pytube/8", "ground_truth": "    seen: Dict[Any, bool] = {}\n    result = []\n    for item in duped_list:\n        if item in seen:\n            continue\n        seen[item] = True\n        result.append(item)\n    return result", "fpath_tuple": ["Utilities", "pytube", "pytube", "helpers.py"], "context_start_lineno": 263, "line_no": 272, "id": "pytube.helpers.uniqueify", "target_function_prompt": "def uniqueify(duped_list: List) -> List:", "function_signature": "def uniqueify(duped_list: List) -> List:"}}
{"prompt": "def target_directory(output_path: Optional[str] = None) -> str:", "metadata": {"task_id": "Utilities/pytube/9", "ground_truth": "    if output_path:\n        if not os.path.isabs(output_path):\n            output_path = os.path.join(os.getcwd(), output_path)\n    else:\n        output_path = os.getcwd()\n    os.makedirs(output_path, exist_ok=True)\n    return output_path", "fpath_tuple": ["Utilities", "pytube", "pytube", "helpers.py"], "context_start_lineno": 237, "line_no": 248, "id": "pytube.helpers.target_directory", "target_function_prompt": "def target_directory(output_path: Optional[str] = None) -> str:", "function_signature": "def target_directory(output_path: Optional[str] = None) -> str:"}}
{"prompt": "def is_private(watch_html):", "metadata": {"task_id": "Utilities/pytube/10", "ground_truth": "    private_strings = [\n        \"This is a private video. Please sign in to verify that you may see it.\",\n        \"\\\"simpleText\\\":\\\"Private video\\\"\",\n        \"This video is private.\"\n    ]\n    for string in private_strings:\n        if string in watch_html:\n            return True\n    return False", "fpath_tuple": ["Utilities", "pytube", "pytube", "extract.py"], "context_start_lineno": 55, "line_no": 64, "id": "pytube.extract.is_private", "target_function_prompt": "def is_private(watch_html):", "function_signature": "def is_private(watch_html):"}}
{"prompt": "def cartesian(*arrays):", "metadata": {"task_id": "Scientific-Engineering/pymc/0", "ground_truth": "    N = len(arrays)\n    arrays_np = [np.asarray(x) for x in arrays]\n    arrays_2d = [x[:, None] if np.asarray(x).ndim == 1 else x for x in arrays_np]\n    arrays_integer = [np.arange(len(x)) for x in arrays_2d]\n    product_integers = np.stack(np.meshgrid(*arrays_integer, indexing=\"ij\"), -1).reshape(-1, N)\n    return np.concatenate(\n        [array[product_integers[:, i]] for i, array in enumerate(arrays_2d)], axis=-1\n    )", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "math.py"], "context_start_lineno": 186, "line_no": 194, "id": "pymc.math.cartesian", "target_function_prompt": "def cartesian(*arrays):", "function_signature": "def cartesian(*arrays):"}}
{"prompt": "def log1mexp(x, *, negative_input=False):", "metadata": {"task_id": "Scientific-Engineering/pymc/1", "ground_truth": "    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    return pt.log1mexp(x)", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "math.py"], "context_start_lineno": 292, "line_no": 306, "id": "pymc.math.log1mexp", "target_function_prompt": "def log1mexp(x, *, negative_input=False):", "function_signature": "def log1mexp(x, *, negative_input=False):"}}
{"prompt": "def log1mexp_numpy(x, *, negative_input=False):", "metadata": {"task_id": "Scientific-Engineering/pymc/2", "ground_truth": "    x = np.asarray(x, dtype=\"float\")\n\n    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp_numpy will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    out = np.empty_like(x)\n    mask = x < -0.6931471805599453  # log(1/2)\n    out[mask] = np.log1p(-np.exp(x[mask]))\n    mask = ~mask\n    out[mask] = np.log(-np.expm1(x[mask]))\n    return out", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "math.py"], "context_start_lineno": 318, "line_no": 324, "id": "pymc.math.log1mexp_numpy", "target_function_prompt": "def log1mexp_numpy(x, *, negative_input=False):", "function_signature": "def log1mexp_numpy(x, *, negative_input=False):"}}
{"prompt": "def drop_warning_stat(idata: arviz.InferenceData) -> arviz.InferenceData:", "metadata": {"task_id": "Scientific-Engineering/pymc/3", "ground_truth": "    nidata = arviz.InferenceData(attrs=idata.attrs)\n    for gname, group in idata.items():\n        if \"sample_stat\" in gname:\n            group = group.drop_vars(names=[\"warning\", \"warning_dim_0\"], errors=\"ignore\")\n        nidata.add_groups({gname: group}, coords=group.coords, dims=group.dims)\n    return nidata", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "util.py"], "context_start_lineno": 262, "line_no": 268, "id": "pymc.util.drop_warning_stat", "target_function_prompt": "def drop_warning_stat(idata: arviz.InferenceData) -> arviz.InferenceData:", "function_signature": "def drop_warning_stat(idata: arviz.InferenceData) -> arviz.InferenceData:"}}
{"prompt": "def walk_model(\n    graphs: Iterable[TensorVariable],\n    stop_at_vars: Optional[Set[TensorVariable]] = None,\n    expand_fn: Callable[[TensorVariable], Iterable[TensorVariable]] = lambda var: [],\n) -> Generator[TensorVariable, None, None]:", "metadata": {"task_id": "Scientific-Engineering/pymc/4", "ground_truth": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    def expand(var):\n        new_vars = expand_fn(var)\n\n        if var.owner and var not in stop_at_vars:\n            new_vars.extend(reversed(var.owner.inputs))\n\n        return new_vars\n\n    yield from walk(graphs, expand, bfs=False)", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "pytensorf.py"], "context_start_lineno": 178, "line_no": 194, "id": "pymc.pytensorf.walk_model", "target_function_prompt": "def walk_model(\n    graphs: Iterable[TensorVariable],\n    stop_at_vars: Optional[Set[TensorVariable]] = None,\n    expand_fn: Callable[[TensorVariable], Iterable[TensorVariable]] = lambda var: [],\n) -> Generator[TensorVariable, None, None]:", "function_signature": "def walk_model(\n    graphs: Iterable[TensorVariable],\n    stop_at_vars: Optional[Set[TensorVariable]] = None,\n    expand_fn: Callable[[TensorVariable], Iterable[TensorVariable]] = lambda var: [],\n) -> Generator[TensorVariable, None, None]:"}}
{"prompt": "def select_by_precision(float64, float32):", "metadata": {"task_id": "Scientific-Engineering/pymc/5", "ground_truth": "    decimal = float64 if pytensor.config.floatX == \"float64\" else float32\n    return decimal", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "testing.py"], "context_start_lineno": 224, "line_no": 226, "id": "pymc.testing.select_by_precision", "target_function_prompt": "def select_by_precision(float64, float32):", "function_signature": "def select_by_precision(float64, float32):"}}
{"prompt": "def handle_args(func: Callable) -> Callable:", "metadata": {"task_id": "Scientific-Engineering/pymc/6", "ground_truth": "    def f(x, args):\n        if args is None:\n            return func(x)\n        else:\n            if not isinstance(args, tuple):\n                args = (args,)\n            return func(x, *args)\n\n    return f", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "gp", "cov.py"], "context_start_lineno": 1148, "line_no": 1149, "id": "pymc.gp.cov.handle_args", "target_function_prompt": "def handle_args(func: Callable) -> Callable:", "function_signature": "def handle_args(func: Callable) -> Callable:"}}
{"prompt": "def kmeans_inducing_points(n_inducing, X, **kmeans_kwargs):", "metadata": {"task_id": "Scientific-Engineering/pymc/7", "ground_truth": "    if isinstance(X, TensorConstant):\n        X = X.value\n    elif isinstance(X, (np.ndarray, tuple, list)):\n        X = np.asarray(X)\n    else:\n        raise TypeError(\n            \"To use K-means initialization, \"\n            \"please provide X as a type that \"\n            \"can be cast to np.ndarray, instead \"\n            \"of {}\".format(type(X))\n        )\n    scaling = np.std(X, 0)\n    # if std of a column is very small (zero), don't normalize that column\n    scaling[scaling <= 1e-6] = 1.0\n    Xw = X / scaling\n\n    if \"k_or_guess\" in kmeans_kwargs:\n        warnings.warn(\"Use `n_inducing` to set the `k_or_guess` parameter instead.\")\n\n    Xu, distortion = kmeans(Xw, k_or_guess=n_inducing, **kmeans_kwargs)\n    return Xu * scaling", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "gp", "util.py"], "context_start_lineno": 97, "line_no": 112, "id": "pymc.gp.util.kmeans_inducing_points", "target_function_prompt": "def kmeans_inducing_points(n_inducing, X, **kmeans_kwargs):", "function_signature": "def kmeans_inducing_points(n_inducing, X, **kmeans_kwargs):"}}
{"prompt": "def floatX(X):", "metadata": {"task_id": "Scientific-Engineering/pymc/8", "ground_truth": "    try:\n        return X.astype(pytensor.config.floatX)\n    except AttributeError:\n        # Scalar passed\n        return np.asarray(X, dtype=pytensor.config.floatX)", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "pytensorf.py"], "context_start_lineno": 435, "line_no": 439, "id": "pymc.pytensorf.floatX", "target_function_prompt": "def floatX(X):", "function_signature": "def floatX(X):"}}
{"prompt": "def posdef(AA):\n", "metadata": {"task_id": "Scientific-Engineering/pymc/9", "ground_truth": "    try:\n        np.linalg.cholesky(AA)\n        return True\n    except np.linalg.LinAlgError:\n        return False", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "distributions", "multivariate.py"], "context_start_lineno": 828, "line_no": 830, "id": "pymc.distributions.multivariate.posdef", "target_function_prompt": "def posdef(AA):\n", "function_signature": "def posdef(AA):\n"}}
{"prompt": "def multigammaln(a, p):", "metadata": {"task_id": "Scientific-Engineering/pymc/10", "ground_truth": "    i = pt.arange(1, p + 1)\n    return p * (p - 1) * pt.log(np.pi) / 4.0 + pt.sum(gammaln(a + (1.0 - i) / 2.0), axis=0)", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "distributions", "dist_math.py"], "context_start_lineno": 397, "line_no": 406, "id": "pymc.distributions.dist_math.multigammaln", "target_function_prompt": "def multigammaln(a, p):", "function_signature": "def multigammaln(a, p):"}}
{"prompt": "def incomplete_beta(a, b, value):", "metadata": {"task_id": "Scientific-Engineering/pymc/11", "ground_truth": "    warnings.warn(\n        \"incomplete_beta has been deprecated. Use pytensor.tensor.betainc instead.\",\n        FutureWarning,\n        stacklevel=2,\n    )\n    return pt.betainc(a, b, value)", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "distributions", "dist_math.py"], "context_start_lineno": 435, "line_no": 436, "id": "pymc.distributions.dist_math.incomplete_beta", "target_function_prompt": "def incomplete_beta(a, b, value):", "function_signature": "def incomplete_beta(a, b, value):"}}
{"prompt": "def observed_dependent_deterministics(model: Model):", "metadata": {"task_id": "Scientific-Engineering/pymc/12", "ground_truth": "    deterministics = model.deterministics\n    observed_rvs = set(model.observed_RVs)\n    blockers = model.basic_RVs\n    return [\n        deterministic\n        for deterministic in deterministics\n        if observed_rvs & set(ancestors([deterministic], blockers=blockers))\n    ]", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "sampling", "forward.py"], "context_start_lineno": 334, "line_no": 336, "id": "pymc.sampling.forward.observed_dependent_deterministics", "target_function_prompt": "def observed_dependent_deterministics(model: Model):", "function_signature": "def observed_dependent_deterministics(model: Model):"}}
{"prompt": "def systematic_resampling(weights, rng):", "metadata": {"task_id": "Scientific-Engineering/pymc/13", "ground_truth": "    lnw = len(weights)\n    arange = np.arange(lnw)\n    uniform = (rng.random(1) + arange) / lnw\n\n    idx = 0\n    weight_accu = weights[0]\n    new_indices = np.empty(lnw, dtype=int)\n    for i in arange:\n        while uniform[i] > weight_accu:\n            idx += 1\n            weight_accu += weights[idx]\n        new_indices[i] = idx\n\n    return new_indices", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "smc", "kernels.py"], "context_start_lineno": 570, "line_no": 584, "id": "pymc.smc.kernels.systematic_resampling", "target_function_prompt": "def systematic_resampling(weights, rng):", "function_signature": "def systematic_resampling(weights, rng):"}}
{"prompt": "def _squeeze_cat(results, combine: bool, squeeze: bool):", "metadata": {"task_id": "Scientific-Engineering/pymc/14", "ground_truth": "    if combine:\n        results = np.concatenate(results)\n        if not squeeze:\n            results = [results]\n    else:\n        if squeeze and len(results) == 1:\n            results = results[0]\n    return results", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "backends", "base.py"], "context_start_lineno": 570, "line_no": 573, "id": "pymc.backends.base._squeeze_cat", "target_function_prompt": "def _squeeze_cat(results, combine: bool, squeeze: bool):", "function_signature": "def _squeeze_cat(results, combine: bool, squeeze: bool):"}}
{"prompt": "    def forward(self, value, *inputs):", "metadata": {"task_id": "Scientific-Engineering/pymc/15", "ground_truth": "        value = pt.as_tensor(value)\n        log_value = pt.log(value)\n        N = value.shape[-1].astype(value.dtype)\n        shift = pt.sum(log_value, -1, keepdims=True) / N\n        return log_value[..., :-1] - shift", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "logprob", "transforms.py"], "context_start_lineno": 1112, "line_no": 1113, "id": "pymc.logprob.transforms.SimplexTransform.forward", "target_function_prompt": "    def forward(self, value, *inputs):", "function_signature": "    def forward(self, value, *inputs):"}}
{"prompt": "    def backward(self, value, *inputs):", "metadata": {"task_id": "Scientific-Engineering/pymc/16", "ground_truth": "        value = pt.concatenate([value, -pt.sum(value, -1, keepdims=True)], axis=-1)\n        exp_value_max = pt.exp(value - pt.max(value, -1, keepdims=True))\n        return exp_value_max / pt.sum(exp_value_max, -1, keepdims=True)", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "logprob", "transforms.py"], "context_start_lineno": 1119, "line_no": 1120, "id": "pymc.logprob.transforms.SimplexTransform.backward", "target_function_prompt": "    def backward(self, value, *inputs):", "function_signature": "    def backward(self, value, *inputs):"}}
{"prompt": "def walk_model(\n    graphs: Iterable[TensorVariable],\n    walk_past_rvs: bool = False,\n    stop_at_vars: Optional[Set[TensorVariable]] = None,\n    expand_fn: Callable[[TensorVariable], List[TensorVariable]] = lambda var: [],\n) -> Generator[TensorVariable, None, None]:", "metadata": {"task_id": "Scientific-Engineering/pymc/17", "ground_truth": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    def expand(var: TensorVariable, stop_at_vars=stop_at_vars) -> List[TensorVariable]:\n        new_vars = expand_fn(var)\n\n        if (\n            var.owner\n            and (walk_past_rvs or not isinstance(var.owner.op, MeasurableVariable))\n            and (var not in stop_at_vars)\n        ):\n            new_vars.extend(reversed(var.owner.inputs))\n\n        return new_vars\n\n    yield from walk(graphs, expand, False)", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "logprob", "utils.py"], "context_start_lineno": 69, "line_no": 90, "id": "pymc.logprob.utils.walk_model", "target_function_prompt": "def walk_model(\n    graphs: Iterable[TensorVariable],\n    walk_past_rvs: bool = False,\n    stop_at_vars: Optional[Set[TensorVariable]] = None,\n    expand_fn: Callable[[TensorVariable], List[TensorVariable]] = lambda var: [],\n) -> Generator[TensorVariable, None, None]:", "function_signature": "def walk_model(\n    graphs: Iterable[TensorVariable],\n    walk_past_rvs: bool = False,\n    stop_at_vars: Optional[Set[TensorVariable]] = None,\n    expand_fn: Callable[[TensorVariable], List[TensorVariable]] = lambda var: [],\n) -> Generator[TensorVariable, None, None]:"}}
{"prompt": "def linearize_metrics(logged_metrics):", "metadata": {"task_id": "Utilities/sacred/0", "ground_truth": "    metrics_by_name = {}\n    for metric_entry in logged_metrics:\n        if metric_entry.name not in metrics_by_name:\n            metrics_by_name[metric_entry.name] = {\n                \"steps\": [],\n                \"values\": [],\n                \"timestamps\": [],\n                \"name\": metric_entry.name,\n            }\n        metrics_by_name[metric_entry.name][\"steps\"].append(metric_entry.step)\n        metrics_by_name[metric_entry.name][\"values\"].append(metric_entry.value)\n        metrics_by_name[metric_entry.name][\"timestamps\"].append(metric_entry.timestamp)\n    return metrics_by_name", "fpath_tuple": ["Utilities", "sacred", "sacred", "metrics_logger.py"], "context_start_lineno": 79, "line_no": 92, "id": "sacred.metrics_logger.linearize_metrics", "target_function_prompt": "def linearize_metrics(logged_metrics):", "function_signature": "def linearize_metrics(logged_metrics):"}}
{"prompt": "def set_by_dotted_path(d, path, value):", "metadata": {"task_id": "Utilities/sacred/1", "ground_truth": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path[:-1]:\n        if p not in current_option:\n            current_option[p] = dict()\n        current_option = current_option[p]\n    current_option[split_path[-1]] = value", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 456, "line_no": 473, "id": "sacred.utils.set_by_dotted_path", "target_function_prompt": "def set_by_dotted_path(d, path, value):", "function_signature": "def set_by_dotted_path(d, path, value):"}}
{"prompt": "def get_by_dotted_path(d, path, default=None):", "metadata": {"task_id": "Utilities/sacred/2", "ground_truth": "    if not path:\n        return d\n    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path:\n        if p not in current_option:\n            return default\n        current_option = current_option[p]\n    return current_option", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 482, "line_no": 491, "id": "sacred.utils.get_by_dotted_path", "target_function_prompt": "def get_by_dotted_path(d, path, default=None):", "function_signature": "def get_by_dotted_path(d, path, default=None):"}}
{"prompt": "def construct_scan(scan_args: ScanArgs, **kwargs) -> Tuple[List[TensorVariable], OrderedUpdates]:", "metadata": {"task_id": "Scientific-Engineering/pymc/18", "ground_truth": "    scan_op = Scan(scan_args.inner_inputs, scan_args.inner_outputs, scan_args.info, **kwargs)\n    node = scan_op.make_node(*scan_args.outer_inputs)\n    updates = OrderedUpdates(zip(scan_args.outer_in_shared, scan_args.outer_out_shared))\n    return node.outputs, updates", "fpath_tuple": ["Scientific-Engineering", "pymc", "pymc", "logprob", "scan.py"], "context_start_lineno": 294, "line_no": 295, "id": "pymc.logprob.scan.construct_scan", "target_function_prompt": "def construct_scan(scan_args: ScanArgs, **kwargs) -> Tuple[List[TensorVariable], OrderedUpdates]:", "function_signature": "def construct_scan(scan_args: ScanArgs, **kwargs) -> Tuple[List[TensorVariable], OrderedUpdates]:"}}
{"prompt": "def is_prefix(pre_path, path):", "metadata": {"task_id": "Utilities/sacred/3", "ground_truth": "    pre_path = pre_path.strip(\".\")\n    path = path.strip(\".\")\n    return not pre_path or path.startswith(pre_path + \".\")", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 521, "line_no": 523, "id": "sacred.utils.is_prefix", "target_function_prompt": "def is_prefix(pre_path, path):", "function_signature": "def is_prefix(pre_path, path):"}}
{"prompt": "def get_inheritors(cls):", "metadata": {"task_id": "Utilities/sacred/4", "ground_truth": "    subclasses = set()\n    work = [cls]\n    while work:\n        parent = work.pop()\n        for child in parent.__subclasses__():\n            if child not in subclasses:\n                subclasses.add(child)\n                work.append(child)\n    return subclasses", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 612, "line_no": 614, "id": "sacred.utils.get_inheritors", "target_function_prompt": "def get_inheritors(cls):", "function_signature": "def get_inheritors(cls):"}}
{"prompt": "def convert_camel_case_to_snake_case(name):", "metadata": {"task_id": "Utilities/sacred/5", "ground_truth": "    s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 627, "line_no": 629, "id": "sacred.utils.convert_camel_case_to_snake_case", "target_function_prompt": "def convert_camel_case_to_snake_case(name):", "function_signature": "def convert_camel_case_to_snake_case(name):"}}
{"prompt": "def module_exists(modname):", "metadata": {"task_id": "Utilities/sacred/6", "ground_truth": "    try:\n        return pkgutil.find_loader(modname) is not None\n    except ImportError:\n        # TODO: Temporary fix for tf 1.14.0.\n        # Should be removed once fixed in tf.\n        return True", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 673, "line_no": 675, "id": "sacred.utils.module_exists", "target_function_prompt": "def module_exists(modname):", "function_signature": "def module_exists(modname):"}}
{"prompt": "def apply_backspaces_and_linefeeds(text):", "metadata": {"task_id": "Utilities/sacred/7", "ground_truth": "    orig_lines = text.split(\"\\n\")\n    orig_lines_len = len(orig_lines)\n    new_lines = []\n    for orig_line_idx, orig_line in enumerate(orig_lines):\n        chars, cursor = [], 0\n        orig_line_len = len(orig_line)\n        for orig_char_idx, orig_char in enumerate(orig_line):\n            if orig_char == \"\\r\" and (\n                orig_char_idx != orig_line_len - 1\n                or orig_line_idx != orig_lines_len - 1\n            ):\n                cursor = 0\n            elif orig_char == \"\\b\":\n                cursor = max(0, cursor - 1)\n            else:\n                if (\n                    orig_char == \"\\r\"\n                    and orig_char_idx == orig_line_len - 1\n                    and orig_line_idx == orig_lines_len - 1\n                ):\n                    cursor = len(chars)\n                if cursor == len(chars):\n                    chars.append(orig_char)\n                else:\n                    chars[cursor] = orig_char\n                cursor += 1\n        new_lines.append(\"\".join(chars))\n    return \"\\n\".join(new_lines)", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 633, "line_no": 643, "id": "sacred.utils.apply_backspaces_and_linefeeds", "target_function_prompt": "def apply_backspaces_and_linefeeds(text):", "function_signature": "def apply_backspaces_and_linefeeds(text):"}}
{"prompt": "def help_for_command(command):", "metadata": {"task_id": "Utilities/sacred/8", "ground_truth": "    help_text = pydoc.text.document(command)\n    # remove backspaces\n    return re.subn(\".\\\\x08\", \"\", help_text)[0]", "fpath_tuple": ["Utilities", "sacred", "sacred", "commands.py"], "context_start_lineno": 117, "line_no": 119, "id": "sacred.commands.help_for_command", "target_function_prompt": "def help_for_command(command):", "function_signature": "def help_for_command(command):"}}
{"prompt": "def optional_import(*package_names):", "metadata": {"task_id": "Utilities/sacred/9", "ground_truth": "    try:\n        packages = [importlib.import_module(pn) for pn in package_names]\n        return True, packages[0]\n    except ImportError:\n        return False, None", "fpath_tuple": ["Utilities", "sacred", "sacred", "optional.py"], "context_start_lineno": 8, "line_no": 9, "id": "sacred.optional.optional_import", "target_function_prompt": "def optional_import(*package_names):", "function_signature": "def optional_import(*package_names):"}}
{"prompt": "def get_py_file_if_possible(pyc_name):", "metadata": {"task_id": "Utilities/sacred/10", "ground_truth": "    if pyc_name.endswith((\".py\", \".so\", \".pyd\", \".ipynb\")):\n        return pyc_name\n    assert pyc_name.endswith(\".pyc\")\n    non_compiled_file = pyc_name[:-1]\n    if os.path.exists(non_compiled_file):\n        return non_compiled_file\n    return pyc_name", "fpath_tuple": ["Utilities", "sacred", "sacred", "dependencies.py"], "context_start_lineno": 377, "line_no": 379, "id": "sacred.dependencies.get_py_file_if_possible", "target_function_prompt": "def get_py_file_if_possible(pyc_name):", "function_signature": "def get_py_file_if_possible(pyc_name):"}}
{"prompt": "    def update(self, iterable=None, **kwargs):", "metadata": {"task_id": "Utilities/sacred/11", "ground_truth": "        if iterable is not None:\n            if hasattr(iterable, \"keys\"):\n                for key in iterable:\n                    self[key] = iterable[key]\n            else:\n                for (key, value) in iterable:\n                    self[key] = value\n        for key in kwargs:\n            self[key] = kwargs[key]", "fpath_tuple": ["Utilities", "sacred", "sacred", "config", "custom_containers.py"], "context_start_lineno": 96, "line_no": 97, "id": "sacred.config.custom_containers.DogmaticDict.update", "target_function_prompt": "    def update(self, iterable=None, **kwargs):", "function_signature": "    def update(self, iterable=None, **kwargs):"}}
{"prompt": "def is_empty_or_comment(line):", "metadata": {"task_id": "Utilities/sacred/12", "ground_truth": "    sline = line.strip()\n    return sline == \"\" or sline.startswith(\"#\")", "fpath_tuple": ["Utilities", "sacred", "sacred", "config", "config_scope.py"], "context_start_lineno": 147, "line_no": 148, "id": "sacred.config.config_scope.is_empty_or_comment", "target_function_prompt": "def is_empty_or_comment(line):", "function_signature": "def is_empty_or_comment(line):"}}
{"prompt": "def copy_function(orig, copy_dict=True):", "metadata": {"task_id": "Utilities/boltons/0", "ground_truth": "    ret = FunctionType(orig.__code__,\n                       orig.__globals__,\n                       name=orig.__name__,\n                       argdefs=getattr(orig, \"__defaults__\", None),\n                       closure=getattr(orig, \"__closure__\", None))\n    if copy_dict:\n        ret.__dict__.update(orig.__dict__)\n    return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "funcutils.py"], "context_start_lineno": 193, "line_no": 212, "id": "boltons.funcutils.copy_function", "target_function_prompt": "def copy_function(orig, copy_dict=True):", "function_signature": "def copy_function(orig, copy_dict=True):"}}
{"prompt": "def dedent_line(line, indent):", "metadata": {"task_id": "Utilities/sacred/13", "ground_truth": "    for i, (line_sym, indent_sym) in enumerate(zip(line, indent)):\n        if line_sym != indent_sym:\n            start = i\n            break\n    else:\n        start = len(indent)\n    return line[start:]", "fpath_tuple": ["Utilities", "sacred", "sacred", "config", "config_scope.py"], "context_start_lineno": 156, "line_no": 157, "id": "sacred.config.config_scope.dedent_line", "target_function_prompt": "def dedent_line(line, indent):", "function_signature": "def dedent_line(line, indent):"}}
{"prompt": "def format_invocation(name='', args=(), kwargs=None, **kw):", "metadata": {"task_id": "Utilities/boltons/1", "ground_truth": "    _repr = kw.pop('repr', repr)\n    if kw:\n        raise TypeError('unexpected keyword args: %r' % ', '.join(kw.keys()))\n    kwargs = kwargs or {}\n    a_text = ', '.join([_repr(a) for a in args])\n    if isinstance(kwargs, dict):\n        kwarg_items = [(k, kwargs[k]) for k in sorted(kwargs)]\n    else:\n        kwarg_items = kwargs\n    kw_text = ', '.join(['%s=%s' % (k, _repr(v)) for k, v in kwarg_items])\n\n    all_args_text = a_text\n    if all_args_text and kw_text:\n        all_args_text += ', '\n    all_args_text += kw_text\n\n    return '%s(%s)' % (name, all_args_text)", "fpath_tuple": ["Utilities", "boltons", "boltons", "funcutils.py"], "context_start_lineno": 337, "line_no": 349, "id": "boltons.funcutils.format_invocation", "target_function_prompt": "def format_invocation(name='', args=(), kwargs=None, **kw):", "function_signature": "def format_invocation(name='', args=(), kwargs=None, **kw):"}}
{"prompt": "    def shift(self, item_index, dest_index=0):", "metadata": {"task_id": "Utilities/boltons/2", "ground_truth": "        if item_index == dest_index:\n            return\n        item = self.pop(item_index)\n        self.insert(dest_index, item)", "fpath_tuple": ["Utilities", "boltons", "boltons", "listutils.py"], "context_start_lineno": 351, "line_no": 352, "id": "boltons.listutils.SplayList.shift", "target_function_prompt": "    def shift(self, item_index, dest_index=0):", "function_signature": "    def shift(self, item_index, dest_index=0):"}}
{"prompt": "def gzip_bytes(bytestring, level=6):", "metadata": {"task_id": "Utilities/boltons/3", "ground_truth": "    out = StringIO()\n    f = GzipFile(fileobj=out, mode='wb', compresslevel=level)\n    f.write(bytestring)\n    f.close()\n    return out.getvalue()", "fpath_tuple": ["Utilities", "boltons", "boltons", "strutils.py"], "context_start_lineno": 669, "line_no": 684, "id": "boltons.strutils.gzip_bytes", "target_function_prompt": "def gzip_bytes(bytestring, level=6):", "function_signature": "def gzip_bytes(bytestring, level=6):"}}
{"prompt": "def is_uuid(obj, version=4):", "metadata": {"task_id": "Utilities/boltons/4", "ground_truth": "    if not isinstance(obj, uuid.UUID):\n        try:\n            obj = uuid.UUID(obj)\n        except (TypeError, ValueError, AttributeError):\n            return False\n    if version and obj.version != int(version):\n        return False\n    return True", "fpath_tuple": ["Utilities", "boltons", "boltons", "strutils.py"], "context_start_lineno": 744, "line_no": 758, "id": "boltons.strutils.is_uuid", "target_function_prompt": "def is_uuid(obj, version=4):", "function_signature": "def is_uuid(obj, version=4):"}}
{"prompt": "def parse_int_list(range_string, delim=',', range_delim='-'):", "metadata": {"task_id": "Utilities/boltons/5", "ground_truth": "    output = []\n\n    for x in range_string.strip().split(delim):\n\n        # Range\n        if range_delim in x:\n            range_limits = list(map(int, x.split(range_delim)))\n            output += list(range(min(range_limits), max(range_limits)+1))\n\n        # Empty String\n        elif not x:\n            continue\n\n        # Integer\n        else:\n            output.append(int(x))\n\n    return sorted(output)", "fpath_tuple": ["Utilities", "boltons", "boltons", "strutils.py"], "context_start_lineno": 910, "line_no": 927, "id": "boltons.strutils.parse_int_list", "target_function_prompt": "def parse_int_list(range_string, delim=',', range_delim='-'):", "function_signature": "def parse_int_list(range_string, delim=',', range_delim='-'):"}}
{"prompt": "    def get(self, key, default=0):", "metadata": {"task_id": "Utilities/boltons/6", "ground_truth": "        try:\n            return self[key]\n        except KeyError:\n            return default", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 797, "line_no": 799, "id": "boltons.cacheutils.ThresholdCounter.get", "target_function_prompt": "    def get(self, key, default=0):", "function_signature": "    def get(self, key, default=0):"}}
{"prompt": "def backoff_iter(start, stop, count=None, factor=2.0, jitter=False):", "metadata": {"task_id": "Utilities/boltons/7", "ground_truth": "    start = float(start)\n    stop = float(stop)\n    factor = float(factor)\n    if start < 0.0:\n        raise ValueError('expected start >= 0, not %r' % start)\n    if factor < 1.0:\n        raise ValueError('expected factor >= 1.0, not %r' % factor)\n    if stop == 0.0:\n        raise ValueError('expected stop >= 0')\n    if stop < start:\n        raise ValueError('expected stop >= start, not %r' % stop)\n    if count is None:\n        denom = start if start else 1\n        count = 1 + math.ceil(math.log(stop/denom, factor))\n        count = count if start else count + 1\n    if count != 'repeat' and count < 0:\n        raise ValueError('count must be positive or \"repeat\", not %r' % count)\n    if jitter:\n        jitter = float(jitter)\n        if not (-1.0 <= jitter <= 1.0):\n            raise ValueError('expected jitter -1 <= j <= 1, not: %r' % jitter)\n\n    cur, i = start, 0\n    while count == 'repeat' or i < count:\n        if not jitter:\n            cur_ret = cur\n        elif jitter:\n            cur_ret = cur - (cur * jitter * random.random())\n        yield cur_ret\n        i += 1\n        if cur == 0:\n            cur = 1\n        elif cur < stop:\n            cur *= factor\n        if cur > stop:\n            cur = stop\n    return", "fpath_tuple": ["Utilities", "boltons", "boltons", "iterutils.py"], "context_start_lineno": 560, "line_no": 615, "id": "boltons.iterutils.backoff_iter", "target_function_prompt": "def backoff_iter(start, stop, count=None, factor=2.0, jitter=False):", "function_signature": "def backoff_iter(start, stop, count=None, factor=2.0, jitter=False):"}}
{"prompt": "def cached(cache, scoped=True, typed=False, key=None):", "metadata": {"task_id": "Utilities/boltons/8", "ground_truth": "    def cached_func_decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_func_decorator", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 542, "line_no": 575, "id": "boltons.cacheutils.cached", "target_function_prompt": "def cached(cache, scoped=True, typed=False, key=None):", "function_signature": "def cached(cache, scoped=True, typed=False, key=None):"}}
{"prompt": "def total_seconds(td):", "metadata": {"task_id": "Utilities/boltons/9", "ground_truth": "    a_milli = 1000000.0\n    td_ds = td.seconds + (td.days * 86400)  # 24 * 60 * 60\n    td_micro = td.microseconds + (td_ds * a_milli)\n    return td_micro / a_milli", "fpath_tuple": ["Utilities", "boltons", "boltons", "timeutils.py"], "context_start_lineno": 61, "line_no": 74, "id": "boltons.timeutils.total_seconds", "target_function_prompt": "def total_seconds(td):", "function_signature": "def total_seconds(td):"}}
{"prompt": "def get_all(type_obj, include_subtypes=True):", "metadata": {"task_id": "Utilities/boltons/10", "ground_truth": "    if not isinstance(type_obj, type):\n        raise TypeError('expected a type, not %r' % type_obj)\n    try:\n        type_is_tracked = gc.is_tracked(type_obj)\n    except AttributeError:\n        type_is_tracked = False  # Python 2.6 and below don't get the speedup\n    if type_is_tracked:\n        to_check = gc.get_referrers(type_obj)\n    else:\n        to_check = gc.get_objects()\n\n    if include_subtypes:\n        ret = [x for x in to_check if isinstance(x, type_obj)]\n    else:\n        ret = [x for x in to_check if type(x) is type_obj]\n    return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "gcutils.py"], "context_start_lineno": 70, "line_no": 103, "id": "boltons.gcutils.get_all", "target_function_prompt": "def get_all(type_obj, include_subtypes=True):", "function_signature": "def get_all(type_obj, include_subtypes=True):"}}
{"prompt": "def daterange(start, stop, step=1, inclusive=False):", "metadata": {"task_id": "Utilities/boltons/11", "ground_truth": "    if not isinstance(start, date):\n        raise TypeError(\"start expected datetime.date instance\")\n    if stop and not isinstance(stop, date):\n        raise TypeError(\"stop expected datetime.date instance or None\")\n    try:\n        y_step, m_step, d_step = step\n    except TypeError:\n        y_step, m_step, d_step = 0, 0, step\n    else:\n        y_step, m_step = int(y_step), int(m_step)\n    if isinstance(d_step, int):\n        d_step = timedelta(days=int(d_step))\n    elif isinstance(d_step, timedelta):\n        pass\n    else:\n        raise ValueError('step expected int, timedelta, or tuple'\n                         ' (year, month, day), not: %r' % step)\n    \n    m_step += y_step * 12\n\n    if stop is None:\n        finished = lambda now, stop: False\n    elif start <= stop:\n        finished = operator.gt if inclusive else operator.ge\n    else:\n        finished = operator.lt if inclusive else operator.le\n    now = start\n\n    while not finished(now, stop):\n        yield now\n        if m_step:\n            m_y_step, cur_month = divmod((now.month - 1) + m_step, 12)\n            now = now.replace(year=now.year + m_y_step,\n                              month=(cur_month + 1))\n        now = now + d_step\n    return", "fpath_tuple": ["Utilities", "boltons", "boltons", "timeutils.py"], "context_start_lineno": 311, "line_no": 362, "id": "boltons.timeutils.daterange", "target_function_prompt": "def daterange(start, stop, step=1, inclusive=False):", "function_signature": "def daterange(start, stop, step=1, inclusive=False):"}}
{"prompt": "def clamp(x, lower=float('-inf'), upper=float('inf')):", "metadata": {"task_id": "Utilities/boltons/12", "ground_truth": "    if upper < lower:\n        raise ValueError('expected upper bound (%r) >= lower bound (%r)'\n                         % (upper, lower))\n    return min(max(x, lower), upper)", "fpath_tuple": ["Utilities", "boltons", "boltons", "mathutils.py"], "context_start_lineno": 42, "line_no": 68, "id": "boltons.mathutils.clamp", "target_function_prompt": "def clamp(x, lower=float('-inf'), upper=float('inf')):", "function_signature": "def clamp(x, lower=float('-inf'), upper=float('inf')):"}}
{"prompt": "def ceil(x, options=None):", "metadata": {"task_id": "Utilities/boltons/13", "ground_truth": "    if options is None:\n        return _ceil(x)\n    options = sorted(options)\n    i = bisect.bisect_left(options, x)\n    if i == len(options):\n        raise ValueError(\"no ceil options greater than or equal to: %r\" % x)\n    return options[i]", "fpath_tuple": ["Utilities", "boltons", "boltons", "mathutils.py"], "context_start_lineno": 74, "line_no": 90, "id": "boltons.mathutils.ceil", "target_function_prompt": "def ceil(x, options=None):", "function_signature": "def ceil(x, options=None):"}}
{"prompt": "def get_format_args(fstr):", "metadata": {"task_id": "Utilities/boltons/14", "ground_truth": "    formatter = Formatter()\n    fargs, fkwargs, _dedup = [], [], set()\n\n    def _add_arg(argname, type_char='s'):\n        if argname not in _dedup:\n            _dedup.add(argname)\n            argtype = _TYPE_MAP.get(type_char, str)  # TODO: unicode\n            try:\n                fargs.append((int(argname), argtype))\n            except ValueError:\n                fkwargs.append((argname, argtype))\n\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if fname is not None:\n            type_char = fspec[-1:]\n            fname_list = re.split('[.[]', fname)\n            if len(fname_list) > 1:\n                raise ValueError('encountered compound format arg: %r' % fname)\n            try:\n                base_fname = fname_list[0]\n                assert base_fname\n            except (IndexError, AssertionError):\n                raise ValueError('encountered anonymous positional argument')\n            _add_arg(fname, type_char)\n            for sublit, subfname, _, _ in formatter.parse(fspec):\n                # TODO: positional and anon args not allowed here.\n                if subfname is not None:\n                    _add_arg(subfname)\n    return fargs, fkwargs", "fpath_tuple": ["Utilities", "boltons", "boltons", "formatutils.py"], "context_start_lineno": 155, "line_no": 171, "id": "boltons.formatutils.get_format_args", "target_function_prompt": "def get_format_args(fstr):", "function_signature": "def get_format_args(fstr):"}}
{"prompt": "def floor(x, options=None):", "metadata": {"task_id": "Utilities/boltons/15", "ground_truth": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n\n    i = bisect.bisect_right(options, x)\n    if not i:\n        raise ValueError(\"no floor options less than or equal to: %r\" % x)\n    return options[i - 1]", "fpath_tuple": ["Utilities", "boltons", "boltons", "mathutils.py"], "context_start_lineno": 99, "line_no": 116, "id": "boltons.mathutils.floor", "target_function_prompt": "def floor(x, options=None):", "function_signature": "def floor(x, options=None):"}}
{"prompt": "    def setdefault(self, key, default=None):", "metadata": {"task_id": "Utilities/boltons/16", "ground_truth": "        if key not in self:\n            self[key] = default\n        return self[key]", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 868, "line_no": 869, "id": "boltons.dictutils.OneToOne.setdefault", "target_function_prompt": "    def setdefault(self, key, default=None):", "function_signature": "    def setdefault(self, key, default=None):"}}
{"prompt": "    def update(self, dict_or_iterable, **kw):", "metadata": {"task_id": "Utilities/boltons/17", "ground_truth": "        if isinstance(dict_or_iterable, dict):\n            for val in dict_or_iterable.values():\n                hash(val)\n                keys_vals = list(dict_or_iterable.items())\n        else:\n            for key, val in dict_or_iterable:\n                hash(key)\n                hash(val)\n                keys_vals = list(dict_or_iterable)\n        for val in kw.values():\n            hash(val)\n        keys_vals.extend(kw.items())\n        for key, val in keys_vals:\n            self[key] = val", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 873, "line_no": 874, "id": "boltons.dictutils.OneToOne.update", "target_function_prompt": "    def update(self, dict_or_iterable, **kw):", "function_signature": "    def update(self, dict_or_iterable, **kw):"}}
{"prompt": "    def get(self, key, default=frozenset()):", "metadata": {"task_id": "Utilities/boltons/18", "ground_truth": "        try:\n            return self[key]\n        except KeyError:\n            return default", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 919, "line_no": 920, "id": "boltons.dictutils.ManyToMany.get", "target_function_prompt": "    def get(self, key, default=frozenset()):", "function_signature": "    def get(self, key, default=frozenset()):"}}
{"prompt": "    def updated(self, *a, **kw):", "metadata": {"task_id": "Utilities/boltons/19", "ground_truth": "        data = dict(self)\n        data.update(*a, **kw)\n        return type(self)(data)", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 1070, "line_no": 1075, "id": "boltons.dictutils.FrozenDict.updated", "target_function_prompt": "    def updated(self, *a, **kw):", "function_signature": "    def updated(self, *a, **kw):"}}
{"prompt": "def subdict(d, keep=None, drop=None):", "metadata": {"task_id": "Utilities/boltons/20", "ground_truth": "    if keep is None:\n        keep = d.keys()\n    if drop is None:\n        drop = []\n\n    keys = set(keep) - set(drop)\n\n    return type(d)([(k, v) for k, v in d.items() if k in keys])", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 1019, "line_no": 1041, "id": "boltons.dictutils.subdict", "target_function_prompt": "def subdict(d, keep=None, drop=None):", "function_signature": "def subdict(d, keep=None, drop=None):"}}
{"prompt": "    def __repr__(self):", "metadata": {"task_id": "Utilities/boltons/21", "ground_truth": "        cn = self.__class__.__name__\n        return '%s(%s)' % (cn, dict.__repr__(self))", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 1084, "line_no": 1085, "id": "boltons.dictutils.FrozenDict.__repr__", "target_function_prompt": "    def __repr__(self):", "function_signature": "    def __repr__(self):"}}
{"prompt": "def validate_callable(arity):", "metadata": {"task_id": "Utilities/gunicorn/0", "ground_truth": "    def _validate_callable(val):\n        if isinstance(val, str):\n            try:\n                mod_name, obj_name = val.rsplit(\".\", 1)\n            except ValueError:\n                raise TypeError(\"Value '%s' is not import string. \"\n                                \"Format: module[.submodules...].object\" % val)\n            try:\n                mod = __import__(mod_name, fromlist=[obj_name])\n                val = getattr(mod, obj_name)\n            except ImportError as e:\n                raise TypeError(str(e))\n            except AttributeError:\n                raise TypeError(\"Can not load '%s' from '%s'\"\n                                \"\" % (obj_name, mod_name))\n        if not callable(val):\n            raise TypeError(\"Value is not callable: %s\" % val)\n        if arity != -1 and arity != util.get_arity(val):\n            raise TypeError(\"Value must have an arity of: %s\" % arity)\n        return val\n    return _validate_callable", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "config.py"], "context_start_lineno": 419, "line_no": 420, "id": "gunicorn.config.validate_callable", "target_function_prompt": "def validate_callable(arity):", "function_signature": "def validate_callable(arity):"}}
{"prompt": "def get_default_config_file():", "metadata": {"task_id": "Utilities/gunicorn/1", "ground_truth": "    config_path = os.path.join(os.path.abspath(os.getcwd()),\n                               'gunicorn.conf.py')\n    if os.path.exists(config_path):\n        return config_path\n    return None", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "config.py"], "context_start_lineno": 528, "line_no": 529, "id": "gunicorn.config.get_default_config_file", "target_function_prompt": "def get_default_config_file():", "function_signature": "def get_default_config_file():"}}
{"prompt": "def is_ipv6(addr):", "metadata": {"task_id": "Utilities/gunicorn/2", "ground_truth": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n    except socket.error:  # not a valid address\n        return False\n    except ValueError:  # ipv6 not supported on this platform\n        return False\n    return True", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "util.py"], "context_start_lineno": 216, "line_no": 217, "id": "gunicorn.util.is_ipv6", "target_function_prompt": "def is_ipv6(addr):", "function_signature": "def is_ipv6(addr):"}}
{"prompt": "def listen_fds(unset_environment=True):", "metadata": {"task_id": "Utilities/gunicorn/3", "ground_truth": "    fds = int(os.environ.get('LISTEN_FDS', 0))\n    listen_pid = int(os.environ.get('LISTEN_PID', 0))\n\n    if listen_pid != os.getpid():\n        return 0\n\n    if unset_environment:\n        os.environ.pop('LISTEN_PID', None)\n        os.environ.pop('LISTEN_FDS', None)\n\n    return fds", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "systemd.py"], "context_start_lineno": 11, "line_no": 35, "id": "gunicorn.systemd.listen_fds", "target_function_prompt": "def listen_fds(unset_environment=True):", "function_signature": "def listen_fds(unset_environment=True):"}}
{"prompt": "def http_date(timestamp=None):", "metadata": {"task_id": "Utilities/gunicorn/4", "ground_truth": "    if timestamp is None:\n        timestamp = time.time()\n    s = email.utils.formatdate(timestamp, localtime=False, usegmt=True)\n    return s", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "util.py"], "context_start_lineno": 459, "line_no": 461, "id": "gunicorn.util.http_date", "target_function_prompt": "def http_date(timestamp=None):", "function_signature": "def http_date(timestamp=None):"}}
{"prompt": "def parse_address(netloc, default_port='8000'):", "metadata": {"task_id": "Utilities/gunicorn/5", "ground_truth": "    if re.match(r'unix:(//)?', netloc):\n        return re.split(r'unix:(//)?', netloc)[-1]\n\n    if netloc.startswith(\"fd://\"):\n        fd = netloc[5:]\n        try:\n            return int(fd)\n        except ValueError:\n            raise RuntimeError(\"%r is not a valid file descriptor.\" % fd) from None\n\n    if netloc.startswith(\"tcp://\"):\n        netloc = netloc.split(\"tcp://\")[1]\n    host, port = netloc, default_port\n\n    if '[' in netloc and ']' in netloc:\n        host = netloc.split(']')[0][1:]\n        port = (netloc.split(']:') + [default_port])[1]\n    elif ':' in netloc:\n        host, port = (netloc.split(':') + [default_port])[:2]\n    elif netloc == \"\":\n        host, port = \"0.0.0.0\", default_port\n\n    try:\n        port = int(port)\n    except ValueError:\n        raise RuntimeError(\"%r is not a valid port number.\" % port)\n\n    return host.lower(), port", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "util.py"], "context_start_lineno": 226, "line_no": 227, "id": "gunicorn.util.parse_address", "target_function_prompt": "def parse_address(netloc, default_port='8000'):", "function_signature": "def parse_address(netloc, default_port='8000'):"}}
{"prompt": "def to_bytestring(value, encoding=\"utf8\"):", "metadata": {"task_id": "Utilities/gunicorn/6", "ground_truth": "    if isinstance(value, bytes):\n        return value\n    if not isinstance(value, str):\n        raise TypeError('%r is not a string' % value)\n\n    return value.encode(encoding)", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "util.py"], "context_start_lineno": 572, "line_no": 574, "id": "gunicorn.util.to_bytestring", "target_function_prompt": "def to_bytestring(value, encoding=\"utf8\"):", "function_signature": "def to_bytestring(value, encoding=\"utf8\"):"}}
{"prompt": "def warn(msg):", "metadata": {"task_id": "Utilities/gunicorn/7", "ground_truth": "    print(\"!!!\", file=sys.stderr)\n\n    lines = msg.splitlines()\n    for i, line in enumerate(lines):\n        if i == 0:\n            line = \"WARNING: %s\" % line\n        print(\"!!! %s\" % line, file=sys.stderr)\n\n    print(\"!!!\\n\", file=sys.stderr)\n    sys.stderr.flush()", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "util.py"], "context_start_lineno": 595, "line_no": 596, "id": "gunicorn.util.warn", "target_function_prompt": "def warn(msg):", "function_signature": "def warn(msg):"}}
{"prompt": "def split_request_uri(uri):", "metadata": {"task_id": "Utilities/gunicorn/8", "ground_truth": "    if uri.startswith(\"//\"):\n        # When the path starts with //, urlsplit considers it as a\n        # relative uri while the RFC says we should consider it as abs_path\n        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.2\n        # We use temporary dot prefix to workaround this behaviour\n        parts = urllib.parse.urlsplit(\".\" + uri)\n        return parts._replace(path=parts.path[1:])\n\n    return urllib.parse.urlsplit(uri)", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "util.py"], "context_start_lineno": 621, "line_no": 622, "id": "gunicorn.util.split_request_uri", "target_function_prompt": "def split_request_uri(uri):", "function_signature": "def split_request_uri(uri):"}}
{"prompt": "    def after(self) -> Optional[Any]:", "metadata": {"task_id": "Utilities/praw/0", "ground_truth": "        if not getattr(self, \"has_next_page\", True):\n            return None\n        return getattr(self, \"end_cursor\", None)", "fpath_tuple": ["Utilities", "praw", "praw", "models", "listing", "listing.py"], "context_start_lineno": 45, "line_no": 47, "id": "praw.models.listing.listing.ModNoteListing.after", "target_function_prompt": "    def after(self) -> Optional[Any]:", "function_signature": "    def after(self) -> Optional[Any]:"}}
{"prompt": "def permissions_string(\n    *, known_permissions: Set[str], permissions: Optional[List[str]]\n) -> str:", "metadata": {"task_id": "Utilities/praw/1", "ground_truth": "    if permissions is None:\n        to_set = [\"+all\"]\n    else:\n        to_set = [\"-all\"]\n        omitted = sorted(known_permissions - set(permissions))\n        to_set.extend(f\"-{x}\" for x in omitted)\n        to_set.extend(f\"+{x}\" for x in permissions)\n    return \",\".join(to_set)", "fpath_tuple": ["Utilities", "praw", "praw", "models", "util.py"], "context_start_lineno": 10, "line_no": 24, "id": "praw.models.util.permissions_string", "target_function_prompt": "def permissions_string(\n    *, known_permissions: Set[str], permissions: Optional[List[str]]\n) -> str:", "function_signature": "def permissions_string(\n    *, known_permissions: Set[str], permissions: Optional[List[str]]\n) -> str:"}}
{"prompt": "    def json_out(self) -> str:", "metadata": {"task_id": "Utilities/jc/0", "ground_truth": "        import json\n\n        if self.pretty:\n            self.json_indent = 2\n            self.json_separators = None\n\n        j_string = json.dumps(\n            self.data_out,\n            indent=self.json_indent,\n            separators=self.json_separators,\n            ensure_ascii=self.ascii_only\n        )\n\n        if not self.mono:\n            class JcStyle(Style):\n                styles: CustomColorType = self.custom_colors\n\n            return str(highlight(j_string, JsonLexer(), Terminal256Formatter(style=JcStyle))[0:-1])\n\n        return j_string", "fpath_tuple": ["Utilities", "jc", "jc", "cli.py"], "context_start_lineno": 381, "line_no": 386, "id": "jc.cli.JcCli.json_out", "target_function_prompt": "    def json_out(self) -> str:", "function_signature": "    def json_out(self) -> str:"}}
{"prompt": "def transform_dep_for_pip(dependency):", "metadata": {"task_id": "Utilities/python-for-android/0", "ground_truth": "    if dependency.find(\"@\") > 0 and (\n            dependency.find(\"@\") < dependency.find(\"://\") or\n            \"://\" not in dependency\n            ):\n        # WORKAROUND FOR UPSTREAM BUG:\n        # https://github.com/pypa/pip/issues/6097\n        # (Please REMOVE workaround once that is fixed & released upstream!)\n        #\n        # Basically, setup_requires() can contain a format pip won't install\n        # from a requirements.txt (PEP 508 URLs).\n        # To avoid this, translate to an #egg= reference:\n        if dependency.endswith(\"#\"):\n            dependency = dependency[:-1]\n        url = (dependency.partition(\"@\")[2].strip().partition(\"#egg\")[0] +\n               \"#egg=\" +\n               dependency.partition(\"@\")[0].strip()\n              )\n        return url\n    return dependency", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "pythonpackage.py"], "context_start_lineno": 54, "line_no": 55, "id": "pythonforandroid.pythonpackage.transform_dep_for_pip", "target_function_prompt": "def transform_dep_for_pip(dependency):", "function_signature": "def transform_dep_for_pip(dependency):"}}
{"prompt": "def fix_deplist(deps):", "metadata": {"task_id": "Utilities/python-for-android/1", "ground_truth": "    deps = [\n        ((dep.lower(),)\n         if not isinstance(dep, (list, tuple))\n         else tuple([dep_entry.lower()\n                     for dep_entry in dep\n                    ]))\n        for dep in deps\n    ]\n    return deps", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "graph.py"], "context_start_lineno": 9, "line_no": 13, "id": "pythonforandroid.graph.fix_deplist", "target_function_prompt": "def fix_deplist(deps):", "function_signature": "def fix_deplist(deps):"}}
{"prompt": "def walk_valid_filens(base_dir, invalid_dir_names, invalid_file_patterns):", "metadata": {"task_id": "Utilities/python-for-android/2", "ground_truth": "    for dirn, subdirs, filens in walk(base_dir):\n\n        # Remove invalid subdirs so that they will not be walked\n        for i in reversed(range(len(subdirs))):\n            subdir = subdirs[i]\n            if subdir in invalid_dir_names:\n                subdirs.pop(i)\n\n        for filen in filens:\n            for pattern in invalid_file_patterns:\n                if fnmatch(filen, pattern):\n                    break\n            else:\n                yield join(dirn, filen)", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "util.py"], "context_start_lineno": 47, "line_no": 61, "id": "pythonforandroid.util.walk_valid_filens", "target_function_prompt": "def walk_valid_filens(base_dir, invalid_dir_names, invalid_file_patterns):", "function_signature": "def walk_valid_filens(base_dir, invalid_dir_names, invalid_file_patterns):"}}
{"prompt": "def _cmp_bootstraps_by_priority(a, b):", "metadata": {"task_id": "Utilities/python-for-android/3", "ground_truth": "    def rank_bootstrap(bootstrap):\n        \"\"\" Returns a ranking index for each bootstrap,\n            with higher priority ranked with higher number. \"\"\"\n        if bootstrap.name in default_recipe_priorities:\n            return default_recipe_priorities.index(bootstrap.name) + 1\n        return 0\n\n    # Rank bootstraps in order:\n    rank_a = rank_bootstrap(a)\n    rank_b = rank_bootstrap(b)\n    if rank_a != rank_b:\n        return (rank_b - rank_a)\n    else:\n        if a.name < b.name:  # alphabetic sort for determinism\n            return -1\n        else:\n            return 1", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "bootstrap.py"], "context_start_lineno": 49, "line_no": 50, "id": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "target_function_prompt": "def _cmp_bootstraps_by_priority(a, b):", "function_signature": "def _cmp_bootstraps_by_priority(a, b):"}}
{"prompt": "    def all_bootstraps(cls):", "metadata": {"task_id": "Utilities/python-for-android/4", "ground_truth": "        forbidden_dirs = ('__pycache__', 'common')\n        bootstraps_dir = join(dirname(__file__), 'bootstraps')\n        result = set()\n        for name in listdir(bootstraps_dir):\n            if name in forbidden_dirs:\n                continue\n            filen = join(bootstraps_dir, name)\n            if isdir(filen):\n                result.add(name)\n        return result", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "bootstrap.py"], "context_start_lineno": 193, "line_no": 195, "id": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "target_function_prompt": "    def all_bootstraps(cls):", "function_signature": "    def all_bootstraps(cls):"}}
{"prompt": "def _convert_input_type_range(img: np.ndarray) -> np.ndarray:", "metadata": {"task_id": "Utilities/mmcv/0", "ground_truth": "    img_type = img.dtype\n    img = img.astype(np.float32)\n    if img_type == np.float32:\n        pass\n    elif img_type == np.uint8:\n        img /= 255.\n    else:\n        raise TypeError('The img type should be np.float32 or np.uint8, '\n                        f'but got {img_type}')\n    return img", "fpath_tuple": ["Utilities", "mmcv", "mmcv", "image", "colorspace.py"], "context_start_lineno": 85, "line_no": 101, "id": "mmcv.image.colorspace._convert_input_type_range", "target_function_prompt": "def _convert_input_type_range(img: np.ndarray) -> np.ndarray:", "function_signature": "def _convert_input_type_range(img: np.ndarray) -> np.ndarray:"}}
{"prompt": "def error(message):", "metadata": {"task_id": "Utilities/mackup/0", "ground_truth": "    fail = \"\\033[91m\"\n    end = \"\\033[0m\"\n    sys.exit(fail + \"Error: {}\".format(message) + end)", "fpath_tuple": ["Utilities", "mackup", "mackup", "utils.py"], "context_start_lineno": 183, "line_no": 190, "id": "mackup.utils.error", "target_function_prompt": "def error(message):", "function_signature": "def error(message):"}}
{"prompt": "def _convert_output_type_range(\n        img: np.ndarray, dst_type: Union[np.uint8, np.float32]) -> np.ndarray:", "metadata": {"task_id": "Utilities/mmcv/1", "ground_truth": "    if dst_type not in (np.uint8, np.float32):\n        raise TypeError('The dst_type should be np.float32 or np.uint8, '\n                        f'but got {dst_type}')\n    if dst_type == np.uint8:\n        img = img.round()\n    else:\n        img /= 255.\n    return img.astype(dst_type)", "fpath_tuple": ["Utilities", "mmcv", "mmcv", "image", "colorspace.py"], "context_start_lineno": 113, "line_no": 135, "id": "mmcv.image.colorspace._convert_output_type_range", "target_function_prompt": "def _convert_output_type_range(\n        img: np.ndarray, dst_type: Union[np.uint8, np.float32]) -> np.ndarray:", "function_signature": "def _convert_output_type_range(\n        img: np.ndarray, dst_type: Union[np.uint8, np.float32]) -> np.ndarray:"}}
{"prompt": "def is_process_running(process_name):", "metadata": {"task_id": "Utilities/mackup/1", "ground_truth": "    is_running = False\n\n    # On systems with pgrep, check if the given process is running\n    if os.path.isfile(\"/usr/bin/pgrep\"):\n        dev_null = open(os.devnull, \"wb\")\n        returncode = subprocess.call([\"/usr/bin/pgrep\", process_name], stdout=dev_null)\n        is_running = bool(returncode == 0)\n\n    return is_running", "fpath_tuple": ["Utilities", "mackup", "mackup", "utils.py"], "context_start_lineno": 300, "line_no": 310, "id": "mackup.utils.is_process_running", "target_function_prompt": "def is_process_running(process_name):", "function_signature": "def is_process_running(process_name):"}}
{"prompt": "def _get_pid_column(raw_conn):\n    # Some distros (e.g Debian) may inject their branding into server_version", "metadata": {"task_id": "Utilities/stellar/0", "ground_truth": "    server_version = raw_conn.execute('SHOW server_version;').first()[0]\n    version_string = re.search('^(\\d+\\.\\d+)', server_version).group(0)\n    version = [int(x) for x in version_string.split('.')]\n    return 'pid' if version >= [9, 2] else 'procpid'", "fpath_tuple": ["Utilities", "stellar", "stellar", "operations.py"], "context_start_lineno": 27, "line_no": 29, "id": "stellar.operations._get_pid_column", "target_function_prompt": "def _get_pid_column(raw_conn):\n    # Some distros (e.g Debian) may inject their branding into server_version", "function_signature": "def _get_pid_column(raw_conn):\n    # Some distros (e.g Debian) may inject their branding into server_version"}}
{"prompt": "def encode(s: Union[str, bytes]) -> bytes:", "metadata": {"task_id": "Communications/IMAPClient/0", "ground_truth": "    if not isinstance(s, str):\n        return s\n\n    res = bytearray()\n\n    b64_buffer: List[str] = []\n\n    def consume_b64_buffer(buf: List[str]) -> None:\n        \"\"\"\n        Consume the buffer by encoding it into a modified base 64 representation\n        and surround it with shift characters & and -\n        \"\"\"\n        if buf:\n            res.extend(b\"&\" + base64_utf7_encode(buf) + b\"-\")\n            del buf[:]\n\n    for c in s:\n        # printable ascii case should not be modified\n        o = ord(c)\n        if 0x20 <= o <= 0x7E:\n            consume_b64_buffer(b64_buffer)\n            # Special case: & is used as shift character so we need to escape it in ASCII\n            if o == 0x26:  # & = 0x26\n                res.extend(b\"&-\")\n            else:\n                res.append(o)\n\n        # Bufferize characters that will be encoded in base64 and append them later\n        # in the result, when iterating over ASCII character or the end of string\n        else:\n            b64_buffer.append(c)\n\n    # Consume the remaining buffer if the string finish with non-ASCII characters\n    consume_b64_buffer(b64_buffer)\n\n    return bytes(res)", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imap_utf7.py"], "context_start_lineno": 13, "line_no": 19, "id": "imapclient.imap_utf7.encode", "target_function_prompt": "def encode(s: Union[str, bytes]) -> bytes:", "function_signature": "def encode(s: Union[str, bytes]) -> bytes:"}}
{"prompt": "def _imapclient_version_string(vinfo: Tuple[int, int, int, str]) -> str:", "metadata": {"task_id": "Communications/IMAPClient/1", "ground_truth": "    major, minor, micro, releaselevel = vinfo\n    v = \"%d.%d.%d\" % (major, minor, micro)\n    if releaselevel != \"final\":\n        v += \"-\" + releaselevel\n    return v", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "version.py"], "context_start_lineno": 9, "line_no": 10, "id": "imapclient.version._imapclient_version_string", "target_function_prompt": "def _imapclient_version_string(vinfo: Tuple[int, int, int, str]) -> str:", "function_signature": "def _imapclient_version_string(vinfo: Tuple[int, int, int, str]) -> str:"}}
{"prompt": "def generate_key_data_from_nonce(server_nonce, new_nonce):", "metadata": {"task_id": "Communications/Telethon/0", "ground_truth": "    server_nonce = server_nonce.to_bytes(16, 'little', signed=True)\n    new_nonce = new_nonce.to_bytes(32, 'little', signed=True)\n    hash1 = sha1(new_nonce + server_nonce).digest()\n    hash2 = sha1(server_nonce + new_nonce).digest()\n    hash3 = sha1(new_nonce + new_nonce).digest()\n\n    key = hash1 + hash2[:12]\n    iv = hash2[12:20] + hash3 + new_nonce[:4]\n    return key, iv", "fpath_tuple": ["Communications", "Telethon", "telethon", "helpers.py"], "context_start_lineno": 270, "line_no": 272, "id": "telethon.helpers.generate_key_data_from_nonce", "target_function_prompt": "def generate_key_data_from_nonce(server_nonce, new_nonce):", "function_signature": "def generate_key_data_from_nonce(server_nonce, new_nonce):"}}
{"prompt": "def bytes_to_int(data):", "metadata": {"task_id": "Communications/hbmqtt/0", "ground_truth": "    try:\n        return int.from_bytes(data, byteorder='big')\n    except:\n        return data", "fpath_tuple": ["Communications", "hbmqtt", "hbmqtt", "codecs.py"], "context_start_lineno": 17, "line_no": 23, "id": "hbmqtt.codecs.bytes_to_int", "target_function_prompt": "def bytes_to_int(data):", "function_signature": "def bytes_to_int(data):"}}
{"prompt": "def display_error_if_present(response: Dict[str, Any], controller: Any) -> None:", "metadata": {"task_id": "Communications/zulip-term/0", "ground_truth": "    if response[\"result\"] == \"error\" and hasattr(controller, \"view\"):\n        controller.report_error([response[\"msg\"]])", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "helper.py"], "context_start_lineno": 657, "line_no": 658, "id": "zulipterminal.helper.display_error_if_present", "target_function_prompt": "def display_error_if_present(response: Dict[str, Any], controller: Any) -> None:", "function_signature": "def display_error_if_present(response: Dict[str, Any], controller: Any) -> None:"}}
{"prompt": "    def _decode_message_id(message_id: str) -> Optional[int]:", "metadata": {"task_id": "Communications/zulip-term/1", "ground_truth": "        try:\n            return int(message_id)\n        except ValueError:\n            return None", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "buttons.py"], "context_start_lineno": 454, "line_no": 458, "id": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "target_function_prompt": "    def _decode_message_id(message_id: str) -> Optional[int]:", "function_signature": "    def _decode_message_id(message_id: str) -> Optional[int]:"}}
{"prompt": "    def handle_narrow_link(self) -> None:", "metadata": {"task_id": "Communications/zulip-term/2", "ground_truth": "        parsed_link = self._parse_narrow_link(self.link)\n        error = self._validate_narrow_link(parsed_link)\n\n        if error:\n            self.controller.report_error([f\" {error}\"])\n        else:\n            self._switch_narrow_to(parsed_link)\n\n            # Exit pop-up if MessageLinkButton exists in one.\n            if self.controller.is_any_popup_open():\n                self.controller.exit_popup()", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "buttons.py"], "context_start_lineno": 607, "line_no": 612, "id": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "target_function_prompt": "    def handle_narrow_link(self) -> None:", "function_signature": "    def handle_narrow_link(self) -> None:"}}
{"prompt": "def color_properties(colors: Any, *prop: str) -> Any:", "metadata": {"task_id": "Communications/zulip-term/3", "ground_truth": "    prop_n = \"_\".join([p.upper() for p in prop])\n    prop_v = \" , \".join([p.lower() for p in prop])\n    updated_colors: Any = Enum(  # type: ignore # Ref: python/mypy#529, #535 and #5317\n        \"Color\",\n        {\n            **{c.name: c.value for c in colors},\n            **{c.name + f\"__{prop_n}\": c.value + f\" , {prop_v}\" for c in colors},\n        },\n    )\n    return updated_colors", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "config", "color.py"], "context_start_lineno": 54, "line_no": 67, "id": "zulipterminal.config.color.color_properties", "target_function_prompt": "def color_properties(colors: Any, *prop: str) -> Any:", "function_signature": "def color_properties(colors: Any, *prop: str) -> Any:"}}
{"prompt": "def decimal(d: Optional[str]) -> Union[Decimal, str]:", "metadata": {"task_id": "Communications/twilio-fatisar/0", "ground_truth": "    if not d:\n        return d\n    return Decimal(d, BasicContext)", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "base", "deserialize.py"], "context_start_lineno": 55, "line_no": 60, "id": "twilio.base.deserialize.decimal", "target_function_prompt": "def decimal(d: Optional[str]) -> Union[Decimal, str]:", "function_signature": "def decimal(d: Optional[str]) -> Union[Decimal, str]:"}}
{"prompt": "def integer(i: str) -> Union[int, str]:", "metadata": {"task_id": "Communications/twilio-fatisar/1", "ground_truth": "    try:\n        return int(i)\n    except (TypeError, ValueError):\n        return i", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "base", "deserialize.py"], "context_start_lineno": 65, "line_no": 71, "id": "twilio.base.deserialize.integer", "target_function_prompt": "def integer(i: str) -> Union[int, str]:", "function_signature": "def integer(i: str) -> Union[int, str]:"}}
{"prompt": "def object(obj):", "metadata": {"task_id": "Communications/twilio-fatisar/2", "ground_truth": "    if isinstance(obj, dict) or isinstance(obj, list):\n        return json.dumps(obj)\n    return obj", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "base", "serialize.py"], "context_start_lineno": 63, "line_no": 68, "id": "twilio.base.serialize.object", "target_function_prompt": "def object(obj):", "function_signature": "def object(obj):"}}
{"prompt": "def map(lst, serialize_func):", "metadata": {"task_id": "Communications/twilio-fatisar/3", "ground_truth": "    if not isinstance(lst, list):\n        return lst\n    return [serialize_func(e) for e in lst]", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "base", "serialize.py"], "context_start_lineno": 73, "line_no": 77, "id": "twilio.base.serialize.map", "target_function_prompt": "def map(lst, serialize_func):", "function_signature": "def map(lst, serialize_func):"}}
{"prompt": "def deprecated_method(new_func=None):", "metadata": {"task_id": "Communications/twilio-fatisar/4", "ground_truth": "    def deprecated_method_wrapper(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            msg = \"Function method .{}() is deprecated\".format(func.__name__)\n            msg += (\n                \" in favor of .{}()\".format(new_func)\n                if isinstance(new_func, str)\n                else \"\"\n            )\n            warnings.warn(msg, DeprecationWarning)\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    if callable(new_func):\n        return deprecated_method_wrapper(new_func)\n\n    return deprecated_method_wrapper", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "base", "obsolete.py"], "context_start_lineno": 23, "line_no": 29, "id": "twilio.base.obsolete.deprecated_method", "target_function_prompt": "def deprecated_method(new_func=None):", "function_signature": "def deprecated_method(new_func=None):"}}
{"prompt": "def sample_indulgent(array, nb_items):", "metadata": {"task_id": "Communications/chatette/0", "ground_truth": "    if nb_items <= len(array):\n        return sample(array, nb_items)\n    return deepcopy(array)", "fpath_tuple": ["Communications", "chatette", "chatette", "utils.py"], "context_start_lineno": 86, "line_no": 92, "id": "chatette.utils.sample_indulgent", "target_function_prompt": "def sample_indulgent(array, nb_items):", "function_signature": "def sample_indulgent(array, nb_items):"}}
{"prompt": "def rchop(string, ending):", "metadata": {"task_id": "Communications/chatette/1", "ground_truth": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string", "fpath_tuple": ["Communications", "chatette", "chatette", "utils.py"], "context_start_lineno": 97, "line_no": 99, "id": "chatette.utils.rchop", "target_function_prompt": "def rchop(string, ending):", "function_signature": "def rchop(string, ending):"}}
{"prompt": "def str_to_bool(text):", "metadata": {"task_id": "Communications/chatette/2", "ground_truth": "    text = text.lower()\n    if text == \"true\":\n        return True\n    if text == \"false\":\n        return False\n    raise ValueError(\"Cannot convert '\" + str(text) + \"' into a boolean\")", "fpath_tuple": ["Communications", "chatette", "chatette", "utils.py"], "context_start_lineno": 104, "line_no": 109, "id": "chatette.utils.str_to_bool", "target_function_prompt": "def str_to_bool(text):", "function_signature": "def str_to_bool(text):"}}
{"prompt": "def min_if_exist(n1, n2):", "metadata": {"task_id": "Communications/chatette/3", "ground_truth": "    if n1 is None and n2 is None:\n        return None\n    elif n1 is None:\n        return n2\n    elif n2 is None:\n        return n1\n    return min(n1, n2)", "fpath_tuple": ["Communications", "chatette", "chatette", "utils.py"], "context_start_lineno": 121, "line_no": 126, "id": "chatette.utils.min_if_exist", "target_function_prompt": "def min_if_exist(n1, n2):", "function_signature": "def min_if_exist(n1, n2):"}}
{"prompt": "def append_to_list_in_dict(dict_of_lists, key, value):", "metadata": {"task_id": "Communications/chatette/4", "ground_truth": "    if key not in dict_of_lists:\n        dict_of_lists[key] = [value]\n    else:\n        dict_of_lists[key].append(value)", "fpath_tuple": ["Communications", "chatette", "chatette", "utils.py"], "context_start_lineno": 142, "line_no": 148, "id": "chatette.utils.append_to_list_in_dict", "target_function_prompt": "def append_to_list_in_dict(dict_of_lists, key, value):", "function_signature": "def append_to_list_in_dict(dict_of_lists, key, value):"}}
{"prompt": "def extend_list_in_dict(dict_of_lists, key, values):", "metadata": {"task_id": "Communications/chatette/5", "ground_truth": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)", "fpath_tuple": ["Communications", "chatette", "chatette", "utils.py"], "context_start_lineno": 153, "line_no": 159, "id": "chatette.utils.extend_list_in_dict", "target_function_prompt": "def extend_list_in_dict(dict_of_lists, key, values):", "function_signature": "def extend_list_in_dict(dict_of_lists, key, values):"}}
{"prompt": "    def _is_end_regex(word):", "metadata": {"task_id": "Communications/chatette/6", "ground_truth": "        return \\\n            word.endswith(\"/\") or word.endswith(\"/g\") \\\n            or word.endswith(\"/i\") or word.endswith(\"/ig\") \\\n            or word.endswith(\"/gi\")", "fpath_tuple": ["Communications", "chatette", "chatette", "cli", "interactive_commands", "command_strategy.py"], "context_start_lineno": 86, "line_no": 88, "id": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "target_function_prompt": "    def _is_end_regex(word):", "function_signature": "    def _is_end_regex(word):"}}
{"prompt": "    def execute(self):", "metadata": {"task_id": "Communications/chatette/7", "ground_truth": "        if len(self.command_tokens) < 3:\n            self.print_wrapper.error_log(\n                \"Missing some arguments\\nUsage: \" + self.usage_str\n            )\n            return\n\n        unit_type = CommandStrategy.get_unit_type_from_str(self.command_tokens[1])\n        if unit_type is None:\n            self.print_wrapper.error_log(\n                \"Unknown unit type: '\" + str(self.command_tokens[1]) + \"'.\"\n            )\n            return\n\n        unit_regex = self.get_regex_name(self.command_tokens[2])\n        if unit_regex is None:\n            try:\n                [unit_name, variation_name] = \\\n                    CommandStrategy.split_exact_unit_name(\n                        self.command_tokens[2]\n                    )\n            except SyntaxError:\n                self.print_wrapper.error_log(\n                    \"Unit identifier couldn't be interpreted. \" + \\\n                    \"Did you mean to escape some hashtags '#'?\")\n                return\n            self.execute_on_unit(unit_type, unit_name, variation_name)\n        else:\n            count = 0\n            for unit_name in self.next_matching_unit_name(\n                unit_type, unit_regex\n            ):\n                self.execute_on_unit(unit_type, unit_name)\n                count += 1\n            if count == 0:\n                self.print_wrapper.write(\"No \" + unit_type.name + \" matched.\")\n        self.finish_execution()", "fpath_tuple": ["Communications", "chatette", "chatette", "cli", "interactive_commands", "command_strategy.py"], "context_start_lineno": 271, "line_no": 278, "id": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "target_function_prompt": "    def execute(self):", "function_signature": "    def execute(self):"}}
{"prompt": "def group_and_order_srv_records(all_records, rng=None):", "metadata": {"task_id": "Communications/aioxmpp/0", "ground_truth": "    rng = rng or random\n\n    all_records.sort(key=lambda x: x[:2])\n\n    for priority, records in itertools.groupby(\n            all_records,\n            lambda x: x[0]):\n\n        records = list(records)\n        total_weight = sum(\n            weight\n            for _, weight, _ in records)\n\n        while records:\n            if len(records) == 1:\n                yield records[0][-1]\n                break\n\n            value = rng.randint(0, total_weight)\n            running_weight_sum = 0\n            for i, (_, weight, addr) in enumerate(records):\n                running_weight_sum += weight\n                if running_weight_sum >= value:\n                    yield addr\n                    del records[i]\n                    total_weight -= weight\n                    break", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "network.py"], "context_start_lineno": 398, "line_no": 408, "id": "aioxmpp.network.group_and_order_srv_records", "target_function_prompt": "def group_and_order_srv_records(all_records, rng=None):", "function_signature": "def group_and_order_srv_records(all_records, rng=None):"}}
{"prompt": "    def get_feature(self, feature_cls, default=None):", "metadata": {"task_id": "Communications/aioxmpp/1", "ground_truth": "        try:\n            return self[feature_cls]\n        except KeyError:\n            return default", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "nonza.py"], "context_start_lineno": 271, "line_no": 278, "id": "aioxmpp.nonza.StreamFeatures.get_feature", "target_function_prompt": "    def get_feature(self, feature_cls, default=None):", "function_signature": "    def get_feature(self, feature_cls, default=None):"}}
{"prompt": "    def _context_factory_factory(self, logger, metadata, verifier):", "metadata": {"task_id": "Communications/aioxmpp/2", "ground_truth": "        def context_factory(transport):\n            ssl_context = metadata.ssl_context_factory()\n\n            if hasattr(ssl_context, \"set_alpn_protos\"):\n                try:\n                    ssl_context.set_alpn_protos([b'xmpp-client'])\n                except NotImplementedError:\n                    logger.warning(\n                        \"the underlying OpenSSL library does not support ALPN\"\n                    )\n            else:\n                logger.warning(\n                    \"OpenSSL.SSL.Context lacks set_alpn_protos - \"\n                    \"please update pyOpenSSL to a recent version\"\n                )\n\n            verifier.setup_context(ssl_context, transport)\n            return ssl_context\n        return context_factory", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "connector.py"], "context_start_lineno": 290, "line_no": 291, "id": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "target_function_prompt": "    def _context_factory_factory(self, logger, metadata, verifier):", "function_signature": "    def _context_factory_factory(self, logger, metadata, verifier):"}}
{"prompt": "def element_path(el, upto=None):", "metadata": {"task_id": "Communications/aioxmpp/3", "ground_truth": "    segments = []\n    parent = el.getparent()\n\n    while parent != upto:\n        similar = list(parent.iterchildren(el.tag))\n        index = similar.index(el)\n        segments.insert(0, (el.tag, index))\n        el = parent\n        parent = el.getparent()\n\n    base = \"/\" + el.tag\n    if segments:\n        return base + \"/\" + \"/\".join(\n            \"{}[{}]\".format(tag, index)\n            for tag, index in segments\n        )\n    return base", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "xmltestutils.py"], "context_start_lineno": 24, "line_no": 25, "id": "aioxmpp.xmltestutils.element_path", "target_function_prompt": "def element_path(el, upto=None):", "function_signature": "def element_path(el, upto=None):"}}
{"prompt": "    def fromstr(cls, s, *, strict=True):", "metadata": {"task_id": "Communications/aioxmpp/4", "ground_truth": "        nodedomain, sep, resource = s.partition(\"/\")\n        if not sep:\n            resource = None\n\n        localpart, sep, domain = nodedomain.partition(\"@\")\n        if not sep:\n            domain = localpart\n            localpart = None\n        return cls(localpart, domain, resource, strict=strict)", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "structs.py"], "context_start_lineno": 795, "line_no": 810, "id": "aioxmpp.structs.JID.fromstr", "target_function_prompt": "    def fromstr(cls, s, *, strict=True):", "function_signature": "    def fromstr(cls, s, *, strict=True):"}}
{"prompt": "def extract_python_dict_from_x509(x509):", "metadata": {"task_id": "Communications/aioxmpp/5", "ground_truth": "    result = {\n        \"subject\": (\n            ((\"commonName\", x509.get_subject().commonName),),\n        )\n    }\n\n    for ext_idx in range(x509.get_extension_count()):\n        ext = x509.get_extension(ext_idx)\n        sn = ext.get_short_name()\n        if sn != b\"subjectAltName\":\n            continue\n\n        data = pyasn1.codec.der.decoder.decode(\n            ext.get_data(),\n            asn1Spec=pyasn1_modules.rfc2459.SubjectAltName())[0]\n        for name in data:\n            dNSName = name.getComponentByPosition(2)\n            if dNSName is None:\n                continue\n\n            if hasattr(dNSName, \"isValue\") and not dNSName.isValue:\n                continue\n\n            result.setdefault(\"subjectAltName\", []).append(\n                (\"DNS\", str(dNSName))\n            )\n\n    return result", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "security_layer.py"], "context_start_lineno": 139, "line_no": 150, "id": "aioxmpp.security_layer.extract_python_dict_from_x509", "target_function_prompt": "def extract_python_dict_from_x509(x509):", "function_signature": "def extract_python_dict_from_x509(x509):"}}
{"prompt": "def extract_blob(x509):", "metadata": {"task_id": "Communications/aioxmpp/6", "ground_truth": "    return OpenSSL.crypto.dump_certificate(\n        OpenSSL.crypto.FILETYPE_ASN1,\n        x509)", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "security_layer.py"], "context_start_lineno": 180, "line_no": 186, "id": "aioxmpp.security_layer.extract_blob", "target_function_prompt": "def extract_blob(x509):", "function_signature": "def extract_blob(x509):"}}
{"prompt": "def blob_to_pyasn1(blob):", "metadata": {"task_id": "Communications/aioxmpp/7", "ground_truth": "    return pyasn1.codec.der.decoder.decode(\n        blob,\n        asn1Spec=pyasn1_modules.rfc2459.Certificate()\n    )[0]", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "security_layer.py"], "context_start_lineno": 191, "line_no": 197, "id": "aioxmpp.security_layer.blob_to_pyasn1", "target_function_prompt": "def blob_to_pyasn1(blob):", "function_signature": "def blob_to_pyasn1(blob):"}}
{"prompt": "def extract_pk_blob_from_pyasn1(pyasn1_struct):", "metadata": {"task_id": "Communications/aioxmpp/8", "ground_truth": "    pk = pyasn1_struct.getComponentByName(\n        \"tbsCertificate\"\n    ).getComponentByName(\n        \"subjectPublicKeyInfo\"\n    )\n\n    return pyasn1.codec.der.encoder.encode(pk)", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "security_layer.py"], "context_start_lineno": 203, "line_no": 209, "id": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "target_function_prompt": "def extract_pk_blob_from_pyasn1(pyasn1_struct):", "function_signature": "def extract_pk_blob_from_pyasn1(pyasn1_struct):"}}
{"prompt": "    def ASYNC_WITH_LOOP(cls, loop):", "metadata": {"task_id": "Communications/aioxmpp/9", "ground_truth": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def create_wrapper(f):\n            if not hasattr(f, \"__call__\"):\n                raise TypeError(\"must be callable, got {!r}\".format(f))\n            return functools.partial(cls._async_wrapper,\n                                     f,\n                                     loop)\n\n        return create_wrapper", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "callbacks.py"], "context_start_lineno": 388, "line_no": 389, "id": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "target_function_prompt": "    def ASYNC_WITH_LOOP(cls, loop):", "function_signature": "    def ASYNC_WITH_LOOP(cls, loop):"}}
{"prompt": "    def SPAWN_WITH_LOOP(cls, loop):", "metadata": {"task_id": "Communications/aioxmpp/10", "ground_truth": "        loop = asyncio.get_event_loop() if loop is None else loop\n\n        def spawn(f):\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be coroutine, got {!r}\".format(f))\n\n            def wrapper(args, kwargs):\n                task = asyncio.ensure_future(f(*args, **kwargs), loop=loop)\n                task.add_done_callback(\n                    functools.partial(\n                        log_spawned,\n                        logger,\n                    )\n                )\n                return True\n\n            return wrapper\n\n        return spawn", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "callbacks.py"], "context_start_lineno": 430, "line_no": 431, "id": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "target_function_prompt": "    def SPAWN_WITH_LOOP(cls, loop):", "function_signature": "    def SPAWN_WITH_LOOP(cls, loop):"}}
{"prompt": "def first_signal(*signals):", "metadata": {"task_id": "Communications/aioxmpp/11", "ground_truth": "    fut = asyncio.Future()\n    for signal in signals:\n        signal.connect(fut, signal.AUTO_FUTURE)\n    return fut", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "callbacks.py"], "context_start_lineno": 849, "line_no": 891, "id": "aioxmpp.callbacks.first_signal", "target_function_prompt": "def first_signal(*signals):", "function_signature": "def first_signal(*signals):"}}
{"prompt": "    def spawn(self, __groups, __coro_fun, *args, **kwargs):", "metadata": {"task_id": "Communications/aioxmpp/12", "ground_truth": "        __groups = set(__groups) | {()}\n\n        return asyncio.ensure_future(__coro_fun(*args, **kwargs))", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "tasks.py"], "context_start_lineno": 164, "line_no": 197, "id": "aioxmpp.tasks.TaskPool.spawn", "target_function_prompt": "    def spawn(self, __groups, __coro_fun, *args, **kwargs):", "function_signature": "    def spawn(self, __groups, __coro_fun, *args, **kwargs):"}}
{"prompt": "async def send_and_wait_for(xmlstream, send, wait_for,\n                            timeout=None,\n                            cb=None):", "metadata": {"task_id": "Communications/aioxmpp/13", "ground_truth": "    fut = asyncio.Future()\n    wait_for = list(wait_for)\n\n    def receive(obj):\n        nonlocal fut, stack\n        if cb is not None:\n            cb(obj)\n        fut.set_result(obj)\n        stack.close()\n\n    failure_future = xmlstream.error_future()\n\n    with contextlib.ExitStack() as stack:\n        for anticipated_cls in wait_for:\n            xmlstream.stanza_parser.add_class(\n                anticipated_cls,\n                receive)\n            stack.callback(\n                xmlstream.stanza_parser.remove_class,\n                anticipated_cls,\n            )\n\n        for to_send in send:\n            xmlstream.send_xso(to_send)\n\n        done, pending = await asyncio.wait(\n            [\n                fut,\n                failure_future,\n            ],\n            timeout=timeout,\n            return_when=asyncio.FIRST_COMPLETED,\n        )\n\n        for other_fut in pending:\n            other_fut.cancel()\n\n        if fut in done:\n            return fut.result()\n\n        if failure_future in done:\n            failure_future.result()\n        else:\n            failure_future.cancel()\n\n        raise TimeoutError()", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "protocol.py"], "context_start_lineno": 871, "line_no": 874, "id": "aioxmpp.protocol.send_and_wait_for", "target_function_prompt": "async def send_and_wait_for(xmlstream, send, wait_for,\n                            timeout=None,\n                            cb=None):", "function_signature": "async def send_and_wait_for(xmlstream, send, wait_for,\n                            timeout=None,\n                            cb=None):"}}
{"prompt": "def run_coroutine_with_peer(\n        coroutine,\n        peer_coroutine,\n        timeout=1.0,\n        loop=None):", "metadata": {"task_id": "Communications/aioxmpp/14", "ground_truth": "    loop = loop or asyncio.get_event_loop()\n\n    local_future = asyncio.ensure_future(coroutine, loop=loop)\n    remote_future = asyncio.ensure_future(peer_coroutine, loop=loop)\n\n    done, pending = loop.run_until_complete(\n        asyncio.wait(\n            [\n                local_future,\n                remote_future,\n            ],\n            timeout=timeout,\n            return_when=asyncio.FIRST_EXCEPTION)\n    )\n    if not done:\n        raise asyncio.TimeoutError(\"Test timed out\")\n\n    if pending:\n        pending_fut = next(iter(pending))\n        pending_fut.cancel()\n        fut = next(iter(done))\n        try:\n            fut.result()\n        except:  # NOQA: E722\n            # everything is fine, the other one failed\n            raise\n        else:\n            if pending_fut == remote_future:\n                raise asyncio.TimeoutError(\n                    \"Peer coroutine did not return in time\")\n            else:\n                raise asyncio.TimeoutError(\n                    \"Coroutine under test did not return in time\")\n\n    if local_future.exception():\n        # re-throw the error properly\n        local_future.result()\n\n    remote_future.result()\n    return local_future.result()", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "testutils.py"], "context_start_lineno": 90, "line_no": 95, "id": "aioxmpp.testutils.run_coroutine_with_peer", "target_function_prompt": "def run_coroutine_with_peer(\n        coroutine,\n        peer_coroutine,\n        timeout=1.0,\n        loop=None):", "function_signature": "def run_coroutine_with_peer(\n        coroutine,\n        peer_coroutine,\n        timeout=1.0,\n        loop=None):"}}
{"prompt": "def make_listener(instance):", "metadata": {"task_id": "Communications/aioxmpp/15", "ground_truth": "    result = unittest.mock.Mock([])\n    names = {\n        name\n        for type_ in type(instance).__mro__\n        for name in type_.__dict__\n    }\n    for name in names:\n        signal = getattr(instance, name)\n        if not isinstance(signal, callbacks.AdHocSignal):\n            continue\n        cb = unittest.mock.Mock()\n        setattr(result, name, cb)\n        cb.return_value = None\n        signal.connect(cb)\n    return result", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "testutils.py"], "context_start_lineno": 137, "line_no": 144, "id": "aioxmpp.testutils.make_listener", "target_function_prompt": "def make_listener(instance):", "function_signature": "def make_listener(instance):"}}
{"prompt": "    async def set_vcard(self, vcard, jid=None):", "metadata": {"task_id": "Communications/aioxmpp/16", "ground_truth": "        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            payload=vcard,\n            to=jid,\n        )\n        await self.client.send(iq)", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "vcard", "service.py"], "context_start_lineno": 68, "line_no": 85, "id": "aioxmpp.vcard.service.VCardService.set_vcard", "target_function_prompt": "    async def set_vcard(self, vcard, jid=None):", "function_signature": "    async def set_vcard(self, vcard, jid=None):"}}
{"prompt": "    def limit(self, max_):", "metadata": {"task_id": "Communications/aioxmpp/17", "ground_truth": "        if isinstance(self, type):\n            result = self()\n        else:\n            result = copy.deepcopy(self)\n        result.max_ = max_\n        return result", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "rsm", "xso.py"], "context_start_lineno": 216, "line_no": 231, "id": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "target_function_prompt": "    def limit(self, max_):", "function_signature": "    def limit(self, max_):"}}
{"prompt": "    def features(self):", "metadata": {"task_id": "Communications/aioxmpp/18", "ground_truth": "        return {\n            aioxmpp.im.conversation.ConversationFeature.BAN,\n            aioxmpp.im.conversation.ConversationFeature.BAN_WITH_KICK,\n            aioxmpp.im.conversation.ConversationFeature.KICK,\n            aioxmpp.im.conversation.ConversationFeature.SEND_MESSAGE,\n            aioxmpp.im.conversation.ConversationFeature.SEND_MESSAGE_TRACKED,\n            aioxmpp.im.conversation.ConversationFeature.SET_TOPIC,\n            aioxmpp.im.conversation.ConversationFeature.SET_NICK,\n            aioxmpp.im.conversation.ConversationFeature.INVITE,\n            aioxmpp.im.conversation.ConversationFeature.INVITE_DIRECT,\n        }", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "muc", "service.py"], "context_start_lineno": 1018, "line_no": 1025, "id": "aioxmpp.muc.service.Room.features", "target_function_prompt": "    def features(self):", "function_signature": "    def features(self):"}}
{"prompt": "    def eval_bool(self, expr):", "metadata": {"task_id": "Communications/aioxmpp/19", "ground_truth": "        result = expr.eval(self)\n        iterator = iter(result)\n        try:\n            next(iterator)\n        except StopIteration:\n            return False\n        else:\n            return True\n        finally:\n            if hasattr(iterator, \"close\"):\n                iterator.close()", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "xso", "query.py"], "context_start_lineno": 167, "line_no": 173, "id": "aioxmpp.xso.query.EvaluationContext.eval_bool", "target_function_prompt": "    def eval_bool(self, expr):", "function_signature": "    def eval_bool(self, expr):"}}
{"prompt": "    def eval(self, ec):", "metadata": {"task_id": "Communications/aioxmpp/20", "ground_truth": "        if self.eval_leaf(ec):\n            yield True", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "xso", "query.py"], "context_start_lineno": 340, "line_no": 341, "id": "aioxmpp.xso.query._BoolOpMixin.eval", "target_function_prompt": "    def eval(self, ec):", "function_signature": "    def eval(self, ec):"}}
{"prompt": "def drop_handler(ev_args):", "metadata": {"task_id": "Communications/aioxmpp/21", "ground_truth": "    depth = 1\n    while depth:\n        ev = yield\n        if ev[0] == \"start\":\n            depth += 1\n        elif ev[0] == \"end\":\n            depth -= 1", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "xso", "model.py"], "context_start_lineno": 2680, "line_no": 2681, "id": "aioxmpp.xso.model.drop_handler", "target_function_prompt": "def drop_handler(ev_args):", "function_signature": "def drop_handler(ev_args):"}}
{"prompt": "def guard(dest, ev_args):", "metadata": {"task_id": "Communications/aioxmpp/22", "ground_truth": "    depth = 1\n    try:\n        next(dest)\n        while True:\n            ev = yield\n            if ev[0] == \"start\":\n                depth += 1\n            elif ev[0] == \"end\":\n                depth -= 1\n            try:\n                dest.send(ev)\n            except StopIteration as exc:\n                return exc.value\n    finally:\n        while depth > 0:\n            ev_type, *_ = yield\n            if ev_type == \"end\":\n                depth -= 1\n            elif ev_type == \"start\":\n                depth += 1", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "xso", "model.py"], "context_start_lineno": 2701, "line_no": 2702, "id": "aioxmpp.xso.model.guard", "target_function_prompt": "def guard(dest, ev_args):", "function_signature": "def guard(dest, ev_args):"}}
{"prompt": "def capture_events(receiver, dest):", "metadata": {"task_id": "Communications/aioxmpp/23", "ground_truth": "    _i = iter(receiver)\n    try:\n        _y = next(_i)\n    except StopIteration as _e:\n        return _e.value\n\n    try:\n        while True:\n            try:\n                _s = yield _y\n            except GeneratorExit as _e:\n                try:\n                    _m = _i.close\n                except AttributeError:\n                    pass\n                else:\n                    _m()\n                    raise _e\n            except BaseException as _e:\n                _x = sys.exc_info()\n                try:\n                    _m = _i.throw\n                except AttributeError:\n                    raise _e\n                else:\n                    try:\n                        _y = _m(*_x)\n                    except StopIteration as _e:\n                        _r = _e.value\n                        break\n            else:\n                dest.append(_s)\n                try:\n                    if _s is None:\n                        _y = next(_i)\n                    else:\n                        _y = _i.send(_s)\n                except StopIteration as _e:\n                    _r = _e.value\n                    break\n    except:  # NOQA\n        dest.clear()\n        raise\n    return _r", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "xso", "model.py"], "context_start_lineno": 2735, "line_no": 2754, "id": "aioxmpp.xso.model.capture_events", "target_function_prompt": "def capture_events(receiver, dest):", "function_signature": "def capture_events(receiver, dest):"}}
{"prompt": "def events_to_sax(events, dest):", "metadata": {"task_id": "Communications/aioxmpp/24", "ground_truth": "    name_stack = []\n\n    for ev_type, *ev_args in events:\n        if ev_type == \"start\":\n            name = (ev_args[0], ev_args[1])\n            dest.startElementNS(name, None, ev_args[2])\n            name_stack.append(name)\n        elif ev_type == \"end\":\n            name = name_stack.pop()\n            dest.endElementNS(name, None)\n        elif ev_type == \"text\":\n            dest.characters(ev_args[0])", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "xso", "model.py"], "context_start_lineno": 2800, "line_no": 2805, "id": "aioxmpp.xso.model.events_to_sax", "target_function_prompt": "def events_to_sax(events, dest):", "function_signature": "def events_to_sax(events, dest):"}}
{"prompt": "    async def get_command_info(self, peer_jid, command_name):", "metadata": {"task_id": "Communications/aioxmpp/25", "ground_truth": "        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_info(\n            peer_jid,\n            node=command_name,\n        )\n        return response", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "adhoc", "service.py"], "context_start_lineno": 92, "line_no": 114, "id": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "target_function_prompt": "    async def get_command_info(self, peer_jid, command_name):", "function_signature": "    async def get_command_info(self, peer_jid, command_name):"}}
{"prompt": "def build_identities_string(identities):", "metadata": {"task_id": "Communications/aioxmpp/26", "ground_truth": "    identities = [\n        b\"/\".join([\n            escape(identity.category).encode(\"utf-8\"),\n            escape(identity.type_).encode(\"utf-8\"),\n            escape(str(identity.lang or \"\")).encode(\"utf-8\"),\n            escape(identity.name or \"\").encode(\"utf-8\"),\n        ])\n        for identity in identities\n    ]\n\n    if len(set(identities)) != len(identities):\n        raise ValueError(\"duplicate identity\")\n\n    identities.sort()\n    identities.append(b\"\")\n    return b\"<\".join(identities)", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "entitycaps", "caps115.py"], "context_start_lineno": 33, "line_no": 34, "id": "aioxmpp.entitycaps.caps115.build_identities_string", "target_function_prompt": "def build_identities_string(identities):", "function_signature": "def build_identities_string(identities):"}}
{"prompt": "def build_features_string(features):", "metadata": {"task_id": "Communications/aioxmpp/27", "ground_truth": "    features = list(escape(feature).encode(\"utf-8\") for feature in features)\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "entitycaps", "caps115.py"], "context_start_lineno": 52, "line_no": 53, "id": "aioxmpp.entitycaps.caps115.build_features_string", "target_function_prompt": "def build_features_string(features):", "function_signature": "def build_features_string(features):"}}
{"prompt": "def build_forms_string(forms):", "metadata": {"task_id": "Communications/aioxmpp/28", "ground_truth": "    types = set()\n    forms_list = []\n    for form in forms:\n        try:\n            form_types = set(\n                value\n                for field in form.fields.filter(attrs={\"var\": \"FORM_TYPE\"})\n                for value in field.values\n            )\n        except KeyError:\n            continue\n\n        if len(form_types) > 1:\n            raise ValueError(\"form with multiple types\")\n        elif not form_types:\n            continue\n\n        type_ = escape(next(iter(form_types))).encode(\"utf-8\")\n        if type_ in types:\n            raise ValueError(\"multiple forms of type {!r}\".format(type_))\n        types.add(type_)\n        forms_list.append((type_, form))\n    forms_list.sort()\n\n    parts = []\n\n    for type_, form in forms_list:\n        parts.append(type_)\n\n        field_list = sorted(\n            (\n                (escape(field.var).encode(\"utf-8\"), field.values)\n                for field in form.fields\n                if field.var != \"FORM_TYPE\"\n            ),\n            key=lambda x: x[0]\n        )\n\n        for var, values in field_list:\n            parts.append(var)\n            parts.extend(sorted(\n                escape(value).encode(\"utf-8\") for value in values\n            ))\n\n    parts.append(b\"\")\n    return b\"<\".join(parts)", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "entitycaps", "caps115.py"], "context_start_lineno": 63, "line_no": 64, "id": "aioxmpp.entitycaps.caps115.build_forms_string", "target_function_prompt": "def build_forms_string(forms):", "function_signature": "def build_forms_string(forms):"}}
{"prompt": "    def path(self):", "metadata": {"task_id": "Communications/aioxmpp/29", "ground_truth": "        quoted = urllib.parse.quote(self.node, safe=\"\")\n        return (pathlib.Path(\"hashes\") /\n                \"{}_{}.xml\".format(self.algo, quoted))", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "entitycaps", "caps115.py"], "context_start_lineno": 132, "line_no": 133, "id": "aioxmpp.entitycaps.caps115.Key.path", "target_function_prompt": "    def path(self):", "function_signature": "    def path(self):"}}
{"prompt": "def _process_features(features):", "metadata": {"task_id": "Communications/aioxmpp/30", "ground_truth": "    parts = [\n        feature.encode(\"utf-8\")+b\"\\x1f\"\n        for feature in features\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "entitycaps", "caps390.py"], "context_start_lineno": 32, "line_no": 44, "id": "aioxmpp.entitycaps.caps390._process_features", "target_function_prompt": "def _process_features(features):", "function_signature": "def _process_features(features):"}}
{"prompt": "def _process_identities(identities):", "metadata": {"task_id": "Communications/aioxmpp/31", "ground_truth": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "entitycaps", "caps390.py"], "context_start_lineno": 61, "line_no": 74, "id": "aioxmpp.entitycaps.caps390._process_identities", "target_function_prompt": "def _process_identities(identities):", "function_signature": "def _process_identities(identities):"}}
{"prompt": "def _process_extensions(exts):", "metadata": {"task_id": "Communications/aioxmpp/32", "ground_truth": "    parts = [\n        _process_form(form)\n        for form in exts\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "entitycaps", "caps390.py"], "context_start_lineno": 102, "line_no": 115, "id": "aioxmpp.entitycaps.caps390._process_extensions", "target_function_prompt": "def _process_extensions(exts):", "function_signature": "def _process_extensions(exts):"}}
{"prompt": "def _calculate_hash(algo, hash_input):", "metadata": {"task_id": "Communications/aioxmpp/33", "ground_truth": "    impl = aioxmpp.hashes.hash_from_algo(algo)\n    impl.update(hash_input)\n    return impl.digest()", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "entitycaps", "caps390.py"], "context_start_lineno": 131, "line_no": 132, "id": "aioxmpp.entitycaps.caps390._calculate_hash", "target_function_prompt": "def _calculate_hash(algo, hash_input):", "function_signature": "def _calculate_hash(algo, hash_input):"}}
{"prompt": "    def node(self):", "metadata": {"task_id": "Communications/aioxmpp/34", "ground_truth": "        return \"urn:xmpp:caps#{}.{}\".format(\n            self.algo,\n            base64.b64encode(self.digest).decode(\"ascii\")\n        )", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "entitycaps", "caps390.py"], "context_start_lineno": 142, "line_no": 143, "id": "aioxmpp.entitycaps.caps390.Key.node", "target_function_prompt": "    def node(self):", "function_signature": "    def node(self):"}}
{"prompt": "    def path(self):", "metadata": {"task_id": "Communications/aioxmpp/35", "ground_truth": "        encoded = base64.b32encode(\n            self.digest\n        ).decode(\"ascii\").rstrip(\"=\").lower()\n        return (pathlib.Path(\"caps2\") /\n                urllib.parse.quote(self.algo, safe=\"\") /\n                encoded[:2] /\n                encoded[2:4] /\n                \"{}.xml\".format(encoded[4:]))", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "entitycaps", "caps390.py"], "context_start_lineno": 149, "line_no": 150, "id": "aioxmpp.entitycaps.caps390.Key.path", "target_function_prompt": "    def path(self):", "function_signature": "    def path(self):"}}
{"prompt": "    def extract_keys(self, presence):", "metadata": {"task_id": "Communications/aioxmpp/36", "ground_truth": "        if presence.xep0390_caps is None:\n            return ()\n\n        return (\n            Key(algo, digest)\n            for algo, digest in presence.xep0390_caps.digests.items()\n            if aioxmpp.hashes.is_algo_supported(algo)\n        )", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "entitycaps", "caps390.py"], "context_start_lineno": 171, "line_no": 172, "id": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "target_function_prompt": "    def extract_keys(self, presence):", "function_signature": "    def extract_keys(self, presence):"}}
{"prompt": "    def approve(self, peer_jid):", "metadata": {"task_id": "Communications/aioxmpp/37", "ground_truth": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBED,\n                            to=peer_jid)\n        )", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "roster", "service.py"], "context_start_lineno": 692, "line_no": 711, "id": "aioxmpp.roster.service.RosterClient.approve", "target_function_prompt": "    def approve(self, peer_jid):", "function_signature": "    def approve(self, peer_jid):"}}
{"prompt": "    def subscribe(self, peer_jid):", "metadata": {"task_id": "Communications/aioxmpp/38", "ground_truth": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "roster", "service.py"], "context_start_lineno": 716, "line_no": 725, "id": "aioxmpp.roster.service.RosterClient.subscribe", "target_function_prompt": "    def subscribe(self, peer_jid):", "function_signature": "    def subscribe(self, peer_jid):"}}
{"prompt": "    def unsubscribe(self, peer_jid):", "metadata": {"task_id": "Communications/aioxmpp/39", "ground_truth": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "roster", "service.py"], "context_start_lineno": 730, "line_no": 734, "id": "aioxmpp.roster.service.RosterClient.unsubscribe", "target_function_prompt": "    def unsubscribe(self, peer_jid):", "function_signature": "    def unsubscribe(self, peer_jid):"}}
{"prompt": "    def value(self):", "metadata": {"task_id": "Communications/aioxmpp/40", "ground_truth": "        try:\n            del self._value\n        except AttributeError:\n            pass", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "forms", "fields.py"], "context_start_lineno": 332, "line_no": 333, "id": "aioxmpp.forms.fields.BoundSingleValueField.value", "target_function_prompt": "    def value(self):", "function_signature": "    def value(self):"}}
{"prompt": "    def value(self):", "metadata": {"task_id": "Communications/aioxmpp/41", "ground_truth": "        try:\n            del self._value\n        except AttributeError:\n            pass", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "forms", "fields.py"], "context_start_lineno": 408, "line_no": 409, "id": "aioxmpp.forms.fields.BoundMultiValueField.value", "target_function_prompt": "    def value(self):", "function_signature": "    def value(self):"}}
{"prompt": "    def options(self):", "metadata": {"task_id": "Communications/aioxmpp/42", "ground_truth": "        try:\n            del self._options\n        except AttributeError:\n            pass", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "forms", "fields.py"], "context_start_lineno": 509, "line_no": 510, "id": "aioxmpp.forms.fields.BoundOptionsField.options", "target_function_prompt": "    def options(self):", "function_signature": "    def options(self):"}}
{"prompt": "    def value(self):", "metadata": {"task_id": "Communications/aioxmpp/43", "ground_truth": "        try:\n            del self._value\n        except AttributeError:\n            pass", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "forms", "fields.py"], "context_start_lineno": 576, "line_no": 577, "id": "aioxmpp.forms.fields.BoundSelectField.value", "target_function_prompt": "    def value(self):", "function_signature": "    def value(self):"}}
{"prompt": "    def value(self):", "metadata": {"task_id": "Communications/aioxmpp/44", "ground_truth": "        try:\n            del self._value\n        except AttributeError:\n            pass", "fpath_tuple": ["Communications", "aioxmpp", "aioxmpp", "forms", "fields.py"], "context_start_lineno": 654, "line_no": 655, "id": "aioxmpp.forms.fields.BoundMultiSelectField.value", "target_function_prompt": "    def value(self):", "function_signature": "    def value(self):"}}
{"prompt": "def reset_states():", "metadata": {"task_id": "Scientific-Engineering/cupy/0", "ground_truth": "    global _random_states\n    _random_states = {}", "fpath_tuple": ["Scientific-Engineering", "cupy", "cupy", "random", "_generator.py"], "context_start_lineno": 1263, "line_no": 1264, "id": "cupy.random._generator.reset_states", "target_function_prompt": "def reset_states():", "function_signature": "def reset_states():"}}
{"prompt": "def _check_and_get_dtype(dtype):", "metadata": {"task_id": "Scientific-Engineering/cupy/1", "ground_truth": "    dtype = numpy.dtype(dtype)\n    if dtype.char not in ('f', 'd'):\n        raise TypeError('cupy.random only supports float32 and float64')\n    return dtype", "fpath_tuple": ["Scientific-Engineering", "cupy", "cupy", "random", "_generator.py"], "context_start_lineno": 1304, "line_no": 1305, "id": "cupy.random._generator._check_and_get_dtype", "target_function_prompt": "def _check_and_get_dtype(dtype):", "function_signature": "def _check_and_get_dtype(dtype):"}}
{"prompt": "def filter_files_by_extension(\n        sources: List[str],\n        extension: str,\n) -> Tuple[List[str], List[str]]:", "metadata": {"task_id": "Scientific-Engineering/cupy/2", "ground_truth": "    sources_selected = []\n    sources_others = []\n    for src in sources:\n        if os.path.splitext(src)[1] == extension:\n            sources_selected.append(src)\n        else:\n            sources_others.append(src)\n    return sources_selected, sources_others", "fpath_tuple": ["Scientific-Engineering", "cupy", "install", "cupy_builder", "_command.py"], "context_start_lineno": 16, "line_no": 20, "id": "cupy_builder._command.filter_files_by_extension", "target_function_prompt": "def filter_files_by_extension(\n        sources: List[str],\n        extension: str,\n) -> Tuple[List[str], List[str]]:", "function_signature": "def filter_files_by_extension(\n        sources: List[str],\n        extension: str,\n) -> Tuple[List[str], List[str]]:"}}
{"prompt": "def _in_memory_arrow_table_from_file(filename: str) -> pa.Table:", "metadata": {"task_id": "Scientific-Engineering/datasets/0", "ground_truth": "    in_memory_stream = pa.input_stream(filename)\n    opened_stream = pa.ipc.open_stream(in_memory_stream)\n    pa_table = opened_stream.read_all()\n    return pa_table", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "table.py"], "context_start_lineno": 34, "line_no": 35, "id": "datasets.table._in_memory_arrow_table_from_file", "target_function_prompt": "def _in_memory_arrow_table_from_file(filename: str) -> pa.Table:", "function_signature": "def _in_memory_arrow_table_from_file(filename: str) -> pa.Table:"}}
{"prompt": "def _in_memory_arrow_table_from_buffer(buffer: pa.Buffer) -> pa.Table:", "metadata": {"task_id": "Scientific-Engineering/datasets/1", "ground_truth": "    stream = pa.BufferReader(buffer)\n    opened_stream = pa.ipc.open_stream(stream)\n    table = opened_stream.read_all()\n    return table", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "table.py"], "context_start_lineno": 41, "line_no": 42, "id": "datasets.table._in_memory_arrow_table_from_buffer", "target_function_prompt": "def _in_memory_arrow_table_from_buffer(buffer: pa.Buffer) -> pa.Table:", "function_signature": "def _in_memory_arrow_table_from_buffer(buffer: pa.Buffer) -> pa.Table:"}}
{"prompt": "def _interpolation_search(arr: List[int], x: int) -> int:", "metadata": {"task_id": "Scientific-Engineering/datasets/2", "ground_truth": "    i, j = 0, len(arr) - 1\n    while i < j and arr[i] <= x < arr[j]:\n        k = i + ((j - i) * (x - arr[i]) // (arr[j] - arr[i]))\n        if arr[k] <= x < arr[k + 1]:\n            return k\n        elif arr[k] < x:\n            i, j = k + 1, j\n        else:\n            i, j = i, k\n    raise IndexError(f\"Invalid query '{x}' for size {arr[-1] if len(arr) else 'none'}.\")", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "table.py"], "context_start_lineno": 89, "line_no": 103, "id": "datasets.table._interpolation_search", "target_function_prompt": "def _interpolation_search(arr: List[int], x: int) -> int:", "function_signature": "def _interpolation_search(arr: List[int], x: int) -> int:"}}
{"prompt": "def _is_inside_unrequested_special_dir(matched_rel_path: str, pattern: str) -> bool:", "metadata": {"task_id": "Scientific-Engineering/datasets/3", "ground_truth": "    data_dirs_to_ignore_in_path = [part for part in PurePath(matched_rel_path).parent.parts if part.startswith(\"__\")]\n    data_dirs_to_ignore_in_pattern = [part for part in PurePath(pattern).parent.parts if part.startswith(\"__\")]\n    return len(data_dirs_to_ignore_in_path) != len(data_dirs_to_ignore_in_pattern)", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "data_files.py"], "context_start_lineno": 134, "line_no": 162, "id": "datasets.data_files._is_inside_unrequested_special_dir", "target_function_prompt": "def _is_inside_unrequested_special_dir(matched_rel_path: str, pattern: str) -> bool:", "function_signature": "def _is_inside_unrequested_special_dir(matched_rel_path: str, pattern: str) -> bool:"}}
{"prompt": "def _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir(matched_rel_path: str, pattern: str) -> bool:", "metadata": {"task_id": "Scientific-Engineering/datasets/4", "ground_truth": "    hidden_directories_in_path = [\n        part for part in PurePath(matched_rel_path).parts if part.startswith(\".\") and not set(part) == {\".\"}\n    ]\n    hidden_directories_in_pattern = [\n        part for part in PurePath(pattern).parts if part.startswith(\".\") and not set(part) == {\".\"}\n    ]\n    return len(hidden_directories_in_path) != len(hidden_directories_in_pattern)", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "data_files.py"], "context_start_lineno": 167, "line_no": 220, "id": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "target_function_prompt": "def _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir(matched_rel_path: str, pattern: str) -> bool:", "function_signature": "def _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir(matched_rel_path: str, pattern: str) -> bool:"}}
{"prompt": "def _batch_to_examples(batch: Dict[str, list]) -> List[Dict[str, Any]]:", "metadata": {"task_id": "Scientific-Engineering/datasets/5", "ground_truth": "    n_examples = len(batch[next(iter(batch))])\n    for i in range(n_examples):\n        yield {col: array[i] for col, array in batch.items()}", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "iterable_dataset.py"], "context_start_lineno": 77, "line_no": 79, "id": "datasets.iterable_dataset._batch_to_examples", "target_function_prompt": "def _batch_to_examples(batch: Dict[str, list]) -> List[Dict[str, Any]]:", "function_signature": "def _batch_to_examples(batch: Dict[str, list]) -> List[Dict[str, Any]]:"}}
{"prompt": "def _examples_to_batch(examples: List[Dict[str, Any]]) -> Dict[str, list]:\n    # we order the columns by order of appearance\n    # to do so, we use a dict as an ordered set", "metadata": {"task_id": "Scientific-Engineering/datasets/6", "ground_truth": "    cols = {col: None for example in examples for col in example}\n    # when an example is missing a column, we set the value to None with .get()\n    arrays = [[example.get(col) for example in examples] for col in cols]\n    return dict(zip(cols, arrays))", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "iterable_dataset.py"], "context_start_lineno": 68, "line_no": 71, "id": "datasets.iterable_dataset._examples_to_batch", "target_function_prompt": "def _examples_to_batch(examples: List[Dict[str, Any]]) -> Dict[str, list]:\n    # we order the columns by order of appearance\n    # to do so, we use a dict as an ordered set", "function_signature": "def _examples_to_batch(examples: List[Dict[str, Any]]) -> Dict[str, list]:\n    # we order the columns by order of appearance\n    # to do so, we use a dict as an ordered set"}}
{"prompt": "    def _iter_random_indices(\n        rng: np.random.Generator,\n        num_sources: int,\n        random_batch_size=1000,\n        p: Optional[List[float]] = None,\n    ) -> Iterator[int]:", "metadata": {"task_id": "Scientific-Engineering/datasets/7", "ground_truth": "        if p is None:\n            while True:\n                yield from (int(i) for i in rng.integers(0, num_sources, size=random_batch_size))\n        else:\n            while True:\n                yield from (int(i) for i in rng.choice(num_sources, size=random_batch_size, p=p))", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "iterable_dataset.py"], "context_start_lineno": 600, "line_no": 607, "id": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "target_function_prompt": "    def _iter_random_indices(\n        rng: np.random.Generator,\n        num_sources: int,\n        random_batch_size=1000,\n        p: Optional[List[float]] = None,\n    ) -> Iterator[int]:", "function_signature": "    def _iter_random_indices(\n        rng: np.random.Generator,\n        num_sources: int,\n        random_batch_size=1000,\n        p: Optional[List[float]] = None,\n    ) -> Iterator[int]:"}}
{"prompt": "    def _iter_random_indices(rng: np.random.Generator, buffer_size: int, random_batch_size=1000) -> Iterator[int]:", "metadata": {"task_id": "Scientific-Engineering/datasets/8", "ground_truth": "        while True:\n            yield from (int(i) for i in rng.integers(0, buffer_size, size=random_batch_size))", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "iterable_dataset.py"], "context_start_lineno": 971, "line_no": 972, "id": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "target_function_prompt": "    def _iter_random_indices(rng: np.random.Generator, buffer_size: int, random_batch_size=1000) -> Iterator[int]:", "function_signature": "    def _iter_random_indices(rng: np.random.Generator, buffer_size: int, random_batch_size=1000) -> Iterator[int]:"}}
{"prompt": "    def remove_columns(self, column_names: Union[str, List[str]]) -> \"IterableDataset\":", "metadata": {"task_id": "Scientific-Engineering/datasets/9", "ground_truth": "        original_features = self._info.features.copy() if self._info.features else None\n        ds_iterable = self.map(remove_columns=column_names)\n        if original_features is not None:\n            ds_iterable._info.features = original_features.copy()\n            for col, _ in original_features.items():\n                if col in column_names:\n                    del ds_iterable._info.features[col]\n            # check that it's still valid, especially with regard to task templates\n            try:\n                ds_iterable._info.copy()\n            except ValueError:\n                ds_iterable._info.task_templates = None\n\n        return ds_iterable", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "iterable_dataset.py"], "context_start_lineno": 1989, "line_no": 2014, "id": "datasets.iterable_dataset.IterableDataset.remove_columns", "target_function_prompt": "    def remove_columns(self, column_names: Union[str, List[str]]) -> \"IterableDataset\":", "function_signature": "    def remove_columns(self, column_names: Union[str, List[str]]) -> \"IterableDataset\":"}}
{"prompt": "    def with_format(\n        self,\n        type: Optional[str] = None,\n        columns: Optional[List] = None,\n        output_all_columns: bool = False,\n        **format_kwargs,\n    ) -> \"DatasetDict\":", "metadata": {"task_id": "Scientific-Engineering/datasets/10", "ground_truth": "        dataset = copy.deepcopy(self)\n        dataset.set_format(type=type, columns=columns, output_all_columns=output_all_columns, **format_kwargs)\n        return dataset", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "dataset_dict.py"], "context_start_lineno": 642, "line_no": 690, "id": "datasets.dataset_dict.DatasetDict.with_format", "target_function_prompt": "    def with_format(\n        self,\n        type: Optional[str] = None,\n        columns: Optional[List] = None,\n        output_all_columns: bool = False,\n        **format_kwargs,\n    ) -> \"DatasetDict\":", "function_signature": "    def with_format(\n        self,\n        type: Optional[str] = None,\n        columns: Optional[List] = None,\n        output_all_columns: bool = False,\n        **format_kwargs,\n    ) -> \"DatasetDict\":"}}
{"prompt": "    def with_transform(\n        self,\n        transform: Optional[Callable],\n        columns: Optional[List] = None,\n        output_all_columns: bool = False,\n    ) -> \"DatasetDict\":", "metadata": {"task_id": "Scientific-Engineering/datasets/11", "ground_truth": "        dataset = copy.deepcopy(self)\n        dataset.set_transform(transform=transform, columns=columns, output_all_columns=output_all_columns)\n        return dataset", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "dataset_dict.py"], "context_start_lineno": 694, "line_no": 744, "id": "datasets.dataset_dict.DatasetDict.with_transform", "target_function_prompt": "    def with_transform(\n        self,\n        transform: Optional[Callable],\n        columns: Optional[List] = None,\n        output_all_columns: bool = False,\n    ) -> \"DatasetDict\":", "function_signature": "    def with_transform(\n        self,\n        transform: Optional[Callable],\n        columns: Optional[List] = None,\n        output_all_columns: bool = False,\n    ) -> \"DatasetDict\":"}}
{"prompt": "    def align_labels_with_mapping(self, label2id: Dict, label_column: str) -> \"DatasetDict\":", "metadata": {"task_id": "Scientific-Engineering/datasets/12", "ground_truth": "        self._check_values_type()\n        return DatasetDict(\n            {\n                k: dataset.align_labels_with_mapping(label2id=label2id, label_column=label_column)\n                for k, dataset in self.items()\n            }\n        )", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "dataset_dict.py"], "context_start_lineno": 1548, "line_no": 1549, "id": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "target_function_prompt": "    def align_labels_with_mapping(self, label2id: Dict, label_column: str) -> \"DatasetDict\":", "function_signature": "    def align_labels_with_mapping(self, label2id: Dict, label_column: str) -> \"DatasetDict\":"}}
{"prompt": "    def map(\n        self,\n        function: Optional[Callable] = None,\n        with_indices: bool = False,\n        input_columns: Optional[Union[str, List[str]]] = None,\n        batched: bool = False,\n        batch_size: int = 1000,\n        drop_last_batch: bool = False,\n        remove_columns: Optional[Union[str, List[str]]] = None,\n        fn_kwargs: Optional[dict] = None,\n    ) -> \"IterableDatasetDict\":", "metadata": {"task_id": "Scientific-Engineering/datasets/13", "ground_truth": "        return IterableDatasetDict(\n            {\n                k: dataset.map(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    drop_last_batch=drop_last_batch,\n                    remove_columns=remove_columns,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "dataset_dict.py"], "context_start_lineno": 1784, "line_no": 1857, "id": "datasets.dataset_dict.IterableDatasetDict.map", "target_function_prompt": "    def map(\n        self,\n        function: Optional[Callable] = None,\n        with_indices: bool = False,\n        input_columns: Optional[Union[str, List[str]]] = None,\n        batched: bool = False,\n        batch_size: int = 1000,\n        drop_last_batch: bool = False,\n        remove_columns: Optional[Union[str, List[str]]] = None,\n        fn_kwargs: Optional[dict] = None,\n    ) -> \"IterableDatasetDict\":", "function_signature": "    def map(\n        self,\n        function: Optional[Callable] = None,\n        with_indices: bool = False,\n        input_columns: Optional[Union[str, List[str]]] = None,\n        batched: bool = False,\n        batch_size: int = 1000,\n        drop_last_batch: bool = False,\n        remove_columns: Optional[Union[str, List[str]]] = None,\n        fn_kwargs: Optional[dict] = None,\n    ) -> \"IterableDatasetDict\":"}}
{"prompt": "    def filter(\n        self,\n        function: Optional[Callable] = None,\n        with_indices=False,\n        input_columns: Optional[Union[str, List[str]]] = None,\n        batched: bool = False,\n        batch_size: Optional[int] = 1000,\n        fn_kwargs: Optional[dict] = None,\n    ) -> \"IterableDatasetDict\":", "metadata": {"task_id": "Scientific-Engineering/datasets/14", "ground_truth": "        return IterableDatasetDict(\n            {\n                k: dataset.filter(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "dataset_dict.py"], "context_start_lineno": 1873, "line_no": 1922, "id": "datasets.dataset_dict.IterableDatasetDict.filter", "target_function_prompt": "    def filter(\n        self,\n        function: Optional[Callable] = None,\n        with_indices=False,\n        input_columns: Optional[Union[str, List[str]]] = None,\n        batched: bool = False,\n        batch_size: Optional[int] = 1000,\n        fn_kwargs: Optional[dict] = None,\n    ) -> \"IterableDatasetDict\":", "function_signature": "    def filter(\n        self,\n        function: Optional[Callable] = None,\n        with_indices=False,\n        input_columns: Optional[Union[str, List[str]]] = None,\n        batched: bool = False,\n        batch_size: Optional[int] = 1000,\n        fn_kwargs: Optional[dict] = None,\n    ) -> \"IterableDatasetDict\":"}}
{"prompt": "    def num_rows(self) -> int:", "metadata": {"task_id": "Scientific-Engineering/datasets/15", "ground_truth": "        if self._indices is not None:\n            return self._indices.num_rows\n        return self._data.num_rows", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "arrow_dataset.py"], "context_start_lineno": 1787, "line_no": 1799, "id": "datasets.arrow_dataset.Dataset.num_rows", "target_function_prompt": "    def num_rows(self) -> int:", "function_signature": "    def num_rows(self) -> int:"}}
{"prompt": "def extract_path_from_uri(dataset_path: str) -> str:", "metadata": {"task_id": "Scientific-Engineering/datasets/16", "ground_truth": "    if \"://\" in dataset_path:\n        dataset_path = dataset_path.split(\"://\")[1]\n    return dataset_path", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "filesystems", "__init__.py"], "context_start_lineno": 32, "line_no": 40, "id": "datasets.filesystems.extract_path_from_uri", "target_function_prompt": "def extract_path_from_uri(dataset_path: str) -> str:", "function_signature": "def extract_path_from_uri(dataset_path: str) -> str:"}}
{"prompt": "def is_remote_filesystem(fs: fsspec.AbstractFileSystem) -> bool:", "metadata": {"task_id": "Scientific-Engineering/datasets/17", "ground_truth": "    if fs is not None:\n        protocols = (p,) if isinstance(p := fs.protocol, str) else p\n        if \"file\" not in protocols:\n            return True\n    return False", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "filesystems", "__init__.py"], "context_start_lineno": 45, "line_no": 53, "id": "datasets.filesystems.is_remote_filesystem", "target_function_prompt": "def is_remote_filesystem(fs: fsspec.AbstractFileSystem) -> bool:", "function_signature": "def is_remote_filesystem(fs: fsspec.AbstractFileSystem) -> bool:"}}
{"prompt": "def hash_url_to_filename(url, etag=None):", "metadata": {"task_id": "Scientific-Engineering/datasets/18", "ground_truth": "    url_bytes = url.encode(\"utf-8\")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\"utf-8\")\n        etag_hash = sha256(etag_bytes)\n        filename += \".\" + etag_hash.hexdigest()\n\n    if url.endswith(\".py\"):\n        filename += \".py\"\n\n    return filename", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "utils", "file_utils.py"], "context_start_lineno": 124, "line_no": 133, "id": "datasets.utils.file_utils.hash_url_to_filename", "target_function_prompt": "def hash_url_to_filename(url, etag=None):", "function_signature": "def hash_url_to_filename(url, etag=None):"}}
{"prompt": "def hf_hub_url(repo_id: str, path: str, revision: Optional[str] = None) -> str:", "metadata": {"task_id": "Scientific-Engineering/datasets/19", "ground_truth": "    if version.parse(hfh.__version__).release < version.parse(\"0.11.0\").release:\n        # old versions of hfh don't url-encode the file path\n        path = quote(path)\n    return hfh.hf_hub_url(repo_id, path, repo_type=\"dataset\", revision=revision)", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "utils", "hub.py"], "context_start_lineno": 7, "line_no": 8, "id": "datasets.utils.hub.hf_hub_url", "target_function_prompt": "def hf_hub_url(repo_id: str, path: str, revision: Optional[str] = None) -> str:", "function_signature": "def hf_hub_url(repo_id: str, path: str, revision: Optional[str] = None) -> str:"}}
{"prompt": "def _number_of_shards_in_gen_kwargs(gen_kwargs: dict) -> int:", "metadata": {"task_id": "Scientific-Engineering/datasets/20", "ground_truth": "    lists_lengths = {key: len(value) for key, value in gen_kwargs.items() if isinstance(value, list)}\n    if len(set(lists_lengths.values())) > 1:\n        raise RuntimeError(\n            (\n                \"Sharding is ambiguous for this dataset: \"\n                + \"we found several data sources lists of different lengths, and we don't know over which list we should parallelize:\\n\"\n                + \"\\n\".join(f\"\\t- key {key} has length {length}\" for key, length in lists_lengths.items())\n                + \"\\nTo fix this, check the 'gen_kwargs' and make sure to use lists only for data sources, \"\n                + \"and use tuples otherwise. In the end there should only be one single list, or several lists with the same length.\"\n            )\n        )\n    max_length = max(lists_lengths.values(), default=0)\n    return max(1, max_length)", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "utils", "sharding.py"], "context_start_lineno": 5, "line_no": 9, "id": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "target_function_prompt": "def _number_of_shards_in_gen_kwargs(gen_kwargs: dict) -> int:", "function_signature": "def _number_of_shards_in_gen_kwargs(gen_kwargs: dict) -> int:"}}
{"prompt": "def _distribute_shards(num_shards: int, max_num_jobs: int) -> List[range]:", "metadata": {"task_id": "Scientific-Engineering/datasets/21", "ground_truth": "    shards_indices_per_group = []\n    for group_idx in range(max_num_jobs):\n        num_shards_to_add = num_shards // max_num_jobs + (group_idx < (num_shards % max_num_jobs))\n        if num_shards_to_add == 0:\n            break\n        start = shards_indices_per_group[-1].stop if shards_indices_per_group else 0\n        shard_indices = range(start, start + num_shards_to_add)\n        shards_indices_per_group.append(shard_indices)\n    return shards_indices_per_group", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "utils", "sharding.py"], "context_start_lineno": 24, "line_no": 40, "id": "datasets.utils.sharding._distribute_shards", "target_function_prompt": "def _distribute_shards(num_shards: int, max_num_jobs: int) -> List[range]:", "function_signature": "def _distribute_shards(num_shards: int, max_num_jobs: int) -> List[range]:"}}
{"prompt": "def temporary_assignment(obj, attr, value):", "metadata": {"task_id": "Scientific-Engineering/datasets/22", "ground_truth": "    original = getattr(obj, attr, None)\n    setattr(obj, attr, value)\n    try:\n        yield\n    finally:\n        setattr(obj, attr, original)", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "utils", "py_utils.py"], "context_start_lineno": 228, "line_no": 230, "id": "datasets.utils.py_utils.temporary_assignment", "target_function_prompt": "def temporary_assignment(obj, attr, value):", "function_signature": "def temporary_assignment(obj, attr, value):"}}
{"prompt": "    def extract(input_path: Union[Path, str], output_path: Union[Path, str]) -> None:", "metadata": {"task_id": "Scientific-Engineering/datasets/23", "ground_truth": "        os.makedirs(output_path, exist_ok=True)\n        tar_file = tarfile.open(input_path)\n        tar_file.extractall(output_path, members=TarExtractor.safemembers(tar_file, output_path))\n        tar_file.close()", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "utils", "extract.py"], "context_start_lineno": 124, "line_no": 125, "id": "datasets.utils.extract.TarExtractor.extract", "target_function_prompt": "    def extract(input_path: Union[Path, str], output_path: Union[Path, str]) -> None:", "function_signature": "    def extract(input_path: Union[Path, str], output_path: Union[Path, str]) -> None:"}}
{"prompt": "    def infer_extractor_format(cls, path: Union[Path, str]) -> str:  # <Added version=\"2.4.0\"/>", "metadata": {"task_id": "Scientific-Engineering/datasets/24", "ground_truth": "        magic_number_max_length = cls._get_magic_number_max_length()\n        magic_number = cls._read_magic_number(path, magic_number_max_length)\n        for extractor_format, extractor in cls.extractors.items():\n            if extractor.is_extractable(path, magic_number=magic_number):\n                return extractor_format", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "utils", "extract.py"], "context_start_lineno": 313, "line_no": 314, "id": "datasets.utils.extract.Extractor.infer_extractor_format", "target_function_prompt": "    def infer_extractor_format(cls, path: Union[Path, str]) -> str:  # <Added version=\"2.4.0\"/>", "function_signature": "    def infer_extractor_format(cls, path: Union[Path, str]) -> str:  # <Added version=\"2.4.0\"/>"}}
{"prompt": "def asdict(obj):", "metadata": {"task_id": "Scientific-Engineering/datasets/25", "ground_truth": "    def _is_dataclass_instance(obj):\n        # https://docs.python.org/3/library/dataclasses.html#dataclasses.is_dataclass\n        return is_dataclass(obj) and not isinstance(obj, type)\n\n    def _asdict_inner(obj):\n        if _is_dataclass_instance(obj):\n            result = {}\n            for f in fields(obj):\n                value = _asdict_inner(getattr(obj, f.name))\n                if not f.init or value != f.default or f.metadata.get(\"include_in_asdict_even_if_is_default\", False):\n                    result[f.name] = value\n            return result\n        elif isinstance(obj, tuple) and hasattr(obj, \"_fields\"):\n            # obj is a namedtuple\n            return type(obj)(*[_asdict_inner(v) for v in obj])\n        elif isinstance(obj, (list, tuple)):\n            # Assume we can create an object of this type by passing in a\n            # generator (which is not true for namedtuples, handled\n            # above).\n            return type(obj)(_asdict_inner(v) for v in obj)\n        elif isinstance(obj, dict):\n            return {_asdict_inner(k): _asdict_inner(v) for k, v in obj.items()}\n        else:\n            return copy.deepcopy(obj)\n\n    if not isinstance(obj, dict) and not _is_dataclass_instance(obj):\n        raise TypeError(f\"{obj} is not a dict or a dataclass\")\n\n    return _asdict_inner(obj)", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "utils", "py_utils.py"], "context_start_lineno": 188, "line_no": 196, "id": "datasets.utils.py_utils.asdict", "target_function_prompt": "def asdict(obj):", "function_signature": "def asdict(obj):"}}
{"prompt": "    def from_dataset_card_data(cls, dataset_card_data: DatasetCardData) -> \"MetadataConfigs\":", "metadata": {"task_id": "Scientific-Engineering/datasets/26", "ground_truth": "        if dataset_card_data.get(cls.FIELD_NAME):\n            metadata_configs = dataset_card_data[cls.FIELD_NAME]\n            if not isinstance(metadata_configs, list):\n                raise ValueError(f\"Expected {cls.FIELD_NAME} to be a list, but got '{metadata_configs}'\")\n            for metadata_config in metadata_configs:\n                if \"config_name\" not in metadata_config:\n                    raise ValueError(\n                        f\"Each config must include `config_name` field with a string name of a config, \"\n                        f\"but got {metadata_config}. \"\n                    )\n                cls._raise_if_data_files_field_not_valid(metadata_config)\n            return cls(\n                {\n                    config[\"config_name\"]: {param: value for param, value in config.items() if param != \"config_name\"}\n                    for config in metadata_configs\n                }\n            )\n        return cls()", "fpath_tuple": ["Scientific-Engineering", "datasets", "src", "datasets", "utils", "metadata.py"], "context_start_lineno": 172, "line_no": 173, "id": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "target_function_prompt": "    def from_dataset_card_data(cls, dataset_card_data: DatasetCardData) -> \"MetadataConfigs\":", "function_signature": "    def from_dataset_card_data(cls, dataset_card_data: DatasetCardData) -> \"MetadataConfigs\":"}}
{"prompt": "    def setmaxsize(self, maxsize):", "metadata": {"task_id": "Utilities/boltons/22", "ground_truth": "        self.maxsize = maxsize\n        self._msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)", "fpath_tuple": ["Utilities", "boltons", "boltons", "socketutils.py"], "context_start_lineno": 666, "line_no": 667, "id": "boltons.socketutils.NetstringSocket.setmaxsize", "target_function_prompt": "    def setmaxsize(self, maxsize):", "function_signature": "    def setmaxsize(self, maxsize):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/4", "ground_truth": "    from boto.regioninfo import connect\n    from boto.datapipeline.layer1 import DataPipelineConnection\n    return connect('datapipeline', region_name,\n                   connection_cls=DataPipelineConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "datapipeline", "__init__.py"], "context_start_lineno": 37, "line_no": 38, "id": "boto.datapipeline.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def __str__(self):", "metadata": {"task_id": "Utilities/gunicorn/9", "ground_truth": "        lines = []\n        kmax = max(len(k) for k in self.settings)\n        for k in sorted(self.settings):\n            v = self.settings[k].value\n            if callable(v):\n                v = \"<{}()>\".format(v.__qualname__)\n            lines.append(\"{k:{kmax}} = {v}\".format(k=k, v=v, kmax=kmax))\n        return \"\\n\".join(lines)", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "config.py"], "context_start_lineno": 53, "line_no": 54, "id": "gunicorn.config.Config.__str__", "target_function_prompt": "    def __str__(self):", "function_signature": "    def __str__(self):"}}
{"prompt": "    def get(self, key, default=None):\n        # get does not use __getitem__ by default so we must override it as well", "metadata": {"task_id": "Database/mongoengine/0", "ground_truth": "        try:\n            return self.__getitem__(key)\n        except KeyError:\n            return default", "fpath_tuple": ["Database", "mongoengine", "mongoengine", "base", "datastructures.py"], "context_start_lineno": 55, "line_no": 57, "id": "mongoengine.base.datastructures.BaseDict.get", "target_function_prompt": "    def get(self, key, default=None):\n        # get does not use __getitem__ by default so we must override it as well", "function_signature": "    def get(self, key, default=None):\n        # get does not use __getitem__ by default so we must override it as well"}}
{"prompt": "def int_to_note(note_int, accidentals=\"#\"):", "metadata": {"task_id": "Multimedia/mingus/7", "ground_truth": "    from mingus.core.mt_exceptions import RangeError\n    from mingus.core.mt_exceptions import FormatError\n    if note_int not in range(12):\n        raise RangeError(\"int out of bounds (0-11): %d\" % note_int)\n    ns = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n    nf = [\"C\", \"Db\", \"D\", \"Eb\", \"E\", \"F\", \"Gb\", \"G\", \"Ab\", \"A\", \"Bb\", \"B\"]\n    if accidentals == \"#\":\n        return ns[note_int]\n    elif accidentals == \"b\":\n        return nf[note_int]\n    else:\n        raise FormatError(\"'%s' not valid as accidental\" % accidentals)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "notes.py"], "context_start_lineno": 35, "line_no": 51, "id": "mingus.core.notes.int_to_note", "target_function_prompt": "def int_to_note(note_int, accidentals=\"#\"):", "function_signature": "def int_to_note(note_int, accidentals=\"#\"):"}}
{"prompt": "    def get_config_schema(self):", "metadata": {"task_id": "Multimedia/Mopidy/10", "ground_truth": "        schema = super().get_config_schema()\n        schema[\"hostname\"] = config_lib.Hostname()\n        schema[\"port\"] = config_lib.Port()\n        schema[\"static_dir\"] = config_lib.Deprecated()\n        schema[\"zeroconf\"] = config_lib.String(optional=True)\n        schema[\"allowed_origins\"] = config_lib.List(\n            optional=True,\n            unique=True,\n            subtype=config_lib.String(transformer=lambda x: x.lower()),\n        )\n        schema[\"csrf_protection\"] = config_lib.Boolean(optional=True)\n        schema[\"default_app\"] = config_lib.String(optional=True)\n        return schema", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "http", "__init__.py"], "context_start_lineno": 19, "line_no": 20, "id": "mopidy.http.Extension.get_config_schema", "target_function_prompt": "    def get_config_schema(self):", "function_signature": "    def get_config_schema(self):"}}
{"prompt": "def find_undeclared_variables(ast: nodes.Template) -> t.Set[str]:", "metadata": {"task_id": "Internet/Jinja2/4", "ground_truth": "    codegen = TrackingCodeGenerator(ast.environment)  # type: ignore\n    codegen.visit(ast)\n    return codegen.undeclared_identifiers", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "meta.py"], "context_start_lineno": 32, "line_no": 51, "id": "jinja2.meta.find_undeclared_variables", "target_function_prompt": "def find_undeclared_variables(ast: nodes.Template) -> t.Set[str]:", "function_signature": "def find_undeclared_variables(ast: nodes.Template) -> t.Set[str]:"}}
{"prompt": "def file(\n    path,\n    present=True,\n    user=None,\n    group=None,\n    mode=None,\n    touch=False,\n    create_remote_dir=True,\n    force=False,\n    force_backup=True,\n    force_backup_dir=None,\n):", "metadata": {"task_id": "System/pyinfra/3", "ground_truth": "    path = _validate_path(path)\n\n    mode = ensure_mode_int(mode)\n    info = host.get_fact(File, path=path)\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    if not present:\n        if info:\n            yield StringCommand(\"rm\", \"-f\", QuoteString(path))\n        else:\n            host.noop(\"file {0} does not exist\")\n        return\n\n    if info is None:  # create\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, path, user, group)\n\n        yield StringCommand(\"touch\", QuoteString(path))\n\n        if mode:\n            yield file_utils.chmod(path, mode)\n        if user or group:\n            yield file_utils.chown(path, user, group)\n\n    else:  # update\n        changed = False\n\n        if touch:\n            changed = True\n            yield StringCommand(\"touch\", QuoteString(path))\n\n        # Check mode\n        if mode and (not info or info[\"mode\"] != mode):\n            yield file_utils.chmod(path, mode)\n            changed = True\n\n        # Check user/group\n        if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n            yield file_utils.chown(path, user, group)\n            changed = True\n\n        if not changed:\n            host.noop(\"file {0} already exists\".format(path))", "fpath_tuple": ["System", "pyinfra", "pyinfra", "operations", "files.py"], "context_start_lineno": 1190, "line_no": 1237, "id": "pyinfra.operations.files.file", "target_function_prompt": "def file(\n    path,\n    present=True,\n    user=None,\n    group=None,\n    mode=None,\n    touch=False,\n    create_remote_dir=True,\n    force=False,\n    force_backup=True,\n    force_backup_dir=None,\n):", "function_signature": "def file(\n    path,\n    present=True,\n    user=None,\n    group=None,\n    mode=None,\n    touch=False,\n    create_remote_dir=True,\n    force=False,\n    force_backup=True,\n    force_backup_dir=None,\n):"}}
{"prompt": "def is_destructive(queries):", "metadata": {"task_id": "Database/litecli/1", "ground_truth": "    keywords = (\"drop\", \"shutdown\", \"delete\", \"truncate\", \"alter\")\n    return queries_start_with(queries, keywords)", "fpath_tuple": ["Database", "litecli", "litecli", "packages", "parseutils.py"], "context_start_lineno": 218, "line_no": 220, "id": "litecli.packages.parseutils.is_destructive", "target_function_prompt": "def is_destructive(queries):", "function_signature": "def is_destructive(queries):"}}
{"prompt": "    def main(self, function):", "metadata": {"task_id": "Utilities/sacred/14", "ground_truth": "        captured = self.command(function)\n        self.default_command = captured.__name__\n        return captured", "fpath_tuple": ["Utilities", "sacred", "sacred", "experiment.py"], "context_start_lineno": 139, "line_no": 148, "id": "sacred.experiment.Experiment.main", "target_function_prompt": "    def main(self, function):", "function_signature": "    def main(self, function):"}}
{"prompt": "def frombase(path1, path2):\n    # type: (Text, Text) -> Text", "metadata": {"task_id": "System/fs/7", "ground_truth": "    if not isparent(path1, path2):\n        raise ValueError(\"path1 must be a prefix of path2\")\n    return path2[len(path1) :]", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 521, "line_no": 537, "id": "fs.path.frombase", "target_function_prompt": "def frombase(path1, path2):\n    # type: (Text, Text) -> Text", "function_signature": "def frombase(path1, path2):\n    # type: (Text, Text) -> Text"}}
{"prompt": "def is_muted(msg: Message, model: Any) -> bool:\n    # PMs cannot be muted", "metadata": {"task_id": "Communications/zulip-term/4", "ground_truth": "    if msg[\"type\"] == \"private\":\n        return False\n    # In a topic narrow\n    elif len(model.narrow) == 2:\n        return False\n    elif model.is_muted_stream(msg[\"stream_id\"]):\n        return True\n    elif model.is_muted_topic(msg[\"stream_id\"], msg[\"subject\"]):\n        return True\n    return False", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "utils.py"], "context_start_lineno": 55, "line_no": 57, "id": "zulipterminal.ui_tools.utils.is_muted", "target_function_prompt": "def is_muted(msg: Message, model: Any) -> bool:\n    # PMs cannot be muted", "function_signature": "def is_muted(msg: Message, model: Any) -> bool:\n    # PMs cannot be muted"}}
{"prompt": "    def remove(self, category_name, discriminator):", "metadata": {"task_id": "Internet/pyramid/11", "ground_truth": "        intr = self.get(category_name, discriminator)\n        if intr is None:\n            return\n        L = self._refs.pop(intr, [])\n        for d in L:\n            L2 = self._refs[d]\n            L2.remove(intr)\n        category = self._categories[intr.category_name]\n        del category[intr.discriminator]\n        del category[intr.discriminator_hash]", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "registry.py"], "context_start_lineno": 162, "line_no": 163, "id": "pyramid.registry.Introspector.remove", "target_function_prompt": "    def remove(self, category_name, discriminator):", "function_signature": "    def remove(self, category_name, discriminator):"}}
{"prompt": "    def set_status(self, msg):", "metadata": {"task_id": "System/mrjob/15", "ground_truth": "        line = 'reporter:status:%s\\n' % (msg,)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 586, "line_no": 593, "id": "mrjob.job.MRJob.set_status", "target_function_prompt": "    def set_status(self, msg):", "function_signature": "    def set_status(self, msg):"}}
{"prompt": "    def make_property(cls, callable, name=None, reify=False):", "metadata": {"task_id": "Internet/pyramid/12", "ground_truth": "        if name is None:\n            if not hasattr(callable, '__name__'):\n                raise ValueError(\n                    'missing __name__, must specify \"name\" for property'\n                )\n            name = callable.__name__\n        name = get_callable_name(name)\n        is_data_descriptor = inspect.isdatadescriptor(callable)\n        if reify and is_data_descriptor:\n            raise ValueError('cannot reify a data descriptor')\n        if is_data_descriptor:\n            fn = callable\n        else:\n            wrapped = lambda this: callable(this)\n            wrapped.__name__ = name\n            wrapped.__doc__ = callable.__doc__\n\n            if reify:\n                import pyramid.decorator  # avoid circular import\n\n                fn = pyramid.decorator.reify(wrapped)\n            else:\n                fn = SettableProperty(wrapped)\n\n        return name, fn", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "util.py"], "context_start_lineno": 103, "line_no": 109, "id": "pyramid.util.InstancePropertyHelper.make_property", "target_function_prompt": "    def make_property(cls, callable, name=None, reify=False):", "function_signature": "    def make_property(cls, callable, name=None, reify=False):"}}
{"prompt": "    def client_prefers(self, media_types):", "metadata": {"task_id": "Internet/falcon/9", "ground_truth": "        try:\n            # NOTE(kgriffs): best_match will return '' if no match is found\n            preferred_type = mimeparse.best_match(media_types, self.accept)\n        except ValueError:\n            # Value for the accept header was not formatted correctly\n            preferred_type = ''\n\n        return preferred_type if preferred_type else None", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 1100, "line_no": 1114, "id": "falcon.request.Request.client_prefers", "target_function_prompt": "    def client_prefers(self, media_types):", "function_signature": "    def client_prefers(self, media_types):"}}
{"prompt": "    def content_length(self):", "metadata": {"task_id": "Internet/falcon/10", "ground_truth": "        try:\n            value = self.env['CONTENT_LENGTH']\n        except KeyError:\n            return None\n\n        # NOTE(kgriffs): Normalize an empty value to behave as if\n        # the header were not included; wsgiref, at least, inserts\n        # an empty CONTENT_LENGTH value if the request does not\n        # set the header. Gunicorn and uWSGI do not do this, but\n        # others might if they are trying to match wsgiref's\n        # behavior too closely.\n        if not value:\n            return None\n\n        try:\n            value_as_int = int(value)\n        except ValueError:\n            msg = 'The value of the header must be a number.'\n            raise errors.HTTPInvalidHeader(msg, 'Content-Length')\n\n        if value_as_int < 0:\n            msg = 'The value of the header must be a positive number.'\n            raise errors.HTTPInvalidHeader(msg, 'Content-Length')\n\n        return value_as_int", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 599, "line_no": 600, "id": "falcon.request.Request.content_length", "target_function_prompt": "    def content_length(self):", "function_signature": "    def content_length(self):"}}
{"prompt": "    def discriminator_hash(self):", "metadata": {"task_id": "Internet/pyramid/13", "ground_truth": "        self._assert_resolved()\n        return hash(self.discriminator)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "registry.py"], "context_start_lineno": 231, "line_no": 232, "id": "pyramid.registry.Introspectable.discriminator_hash", "target_function_prompt": "    def discriminator_hash(self):", "function_signature": "    def discriminator_hash(self):"}}
{"prompt": "def regex_guesses(match):", "metadata": {"task_id": "Security/zxcvbn-python/4", "ground_truth": "    char_class_bases = {\n        'alpha_lower': 26,\n        'alpha_upper': 26,\n        'alpha': 52,\n        'alphanumeric': 62,\n        'digits': 10,\n        'symbols': 33,\n    }\n    if match['regex_name'] in char_class_bases:\n        return char_class_bases[match['regex_name']] ** len(match['token'])\n    elif match['regex_name'] == 'recent_year':\n        # conservative estimate of year space: num years from REFERENCE_YEAR.\n        # if year is close to REFERENCE_YEAR, estimate a year space of\n        # MIN_YEAR_SPACE.\n        year_space = abs(int(match['regex_match'].group(0)) - REFERENCE_YEAR)\n        year_space = max(year_space, MIN_YEAR_SPACE)\n\n        return year_space", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "scoring.py"], "context_start_lineno": 296, "line_no": 297, "id": "zxcvbn.scoring.regex_guesses", "target_function_prompt": "def regex_guesses(match):", "function_signature": "def regex_guesses(match):"}}
{"prompt": "    def stream_box_view(\n        self, stream_id: int, caption: str = \"\", title: str = \"\"\n    ) -> None:", "metadata": {"task_id": "Communications/zulip-term/5", "ground_truth": "        self.stream_write_box = ReadlineEdit(\n            edit_text=caption, max_char=self.model.max_stream_name_length\n        )\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        # Use and set a callback to set the stream marker\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "boxes.py"], "context_start_lineno": 380, "line_no": 383, "id": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "target_function_prompt": "    def stream_box_view(\n        self, stream_id: int, caption: str = \"\", title: str = \"\"\n    ) -> None:", "function_signature": "    def stream_box_view(\n        self, stream_id: int, caption: str = \"\", title: str = \"\"\n    ) -> None:"}}
{"prompt": "    def get_arg_names(self, only_required=False):", "metadata": {"task_id": "Utilities/boltons/23", "ground_truth": "        arg_names = tuple(self.args) + tuple(getattr(self, 'kwonlyargs', ()))\n        if only_required:\n            defaults_dict = self.get_defaults_dict()\n            arg_names = tuple([an for an in arg_names if an not in defaults_dict])\n        return arg_names", "fpath_tuple": ["Utilities", "boltons", "boltons", "funcutils.py"], "context_start_lineno": 961, "line_no": 962, "id": "boltons.funcutils.FunctionBuilder.get_arg_names", "target_function_prompt": "    def get_arg_names(self, only_required=False):", "function_signature": "    def get_arg_names(self, only_required=False):"}}
{"prompt": "    def _stream_history_log_dirs(self, output_dir=None):", "metadata": {"task_id": "System/mrjob/16", "ground_truth": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if _logs_exist(self.fs, log_dir):\n                log.info('Looking for history log in %s...' % log_dir)\n                # logs aren't always in a subdir named history/\n                yield [log_dir]", "fpath_tuple": ["System", "mrjob", "mrjob", "hadoop.py"], "context_start_lineno": 545, "line_no": 547, "id": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "target_function_prompt": "    def _stream_history_log_dirs(self, output_dir=None):", "function_signature": "    def _stream_history_log_dirs(self, output_dir=None):"}}
{"prompt": "    def result_to_console_output(cls, result: SessionRenegotiationScanResult) -> List[str]:", "metadata": {"task_id": "System/sslyze/1", "ground_truth": "        result_txt = [cls._format_title(\"Session Renegotiation\")]\n\n        # Client-initiated reneg\n        client_reneg_txt = (\n            \"VULNERABLE - Server honors client-initiated renegotiations\"\n            if result.is_vulnerable_to_client_renegotiation_dos\n            else \"OK - Not vulnerable\"\n        )\n        result_txt.append(cls._format_field(\"Client Renegotiation DoS Attack:\", client_reneg_txt))\n\n        # Secure reneg\n        secure_txt = (\n            \"OK - Supported\"\n            if result.supports_secure_renegotiation\n            else \"VULNERABLE - Secure renegotiation not supported\"\n        )\n        result_txt.append(cls._format_field(\"Secure Renegotiation:\", secure_txt))\n\n        return result_txt", "fpath_tuple": ["System", "sslyze", "sslyze", "plugins", "session_renegotiation_plugin.py"], "context_start_lineno": 56, "line_no": 57, "id": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "target_function_prompt": "    def result_to_console_output(cls, result: SessionRenegotiationScanResult) -> List[str]:", "function_signature": "    def result_to_console_output(cls, result: SessionRenegotiationScanResult) -> List[str]:"}}
{"prompt": "    def read_value(self, key):", "metadata": {"task_id": "System/prometheus-client/0", "ground_truth": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        return _unpack_two_doubles(self._m, pos)", "fpath_tuple": ["System", "prometheus-client", "prometheus_client", "mmap_dict.py"], "context_start_lineno": 120, "line_no": 121, "id": "prometheus_client.mmap_dict.MmapedDict.read_value", "target_function_prompt": "    def read_value(self, key):", "function_signature": "    def read_value(self, key):"}}
{"prompt": "def near_message_url(server_url: str, message: Message) -> str:", "metadata": {"task_id": "Communications/zulip-term/6", "ground_truth": "    if message[\"type\"] == \"stream\":\n        url = near_stream_message_url(server_url, message)\n    else:\n        url = near_pm_message_url(server_url, message)\n    return url", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "server_url.py"], "context_start_lineno": 72, "line_no": 78, "id": "zulipterminal.server_url.near_message_url", "target_function_prompt": "def near_message_url(server_url: str, message: Message) -> str:", "function_signature": "def near_message_url(server_url: str, message: Message) -> str:"}}
{"prompt": "    def unset_cookie(self, name, domain=None, path=None):", "metadata": {"task_id": "Internet/falcon/11", "ground_truth": "        if self._cookies is None:\n            self._cookies = http_cookies.SimpleCookie()\n\n        self._cookies[name] = ''\n\n        # NOTE(Freezerburn): SimpleCookie apparently special cases the\n        # expires attribute to automatically use strftime and set the\n        # time as a delta from the current time. We use -1 here to\n        # basically tell the browser to immediately expire the cookie,\n        # thus removing it from future request objects.\n        self._cookies[name]['expires'] = -1\n\n        # NOTE(CaselIT): Set SameSite to Lax to avoid setting invalid cookies.\n        # See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie/SameSite#Fixing_common_warnings  # noqa: E501\n        self._cookies[name]['samesite'] = 'Lax'\n\n        if domain:\n            self._cookies[name]['domain'] = domain\n\n        if path:\n            self._cookies[name]['path'] = path", "fpath_tuple": ["Internet", "falcon", "falcon", "response.py"], "context_start_lineno": 509, "line_no": 560, "id": "falcon.response.Response.unset_cookie", "target_function_prompt": "    def unset_cookie(self, name, domain=None, path=None):", "function_signature": "    def unset_cookie(self, name, domain=None, path=None):"}}
{"prompt": "    def _required_auth_capability(self):", "metadata": {"task_id": "Internet/boto/5", "ground_truth": "        if self.anon:\n            return ['anon']\n        else:\n            return ['s3']", "fpath_tuple": ["Internet", "boto", "boto", "s3", "connection.py"], "context_start_lineno": 203, "line_no": 204, "id": "boto.s3.connection.S3Connection._required_auth_capability", "target_function_prompt": "    def _required_auth_capability(self):", "function_signature": "    def _required_auth_capability(self):"}}
{"prompt": "    async def facet_results(self):\n        # self.configs should be a plain list of columns", "metadata": {"task_id": "Database/datasette/11", "ground_truth": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            # https://github.com/simonw/datasette/issues/448\n            facet_sql = \"\"\"\n                with inner as ({sql}),\n                deduped_array_items as (\n                    select\n                        distinct j.value,\n                        inner.*\n                    from\n                        json_each([inner].{col}) j\n                        join inner\n                )\n                select\n                    value as value,\n                    count(*) as count\n                from\n                    deduped_array_items\n                group by\n                    value\n                order by\n                    count(*) desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(\n                                self.request, {\"_facet_array\": column}\n                            )\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value\"])\n                    selected = (f\"{column}__arraycontains\", value) in pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column}__arraycontains\": value}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {f\"{column}__arraycontains\": value}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": value,\n                            \"label\": value,\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request, toggle_path\n                            ),\n                            \"selected\": selected,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out", "fpath_tuple": ["Database", "datasette", "datasette", "facets.py"], "context_start_lineno": 369, "line_no": 371, "id": "datasette.facets.ArrayFacet.facet_results", "target_function_prompt": "    async def facet_results(self):\n        # self.configs should be a plain list of columns", "function_signature": "    async def facet_results(self):\n        # self.configs should be a plain list of columns"}}
{"prompt": "def substitute_diminished_for_diminished(\n    progression, substitute_index, ignore_suffix=False\n):", "metadata": {"task_id": "Multimedia/mingus/8", "ground_truth": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(3):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "progressions.py"], "context_start_lineno": 362, "line_no": 373, "id": "mingus.core.progressions.substitute_diminished_for_diminished", "target_function_prompt": "def substitute_diminished_for_diminished(\n    progression, substitute_index, ignore_suffix=False\n):", "function_signature": "def substitute_diminished_for_diminished(\n    progression, substitute_index, ignore_suffix=False\n):"}}
{"prompt": "def _parse_mode(line: str) -> Optional[Mode]:", "metadata": {"task_id": "Utilities/jc/1", "ground_truth": "    result = re.match(_mode_pattern, line)\n    frequencies: List[Frequency] = []\n    if not result:\n        return None\n\n    d = result.groupdict()\n    resolution_width = int(d[\"resolution_width\"])\n    resolution_height = int(d[\"resolution_height\"])\n    is_high_resolution = d[\"is_high_resolution\"] is not None\n\n    mode: Mode = {\n        \"resolution_width\": resolution_width,\n        \"resolution_height\": resolution_height,\n        \"is_high_resolution\": is_high_resolution,\n        \"frequencies\": frequencies,\n    }\n\n    result = re.finditer(_frequencies_pattern, d[\"rest\"])\n    if not result:\n        return mode\n\n    for match in result:\n        d = match.groupdict()\n        frequency = float(d[\"frequency\"])\n        is_current = len(d[\"star\"].strip()) > 0\n        is_preferred = len(d[\"plus\"].strip()) > 0\n        f: Frequency = {\n            \"frequency\": frequency,\n            \"is_current\": is_current,\n            \"is_preferred\": is_preferred,\n        }\n        mode[\"frequencies\"].append(f)\n    return mode", "fpath_tuple": ["Utilities", "jc", "jc", "parsers", "xrandr.py"], "context_start_lineno": 439, "line_no": 440, "id": "jc.parsers.xrandr._parse_mode", "target_function_prompt": "def _parse_mode(line: str) -> Optional[Mode]:", "function_signature": "def _parse_mode(line: str) -> Optional[Mode]:"}}
{"prompt": "    def add_rule(self, ip_protocol, from_port, to_port,\n                 src_group_name, src_group_owner_id, cidr_ip,\n                 src_group_group_id, dry_run=False):", "metadata": {"task_id": "Internet/boto/6", "ground_truth": "        rule = IPPermissions(self)\n        rule.ip_protocol = ip_protocol\n        rule.from_port = from_port\n        rule.to_port = to_port\n        self.rules.append(rule)\n        rule.add_grant(\n            src_group_name,\n            src_group_owner_id,\n            cidr_ip,\n            src_group_group_id,\n            dry_run=dry_run\n        )", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "securitygroup.py"], "context_start_lineno": 96, "line_no": 104, "id": "boto.ec2.securitygroup.SecurityGroup.add_rule", "target_function_prompt": "    def add_rule(self, ip_protocol, from_port, to_port,\n                 src_group_name, src_group_owner_id, cidr_ip,\n                 src_group_group_id, dry_run=False):", "function_signature": "    def add_rule(self, ip_protocol, from_port, to_port,\n                 src_group_name, src_group_owner_id, cidr_ip,\n                 src_group_group_id, dry_run=False):"}}
{"prompt": "def null_stemmer(object):", "metadata": {"task_id": "Internet/sumy/6", "ground_truth": "    from ..._compat import to_unicode\n    return to_unicode(object).lower()", "fpath_tuple": ["Internet", "sumy", "sumy", "nlp", "stemmers", "__init__.py"], "context_start_lineno": 15, "line_no": 17, "id": "sumy.nlp.stemmers.null_stemmer", "target_function_prompt": "def null_stemmer(object):", "function_signature": "def null_stemmer(object):"}}
{"prompt": "    def make_shell(self):", "metadata": {"task_id": "Internet/pyramid/14", "ground_truth": "        shells = self.find_all_shells()\n\n        shell = None\n        user_shell = self.args.python_shell.lower()\n\n        if not user_shell:\n            preferred_shells = self.preferred_shells\n            if not preferred_shells:\n                # by default prioritize all shells above python\n                preferred_shells = [k for k in shells.keys() if k != 'python']\n            max_weight = len(preferred_shells)\n\n            def order(x):\n                # invert weight to reverse sort the list\n                # (closer to the front is higher priority)\n                try:\n                    return preferred_shells.index(x[0].lower()) - max_weight\n                except ValueError:\n                    return 1\n\n            sorted_shells = sorted(shells.items(), key=order)\n\n            if len(sorted_shells) > 0:\n                shell = sorted_shells[0][1]\n\n        else:\n            runner = shells.get(user_shell)\n\n            if runner is not None:\n                shell = runner\n\n            if shell is None:\n                raise ValueError(\n                    'could not find a shell named \"%s\"' % user_shell\n                )\n\n        if shell is None:\n            # should never happen, but just incase entry points are borked\n            shell = self.default_runner\n\n        return shell", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "scripts", "pshell.py"], "context_start_lineno": 237, "line_no": 238, "id": "pyramid.scripts.pshell.PShellCommand.make_shell", "target_function_prompt": "    def make_shell(self):", "function_signature": "    def make_shell(self):"}}
{"prompt": "    def _find_binaries_and_jars(self):", "metadata": {"task_id": "System/mrjob/17", "ground_truth": "        self.get_hadoop_version()\n\n        if self._has_hadoop_streaming_steps():\n            self.get_hadoop_streaming_jar()\n\n        if self._has_spark_steps():\n            self.get_spark_submit_bin()", "fpath_tuple": ["System", "mrjob", "mrjob", "hadoop.py"], "context_start_lineno": 333, "line_no": 341, "id": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "target_function_prompt": "    def _find_binaries_and_jars(self):", "function_signature": "    def _find_binaries_and_jars(self):"}}
{"prompt": "def split_template_path(template: str) -> t.List[str]:", "metadata": {"task_id": "Internet/Jinja2/5", "ground_truth": "    pieces = []\n    for piece in template.split(\"/\"):\n        if (\n            os.path.sep in piece\n            or (os.path.altsep and os.path.altsep in piece)\n            or piece == os.path.pardir\n        ):\n            raise TemplateNotFound(template)\n        elif piece and piece != \".\":\n            pieces.append(piece)\n    return pieces", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "loaders.py"], "context_start_lineno": 24, "line_no": 28, "id": "jinja2.loaders.split_template_path", "target_function_prompt": "def split_template_path(template: str) -> t.List[str]:", "function_signature": "def split_template_path(template: str) -> t.List[str]:"}}
{"prompt": "    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> tuple[list[t.Any], list[int]]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/9", "ground_truth": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 549, "line_no": 554, "id": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "target_function_prompt": "    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> tuple[list[t.Any], list[int]]:", "function_signature": "    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> tuple[list[t.Any], list[int]]:"}}
{"prompt": "    def to_json(self, handler=None):", "metadata": {"task_id": "Internet/falcon/12", "ground_truth": "        from falcon.constants import MEDIA_JSON\n        obj = self.to_dict(OrderedDict)\n        if handler is None:\n            handler = _DEFAULT_JSON_HANDLER\n        return handler.serialize(obj, MEDIA_JSON)", "fpath_tuple": ["Internet", "falcon", "falcon", "http_error.py"], "context_start_lineno": 177, "line_no": 190, "id": "falcon.http_error.HTTPError.to_json", "target_function_prompt": "    def to_json(self, handler=None):", "function_signature": "    def to_json(self, handler=None):"}}
{"prompt": "    def pluralize(self, singular, plural, n, domain=None, mapping=None):", "metadata": {"task_id": "Internet/pyramid/15", "ground_truth": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(\n            singular, plural, n, domain=domain, mapping=mapping\n        )", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "i18n.py"], "context_start_lineno": 63, "line_no": 102, "id": "pyramid.i18n.Localizer.pluralize", "target_function_prompt": "    def pluralize(self, singular, plural, n, domain=None, mapping=None):", "function_signature": "    def pluralize(self, singular, plural, n, domain=None, mapping=None):"}}
{"prompt": "def assert_can_create():", "metadata": {"task_id": "Text-Processing/pymorphy2/0", "ground_truth": "    if not EXTENSION_AVAILABLE:\n        msg = (\"Creating of DAWGs with DAWG-Python is \"\n               \"not supported; install 'dawg' package.\")\n        raise NotImplementedError(msg)", "fpath_tuple": ["Text-Processing", "pymorphy2", "pymorphy2", "dawg.py"], "context_start_lineno": 12, "line_no": 13, "id": "pymorphy2.dawg.assert_can_create", "target_function_prompt": "def assert_can_create():", "function_signature": "def assert_can_create():"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/7", "ground_truth": "    from boto.regioninfo import connect\n    from boto.s3.connection import S3Connection\n    if 'host' in kw_params:\n        host = kw_params.pop('host')\n        if host not in ['', None]:\n            region = S3RegionInfo(\n                name='custom',\n                endpoint=host,\n                connection_cls=S3Connection\n            )\n            return region.connect(**kw_params)\n\n    return connect('s3', region_name, region_cls=S3RegionInfo,\n                   connection_cls=S3Connection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "s3", "__init__.py"], "context_start_lineno": 61, "line_no": 62, "id": "boto.s3.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def commit(self, blocking=True):", "metadata": {"task_id": "Database/sqlitedict/1", "ground_truth": "        if self.conn is not None:\n            self.conn.commit(blocking)", "fpath_tuple": ["Database", "sqlitedict", "sqlitedict.py"], "context_start_lineno": 369, "line_no": 376, "id": "sqlitedict.SqliteDict.commit", "target_function_prompt": "    def commit(self, blocking=True):", "function_signature": "    def commit(self, blocking=True):"}}
{"prompt": "    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> DropColumnOp:", "metadata": {"task_id": "Database/alembic/1", "ground_truth": "        return cls(\n            tname,\n            col.name,\n            schema=schema,\n            _reverse=AddColumnOp.from_column_and_tablename(schema, tname, col),\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 2201, "line_no": 2207, "id": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "target_function_prompt": "    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> DropColumnOp:", "function_signature": "    def from_column_and_tablename(\n        cls,\n        schema: Optional[str],\n        tname: str,\n        col: Column[Any],\n    ) -> DropColumnOp:"}}
{"prompt": "    def count(self):", "metadata": {"task_id": "Internet/boto/8", "ground_truth": "        info = self.describe()\n        return info['Table'].get('ItemCount', 0)", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 1603, "line_no": 1616, "id": "boto.dynamodb2.table.Table.count", "target_function_prompt": "    def count(self):", "function_signature": "    def count(self):"}}
{"prompt": "    def get_keys(self):", "metadata": {"task_id": "Internet/boto/9", "ground_truth": "        key_fields = self.table.get_key_fields()\n        key_data = {}\n\n        for key in key_fields:\n            key_data[key] = self[key]\n\n        return key_data", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "items.py"], "context_start_lineno": 225, "line_no": 231, "id": "boto.dynamodb2.items.Item.get_keys", "target_function_prompt": "    def get_keys(self):", "function_signature": "    def get_keys(self):"}}
{"prompt": "    def add(self, translations, merge=True):", "metadata": {"task_id": "Internet/pyramid/16", "ground_truth": "        domain = getattr(translations, 'domain', self.DEFAULT_DOMAIN)\n        if domain == self.DEFAULT_DOMAIN and self.plural is DEFAULT_PLURAL:\n            self.plural = translations.plural\n\n        if merge and domain == self.domain:\n            return self.merge(translations)\n\n        existing = self._domains.get(domain)\n        if merge and existing is not None:\n            existing.merge(translations)\n        else:\n            translations.add_fallback(self)\n            self._domains[domain] = translations\n\n        return self", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "i18n.py"], "context_start_lineno": 276, "line_no": 292, "id": "pyramid.i18n.Translations.add", "target_function_prompt": "    def add(self, translations, merge=True):", "function_signature": "    def add(self, translations, merge=True):"}}
{"prompt": "    def add_passthru_arg(self, *args, **kwargs):", "metadata": {"task_id": "System/mrjob/18", "ground_truth": "        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 1186, "line_no": 1204, "id": "mrjob.job.MRJob.add_passthru_arg", "target_function_prompt": "    def add_passthru_arg(self, *args, **kwargs):", "function_signature": "    def add_passthru_arg(self, *args, **kwargs):"}}
{"prompt": "    def client(\n        self,\n        identity=None,\n        url=None,\n        method=None,\n        status_callback_event=None,\n        status_callback=None,\n        status_callback_method=None,\n        **kwargs\n    ):", "metadata": {"task_id": "Communications/twilio-fatisar/5", "ground_truth": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "twiml", "voice_response.py"], "context_start_lineno": 1973, "line_no": 1996, "id": "twilio.twiml.voice_response.Dial.client", "target_function_prompt": "    def client(\n        self,\n        identity=None,\n        url=None,\n        method=None,\n        status_callback_event=None,\n        status_callback=None,\n        status_callback_method=None,\n        **kwargs\n    ):", "function_signature": "    def client(\n        self,\n        identity=None,\n        url=None,\n        method=None,\n        status_callback_event=None,\n        status_callback=None,\n        status_callback_method=None,\n        **kwargs\n    ):"}}
{"prompt": "    def _build_filters(self, filter_kwargs, using=QUERY_OPERATORS):", "metadata": {"task_id": "Internet/boto/10", "ground_truth": "        if filter_kwargs is None:\n            return\n\n        filters = {}\n\n        for field_and_op, value in filter_kwargs.items():\n            field_bits = field_and_op.split('__')\n            fieldname = '__'.join(field_bits[:-1])\n\n            try:\n                op = using[field_bits[-1]]\n            except KeyError:\n                raise exceptions.UnknownFilterTypeError(\n                    \"Operator '%s' from '%s' is not recognized.\" % (\n                        field_bits[-1],\n                        field_and_op\n                    )\n                )\n\n            lookup = {\n                'AttributeValueList': [],\n                'ComparisonOperator': op,\n            }\n\n            # Special-case the ``NULL/NOT_NULL`` case.\n            if field_bits[-1] == 'null':\n                del lookup['AttributeValueList']\n\n                if value is False:\n                    lookup['ComparisonOperator'] = 'NOT_NULL'\n                else:\n                    lookup['ComparisonOperator'] = 'NULL'\n            # Special-case the ``BETWEEN`` case.\n            elif field_bits[-1] == 'between':\n                if len(value) == 2 and isinstance(value, (list, tuple)):\n                    lookup['AttributeValueList'].append(\n                        self._dynamizer.encode(value[0])\n                    )\n                    lookup['AttributeValueList'].append(\n                        self._dynamizer.encode(value[1])\n                    )\n            # Special-case the ``IN`` case\n            elif field_bits[-1] == 'in':\n                for val in value:\n                    lookup['AttributeValueList'].append(self._dynamizer.encode(val))\n            else:\n                # Fix up the value for encoding, because it was built to only work\n                # with ``set``s.\n                if isinstance(value, (list, tuple)):\n                    value = set(value)\n                lookup['AttributeValueList'].append(\n                    self._dynamizer.encode(value)\n                )\n\n            # Finally, insert it into the filters.\n            filters[fieldname] = lookup\n\n        return filters", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 989, "line_no": 994, "id": "boto.dynamodb2.table.Table._build_filters", "target_function_prompt": "    def _build_filters(self, filter_kwargs, using=QUERY_OPERATORS):", "function_signature": "    def _build_filters(self, filter_kwargs, using=QUERY_OPERATORS):"}}
{"prompt": "    def checkpoint(self):", "metadata": {"task_id": "Database/bplustree/5", "ground_truth": "        if self._not_committed_pages:\n            logger.warning('Closing WAL with uncommitted data, discarding it')\n\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        for page, page_start in self._committed_pages.items():\n            page_data = read_from_file(\n                self._fd,\n                page_start,\n                page_start + self._page_size\n            )\n            yield page, page_data\n\n        self._fd.close()\n        os.unlink(self.filename)\n        if self._dir_fd is not None:\n            os.fsync(self._dir_fd)\n            os.close(self._dir_fd)", "fpath_tuple": ["Database", "bplustree", "bplustree", "memory.py"], "context_start_lineno": 313, "line_no": 315, "id": "bplustree.memory.WAL.checkpoint", "target_function_prompt": "    def checkpoint(self):", "function_signature": "    def checkpoint(self):"}}
{"prompt": "    def batch_to_payloads(\n        cls,\n        batch: ext.NpNDArray,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/10", "ground_truth": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 322, "line_no": 328, "id": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "target_function_prompt": "    def batch_to_payloads(\n        cls,\n        batch: ext.NpNDArray,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:", "function_signature": "    def batch_to_payloads(\n        cls,\n        batch: ext.NpNDArray,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:"}}
{"prompt": "def dumps_list(l, *, escape=True, token=\"%\\n\", mapper=None, as_content=True):", "metadata": {"task_id": "Text-Processing/PyLaTeX/0", "ground_truth": "    strings = (\n        _latex_item_to_string(i, escape=escape, as_content=as_content) for i in l\n    )\n\n    if mapper is not None:\n        if not isinstance(mapper, list):\n            mapper = [mapper]\n\n        for m in mapper:\n            strings = [m(s) for s in strings]\n        strings = [_latex_item_to_string(s) for s in strings]\n\n    return NoEscape(token.join(strings))", "fpath_tuple": ["Text-Processing", "PyLaTeX", "pylatex", "utils.py"], "context_start_lineno": 149, "line_no": 188, "id": "pylatex.utils.dumps_list", "target_function_prompt": "def dumps_list(l, *, escape=True, token=\"%\\n\", mapper=None, as_content=True):", "function_signature": "def dumps_list(l, *, escape=True, token=\"%\\n\", mapper=None, as_content=True):"}}
{"prompt": "def uppercase_variations(match):", "metadata": {"task_id": "Security/zxcvbn-python/5", "ground_truth": "    word = match['token']\n\n    if ALL_LOWER.match(word) or word.lower() == word:\n        return 1\n\n    for regex in [START_UPPER, END_UPPER, ALL_UPPER]:\n        if regex.match(word):\n            return 2\n\n    U = sum(1 for c in word if c.isupper())\n    L = sum(1 for c in word if c.islower())\n    variations = 0\n    for i in range(1, min(U, L) + 1):\n        variations += nCk(U + L, i)\n\n    return variations", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "scoring.py"], "context_start_lineno": 373, "line_no": 374, "id": "zxcvbn.scoring.uppercase_variations", "target_function_prompt": "def uppercase_variations(match):", "function_signature": "def uppercase_variations(match):"}}
{"prompt": "def is_consonant(note1, note2, include_fourths=True):", "metadata": {"task_id": "Multimedia/mingus/9", "ground_truth": "    return is_perfect_consonant(\n        note1, note2, include_fourths\n    ) or is_imperfect_consonant(note1, note2)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "intervals.py"], "context_start_lineno": 488, "line_no": 501, "id": "mingus.core.intervals.is_consonant", "target_function_prompt": "def is_consonant(note1, note2, include_fourths=True):", "function_signature": "def is_consonant(note1, note2, include_fourths=True):"}}
{"prompt": "def get_plugins():", "metadata": {"task_id": "Database/datasette/12", "ground_truth": "    plugins = []\n    plugin_to_distinfo = dict(pm.list_plugin_distinfo())\n    for plugin in pm.get_plugins():\n        static_path = None\n        templates_path = None\n        if plugin.__name__ not in DEFAULT_PLUGINS:\n            try:\n                if pkg_resources.resource_isdir(plugin.__name__, \"static\"):\n                    static_path = pkg_resources.resource_filename(\n                        plugin.__name__, \"static\"\n                    )\n                if pkg_resources.resource_isdir(plugin.__name__, \"templates\"):\n                    templates_path = pkg_resources.resource_filename(\n                        plugin.__name__, \"templates\"\n                    )\n            except (KeyError, ImportError):\n                # Caused by --plugins_dir= plugins - KeyError/ImportError thrown in Py3.5\n                pass\n        plugin_info = {\n            \"name\": plugin.__name__,\n            \"static_path\": static_path,\n            \"templates_path\": templates_path,\n            \"hooks\": [h.name for h in pm.get_hookcallers(plugin)],\n        }\n        distinfo = plugin_to_distinfo.get(plugin)\n        if distinfo:\n            plugin_info[\"version\"] = distinfo.version\n            plugin_info[\"name\"] = distinfo.project_name\n        plugins.append(plugin_info)\n    return plugins", "fpath_tuple": ["Database", "datasette", "datasette", "plugins.py"], "context_start_lineno": 34, "line_no": 35, "id": "datasette.plugins.get_plugins", "target_function_prompt": "def get_plugins():", "function_signature": "def get_plugins():"}}
{"prompt": "def _parse_pre_yarn_history_records(lines):", "metadata": {"task_id": "System/mrjob/19", "ground_truth": "    def yield_record_strings(lines):\n        record_lines = []\n        start_line = 0\n\n        for line_num, line in enumerate(lines):\n            record_lines.append(line)\n            if line.endswith(' .\\n'):\n                yield start_line, len(record_lines), ''.join(record_lines)\n                record_lines = []\n                start_line = line_num + 1\n\n    for start_line, num_lines, record_str in yield_record_strings(lines):\n        record_match = _PRE_YARN_HISTORY_RECORD.match(record_str)\n\n        if not record_match:\n            continue\n\n        record_type = record_match.group('type')\n        key_pairs = record_match.group('key_pairs')\n\n        fields = {}\n        for m in _PRE_YARN_HISTORY_KEY_PAIR.finditer(key_pairs):\n            key = m.group('key')\n            value = _pre_yarn_history_unescape(m.group('escaped_value'))\n\n            fields[key] = value\n\n        yield dict(\n            fields=fields,\n            num_lines=num_lines,\n            start_line=start_line,\n            type=record_type,\n        )", "fpath_tuple": ["System", "mrjob", "mrjob", "logs", "history.py"], "context_start_lineno": 335, "line_no": 359, "id": "mrjob.logs.history._parse_pre_yarn_history_records", "target_function_prompt": "def _parse_pre_yarn_history_records(lines):", "function_signature": "def _parse_pre_yarn_history_records(lines):"}}
{"prompt": "def chunk_hashes(bytestring, chunk_size=_MEGABYTE):", "metadata": {"task_id": "Internet/boto/11", "ground_truth": "    chunk_count = int(math.ceil(len(bytestring) / float(chunk_size)))\n    hashes = []\n    for i in range(chunk_count):\n        start = i * chunk_size\n        end = (i + 1) * chunk_size\n        hashes.append(hashlib.sha256(bytestring[start:end]).digest())\n    if not hashes:\n        return [hashlib.sha256(b'').digest()]\n    return hashes", "fpath_tuple": ["Internet", "boto", "boto", "glacier", "utils.py"], "context_start_lineno": 73, "line_no": 74, "id": "boto.glacier.utils.chunk_hashes", "target_function_prompt": "def chunk_hashes(bytestring, chunk_size=_MEGABYTE):", "function_signature": "def chunk_hashes(bytestring, chunk_size=_MEGABYTE):"}}
{"prompt": "def get_package_name(dependency,\n                     use_cache=True):", "metadata": {"task_id": "Utilities/python-for-android/5", "ground_truth": "    def timestamp():\n        try:\n            return time.monotonic()\n        except AttributeError:\n            return time.time()  # Python 2.\n    try:\n        value = package_name_cache[dependency]\n        if value[0] + 600.0 > timestamp() and use_cache:\n            return value[1]\n    except KeyError:\n        pass\n    result = _extract_info_from_package(dependency, extract_type=\"name\")\n    package_name_cache[dependency] = (timestamp(), result)\n    return result", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "pythonpackage.py"], "context_start_lineno": 586, "line_no": 588, "id": "pythonforandroid.pythonpackage.get_package_name", "target_function_prompt": "def get_package_name(dependency,\n                     use_cache=True):", "function_signature": "def get_package_name(dependency,\n                     use_cache=True):"}}
{"prompt": "def make_safe_absolute_uri(base, rel=None):\n    # bail if ACCEPTABLE_URI_SCHEMES is empty", "metadata": {"task_id": "Text-Processing/feedparser/1", "ground_truth": "    if not ACCEPTABLE_URI_SCHEMES:\n        return _urljoin(base, rel or '')\n    if not base:\n        return rel or ''\n    if not rel:\n        try:\n            scheme = urllib.parse.urlparse(base)[0]\n        except ValueError:\n            return ''\n        if not scheme or scheme in ACCEPTABLE_URI_SCHEMES:\n            return base\n        return ''\n    uri = _urljoin(base, rel)\n    if uri.strip().split(':', 1)[0] not in ACCEPTABLE_URI_SCHEMES:\n        return ''\n    return uri", "fpath_tuple": ["Text-Processing", "feedparser", "feedparser", "urls.py"], "context_start_lineno": 85, "line_no": 87, "id": "feedparser.urls.make_safe_absolute_uri", "target_function_prompt": "def make_safe_absolute_uri(base, rel=None):\n    # bail if ACCEPTABLE_URI_SCHEMES is empty", "function_signature": "def make_safe_absolute_uri(base, rel=None):\n    # bail if ACCEPTABLE_URI_SCHEMES is empty"}}
{"prompt": "    def gather(\n        self,\n        input=None,\n        action=None,\n        method=None,\n        timeout=None,\n        speech_timeout=None,\n        max_speech_time=None,\n        profanity_filter=None,\n        finish_on_key=None,\n        num_digits=None,\n        partial_result_callback=None,\n        partial_result_callback_method=None,\n        language=None,\n        hints=None,\n        barge_in=None,\n        debug=None,\n        action_on_empty_result=None,\n        speech_model=None,\n        enhanced=None,\n        **kwargs\n    ):", "metadata": {"task_id": "Communications/twilio-fatisar/6", "ground_truth": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "twiml", "voice_response.py"], "context_start_lineno": 151, "line_no": 198, "id": "twilio.twiml.voice_response.VoiceResponse.gather", "target_function_prompt": "    def gather(\n        self,\n        input=None,\n        action=None,\n        method=None,\n        timeout=None,\n        speech_timeout=None,\n        max_speech_time=None,\n        profanity_filter=None,\n        finish_on_key=None,\n        num_digits=None,\n        partial_result_callback=None,\n        partial_result_callback_method=None,\n        language=None,\n        hints=None,\n        barge_in=None,\n        debug=None,\n        action_on_empty_result=None,\n        speech_model=None,\n        enhanced=None,\n        **kwargs\n    ):", "function_signature": "    def gather(\n        self,\n        input=None,\n        action=None,\n        method=None,\n        timeout=None,\n        speech_timeout=None,\n        max_speech_time=None,\n        profanity_filter=None,\n        finish_on_key=None,\n        num_digits=None,\n        partial_result_callback=None,\n        partial_result_callback_method=None,\n        language=None,\n        hints=None,\n        barge_in=None,\n        debug=None,\n        action_on_empty_result=None,\n        speech_model=None,\n        enhanced=None,\n        **kwargs\n    ):"}}
{"prompt": "    def to_string(self):", "metadata": {"task_id": "Utilities/boltons/24", "ground_truth": "        lines = [u'Traceback (most recent call last):']\n\n        for frame in self.frames:\n            lines.append(u'  File \"%s\", line %s, in %s' % (frame['filepath'],\n                                                           frame['lineno'],\n                                                           frame['funcname']))\n            source_line = frame.get('source_line')\n            if source_line:\n                lines.append(u'    %s' % (source_line,))\n        if self.exc_msg:\n            lines.append(u'%s: %s' % (self.exc_type, self.exc_msg))\n        else:\n            lines.append(u'%s' % (self.exc_type,))\n        return u'\\n'.join(lines)", "fpath_tuple": ["Utilities", "boltons", "boltons", "tbutils.py"], "context_start_lineno": 725, "line_no": 732, "id": "boltons.tbutils.ParsedException.to_string", "target_function_prompt": "    def to_string(self):", "function_signature": "    def to_string(self):"}}
{"prompt": "    def darwin_installer(self):", "metadata": {"task_id": "Utilities/python-for-android/6", "ground_truth": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 346, "line_no": 347, "id": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "target_function_prompt": "    def darwin_installer(self):", "function_signature": "    def darwin_installer(self):"}}
{"prompt": "    def loads(cls, etag_str):", "metadata": {"task_id": "Internet/falcon/13", "ground_truth": "        value = etag_str\n\n        is_weak = False\n        if value.startswith(('W/', 'w/')):\n            is_weak = True\n            value = value[2:]\n\n        # NOTE(kgriffs): We allow for an unquoted entity-tag just in case,\n        #   although it has been non-standard to do so since at least 1999\n        #   with the advent of RFC 2616.\n        if value[:1] == value[-1:] == '\"':\n            value = value[1:-1]\n\n        t = cls(value)\n        t.is_weak = is_weak\n\n        return t", "fpath_tuple": ["Internet", "falcon", "falcon", "util", "structures.py"], "context_start_lineno": 282, "line_no": 302, "id": "falcon.util.structures.ETag.loads", "target_function_prompt": "    def loads(cls, etag_str):", "function_signature": "    def loads(cls, etag_str):"}}
{"prompt": "def compute_values_probs(  # nosec\n    value_counts: Union[StateMatrix, dict],\n    param_value_counts: Union[StateMatrix, dict],\n    unk_token: str,\n) -> Tuple[StateMatrix, StateMatrix]:", "metadata": {"task_id": "Security/msticpy/3", "ground_truth": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    value_cond_param_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for param, values in param_value_counts.items():\n        n_val = sum(values.values())\n        for value, count in values.items():\n            value_cond_param_probs[param][value] = count / n_val\n\n    tot_val = sum(value_counts.values())\n    for value, count in value_counts.items():\n        value_probs[value] = count / tot_val\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    value_cond_param_probs_sm = StateMatrix(\n        states=value_cond_param_probs, unk_token=unk_token\n    )\n\n    return value_probs_sm, value_cond_param_probs_sm", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "probabilities.py"], "context_start_lineno": 132, "line_no": 166, "id": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "target_function_prompt": "def compute_values_probs(  # nosec\n    value_counts: Union[StateMatrix, dict],\n    param_value_counts: Union[StateMatrix, dict],\n    unk_token: str,\n) -> Tuple[StateMatrix, StateMatrix]:", "function_signature": "def compute_values_probs(  # nosec\n    value_counts: Union[StateMatrix, dict],\n    param_value_counts: Union[StateMatrix, dict],\n    unk_token: str,\n) -> Tuple[StateMatrix, StateMatrix]:"}}
{"prompt": "    def add_fs(self, name, fs, disable_if=None):", "metadata": {"task_id": "System/mrjob/20", "ground_truth": "        if name in self._fs_names:\n            raise ValueError('name %r is already taken' % name)\n\n        setattr(self, name, fs)\n        self._fs_names.append(name)\n\n        if disable_if:\n            self._disable_if[name] = disable_if", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "composite.py"], "context_start_lineno": 62, "line_no": 74, "id": "mrjob.fs.composite.CompositeFilesystem.add_fs", "target_function_prompt": "    def add_fs(self, name, fs, disable_if=None):", "function_signature": "    def add_fs(self, name, fs, disable_if=None):"}}
{"prompt": "    def remember(self, request, userid, **kw):", "metadata": {"task_id": "Internet/pyramid/17", "ground_truth": "        request.session[self.userid_key] = userid\n        return []", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "authentication.py"], "context_start_lineno": 1264, "line_no": 1266, "id": "pyramid.authentication.SessionAuthenticationHelper.remember", "target_function_prompt": "    def remember(self, request, userid, **kw):", "function_signature": "    def remember(self, request, userid, **kw):"}}
{"prompt": "    def values(self):", "metadata": {"task_id": "Internet/boto/12", "ground_truth": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()", "fpath_tuple": ["Internet", "boto", "boto", "utils.py"], "context_start_lineno": 344, "line_no": 345, "id": "boto.utils.LazyLoadMetadata.values", "target_function_prompt": "    def values(self):", "function_signature": "    def values(self):"}}
{"prompt": "def inspect_error_handlers(app: App) -> 'List[ErrorHandlerInfo]':", "metadata": {"task_id": "Internet/falcon/14", "ground_truth": "    errors = []\n    for exc, fn in app._error_handlers.items():\n        source_info, name = _get_source_info_and_name(fn)\n        info = ErrorHandlerInfo(exc.__name__, name, source_info, _is_internal(fn))\n        errors.append(info)\n    return errors", "fpath_tuple": ["Internet", "falcon", "falcon", "inspect.py"], "context_start_lineno": 140, "line_no": 151, "id": "falcon.inspect.inspect_error_handlers", "target_function_prompt": "def inspect_error_handlers(app: App) -> 'List[ErrorHandlerInfo]':", "function_signature": "def inspect_error_handlers(app: App) -> 'List[ErrorHandlerInfo]':"}}
{"prompt": "    def get_template(\n        self,\n        name: t.Union[str, \"Template\"],\n        parent: t.Optional[str] = None,\n        globals: t.Optional[t.MutableMapping[str, t.Any]] = None,\n    ) -> \"Template\":", "metadata": {"task_id": "Internet/Jinja2/6", "ground_truth": "        if isinstance(name, Template):\n            return name\n        if parent is not None:\n            name = self.join_path(name, parent)\n\n        return self._load_template(name, globals)", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "environment.py"], "context_start_lineno": 975, "line_no": 1004, "id": "jinja2.environment.Environment.get_template", "target_function_prompt": "    def get_template(\n        self,\n        name: t.Union[str, \"Template\"],\n        parent: t.Optional[str] = None,\n        globals: t.Optional[t.MutableMapping[str, t.Any]] = None,\n    ) -> \"Template\":", "function_signature": "    def get_template(\n        self,\n        name: t.Union[str, \"Template\"],\n        parent: t.Optional[str] = None,\n        globals: t.Optional[t.MutableMapping[str, t.Any]] = None,\n    ) -> \"Template\":"}}
{"prompt": "    def darwin_checker(self):", "metadata": {"task_id": "Utilities/python-for-android/7", "ground_truth": "        return (\n            self._darwin_get_brew_formula_location_prefix(\n                self.homebrew_formula_name, installed=True\n            )\n            is not None\n        )", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 268, "line_no": 269, "id": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "target_function_prompt": "    def darwin_checker(self):", "function_signature": "    def darwin_checker(self):"}}
{"prompt": "    def to_hertz(self, standard_pitch=440):", "metadata": {"task_id": "Multimedia/mingus/10", "ground_truth": "        diff = self.__int__() - 57\n        return 2 ** (diff / 12.0) * standard_pitch", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "note.py"], "context_start_lineno": 225, "line_no": 232, "id": "mingus.containers.note.Note.to_hertz", "target_function_prompt": "    def to_hertz(self, standard_pitch=440):", "function_signature": "    def to_hertz(self, standard_pitch=440):"}}
{"prompt": "    def from_string(\n        self,\n        source: t.Union[str, nodes.Template],\n        globals: t.Optional[t.MutableMapping[str, t.Any]] = None,\n        template_class: t.Optional[t.Type[\"Template\"]] = None,\n    ) -> \"Template\":", "metadata": {"task_id": "Internet/Jinja2/7", "ground_truth": "        gs = self.make_globals(globals)\n        cls = template_class or self.template_class\n        return cls.from_code(self, self.compile(source), gs, None)", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "environment.py"], "context_start_lineno": 1085, "line_no": 1102, "id": "jinja2.environment.Environment.from_string", "target_function_prompt": "    def from_string(\n        self,\n        source: t.Union[str, nodes.Template],\n        globals: t.Optional[t.MutableMapping[str, t.Any]] = None,\n        template_class: t.Optional[t.Type[\"Template\"]] = None,\n    ) -> \"Template\":", "function_signature": "    def from_string(\n        self,\n        source: t.Union[str, nodes.Template],\n        globals: t.Optional[t.MutableMapping[str, t.Any]] = None,\n        template_class: t.Optional[t.Type[\"Template\"]] = None,\n    ) -> \"Template\":"}}
{"prompt": "    def start(self, segment):", "metadata": {"task_id": "System/wal-e/5", "ground_truth": "        if self.closed:\n            raise UserCritical(msg='attempt to transfer wal after closing',\n                               hint='report a bug')\n\n        g = gevent.Greenlet(self.transferer, segment)\n        g.link(self._complete_execution)\n        self.greenlets.add(g)\n\n        # Increment .expect before starting the greenlet, or else a\n        # very unlucky .join could be fooled as to when pool is\n        # complete.\n        self.expect += 1\n\n        g.start()", "fpath_tuple": ["System", "wal-e", "wal_e", "worker", "pg", "wal_transfer.py"], "context_start_lineno": 145, "line_no": 148, "id": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "target_function_prompt": "    def start(self, segment):", "function_signature": "    def start(self, segment):"}}
{"prompt": "    def set_header(self, name, value):", "metadata": {"task_id": "Internet/falcon/15", "ground_truth": "        value = str(value)\n\n        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to set cookies')\n\n        self._headers[name] = value", "fpath_tuple": ["Internet", "falcon", "falcon", "response.py"], "context_start_lineno": 614, "line_no": 640, "id": "falcon.response.Response.set_header", "target_function_prompt": "    def set_header(self, name, value):", "function_signature": "    def set_header(self, name, value):"}}
{"prompt": "    def execute(self):", "metadata": {"task_id": "Communications/chatette/8", "ground_truth": "        if len(self.command_tokens) < 3:\n            self.print_wrapper.error_log(\n                \"Missing some arguments\\nUsage: \" +\n                'unhide <unit-type> \"<unit-name>\"'\n            )\n            return\n\n        unit_type = CommandStrategy.get_unit_type_from_str(self.command_tokens[1])\n        if unit_type is None:\n            self.print_wrapper.error_log(\n                \"Unknown unit type: '\" + str(self.command_tokens[1]) + \"'.\"\n            )\n            return\n\n        unit_regex = self.get_regex_name(self.command_tokens[2])\n        if unit_regex is None:\n            try:\n                [unit_name, variation_name] = \\\n                    CommandStrategy.split_exact_unit_name(self.command_tokens[2])\n            except SyntaxError:\n                self.print_wrapper.error_log(\n                    \"Unit identifier couldn't be interpreted. \" + \\\n                    \"Did you mean to escape some hashtags '#'?\"\n                )\n                return\n            self.execute_on_unit(unit_type, unit_name, variation_name)\n        else:\n            unit_names = [\n                unit_name\n                for unit_name in HideCommand.stored_units[unit_type.name]\n                if unit_regex.match(unit_name)\n            ]\n            if len(unit_names) == 0:\n                self.print_wrapper.write(\"No \" + unit_type.name + \" matched.\")\n\n            for unit_name in unit_names:\n                self.execute_on_unit(unit_type, unit_name)", "fpath_tuple": ["Communications", "chatette", "chatette", "cli", "interactive_commands", "unhide_command.py"], "context_start_lineno": 13, "line_no": 18, "id": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "target_function_prompt": "    def execute(self):", "function_signature": "    def execute(self):"}}
{"prompt": "def reduce_accidentals(note):", "metadata": {"task_id": "Multimedia/mingus/11", "ground_truth": "    val = note_to_int(note[0])\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n        else:\n            raise NoteFormatError(\"Unknown note format '%s'\" % note)\n    if val >= note_to_int(note[0]):\n        return int_to_note(val % 12)\n    else:\n        return int_to_note(val % 12, \"b\")", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "notes.py"], "context_start_lineno": 100, "line_no": 107, "id": "mingus.core.notes.reduce_accidentals", "target_function_prompt": "def reduce_accidentals(note):", "function_signature": "def reduce_accidentals(note):"}}
{"prompt": "    def run_combiner(self, step_num=0):", "metadata": {"task_id": "System/mrjob/21", "ground_truth": "        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 768, "line_no": 783, "id": "mrjob.job.MRJob.run_combiner", "target_function_prompt": "    def run_combiner(self, step_num=0):", "function_signature": "    def run_combiner(self, step_num=0):"}}
{"prompt": "    def lookup(self, mac):", "metadata": {"task_id": "System/trackerjacker/0", "ground_truth": "        try:\n            oui_prefix = mac.upper().replace(':', '')[0:6]\n            if oui_prefix in self.db:\n                return self.db[oui_prefix]\n        except Exception:\n            pass\n\n        return ''", "fpath_tuple": ["System", "trackerjacker", "trackerjacker", "ieee_mac_vendor_db.py"], "context_start_lineno": 13, "line_no": 15, "id": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "target_function_prompt": "    def lookup(self, mac):", "function_signature": "    def lookup(self, mac):"}}
{"prompt": "    async def check_visibility(\n        self,\n        actor: dict,\n        action: Optional[str] = None,\n        resource: Optional[Union[str, Tuple[str, str]]] = None,\n        permissions: Optional[\n            Sequence[Union[Tuple[str, Union[str, Tuple[str, str]]], str]]\n        ] = None,\n    ):", "metadata": {"task_id": "Database/datasette/13", "ground_truth": "        if permissions:\n            assert (\n                not action and not resource\n            ), \"Can't use action= or resource= with permissions=\"\n        else:\n            permissions = [(action, resource)]\n        try:\n            await self.ensure_permissions(actor, permissions)\n        except Forbidden:\n            return False, False\n        # User can see it, but can the anonymous user see it?\n        try:\n            await self.ensure_permissions(None, permissions)\n        except Forbidden:\n            # It's visible but private\n            return True, True\n        # It's visible to everyone\n        return True, False", "fpath_tuple": ["Database", "datasette", "datasette", "app.py"], "context_start_lineno": 725, "line_no": 735, "id": "datasette.app.Datasette.check_visibility", "target_function_prompt": "    async def check_visibility(\n        self,\n        actor: dict,\n        action: Optional[str] = None,\n        resource: Optional[Union[str, Tuple[str, str]]] = None,\n        permissions: Optional[\n            Sequence[Union[Tuple[str, Union[str, Tuple[str, str]]], str]]\n        ] = None,\n    ):", "function_signature": "    async def check_visibility(\n        self,\n        actor: dict,\n        action: Optional[str] = None,\n        resource: Optional[Union[str, Tuple[str, str]]] = None,\n        permissions: Optional[\n            Sequence[Union[Tuple[str, Union[str, Tuple[str, str]]], str]]\n        ] = None,\n    ):"}}
{"prompt": "def suggest_type(full_text, text_before_cursor):", "metadata": {"task_id": "Database/mssql-cli/4", "ground_truth": "    if full_text.startswith('\\\\i '):\n        return (Path(),)\n\n    # This is a temporary hack; the exception handling\n    # here should be removed once sqlparse has been fixed\n    try:\n        stmt = SqlStatement(full_text, text_before_cursor)\n    except (TypeError, AttributeError):\n        return []\n\n    # Check for special commands and handle those separately\n    if stmt.parsed:\n        # Be careful here because trivial whitespace is parsed as a\n        # statement, but the statement won't have a first token\n        tok1 = stmt.parsed.token_first()\n        if tok1 and tok1.value == '\\\\':\n            text = stmt.text_before_cursor + stmt.word_before_cursor\n            return suggest_special(text)\n\n    return suggest_based_on_last_token(stmt.last_token, stmt)", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "packages", "sqlcompletion.py"], "context_start_lineno": 132, "line_no": 140, "id": "mssqlcli.packages.sqlcompletion.suggest_type", "target_function_prompt": "def suggest_type(full_text, text_before_cursor):", "function_signature": "def suggest_type(full_text, text_before_cursor):"}}
{"prompt": "def providers_for_config_string(config_string, netcode):", "metadata": {"task_id": "Security/pycoin/15", "ground_truth": "    providers = []\n    for d in config_string.split():\n        p = provider_for_descriptor_and_netcode(d, netcode)\n        if p:\n            providers.append(p)\n        else:\n            warnings.warn(\"can't parse provider %s in config string\" % d)\n    return providers", "fpath_tuple": ["Security", "pycoin", "pycoin", "services", "providers.py"], "context_start_lineno": 119, "line_no": 120, "id": "pycoin.services.providers.providers_for_config_string", "target_function_prompt": "def providers_for_config_string(config_string, netcode):", "function_signature": "def providers_for_config_string(config_string, netcode):"}}
{"prompt": "def validate_extension_data(data: ExtensionData) -> bool:", "metadata": {"task_id": "Multimedia/Mopidy/11", "ground_truth": "    from mopidy import exceptions\n    logger.debug(\"Validating extension: %s\", data.extension.ext_name)\n\n    if data.extension.ext_name != data.entry_point.name:\n        logger.warning(\n            \"Disabled extension %(ep)s: entry point name (%(ep)s) \"\n            \"does not match extension name (%(ext)s)\",\n            {\"ep\": data.entry_point.name, \"ext\": data.extension.ext_name},\n        )\n        return False\n\n    try:\n        data.entry_point.require()\n    except pkg_resources.DistributionNotFound as exc:\n        logger.info(\n            \"Disabled extension %s: Dependency %s not found\",\n            data.extension.ext_name,\n            exc,\n        )\n        return False\n    except pkg_resources.VersionConflict as exc:\n        if len(exc.args) == 2:\n            found, required = exc.args\n            logger.info(\n                \"Disabled extension %s: %s required, but found %s at %s\",\n                data.extension.ext_name,\n                required,\n                found,\n                found.location,\n            )\n        else:\n            logger.info(\n                \"Disabled extension %s: %s\", data.extension.ext_name, exc\n            )\n        return False\n\n    try:\n        data.extension.validate_environment()\n    except exceptions.ExtensionError as exc:\n        logger.info(\"Disabled extension %s: %s\", data.extension.ext_name, exc)\n        return False\n    except Exception:\n        logger.exception(\n            \"Validating extension %s failed with an exception.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    if not data.config_schema:\n        logger.error(\n            \"Extension %s does not have a config schema, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n    elif not isinstance(data.config_schema.get(\"enabled\"), config_lib.Boolean):\n        logger.error(\n            'Extension %s does not have the required \"enabled\" config'\n            \" option, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    for key, value in data.config_schema.items():\n        if not isinstance(value, config_lib.ConfigValue):\n            logger.error(\n                \"Extension %s config schema contains an invalid value\"\n                ' for the option \"%s\", disabling.',\n                data.extension.ext_name,\n                key,\n            )\n            return False\n\n    if not data.config_defaults:\n        logger.error(\n            \"Extension %s does not have a default config, disabling.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    return True", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "ext.py"], "context_start_lineno": 270, "line_no": 277, "id": "mopidy.ext.validate_extension_data", "target_function_prompt": "def validate_extension_data(data: ExtensionData) -> bool:", "function_signature": "def validate_extension_data(data: ExtensionData) -> bool:"}}
{"prompt": "def load(files, ext_schemas, ext_defaults, overrides):", "metadata": {"task_id": "Multimedia/Mopidy/12", "ground_truth": "    from mopidy.config import keyring\n    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    defaults.extend(ext_defaults)\n    raw_config = _load(files, defaults, keyring.fetch() + (overrides or []))\n\n    schemas = _schemas[:]\n    schemas.extend(ext_schemas)\n    return _validate(raw_config, schemas)", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "__init__.py"], "context_start_lineno": 106, "line_no": 107, "id": "mopidy.config.load", "target_function_prompt": "def load(files, ext_schemas, ext_defaults, overrides):", "function_signature": "def load(files, ext_schemas, ext_defaults, overrides):"}}
{"prompt": "def _parse_task_syslog(lines):", "metadata": {"task_id": "System/mrjob/22", "ground_truth": "    from .log4j import _parse_hadoop_log4j_records\n    return _parse_task_syslog_records(_parse_hadoop_log4j_records(lines))", "fpath_tuple": ["System", "mrjob", "mrjob", "logs", "task.py"], "context_start_lineno": 339, "line_no": 356, "id": "mrjob.logs.task._parse_task_syslog", "target_function_prompt": "def _parse_task_syslog(lines):", "function_signature": "def _parse_task_syslog(lines):"}}
{"prompt": "    def get_worker_count(\n        cls,\n        runnable_class: t.Type[Runnable],\n        resource_request: dict[str, t.Any] | None,\n        workers_per_resource: int | float,\n    ) -> int:", "metadata": {"task_id": "Scientific-Engineering/bentoml/11", "ground_truth": "        if resource_request is None:\n            resource_request = system_resources()\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            return math.ceil(len(nvidia_gpus) * workers_per_resource)\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if \"cpu\" not in runnable_class.SUPPORTED_RESOURCES:\n                logger.warning(\n                    \"No known supported resource available for %s, falling back to using CPU.\",\n                    runnable_class,\n                )\n\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                if isinstance(workers_per_resource, float):\n                    raise ValueError(\n                        \"Fractional CPU multi threading support is not yet supported.\"\n                    )\n                return workers_per_resource\n\n            return math.ceil(cpus) * workers_per_resource\n\n        # this should not be reached by user since we always read system resource as default\n        raise ValueError(\n            f\"No known supported resource available for {runnable_class}. Please check your resource request. \"\n            \"Leaving it blank will allow BentoML to use system resources.\"\n        )", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "strategy.py"], "context_start_lineno": 60, "line_no": 66, "id": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "target_function_prompt": "    def get_worker_count(\n        cls,\n        runnable_class: t.Type[Runnable],\n        resource_request: dict[str, t.Any] | None,\n        workers_per_resource: int | float,\n    ) -> int:", "function_signature": "    def get_worker_count(\n        cls,\n        runnable_class: t.Type[Runnable],\n        resource_request: dict[str, t.Any] | None,\n        workers_per_resource: int | float,\n    ) -> int:"}}
{"prompt": "    def serialize(self, value, display=False):", "metadata": {"task_id": "Multimedia/Mopidy/13", "ground_truth": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 113, "line_no": 114, "id": "mopidy.config.types.String.serialize", "target_function_prompt": "    def serialize(self, value, display=False):", "function_signature": "    def serialize(self, value, display=False):"}}
{"prompt": "def iso8601_date(s: str) -> Union[datetime.date, str]:", "metadata": {"task_id": "Communications/twilio-fatisar/7", "ground_truth": "    try:\n        return (\n            datetime.datetime.strptime(s, ISO8601_DATE_FORMAT)\n            .replace(tzinfo=datetime.timezone.utc)\n            .date()\n        )\n    except (TypeError, ValueError):\n        return s", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "base", "deserialize.py"], "context_start_lineno": 9, "line_no": 16, "id": "twilio.base.deserialize.iso8601_date", "target_function_prompt": "def iso8601_date(s: str) -> Union[datetime.date, str]:", "function_signature": "def iso8601_date(s: str) -> Union[datetime.date, str]:"}}
{"prompt": "def extract_ctes(sql):", "metadata": {"task_id": "Database/mssql-cli/5", "ground_truth": "    p = parse(sql)[0]\n\n    # Make sure the first meaningful token is \"WITH\" which is necessary to\n    # define CTEs\n    idx, tok = p.token_next(-1, skip_ws=True, skip_cm=True)\n    if not (tok and tok.ttype == CTE):\n        return [], sql\n\n    # Get the next (meaningful) token, which should be the first CTE\n    idx, tok = p.token_next(idx)\n    if not tok:\n        return ([], '')\n    start_pos = token_start_pos(p.tokens, idx)\n    ctes = []\n\n    if isinstance(tok, IdentifierList):\n        # Multiple ctes\n        for t in tok.get_identifiers():\n            cte_start_offset = token_start_pos(tok.tokens, tok.token_index(t))\n            cte = get_cte_from_token(t, start_pos + cte_start_offset)\n            if not cte:\n                continue\n            ctes.append(cte)\n    elif isinstance(tok, Identifier):\n        # A single CTE\n        cte = get_cte_from_token(tok, start_pos)\n        if cte:\n            ctes.append(cte)\n\n    idx = p.token_index(tok) + 1\n\n    # Collapse everything after the ctes into a remainder query\n    remainder = u''.join(str(tok) for tok in p.tokens[idx:])\n\n    return ctes, remainder", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "packages", "parseutils", "ctes.py"], "context_start_lineno": 48, "line_no": 58, "id": "mssqlcli.packages.parseutils.ctes.extract_ctes", "target_function_prompt": "def extract_ctes(sql):", "function_signature": "def extract_ctes(sql):"}}
{"prompt": "def imatch(pattern, name):\n    # type: (Text, Text) -> bool", "metadata": {"task_id": "System/fs/8", "ground_truth": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern, case_sensitive=False) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res, re.IGNORECASE)\n    return re_pat.match(name) is not None", "fpath_tuple": ["System", "fs", "fs", "wildcard.py"], "context_start_lineno": 40, "line_no": 52, "id": "fs.wildcard.imatch", "target_function_prompt": "def imatch(pattern, name):\n    # type: (Text, Text) -> bool", "function_signature": "def imatch(pattern, name):\n    # type: (Text, Text) -> bool"}}
{"prompt": "    def is_registered(self):", "metadata": {"task_id": "Utilities/praw/2", "ground_truth": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        return cursor.fetchone() is not None", "fpath_tuple": ["Utilities", "praw", "praw", "util", "token_manager.py"], "context_start_lineno": 159, "line_no": 161, "id": "praw.util.token_manager.SQLiteTokenManager.is_registered", "target_function_prompt": "    def is_registered(self):", "function_signature": "    def is_registered(self):"}}
{"prompt": "    def pop(self, key, default=_MISSING):\n        # NB: hit/miss counts are bypassed for pop()", "metadata": {"task_id": "Utilities/boltons/25", "ground_truth": "        with self._lock:\n            try:\n                ret = super(LRI, self).pop(key)\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                ret = default\n            else:\n                self._remove_from_ll(key)\n            return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 267, "line_no": 269, "id": "boltons.cacheutils.LRI.pop", "target_function_prompt": "    def pop(self, key, default=_MISSING):\n        # NB: hit/miss counts are bypassed for pop()", "function_signature": "    def pop(self, key, default=_MISSING):\n        # NB: hit/miss counts are bypassed for pop()"}}
{"prompt": "def autodl_topic2papers():", "metadata": {"task_id": "Database/awesome-autodl/0", "ground_truth": "    from awesome_autodl.utils import load_yaml, dump_yaml\n    from awesome_autodl.data_cls import AutoDLpaper\n\n    topic2path = autodl_topic2path()\n    topic2papers = OrderedDict()\n    for topic, xpath in topic2path.items():\n        if not xpath.exists():\n            ValueError(f\"Can not find {topic} at {xpath}.\")\n        papers = []\n        raw_data = load_yaml(xpath)\n        assert isinstance(\n            raw_data, (list, tuple)\n        ), f\"invalid type of raw data: {type(raw_data)}\"\n        for per_data in raw_data:\n            papers.append(AutoDLpaper(per_data))\n        topic2papers[topic] = papers\n        print(f\"Load {topic} completed with {len(papers)} papers.\")\n    return topic2papers", "fpath_tuple": ["Database", "awesome-autodl", "awesome_autodl", "__init__.py"], "context_start_lineno": 57, "line_no": 58, "id": "awesome_autodl.autodl_topic2papers", "target_function_prompt": "def autodl_topic2papers():", "function_signature": "def autodl_topic2papers():"}}
{"prompt": "    def __repr__(self):", "metadata": {"task_id": "Utilities/boltons/26", "ground_truth": "        cn = self.__class__.__name__\n        return ('%s(user=%r, group=%r, other=%r)'\n                % (cn, self.user, self.group, self.other))", "fpath_tuple": ["Utilities", "boltons", "boltons", "fileutils.py"], "context_start_lineno": 213, "line_no": 214, "id": "boltons.fileutils.FilePerms.__repr__", "target_function_prompt": "    def __repr__(self):", "function_signature": "    def __repr__(self):"}}
{"prompt": "    def _validate_and_patch_stream_data(self, parsed_link: ParsedNarrowLink) -> str:", "metadata": {"task_id": "Communications/zulip-term/7", "ground_truth": "        stream_id = parsed_link[\"stream\"][\"stream_id\"]\n        stream_name = parsed_link[\"stream\"][\"stream_name\"]\n        assert (stream_id is None and stream_name is not None) or (\n            stream_id is not None and stream_name is None\n        )\n\n        model = self.model\n        # Validate stream ID and name.\n        if (stream_id and not model.is_user_subscribed_to_stream(stream_id)) or (\n            stream_name and not model.is_valid_stream(stream_name)\n        ):\n            # TODO: Narrow to the concerned stream in a 'preview' mode or\n            # report whether the stream id is invalid instead.\n            return \"The stream seems to be either unknown or unsubscribed\"\n\n        # Patch the optional value.\n        if not stream_id:\n            stream_id = cast(int, model.stream_id_from_name(stream_name))\n            parsed_link[\"stream\"][\"stream_id\"] = stream_id\n        else:\n            stream_name = cast(str, model.stream_dict[stream_id][\"name\"])\n            parsed_link[\"stream\"][\"stream_name\"] = stream_name\n\n        return \"\"", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "buttons.py"], "context_start_lineno": 520, "line_no": 525, "id": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "target_function_prompt": "    def _validate_and_patch_stream_data(self, parsed_link: ParsedNarrowLink) -> str:", "function_signature": "    def _validate_and_patch_stream_data(self, parsed_link: ParsedNarrowLink) -> str:"}}
{"prompt": "    def stem(self):\n        # type: () -> Text", "metadata": {"task_id": "System/fs/9", "ground_truth": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\"):\n            return name\n        return name.split(\".\")[0]", "fpath_tuple": ["System", "fs", "fs", "info.py"], "context_start_lineno": 244, "line_no": 254, "id": "fs.info.Info.stem", "target_function_prompt": "    def stem(self):\n        # type: () -> Text", "function_signature": "    def stem(self):\n        # type: () -> Text"}}
{"prompt": "def compute_likelihood_windows_in_session(\n    session: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    value_cond_param_probs: Union[StateMatrix, dict],\n    modellable_params: set,\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str = None,\n    end_token: str = None,\n    use_geo_mean: bool = False,\n) -> List[float]:", "metadata": {"task_id": "Security/msticpy/4", "ground_truth": "    if use_start_end_tokens and (start_token is None or end_token is None):\n        raise MsticpyException(\n            \"start_token and end_token should not be set to None when \"\n            \"use_start_end_tokens is set to True\"\n        )\n\n    likelihoods = []\n    sess = session.copy()\n    if use_start_end_tokens and end_token:\n        sess += [Cmd(name=str(end_token), params={})]\n    end = len(sess) - window_len\n    for i in range(end + 1):\n        window = sess[i : i + window_len]  # noqa E203\n        use_start = use_start_end_tokens if i == 0 else False\n        lik = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=use_start,\n            use_end_token=False,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            k = window_len\n            lik = lik ** (1 / k)\n        likelihoods.append(lik)\n\n    return likelihoods", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "cmds_params_values.py"], "context_start_lineno": 446, "line_no": 509, "id": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "target_function_prompt": "def compute_likelihood_windows_in_session(\n    session: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    value_cond_param_probs: Union[StateMatrix, dict],\n    modellable_params: set,\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str = None,\n    end_token: str = None,\n    use_geo_mean: bool = False,\n) -> List[float]:", "function_signature": "def compute_likelihood_windows_in_session(\n    session: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    value_cond_param_probs: Union[StateMatrix, dict],\n    modellable_params: set,\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str = None,\n    end_token: str = None,\n    use_geo_mean: bool = False,\n) -> List[float]:"}}
{"prompt": "    def mkdir(self, path):", "metadata": {"task_id": "System/mrjob/23", "ground_truth": "        version = self.get_hadoop_version()\n\n        # use -p on Hadoop 2 (see #991, #845)\n        if uses_yarn(version):\n            args = ['fs', '-mkdir', '-p', path]\n        else:\n            args = ['fs', '-mkdir', path]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n        except CalledProcessError:\n            raise IOError(\"Could not mkdir %s\" % path)", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "hadoop.py"], "context_start_lineno": 285, "line_no": 286, "id": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "target_function_prompt": "    def mkdir(self, path):", "function_signature": "    def mkdir(self, path):"}}
{"prompt": "    def to_dict(self):", "metadata": {"task_id": "Internet/boto/13", "ground_truth": "        batch_dict = {}\n        key_list = []\n        for key in self.keys:\n            if isinstance(key, tuple):\n                hash_key, range_key = key\n            else:\n                hash_key = key\n                range_key = None\n            k = self.table.layer2.build_key_from_values(self.table.schema,\n                                                        hash_key, range_key)\n            key_list.append(k)\n        batch_dict['Keys'] = key_list\n        if self.attributes_to_get:\n            batch_dict['AttributesToGet'] = self.attributes_to_get\n        if self.consistent_read:\n            batch_dict['ConsistentRead'] = True\n        else:\n            batch_dict['ConsistentRead'] = False\n        return batch_dict", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb", "batch.py"], "context_start_lineno": 57, "line_no": 61, "id": "boto.dynamodb.batch.Batch.to_dict", "target_function_prompt": "    def to_dict(self):", "function_signature": "    def to_dict(self):"}}
{"prompt": "    def get(self, k, d=None):", "metadata": {"task_id": "Utilities/sacred/15", "ground_truth": "        if dict.__contains__(self, k):\n            return dict.__getitem__(self, k)\n        else:\n            return self.fallback.get(k, d)", "fpath_tuple": ["Utilities", "sacred", "sacred", "config", "custom_containers.py"], "context_start_lineno": 83, "line_no": 84, "id": "sacred.config.custom_containers.DogmaticDict.get", "target_function_prompt": "    def get(self, k, d=None):", "function_signature": "    def get(self, k, d=None):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/14", "ground_truth": "    from boto.regioninfo import connect\n    return connect('ec2', region_name, connection_cls=VPCConnection,\n                   **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "vpc", "__init__.py"], "context_start_lineno": 55, "line_no": 69, "id": "boto.vpc.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def steps(self):", "metadata": {"task_id": "System/mrjob/24", "ground_truth": "        from mrjob.step import _JOB_STEP_FUNC_PARAMS\n        kwargs = dict(\n            (func_name, getattr(self, func_name))\n            for func_name in _JOB_STEP_FUNC_PARAMS + ('spark',)\n            if (_im_func(getattr(self, func_name)) is not\n                _im_func(getattr(MRJob, func_name))))\n\n        # special case for spark()\n        # TODO: support jobconf as well\n        if 'spark' in kwargs:\n            if sorted(kwargs) != ['spark']:\n                raise ValueError(\n                    \"Can't mix spark() and streaming functions\")\n            return [SparkStep(\n                spark=kwargs['spark'],\n                spark_args=self.spark_args())]\n\n        # MRStep takes commands as strings, but the user defines them in the\n        # class as functions that return strings, so call the functions.\n        updates = {}\n        for k, v in kwargs.items():\n            if k.endswith('_cmd') or k.endswith('_pre_filter'):\n                updates[k] = v()\n\n        kwargs.update(updates)\n\n        if kwargs:\n            return [MRStep(**kwargs)]\n        else:\n            return []", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 496, "line_no": 516, "id": "mrjob.job.MRJob.steps", "target_function_prompt": "    def steps(self):", "function_signature": "    def steps(self):"}}
{"prompt": "    def plural_post(self):", "metadata": {"task_id": "Internet/kinto/9", "ground_truth": "        new_object = self.request.validated[\"body\"].get(\"data\", {})\n\n        existing = None\n        # If id was specified, then add it to posted body and look-up\n        # the existing object.\n        if self.object_id is not None:\n            new_object[self.model.id_field] = self.object_id\n            try:\n                existing = self._get_object_or_404(self.object_id)\n            except HTTPNotFound:\n                pass\n\n        self._raise_412_if_modified(obj=existing)\n\n        if existing:\n            obj = existing\n            action = ACTIONS.READ\n        else:\n            new_object = self.process_object(new_object)\n            obj = self.model.create_object(new_object)\n            self.request.response.status_code = 201\n            action = ACTIONS.CREATE\n\n        timestamp = obj[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(obj, action=action)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "resource", "__init__.py"], "context_start_lineno": 410, "line_no": 427, "id": "kinto.core.resource.Resource.plural_post", "target_function_prompt": "    def plural_post(self):", "function_signature": "    def plural_post(self):"}}
{"prompt": "    def run_mapper(self, step_num=0):", "metadata": {"task_id": "System/mrjob/25", "ground_truth": "        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 753, "line_no": 763, "id": "mrjob.job.MRJob.run_mapper", "target_function_prompt": "    def run_mapper(self, step_num=0):", "function_signature": "    def run_mapper(self, step_num=0):"}}
{"prompt": "    def __repr__(self):", "metadata": {"task_id": "Utilities/boltons/27", "ground_truth": "        cn = self.__class__.__name__\n        if self.typed or not self.scoped:\n            return (\"%s(func=%r, scoped=%r, typed=%r)\"\n                    % (cn, self.func, self.scoped, self.typed))\n        return \"%s(func=%r)\" % (cn, self.func)", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 478, "line_no": 479, "id": "boltons.cacheutils.CachedFunction.__repr__", "target_function_prompt": "    def __repr__(self):", "function_signature": "    def __repr__(self):"}}
{"prompt": "def determine(chord, shorthand=False, no_inversions=False, no_polychords=False):", "metadata": {"task_id": "Multimedia/mingus/12", "ground_truth": "    if chord == []:\n        return []\n    elif len(chord) == 1:\n        return chord\n    elif len(chord) == 2:\n        return [intervals.determine(chord[0], chord[1])]\n    elif len(chord) == 3:\n        return determine_triad(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 4:\n        return determine_seventh(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 5:\n        return determine_extended_chord5(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 6:\n        return determine_extended_chord6(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 7:\n        return determine_extended_chord7(chord, shorthand, no_inversions, no_polychords)\n    else:\n        return determine_polychords(chord, shorthand)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "chords.py"], "context_start_lineno": 919, "line_no": 924, "id": "mingus.core.chords.determine", "target_function_prompt": "def determine(chord, shorthand=False, no_inversions=False, no_polychords=False):", "function_signature": "def determine(chord, shorthand=False, no_inversions=False, no_polychords=False):"}}
{"prompt": "def dedent_function_body(body):", "metadata": {"task_id": "Utilities/sacred/16", "ground_truth": "    lines = body.split(\"\\n\")\n    # find indentation by first line\n    indent = \"\"\n    for line in lines:\n        if is_empty_or_comment(line):\n            continue\n        else:\n            indent = re.match(r\"^\\s*\", line).group()\n            break\n\n    out_lines = [dedent_line(line, indent) for line in lines]\n    return \"\\n\".join(out_lines)", "fpath_tuple": ["Utilities", "sacred", "sacred", "config", "config_scope.py"], "context_start_lineno": 166, "line_no": 167, "id": "sacred.config.config_scope.dedent_function_body", "target_function_prompt": "def dedent_function_body(body):", "function_signature": "def dedent_function_body(body):"}}
{"prompt": "def encrypt(fingerprint, data, *, use_old=False):", "metadata": {"task_id": "Communications/Telethon/1", "ground_truth": "    global _server_keys\n    key, old = _server_keys.get(fingerprint, [None, None])\n    if (not key) or (old and not use_old):\n        return None\n\n    # len(sha1.digest) is always 20, so we're left with 255 - 20 - x padding\n    to_encrypt = sha1(data).digest() + data + os.urandom(235 - len(data))\n\n    # rsa module rsa.encrypt adds 11 bits for padding which we don't want\n    # rsa module uses rsa.transform.bytes2int(to_encrypt), easier way:\n    payload = int.from_bytes(to_encrypt, 'big')\n    encrypted = rsa.core.encrypt_int(payload, key.e, key.n)\n    # rsa module uses transform.int2bytes(encrypted, keylength), easier:\n    block = encrypted.to_bytes(256, 'big')\n    return block", "fpath_tuple": ["Communications", "Telethon", "telethon", "crypto", "rsa.py"], "context_start_lineno": 56, "line_no": 67, "id": "telethon.crypto.rsa.encrypt", "target_function_prompt": "def encrypt(fingerprint, data, *, use_old=False):", "function_signature": "def encrypt(fingerprint, data, *, use_old=False):"}}
{"prompt": "def list_to_scope(scope):", "metadata": {"task_id": "Internet/Authlib/4", "ground_truth": "    if isinstance(scope, (set, tuple, list)):\n        return \" \".join([to_unicode(s) for s in scope])\n    if scope is None:\n        return scope\n    return to_unicode(scope)", "fpath_tuple": ["Internet", "Authlib", "authlib", "oauth2", "rfc6749", "util.py"], "context_start_lineno": 5, "line_no": 7, "id": "authlib.oauth2.rfc6749.util.list_to_scope", "target_function_prompt": "def list_to_scope(scope):", "function_signature": "def list_to_scope(scope):"}}
{"prompt": "    def join(self):", "metadata": {"task_id": "System/wal-e/6", "ground_truth": "        self.closed = True\n\n        while self.expect > 0:\n            val = self.wait_change.get()\n            self.expect -= 1\n\n            if val is not None:\n                # Wait a while for all running greenlets to exit, and\n                # then attempt to force them to exit so join()\n                # terminates in a reasonable amount of time.\n                gevent.joinall(list(self.greenlets), timeout=30)\n                gevent.killall(list(self.greenlets), block=True, timeout=30)\n                raise val", "fpath_tuple": ["System", "wal-e", "wal_e", "worker", "pg", "wal_transfer.py"], "context_start_lineno": 129, "line_no": 131, "id": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "target_function_prompt": "    def join(self):", "function_signature": "    def join(self):"}}
{"prompt": "def load_opts_from_mrjob_confs(runner_alias, conf_paths=None):", "metadata": {"task_id": "System/mrjob/26", "ground_truth": "    if conf_paths is None:\n        results = load_opts_from_mrjob_conf(runner_alias)\n    else:\n        # don't include conf files that were loaded earlier in conf_paths\n        already_loaded = []\n\n        # load configs in reversed order so that order of conf paths takes\n        # precedence over inheritance\n        results = []\n\n        for path in reversed(conf_paths):\n            results = load_opts_from_mrjob_conf(\n                runner_alias, path, already_loaded=already_loaded) + results\n\n    if runner_alias and not any(conf for path, conf in results):\n        log.warning('No configs specified for %s runner' % runner_alias)\n\n    return results", "fpath_tuple": ["System", "mrjob", "mrjob", "conf.py"], "context_start_lineno": 305, "line_no": 323, "id": "mrjob.conf.load_opts_from_mrjob_confs", "target_function_prompt": "def load_opts_from_mrjob_confs(runner_alias, conf_paths=None):", "function_signature": "def load_opts_from_mrjob_confs(runner_alias, conf_paths=None):"}}
{"prompt": "    def update(self, validate=False, dry_run=False):", "metadata": {"task_id": "Internet/boto/15", "ground_truth": "        rs = self.connection.get_all_network_interfaces(\n            [self.id],\n            dry_run=dry_run\n        )\n        if len(rs) > 0:\n            self._update(rs[0])\n        elif validate:\n            raise ValueError('%s is not a valid ENI ID' % self.id)\n        return self.status", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "networkinterface.py"], "context_start_lineno": 171, "line_no": 182, "id": "boto.ec2.networkinterface.NetworkInterface.update", "target_function_prompt": "    def update(self, validate=False, dry_run=False):", "function_signature": "    def update(self, validate=False, dry_run=False):"}}
{"prompt": "def _get_constraint_final_name(\n    constraint: Union[Index, Constraint], dialect: Optional[Dialect]\n) -> Optional[str]:", "metadata": {"task_id": "Database/alembic/2", "ground_truth": "    if constraint.name is None:\n        return None\n    assert dialect is not None\n    if sqla_14:\n        # for SQLAlchemy 1.4 we would like to have the option to expand\n        # the use of \"deferred\" names for constraints as well as to have\n        # some flexibility with \"None\" name and similar; make use of new\n        # SQLAlchemy API to return what would be the final compiled form of\n        # the name for this dialect.\n        return dialect.identifier_preparer.format_constraint(\n            constraint, _alembic_quote=False\n        )\n    else:\n        # prior to SQLAlchemy 1.4, work around quoting logic to get at the\n        # final compiled name without quotes.\n        if hasattr(constraint.name, \"quote\"):\n            # might be quoted_name, might be truncated_name, keep it the\n            # same\n            quoted_name_cls: type = type(constraint.name)\n        else:\n            quoted_name_cls = quoted_name\n\n        new_name = quoted_name_cls(str(constraint.name), quote=False)\n        constraint = constraint.__class__(name=new_name)\n\n        if isinstance(constraint, schema.Index):\n            # name should not be quoted.\n            d = dialect.ddl_compiler(dialect, None)  # type: ignore[arg-type]\n            return d._prepared_index_name(  # type: ignore[attr-defined]\n                constraint\n            )\n        else:\n            # name should not be quoted.\n            return dialect.identifier_preparer.format_constraint(constraint)", "fpath_tuple": ["Database", "alembic", "alembic", "util", "sqla_compat.py"], "context_start_lineno": 541, "line_no": 544, "id": "alembic.util.sqla_compat._get_constraint_final_name", "target_function_prompt": "def _get_constraint_final_name(\n    constraint: Union[Index, Constraint], dialect: Optional[Dialect]\n) -> Optional[str]:", "function_signature": "def _get_constraint_final_name(\n    constraint: Union[Index, Constraint], dialect: Optional[Dialect]\n) -> Optional[str]:"}}
{"prompt": "    def get_category(self, category_name, default=None, sort_key=None):", "metadata": {"task_id": "Internet/pyramid/18", "ground_truth": "        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        category = self._categories.get(category_name)\n        if category is None:\n            return default\n        values = category.values()\n        values = sorted(set(values), key=sort_key)\n        return [\n            {'introspectable': intr, 'related': self.related(intr)}\n            for intr in values\n        ]", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "registry.py"], "context_start_lineno": 135, "line_no": 136, "id": "pyramid.registry.Introspector.get_category", "target_function_prompt": "    def get_category(self, category_name, default=None, sort_key=None):", "function_signature": "    def get_category(self, category_name, default=None, sort_key=None):"}}
{"prompt": "def iso8601_date(d):", "metadata": {"task_id": "Communications/twilio-fatisar/8", "ground_truth": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return str(d.date())\n    elif isinstance(d, datetime.date):\n        return str(d)\n    elif isinstance(d, str):\n        return d", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "base", "serialize.py"], "context_start_lineno": 6, "line_no": 11, "id": "twilio.base.serialize.iso8601_date", "target_function_prompt": "def iso8601_date(d):", "function_signature": "def iso8601_date(d):"}}
{"prompt": "", "metadata": {"task_id": "Database/datasette/14", "ground_truth": "    def fake(cls, path_with_query_string, method=\"GET\", scheme=\"http\", url_vars=None):\n        \"\"\"Useful for constructing Request objects for tests\"\"\"\n        path, _, query_string = path_with_query_string.partition(\"?\")\n        scope = {\n            \"http_version\": \"1.1\",\n            \"method\": method,\n            \"path\": path,\n            \"raw_path\": path_with_query_string.encode(\"latin-1\"),\n            \"query_string\": query_string.encode(\"latin-1\"),\n            \"scheme\": scheme,\n            \"type\": \"http\",\n        }\n        if url_vars:", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "asgi.py"], "context_start_lineno": 127, "line_no": 129, "id": "datasette.utils.asgi.Request.fake", "target_function_prompt": "", "function_signature": ""}}
{"prompt": "def get_mac(mac_alg: bytes, key: bytes) -> MAC:", "metadata": {"task_id": "Security/asyncssh/1", "ground_truth": "    handler, hash_size, args = _mac_handler[mac_alg]\n    return handler(key, hash_size, *args)", "fpath_tuple": ["Security", "asyncssh", "asyncssh", "mac.py"], "context_start_lineno": 157, "line_no": 165, "id": "asyncssh.mac.get_mac", "target_function_prompt": "def get_mac(mac_alg: bytes, key: bytes) -> MAC:", "function_signature": "def get_mac(mac_alg: bytes, key: bytes) -> MAC:"}}
{"prompt": "    def get(self):", "metadata": {"task_id": "Internet/kinto/10", "ground_truth": "        self._raise_400_if_invalid_id(self.object_id)\n        obj = self._get_object_or_404(self.object_id)\n        timestamp = obj[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n        self._add_cache_header(self.request.response)\n        self._raise_304_if_not_modified(obj)\n        self._raise_412_if_modified(obj)\n\n        partial_fields = self._extract_partial_fields()\n        if partial_fields:\n            obj = dict_subset(obj, partial_fields)\n\n        return self.postprocess(obj)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "resource", "__init__.py"], "context_start_lineno": 496, "line_no": 511, "id": "kinto.core.resource.Resource.get", "target_function_prompt": "    def get(self):", "function_signature": "    def get(self):"}}
{"prompt": "    def __repr__(self):", "metadata": {"task_id": "Communications/Wikipedia-API/0", "ground_truth": "        if any(self._called.values()):\n            return f\"{self.title} (id: {self.pageid}, ns: {self.ns})\"\n        return f\"{self.title} (id: ??, ns: {self.ns})\"", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 1067, "line_no": 1068, "id": "wikipediaapi.WikipediaPage.__repr__", "target_function_prompt": "    def __repr__(self):", "function_signature": "    def __repr__(self):"}}
{"prompt": "    def about_jc() -> JSONDictType:", "metadata": {"task_id": "Utilities/jc/2", "ground_truth": "        from .lib import plugin_parser_mod_list\n        from .lib import streaming_parser_mod_list\n        from .lib import parser_mod_list\n        from .lib import standard_parser_mod_list\n        return {\n            'name': 'jc',\n            'version': info.version,\n            'description': info.description,\n            'author': info.author,\n            'author_email': info.author_email,\n            'website': info.website,\n            'copyright': info.copyright,\n            'license': info.license,\n            'python_version': '.'.join((str(sys.version_info.major), str(sys.version_info.minor), str(sys.version_info.micro))),\n            'python_path': sys.executable,\n            'parser_count': len(parser_mod_list(show_hidden=True, show_deprecated=True)),\n            'standard_parser_count': len(standard_parser_mod_list(show_hidden=True, show_deprecated=True)),\n            'streaming_parser_count': len(streaming_parser_mod_list(show_hidden=True, show_deprecated=True)),\n            'plugin_parser_count': len(plugin_parser_mod_list(show_hidden=True, show_deprecated=True)),\n            'parsers': all_parser_info(show_hidden=True, show_deprecated=True)\n        }", "fpath_tuple": ["Utilities", "jc", "jc", "cli.py"], "context_start_lineno": 261, "line_no": 263, "id": "jc.cli.JcCli.about_jc", "target_function_prompt": "    def about_jc() -> JSONDictType:", "function_signature": "    def about_jc() -> JSONDictType:"}}
{"prompt": "    def locale_name(self):", "metadata": {"task_id": "Internet/pyramid/19", "ground_truth": "        locale_name = negotiate_locale_name(self)\n        return locale_name", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "i18n.py"], "context_start_lineno": 375, "line_no": 376, "id": "pyramid.i18n.LocalizerRequestMixin.locale_name", "target_function_prompt": "    def locale_name(self):", "function_signature": "    def locale_name(self):"}}
{"prompt": "    def get_routes(self, include_static=False):", "metadata": {"task_id": "Internet/pyramid/20", "ground_truth": "        if include_static is True:\n            return self.routelist + self.static_routes\n\n        return self.routelist", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "urldispatch.py"], "context_start_lineno": 36, "line_no": 37, "id": "pyramid.urldispatch.RoutesMapper.get_routes", "target_function_prompt": "    def get_routes(self, include_static=False):", "function_signature": "    def get_routes(self, include_static=False):"}}
{"prompt": "    def run_validation(self, data=empty):", "metadata": {"task_id": "Internet/djangorestframework/3", "ground_truth": "        (is_empty_value, data) = self.validate_empty_values(data)\n        if is_empty_value:\n            return data\n        value = self.to_internal_value(data)\n        self.run_validators(value)\n        return value", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "fields.py"], "context_start_lineno": 533, "line_no": 543, "id": "rest_framework.fields.Field.run_validation", "target_function_prompt": "    def run_validation(self, data=empty):", "function_signature": "    def run_validation(self, data=empty):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/16", "ground_truth": "    from boto.regioninfo import connect\n    from boto.regioninfo import RegionInfo\n    if 'region' in kw_params and isinstance(kw_params['region'], RegionInfo)\\\n       and region_name == kw_params['region'].name:\n        return EC2Connection(**kw_params)\n\n    return connect('ec2', region_name,\n                   connection_cls=EC2Connection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "__init__.py"], "context_start_lineno": 46, "line_no": 60, "id": "boto.ec2.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def session(self):", "metadata": {"task_id": "Internet/pyramid/21", "ground_truth": "        from pyramid.interfaces import ISessionFactory\n        factory = self.registry.queryUtility(ISessionFactory)\n        if factory is None:\n            raise AttributeError(\n                'No session factory registered '\n                '(see the Sessions chapter of the Pyramid documentation)'\n            )\n        return factory(self)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "request.py"], "context_start_lineno": 184, "line_no": 189, "id": "pyramid.request.Request.session", "target_function_prompt": "    def session(self):", "function_signature": "    def session(self):"}}
{"prompt": "    def expunge(self, messages=None):", "metadata": {"task_id": "Communications/IMAPClient/2", "ground_truth": "        if messages:\n            if not self.use_uid:\n                raise ValueError(\"cannot EXPUNGE by ID when not using uids\")\n            return self._command_and_check(\n                \"EXPUNGE\", join_message_ids(messages), uid=True\n            )\n        tag = self._imap._command(\"EXPUNGE\")\n        return self._consume_until_tagged_response(tag, \"EXPUNGE\")", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1487, "line_no": 1520, "id": "imapclient.imapclient.IMAPClient.expunge", "target_function_prompt": "    def expunge(self, messages=None):", "function_signature": "    def expunge(self, messages=None):"}}
{"prompt": "def substitute(progression, substitute_index, depth=0):", "metadata": {"task_id": "Multimedia/mingus/13", "ground_truth": "    res = []\n    simple_substitutions = [\n        (\"I\", \"III\"),\n        (\"I\", \"VI\"),\n        (\"IV\", \"II\"),\n        (\"IV\", \"VI\"),\n        (\"V\", \"VII\"),\n        (\"V\", \"VIIdim7\"),\n        (\"V\", \"IIdim7\"),\n        (\"V\", \"IVdim7\"),\n        (\"V\", \"bVIIdim7\"),\n    ]\n    p = progression[substitute_index]\n    (roman, acc, suff) = parse_string(p)\n\n    # Do the simple harmonic substitutions\n    if suff == \"\" or suff == \"7\":\n        for subs in simple_substitutions:\n            r = None\n            if roman == subs[0]:\n                r = subs[1]\n            elif roman == subs[1]:\n                r = subs[0]\n            if r != None:\n                res.append(tuple_to_string((r, acc, \"\")))\n\n                # Add seventh or triad depending on r\n                if r[-1] != \"7\":\n                    res.append(tuple_to_string((r, acc, \"7\")))\n                else:\n                    res.append(tuple_to_string((r[:-1], acc, \"\")))\n\n    if suff == \"\" or suff == \"M\" or suff == \"m\":\n        res.append(tuple_to_string((roman, acc, suff + \"7\")))\n\n    if suff == \"m\" or suff == \"m7\":\n        n = skip(roman, 2)\n        a = interval_diff(roman, n, 3) + acc\n        res.append(tuple_to_string((n, a, \"M\")))\n        res.append(tuple_to_string((n, a, \"M7\")))\n\n    # Major to minor substitution\n    if suff == \"M\" or suff == \"M7\":\n        n = skip(roman, 5)\n        a = interval_diff(roman, n, 9) + acc\n        res.append(tuple_to_string((n, a, \"m\")))\n        res.append(tuple_to_string((n, a, \"m7\")))\n\n    if suff == \"dim7\" or suff == \"dim\":\n        # Add the corresponding dominant seventh\n        res.append(tuple_to_string((skip(roman, 5), acc, \"dom7\")))\n\n        n = skip(roman, 1)\n        res.append(tuple_to_string((n, acc + interval_diff(roman, n, 1), \"dom7\")))\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    res2 = []\n    if depth > 0:\n        for x in res:\n            new_progr = progression\n            new_progr[substitute_index] = x\n            res2 += substitute(new_progr, substitute_index, depth - 1)\n    return res + res2", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "progressions.py"], "context_start_lineno": 425, "line_no": 435, "id": "mingus.core.progressions.substitute", "target_function_prompt": "def substitute(progression, substitute_index, depth=0):", "function_signature": "def substitute(progression, substitute_index, depth=0):"}}
{"prompt": "    def popitem(self):", "metadata": {"task_id": "Utilities/boltons/28", "ground_truth": "        with self._lock:\n            item = super(LRI, self).popitem()\n            self._remove_from_ll(item[0])\n            return item", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 280, "line_no": 281, "id": "boltons.cacheutils.LRI.popitem", "target_function_prompt": "    def popitem(self):", "function_signature": "    def popitem(self):"}}
{"prompt": "    def get_revision(self, id_: Optional[str]) -> Optional[Revision]:", "metadata": {"task_id": "Database/alembic/3", "ground_truth": "        resolved_id, branch_label = self._resolve_revision_number(id_)\n        if len(resolved_id) > 1:\n            raise MultipleHeads(resolved_id, id_)\n\n        resolved: Union[str, Tuple[()]] = resolved_id[0] if resolved_id else ()\n        return self._revision_for_ident(resolved, branch_label)", "fpath_tuple": ["Database", "alembic", "alembic", "script", "revision.py"], "context_start_lineno": 557, "line_no": 571, "id": "alembic.script.revision.RevisionMap.get_revision", "target_function_prompt": "    def get_revision(self, id_: Optional[str]) -> Optional[Revision]:", "function_signature": "    def get_revision(self, id_: Optional[str]) -> Optional[Revision]:"}}
{"prompt": "def major_seventh(note):", "metadata": {"task_id": "Multimedia/mingus/14", "ground_truth": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 11)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "intervals.py"], "context_start_lineno": 230, "line_no": 231, "id": "mingus.core.intervals.major_seventh", "target_function_prompt": "def major_seventh(note):", "function_signature": "def major_seventh(note):"}}
{"prompt": "    def _verify_token(self, access_token):", "metadata": {"task_id": "Internet/kinto/11", "ground_truth": "        from kinto.core import logger\n        uri = self.oid_config[\"userinfo_endpoint\"]\n        # Opaque access token string. Fetch user info from profile.\n        try:\n            resp = requests.get(uri, headers={\"Authorization\": \"Bearer \" + access_token})\n            resp.raise_for_status()\n            userprofile = resp.json()\n            return userprofile\n\n        except (requests.exceptions.HTTPError, ValueError, KeyError) as e:\n            logger.debug(\"Unable to fetch user profile from %s (%s)\" % (uri, e))\n            return None", "fpath_tuple": ["Internet", "kinto", "kinto", "plugins", "openid", "__init__.py"], "context_start_lineno": 67, "line_no": 68, "id": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "target_function_prompt": "    def _verify_token(self, access_token):", "function_signature": "    def _verify_token(self, access_token):"}}
{"prompt": "    def schema(self):", "metadata": {"task_id": "Internet/boto/17", "ground_truth": "        schema_data = super(GlobalBaseIndexField, self).schema()\n        schema_data['ProvisionedThroughput'] = {\n            'ReadCapacityUnits': int(self.throughput['read']),\n            'WriteCapacityUnits': int(self.throughput['write']),\n        }\n        return schema_data", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "fields.py"], "context_start_lineno": 234, "line_no": 259, "id": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "target_function_prompt": "    def schema(self):", "function_signature": "    def schema(self):"}}
{"prompt": "    def get_raw_keys(self):", "metadata": {"task_id": "Internet/boto/18", "ground_truth": "        raw_key_data = {}\n\n        for key, value in self.get_keys().items():\n            raw_key_data[key] = self._dynamizer.encode(value)\n\n        return raw_key_data", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "items.py"], "context_start_lineno": 239, "line_no": 245, "id": "boto.dynamodb2.items.Item.get_raw_keys", "target_function_prompt": "    def get_raw_keys(self):", "function_signature": "    def get_raw_keys(self):"}}
{"prompt": "    def following(self):", "metadata": {"task_id": "Communications/twtxt/0", "ground_truth": "        following = []\n        try:\n            for (nick, url) in self.cfg.items(\"following\"):\n                source = Source(nick, url)\n                following.append(source)\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n\n        return following", "fpath_tuple": ["Communications", "twtxt", "twtxt", "config.py"], "context_start_lineno": 100, "line_no": 102, "id": "twtxt.config.Config.following", "target_function_prompt": "    def following(self):", "function_signature": "    def following(self):"}}
{"prompt": "    def delete(self):", "metadata": {"task_id": "Internet/boto/19", "ground_truth": "        self.connection.delete_table(self.table_name)\n        return True", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 609, "line_no": 623, "id": "boto.dynamodb2.table.Table.delete", "target_function_prompt": "    def delete(self):", "function_signature": "    def delete(self):"}}
{"prompt": "def register_kex_alg(alg: bytes, handler: Type[Kex], hash_alg: HashType,\n                     args: Tuple, default: bool) -> None:", "metadata": {"task_id": "Security/asyncssh/2", "ground_truth": "    _kex_algs.append(alg)\n\n    if default:\n        _default_kex_algs.append(alg)\n\n    _kex_handlers[alg] = (handler, hash_alg, args)", "fpath_tuple": ["Security", "asyncssh", "asyncssh", "kex.py"], "context_start_lineno": 91, "line_no": 95, "id": "asyncssh.kex.register_kex_alg", "target_function_prompt": "def register_kex_alg(alg: bytes, handler: Type[Kex], hash_alg: HashType,\n                     args: Tuple, default: bool) -> None:", "function_signature": "def register_kex_alg(alg: bytes, handler: Type[Kex], hash_alg: HashType,\n                     args: Tuple, default: bool) -> None:"}}
{"prompt": "def update_query_params(uri, params):", "metadata": {"task_id": "Internet/google-api-python-client/1", "ground_truth": "    parts = urllib.parse.urlparse(uri)\n    query_params = parse_unique_urlencoded(parts.query)\n    query_params.update(params)\n    new_query = urllib.parse.urlencode(query_params)\n    new_parts = parts._replace(query=new_query)\n    return urllib.parse.urlunparse(new_parts)", "fpath_tuple": ["Internet", "google-api-python-client", "googleapiclient", "_helpers.py"], "context_start_lineno": 165, "line_no": 182, "id": "googleapiclient._helpers.update_query_params", "target_function_prompt": "def update_query_params(uri, params):", "function_signature": "def update_query_params(uri, params):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/20", "ground_truth": "    from boto.regioninfo import connect\n    from boto.rds2.layer1 import RDSConnection\n    return connect('rds', region_name, connection_cls=RDSConnection,\n                   **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "rds2", "__init__.py"], "context_start_lineno": 36, "line_no": 50, "id": "boto.rds2.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def write(self, s):", "metadata": {"task_id": "Utilities/boltons/29", "ground_truth": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                text_type.__name__,\n                type(s).__name__\n            ))\n        current_pos = self.tell()\n        if self.buffer.tell() + len(s.encode('utf-8')) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s.encode('utf-8'))\n        self._tell = current_pos + len(s)", "fpath_tuple": ["Utilities", "boltons", "boltons", "ioutils.py"], "context_start_lineno": 410, "line_no": 411, "id": "boltons.ioutils.SpooledStringIO.write", "target_function_prompt": "    def write(self, s):", "function_signature": "    def write(self, s):"}}
{"prompt": "def multi_heads_fixture(cfg, a, b, c):", "metadata": {"task_id": "Database/alembic/4", "ground_truth": "    d = util.rev_id()\n    e = util.rev_id()\n    f = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(\n        d, \"revision d from b\", head=b, splice=True, refresh=True\n    )\n    write_script(\n        script,\n        d,\n        \"\"\"\\\n\"Rev D\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 4\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 4\")\n\n\"\"\"\n        % (d, b),\n    )\n\n    script.generate_revision(\n        e, \"revision e from d\", head=d, splice=True, refresh=True\n    )\n    write_script(\n        script,\n        e,\n        \"\"\"\\\n\"Rev E\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 5\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 5\")\n\n\"\"\"\n        % (e, d),\n    )\n\n    script.generate_revision(\n        f, \"revision f from b\", head=b, splice=True, refresh=True\n    )\n    write_script(\n        script,\n        f,\n        \"\"\"\\\n\"Rev F\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 6\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 6\")\n\n\"\"\"\n        % (f, b),\n    )\n\n    return d, e, f", "fpath_tuple": ["Database", "alembic", "alembic", "testing", "env.py"], "context_start_lineno": 381, "line_no": 387, "id": "alembic.testing.env.multi_heads_fixture", "target_function_prompt": "def multi_heads_fixture(cfg, a, b, c):", "function_signature": "def multi_heads_fixture(cfg, a, b, c):"}}
{"prompt": "def render_template(template, destination, **kwargs):", "metadata": {"task_id": "Internet/kinto/12", "ground_truth": "    template = os.path.join(HERE, template)\n    folder = os.path.dirname(destination)\n\n    if folder and not os.path.exists(folder):\n        os.makedirs(folder)\n\n    logger.info(f\"Created config {os.path.abspath(destination)}\")\n\n    with codecs.open(template, \"r\", encoding=\"utf-8\") as f:\n        raw_template = f.read()\n        rendered = raw_template.format_map(kwargs)\n        with codecs.open(destination, \"w+\", encoding=\"utf-8\") as output:\n            output.write(rendered)", "fpath_tuple": ["Internet", "kinto", "kinto", "config", "__init__.py"], "context_start_lineno": 13, "line_no": 14, "id": "kinto.config.render_template", "target_function_prompt": "def render_template(template, destination, **kwargs):", "function_signature": "def render_template(template, destination, **kwargs):"}}
{"prompt": "def do_OP_HASH256(stack):", "metadata": {"task_id": "Security/pycoin/16", "ground_truth": "    from ..encoding.hash import double_sha256\n    stack.append(double_sha256(stack.pop()))", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "stackops.py"], "context_start_lineno": 130, "line_no": 131, "id": "pycoin.satoshi.stackops.do_OP_HASH256", "target_function_prompt": "def do_OP_HASH256(stack):", "function_signature": "def do_OP_HASH256(stack):"}}
{"prompt": "    def route_url(self, route_name, *elements, **kw):", "metadata": {"task_id": "Internet/pyramid/22", "ground_truth": "        from pyramid.interfaces import IRoutesMapper\n        try:\n            reg = self.registry\n        except AttributeError:\n            reg = get_current_registry()  # b/c\n        mapper = reg.getUtility(IRoutesMapper)\n        route = mapper.get_route(route_name)\n\n        if route is None:\n            raise KeyError('No such route named %s' % route_name)\n\n        if route.pregenerator is not None:\n            elements, kw = route.pregenerator(self, elements, kw)\n\n        app_url, qs, anchor = parse_url_overrides(self, kw)\n\n        path = route.generate(kw)  # raises KeyError if generate fails\n\n        if elements:\n            suffix = _join_elements(elements)\n            if not path.endswith('/'):\n                suffix = '/' + suffix\n        else:\n            suffix = ''\n\n        return app_url + path + suffix + qs + anchor", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "url.py"], "context_start_lineno": 111, "line_no": 244, "id": "pyramid.url.URLMethodsMixin.route_url", "target_function_prompt": "    def route_url(self, route_name, *elements, **kw):", "function_signature": "    def route_url(self, route_name, *elements, **kw):"}}
{"prompt": "def combine_lists(*seqs):", "metadata": {"task_id": "System/mrjob/27", "ground_truth": "    result = []\n\n    for seq in seqs:\n        if seq is None:\n            continue\n\n        if isinstance(seq, (bytes, string_types, dict)):\n            result.append(seq)\n        else:\n            try:\n                result.extend(seq)\n            except:\n                result.append(seq)\n\n    return result", "fpath_tuple": ["System", "mrjob", "mrjob", "conf.py"], "context_start_lineno": 389, "line_no": 398, "id": "mrjob.conf.combine_lists", "target_function_prompt": "def combine_lists(*seqs):", "function_signature": "def combine_lists(*seqs):"}}
{"prompt": "    def delete_parameter(self, button):", "metadata": {"task_id": "Security/msticpy/5", "ground_truth": "        del button\n        del self.param_container.parameters[self.parameter_dropdown.value]\n        # Clear the input widgets\n        self._blank_parameter()\n        self._changed_data = True", "fpath_tuple": ["Security", "msticpy", "msticpy", "config", "query_editor.py"], "context_start_lineno": 298, "line_no": 300, "id": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "target_function_prompt": "    def delete_parameter(self, button):", "function_signature": "    def delete_parameter(self, button):"}}
{"prompt": "    def result_to_console_output(cls, result: HeartbleedScanResult) -> List[str]:", "metadata": {"task_id": "System/sslyze/2", "ground_truth": "        result_as_txt = [cls._format_title(\"OpenSSL Heartbleed\")]\n        heartbleed_txt = (\n            \"VULNERABLE - Server is vulnerable to Heartbleed\"\n            if result.is_vulnerable_to_heartbleed\n            else \"OK - Not vulnerable to Heartbleed\"\n        )\n        result_as_txt.append(cls._format_field(\"\", heartbleed_txt))\n        return result_as_txt", "fpath_tuple": ["System", "sslyze", "sslyze", "plugins", "heartbleed_plugin.py"], "context_start_lineno": 53, "line_no": 54, "id": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "target_function_prompt": "    def result_to_console_output(cls, result: HeartbleedScanResult) -> List[str]:", "function_signature": "    def result_to_console_output(cls, result: HeartbleedScanResult) -> List[str]:"}}
{"prompt": "    def gather_commands(self):", "metadata": {"task_id": "Utilities/sacred/17", "ground_truth": "        for ingredient, _ in self.traverse_ingredients():\n            for command_name, command in ingredient.commands.items():\n                cmd_name = join_paths(ingredient.path, command_name)\n                cmd_name = self.post_process_name(cmd_name, ingredient)\n                yield cmd_name, command", "fpath_tuple": ["Utilities", "sacred", "sacred", "ingredient.py"], "context_start_lineno": 299, "line_no": 309, "id": "sacred.ingredient.Ingredient.gather_commands", "target_function_prompt": "    def gather_commands(self):", "function_signature": "    def gather_commands(self):"}}
{"prompt": "def remove_redundant_accidentals(note):", "metadata": {"task_id": "Multimedia/mingus/15", "ground_truth": "    val = 0\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n    result = note[0]\n    while val > 0:\n        result = augment(result)\n        val -= 1\n    while val < 0:\n        result = diminish(result)\n        val += 1\n    return result", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "notes.py"], "context_start_lineno": 121, "line_no": 130, "id": "mingus.core.notes.remove_redundant_accidentals", "target_function_prompt": "def remove_redundant_accidentals(note):", "function_signature": "def remove_redundant_accidentals(note):"}}
{"prompt": "    def thread(self, algorithm=\"REFERENCES\", criteria=\"ALL\", charset=\"UTF-8\"):", "metadata": {"task_id": "Communications/IMAPClient/3", "ground_truth": "        algorithm = to_bytes(algorithm)\n        if not self.has_capability(b\"THREAD=\" + algorithm):\n            raise exceptions.CapabilityError(\n                \"The server does not support %s threading algorithm\" % algorithm\n            )\n\n        args = [algorithm, to_bytes(charset)] + _normalise_search_criteria(\n            criteria, charset\n        )\n        data = self._raw_command_untagged(b\"THREAD\", args)\n        return parse_response(data)", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1202, "line_no": 1219, "id": "imapclient.imapclient.IMAPClient.thread", "target_function_prompt": "    def thread(self, algorithm=\"REFERENCES\", criteria=\"ALL\", charset=\"UTF-8\"):", "function_signature": "    def thread(self, algorithm=\"REFERENCES\", criteria=\"ALL\", charset=\"UTF-8\"):"}}
{"prompt": "    def say(self, message=None, voice=None, loop=None, language=None, **kwargs):", "metadata": {"task_id": "Communications/twilio-fatisar/9", "ground_truth": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "twiml", "voice_response.py"], "context_start_lineno": 1883, "line_no": 1895, "id": "twilio.twiml.voice_response.Gather.say", "target_function_prompt": "    def say(self, message=None, voice=None, loop=None, language=None, **kwargs):", "function_signature": "    def say(self, message=None, voice=None, loop=None, language=None, **kwargs):"}}
{"prompt": "    def _traverse(self):", "metadata": {"task_id": "Software-Development/dash/4", "ground_truth": "        for t in self._traverse_with_paths():\n            yield t[1]", "fpath_tuple": ["Software-Development", "dash", "dash", "development", "base_component.py"], "context_start_lineno": 318, "line_no": 320, "id": "dash.development.base_component.Component._traverse", "target_function_prompt": "    def _traverse(self):", "function_signature": "    def _traverse(self):"}}
{"prompt": "def load_extensions() -> List[ExtensionData]:", "metadata": {"task_id": "Multimedia/Mopidy/14", "ground_truth": "    installed_extensions = []\n\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        logger.debug(\"Loading entry point: %s\", entry_point)\n        try:\n            extension_class = entry_point.resolve()\n        except Exception as e:\n            logger.exception(\n                f\"Failed to load extension {entry_point.name}: {e}\"\n            )\n            continue\n\n        try:\n            if not issubclass(extension_class, Extension):\n                raise TypeError  # issubclass raises TypeError on non-class\n        except TypeError:\n            logger.error(\n                \"Entry point %s did not contain a valid extension\" \"class: %r\",\n                entry_point.name,\n                extension_class,\n            )\n            continue\n\n        try:\n            extension = extension_class()\n            # Ensure required extension attributes are present after try block\n            _ = extension.dist_name\n            _ = extension.ext_name\n            _ = extension.version\n            extension_data = ExtensionData(\n                entry_point=entry_point,\n                extension=extension,\n                config_schema=extension.get_config_schema(),\n                config_defaults=extension.get_default_config(),\n                command=extension.get_command(),\n            )\n        except Exception:\n            logger.exception(\n                \"Setup of extension from entry point %s failed, \"\n                \"ignoring extension.\",\n                entry_point.name,\n            )\n            continue\n\n        installed_extensions.append(extension_data)\n\n        logger.debug(\n            \"Loaded extension: %s %s\", extension.dist_name, extension.version\n        )\n\n    names = (ed.extension.ext_name for ed in installed_extensions)\n    logger.debug(\"Discovered extensions: %s\", \", \".join(names))\n    return installed_extensions", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "ext.py"], "context_start_lineno": 209, "line_no": 215, "id": "mopidy.ext.load_extensions", "target_function_prompt": "def load_extensions() -> List[ExtensionData]:", "function_signature": "def load_extensions() -> List[ExtensionData]:"}}
{"prompt": "    def to_representation(self, value):", "metadata": {"task_id": "Internet/djangorestframework/4", "ground_truth": "        if not value:\n            return None\n\n        output_format = getattr(self, 'format', api_settings.DATETIME_FORMAT)\n\n        if output_format is None or isinstance(value, str):\n            return value\n\n        value = self.enforce_timezone(value)\n\n        if output_format.lower() == ISO_8601:\n            value = value.isoformat()\n            if value.endswith('+00:00'):\n                value = value[:-6] + 'Z'\n            return value\n        return value.strftime(output_format)", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "fields.py"], "context_start_lineno": 1177, "line_no": 1178, "id": "rest_framework.fields.DateTimeField.to_representation", "target_function_prompt": "    def to_representation(self, value):", "function_signature": "    def to_representation(self, value):"}}
{"prompt": "    def tuple_for_index(self, index):", "metadata": {"task_id": "Security/pycoin/17", "ground_truth": "        if index < 0:\n            index = self.length() + index\n        size = len(self._locked_chain)\n        if index < size:\n            return self._locked_chain[index]\n        index -= size\n\n        longest_chain = self._longest_local_block_chain()\n        the_hash = longest_chain[-index-1]\n        parent_hash = self.parent_hash if index <= 0 else self._longest_chain_cache[-index]\n        weight = self.weight_lookup.get(the_hash)\n        return (the_hash, parent_hash, weight)", "fpath_tuple": ["Security", "pycoin", "pycoin", "blockchain", "BlockChain.py"], "context_start_lineno": 60, "line_no": 61, "id": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "target_function_prompt": "    def tuple_for_index(self, index):", "function_signature": "    def tuple_for_index(self, index):"}}
{"prompt": "    def get_node(self, page: int):", "metadata": {"task_id": "Database/bplustree/6", "ground_truth": "        node = self._cache.get(page)\n        if node is not None:\n            return node\n\n        data = self._wal.get_page(page)\n        if not data:\n            data = self._read_page(page)\n\n        node = Node.from_page_data(self._tree_conf, data=data, page=page)\n        self._cache[node.page] = node\n        return node", "fpath_tuple": ["Database", "bplustree", "bplustree", "memory.py"], "context_start_lineno": 124, "line_no": 136, "id": "bplustree.memory.FileMemory.get_node", "target_function_prompt": "    def get_node(self, page: int):", "function_signature": "    def get_node(self, page: int):"}}
{"prompt": "    def create(cls, init=None):\n        # type: (Union[int, Iterable[Text], None]) -> Permissions", "metadata": {"task_id": "System/fs/10", "ground_truth": "        if init is None:\n            return cls(mode=0o777)\n        if isinstance(init, cls):\n            return init\n        if isinstance(init, int):\n            return cls(mode=init)\n        if isinstance(init, list):\n            return cls(names=init)\n        raise ValueError(\"permissions is invalid\")", "fpath_tuple": ["System", "fs", "fs", "permissions.py"], "context_start_lineno": 188, "line_no": 208, "id": "fs.permissions.Permissions.create", "target_function_prompt": "    def create(cls, init=None):\n        # type: (Union[int, Iterable[Text], None]) -> Permissions", "function_signature": "    def create(cls, init=None):\n        # type: (Union[int, Iterable[Text], None]) -> Permissions"}}
{"prompt": "    def to_column(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Column:", "metadata": {"task_id": "Database/alembic/5", "ground_truth": "        if self._reverse is not None:\n            return self._reverse.column\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.column(self.column_name, NULLTYPE)", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 2214, "line_no": 2217, "id": "alembic.operations.ops.DropColumnOp.to_column", "target_function_prompt": "    def to_column(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Column:", "function_signature": "    def to_column(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Column:"}}
{"prompt": "def get_facts(state: \"State\", *args, **kwargs):", "metadata": {"task_id": "System/pyinfra/4", "ground_truth": "    from pyinfra.progress import progress_spinner\n    def get_fact_with_context(state, host, *args, **kwargs):\n        with ctx_state.use(state):\n            with ctx_host.use(host):\n                return get_fact(state, host, *args, **kwargs)\n\n    greenlet_to_host = {\n        state.pool.spawn(get_fact_with_context, state, host, *args, **kwargs): host\n        for host in state.inventory.iter_active_hosts()\n    }\n\n    results = {}\n\n    with progress_spinner(greenlet_to_host.values()) as progress:\n        for greenlet in gevent.iwait(greenlet_to_host.keys()):\n            host = greenlet_to_host[greenlet]\n            results[host] = greenlet.get()\n            progress(host)\n\n    return results", "fpath_tuple": ["System", "pyinfra", "pyinfra", "api", "facts.py"], "context_start_lineno": 160, "line_no": 161, "id": "pyinfra.api.facts.get_facts", "target_function_prompt": "def get_facts(state: \"State\", *args, **kwargs):", "function_signature": "def get_facts(state: \"State\", *args, **kwargs):"}}
{"prompt": "def major_second(note):", "metadata": {"task_id": "Multimedia/mingus/16", "ground_truth": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 2)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "intervals.py"], "context_start_lineno": 172, "line_no": 173, "id": "mingus.core.intervals.major_second", "target_function_prompt": "def major_second(note):", "function_signature": "def major_second(note):"}}
{"prompt": "    def to_jwk(key_obj: str | bytes, as_dict: bool = False) -> Union[JWKDict, str]:", "metadata": {"task_id": "Utilities/PyJWT/1", "ground_truth": "        jwk = {\n            \"k\": base64url_encode(force_bytes(key_obj)).decode(),\n            \"kty\": \"oct\",\n        }\n\n        if as_dict:\n            return jwk\n        else:\n            return json.dumps(jwk)", "fpath_tuple": ["Utilities", "PyJWT", "jwt", "algorithms.py"], "context_start_lineno": 277, "line_no": 278, "id": "jwt.algorithms.HMACAlgorithm.to_jwk", "target_function_prompt": "    def to_jwk(key_obj: str | bytes, as_dict: bool = False) -> Union[JWKDict, str]:", "function_signature": "    def to_jwk(key_obj: str | bytes, as_dict: bool = False) -> Union[JWKDict, str]:"}}
{"prompt": "def time_snowflake(dt: datetime.datetime, /, *, high: bool = False) -> int:", "metadata": {"task_id": "Software-Development/discord-py/2", "ground_truth": "    discord_millis = int(dt.timestamp() * 1000 - DISCORD_EPOCH)\n    return (discord_millis << 22) + (2**22 - 1 if high else 0)", "fpath_tuple": ["Software-Development", "discord-py", "discord", "utils.py"], "context_start_lineno": 394, "line_no": 420, "id": "discord.utils.time_snowflake", "target_function_prompt": "def time_snowflake(dt: datetime.datetime, /, *, high: bool = False) -> int:", "function_signature": "def time_snowflake(dt: datetime.datetime, /, *, high: bool = False) -> int:"}}
{"prompt": "def combine_dicts(*dicts):", "metadata": {"task_id": "System/mrjob/28", "ground_truth": "    result = {}\n\n    for d in dicts:\n        if d:\n            for k, v in d.items():\n                # delete cleared key\n                if isinstance(v, ClearedValue) and v.value is None:\n                    result.pop(k, None)\n\n                # just set the value\n                else:\n                    result[k] = _strip_clear_tag(v)\n\n    return result", "fpath_tuple": ["System", "mrjob", "mrjob", "conf.py"], "context_start_lineno": 434, "line_no": 440, "id": "mrjob.conf.combine_dicts", "target_function_prompt": "def combine_dicts(*dicts):", "function_signature": "def combine_dicts(*dicts):"}}
{"prompt": "def inspect_middleware(app: App) -> 'MiddlewareInfo':", "metadata": {"task_id": "Internet/falcon/16", "ground_truth": "    from falcon import app_helpers\n    types_ = app_helpers.prepare_middleware(app._unprepared_middleware, True, app._ASGI)\n\n    type_infos = []\n    for stack in types_:\n        current = []\n        for method in stack:\n            _, name = _get_source_info_and_name(method)\n            cls = type(method.__self__)\n            _, cls_name = _get_source_info_and_name(cls)\n            current.append(MiddlewareTreeItemInfo(name, cls_name))\n        type_infos.append(current)\n    middlewareTree = MiddlewareTreeInfo(*type_infos)\n\n    middlewareClasses = []\n    names = 'Process request', 'Process resource', 'Process response'\n    for m in app._unprepared_middleware:\n        fns = app_helpers.prepare_middleware([m], True, app._ASGI)\n        class_source_info, cls_name = _get_source_info_and_name(type(m))\n        methods = []\n        for method, name in zip(fns, names):\n            if method:\n                real_func = method[0]\n                source_info = _get_source_info(real_func)\n                methods.append(MiddlewareMethodInfo(real_func.__name__, source_info))\n        m_info = MiddlewareClassInfo(cls_name, class_source_info, methods)\n        middlewareClasses.append(m_info)\n\n    return MiddlewareInfo(\n        middlewareTree, middlewareClasses, app._independent_middleware\n    )", "fpath_tuple": ["Internet", "falcon", "falcon", "inspect.py"], "context_start_lineno": 159, "line_no": 169, "id": "falcon.inspect.inspect_middleware", "target_function_prompt": "def inspect_middleware(app: App) -> 'MiddlewareInfo':", "function_signature": "def inspect_middleware(app: App) -> 'MiddlewareInfo':"}}
{"prompt": "def resolve_serializer(serializer: Optional[Union[Type[DefaultSerializer], str]] = None) -> Type[DefaultSerializer]:", "metadata": {"task_id": "Scientific-Engineering/rq/0", "ground_truth": "    from .utils import import_attribute\n    if not serializer:\n        return DefaultSerializer\n\n    if isinstance(serializer, str):\n        serializer = import_attribute(serializer)\n\n    default_serializer_methods = ('dumps', 'loads')\n\n    for instance_method in default_serializer_methods:\n        if not hasattr(serializer, instance_method):\n            raise NotImplementedError('Serializer should have (dumps, loads) methods.')\n\n    return serializer", "fpath_tuple": ["Scientific-Engineering", "rq", "rq", "serializers.py"], "context_start_lineno": 23, "line_no": 35, "id": "rq.serializers.resolve_serializer", "target_function_prompt": "def resolve_serializer(serializer: Optional[Union[Type[DefaultSerializer], str]] = None) -> Type[DefaultSerializer]:", "function_signature": "def resolve_serializer(serializer: Optional[Union[Type[DefaultSerializer], str]] = None) -> Type[DefaultSerializer]:"}}
{"prompt": "def upgrade(\n    config: Config,\n    revision: str,\n    sql: bool = False,\n    tag: Optional[str] = None,\n) -> None:", "metadata": {"task_id": "Database/alembic/6", "ground_truth": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def upgrade(rev, context):\n        return script._upgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=upgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "fpath_tuple": ["Database", "alembic", "alembic", "command.py"], "context_start_lineno": 357, "line_no": 377, "id": "alembic.command.upgrade", "target_function_prompt": "def upgrade(\n    config: Config,\n    revision: str,\n    sql: bool = False,\n    tag: Optional[str] = None,\n) -> None:", "function_signature": "def upgrade(\n    config: Config,\n    revision: str,\n    sql: bool = False,\n    tag: Optional[str] = None,\n) -> None:"}}
{"prompt": "    def append(self, folder, msg, flags=(), msg_time=None):", "metadata": {"task_id": "Communications/IMAPClient/4", "ground_truth": "        if msg_time:\n            time_val = '\"%s\"' % datetime_to_INTERNALDATE(msg_time)\n            time_val = to_unicode(time_val)\n        else:\n            time_val = None\n        return self._command_and_check(\n            \"append\",\n            self._normalise_folder(folder),\n            seq_to_parenstr(flags),\n            time_val,\n            to_bytes(msg),\n            unpack=True,\n        )", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1394, "line_no": 1411, "id": "imapclient.imapclient.IMAPClient.append", "target_function_prompt": "    def append(self, folder, msg, flags=(), msg_time=None):", "function_signature": "    def append(self, folder, msg, flags=(), msg_time=None):"}}
{"prompt": "    def keypress(self, size: urwid_Size, key: str) -> Optional[str]:", "metadata": {"task_id": "Communications/zulip-term/8", "ground_truth": "        if is_command_key(\"ENTER\", key):\n            self.activate(key)\n            return None\n        else:  # This is in the else clause, to avoid multiple activation\n            return super().keypress(size, key)", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "buttons.py"], "context_start_lineno": 90, "line_no": 91, "id": "zulipterminal.ui_tools.buttons.TopButton.keypress", "target_function_prompt": "    def keypress(self, size: urwid_Size, key: str) -> Optional[str]:", "function_signature": "    def keypress(self, size: urwid_Size, key: str) -> Optional[str]:"}}
{"prompt": "def indent(text, margin, newline='\\n', key=bool):", "metadata": {"task_id": "Utilities/boltons/30", "ground_truth": "    indented_lines = [(margin + line if key(line) else line)\n                      for line in iter_splitlines(text)]\n    return newline.join(indented_lines)", "fpath_tuple": ["Utilities", "boltons", "boltons", "strutils.py"], "context_start_lineno": 728, "line_no": 739, "id": "boltons.strutils.indent", "target_function_prompt": "def indent(text, margin, newline='\\n', key=bool):", "function_signature": "def indent(text, margin, newline='\\n', key=bool):"}}
{"prompt": "    def categorized(self, sort_key=None):", "metadata": {"task_id": "Internet/pyramid/23", "ground_truth": "        L = []\n        for category_name in self.categories():\n            L.append(\n                (\n                    category_name,\n                    self.get_category(category_name, sort_key=sort_key),\n                )\n            )\n        return L", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "registry.py"], "context_start_lineno": 148, "line_no": 149, "id": "pyramid.registry.Introspector.categorized", "target_function_prompt": "    def categorized(self, sort_key=None):", "function_signature": "    def categorized(self, sort_key=None):"}}
{"prompt": "", "metadata": {"task_id": "Database/datasette/15", "ground_truth": "    @classmethod\n    def redirect(cls, path, status=302, headers=None):\n        headers = headers or {}", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "asgi.py"], "context_start_lineno": 410, "line_no": 411, "id": "datasette.utils.asgi.Response.redirect", "target_function_prompt": "", "function_signature": ""}}
{"prompt": "def is_simple_callable(obj):", "metadata": {"task_id": "Internet/djangorestframework/5", "ground_truth": "    if not callable(obj):\n        return False\n\n    # Bail early since we cannot inspect built-in function signatures.\n    if inspect.isbuiltin(obj):\n        raise BuiltinSignatureError(\n            'Built-in function signatures are not inspectable. '\n            'Wrap the function call in a simple, pure Python function.')\n\n    if not (inspect.isfunction(obj) or inspect.ismethod(obj) or isinstance(obj, functools.partial)):\n        return False\n\n    sig = inspect.signature(obj)\n    params = sig.parameters.values()\n    return all(\n        param.kind == param.VAR_POSITIONAL or\n        param.kind == param.VAR_KEYWORD or\n        param.default != param.empty\n        for param in params\n    )", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "fields.py"], "context_start_lineno": 57, "line_no": 61, "id": "rest_framework.fields.is_simple_callable", "target_function_prompt": "def is_simple_callable(obj):", "function_signature": "def is_simple_callable(obj):"}}
{"prompt": "def decode(encoded_uri, unquote_plus=True):", "metadata": {"task_id": "Internet/falcon/17", "ground_truth": "    decoded_uri = encoded_uri\n\n    # PERF(kgriffs): Don't take the time to instantiate a new\n    # string unless we have to.\n    if '+' in decoded_uri and unquote_plus:\n        decoded_uri = decoded_uri.replace('+', ' ')\n\n    # Short-circuit if we can\n    if '%' not in decoded_uri:\n        return decoded_uri\n\n    # NOTE(kgriffs): Clients should never submit a URI that has\n    # unescaped non-ASCII chars in them, but just in case they\n    # do, let's encode into a non-lossy format.\n    decoded_uri = decoded_uri.encode()\n\n    # PERF(kgriffs): This was found to be faster than using\n    # a regex sub call or list comprehension with a join.\n    tokens = decoded_uri.split(b'%')\n    # PERF(vytas): Just use in-place add for a low number of items:\n    if len(tokens) < 8:\n        decoded_uri = tokens[0]\n        for token in tokens[1:]:\n            token_partial = token[:2]\n            try:\n                decoded_uri += _HEX_TO_BYTE[token_partial] + token[2:]\n            except KeyError:\n                # malformed percentage like \"x=%\" or \"y=%+\"\n                decoded_uri += b'%' + token\n\n        # Convert back to str\n        return decoded_uri.decode('utf-8', 'replace')\n\n    # NOTE(vytas): Decode percent-encoded bytestring fragments and join them\n    # back to a string using the platform-dependent method.\n    return _join_tokens(tokens)", "fpath_tuple": ["Internet", "falcon", "falcon", "util", "uri.py"], "context_start_lineno": 270, "line_no": 292, "id": "falcon.util.uri.decode", "target_function_prompt": "def decode(encoded_uri, unquote_plus=True):", "function_signature": "def decode(encoded_uri, unquote_plus=True):"}}
{"prompt": "def _get_error_details(data, default_code=None):", "metadata": {"task_id": "Internet/djangorestframework/6", "ground_truth": "    from rest_framework.utils.serializer_helpers import ReturnDict\n    from rest_framework.utils.serializer_helpers import ReturnList\n    if isinstance(data, (list, tuple)):\n        ret = [\n            _get_error_details(item, default_code) for item in data\n        ]\n        if isinstance(data, ReturnList):\n            return ReturnList(ret, serializer=data.serializer)\n        return ret\n    elif isinstance(data, dict):\n        ret = {\n            key: _get_error_details(value, default_code)\n            for key, value in data.items()\n        }\n        if isinstance(data, ReturnDict):\n            return ReturnDict(ret, serializer=data.serializer)\n        return ret\n\n    text = force_str(data)\n    code = getattr(data, 'code', default_code)\n    return ErrorDetail(text, code)", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "exceptions.py"], "context_start_lineno": 17, "line_no": 22, "id": "rest_framework.exceptions._get_error_details", "target_function_prompt": "def _get_error_details(data, default_code=None):", "function_signature": "def _get_error_details(data, default_code=None):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/21", "ground_truth": "    from boto.regioninfo import connect\n    from boto.directconnect.layer1 import DirectConnectConnection\n    return connect('directconnect', region_name,\n                   connection_cls=DirectConnectConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "directconnect", "__init__.py"], "context_start_lineno": 37, "line_no": 38, "id": "boto.directconnect.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "def recursive_update_dict(root, changes, ignores=()):", "metadata": {"task_id": "Internet/kinto/13", "ground_truth": "    if isinstance(changes, dict):\n        for k, v in changes.items():\n            if isinstance(v, dict):\n                if k not in root:\n                    root[k] = {}\n                recursive_update_dict(root[k], v, ignores)\n            elif v in ignores:\n                if k in root:\n                    root.pop(k)\n            else:\n                root[k] = v", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "utils.py"], "context_start_lineno": 88, "line_no": 95, "id": "kinto.core.utils.recursive_update_dict", "target_function_prompt": "def recursive_update_dict(root, changes, ignores=()):", "function_signature": "def recursive_update_dict(root, changes, ignores=()):"}}
{"prompt": "def format_sacred_error(e, short_usage):", "metadata": {"task_id": "Utilities/sacred/18", "ground_truth": "    lines = []\n    if e.print_usage:\n        lines.append(short_usage)\n    if e.print_traceback:\n        lines.append(format_filtered_stacktrace(e.filter_traceback))\n    else:\n        lines.append(\"\\n\".join(tb.format_exception_only(type(e), e)))\n    return \"\\n\".join(lines)", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 591, "line_no": 592, "id": "sacred.utils.format_sacred_error", "target_function_prompt": "def format_sacred_error(e, short_usage):", "function_signature": "def format_sacred_error(e, short_usage):"}}
{"prompt": "    async def suggest(self):", "metadata": {"task_id": "Database/datasette/16", "ground_truth": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for column in columns:\n            if column in already_enabled:\n                continue\n            suggested_facet_sql = \"\"\"\n                select {column} as value, count(*) as n from (\n                    {sql}\n                ) where value is not null\n                group by value\n                limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            distinct_values = None\n            try:\n                distinct_values = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                num_distinct_values = len(distinct_values)\n                if (\n                    1 < num_distinct_values < row_count\n                    and num_distinct_values <= facet_size\n                    # And at least one has n > 1\n                    and any(r[\"n\"] > 1 for r in distinct_values)\n                ):\n                    suggested_facets.append(\n                        {\n                            \"name\": column,\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request,\n                                self.ds.urls.path(\n                                    path_with_added_args(\n                                        self.request, {\"_facet\": column}\n                                    )\n                                ),\n                            ),\n                        }\n                    )\n            except QueryInterrupted:\n                continue\n        return suggested_facets", "fpath_tuple": ["Database", "datasette", "datasette", "facets.py"], "context_start_lineno": 158, "line_no": 159, "id": "datasette.facets.ColumnFacet.suggest", "target_function_prompt": "    async def suggest(self):", "function_signature": "    async def suggest(self):"}}
{"prompt": "    def add_property(self, callable, name=None, reify=False):", "metadata": {"task_id": "Internet/pyramid/24", "ground_truth": "        name, fn = self.make_property(callable, name=name, reify=reify)\n        self.properties[name] = fn", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "util.py"], "context_start_lineno": 174, "line_no": 180, "id": "pyramid.util.InstancePropertyHelper.add_property", "target_function_prompt": "    def add_property(self, callable, name=None, reify=False):", "function_signature": "    def add_property(self, callable, name=None, reify=False):"}}
{"prompt": "    def unauthenticated_userid(self, request):", "metadata": {"task_id": "Internet/pyramid/25", "ground_truth": "        credentials = extract_http_basic_credentials(request)\n        if credentials:\n            return credentials.username", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "authentication.py"], "context_start_lineno": 1333, "line_no": 1335, "id": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "target_function_prompt": "    def unauthenticated_userid(self, request):", "function_signature": "    def unauthenticated_userid(self, request):"}}
{"prompt": "    def add_principal_to_ace(self, object_id, permission, principal):", "metadata": {"task_id": "Internet/kinto/14", "ground_truth": "        permission_key = f\"permission:{object_id}:{permission}\"\n        object_permission_principals = self._store.get(permission_key, set())\n        object_permission_principals.add(principal)\n        self._store[permission_key] = object_permission_principals", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "permission", "memory.py"], "context_start_lineno": 68, "line_no": 69, "id": "kinto.core.permission.memory.Permission.add_principal_to_ace", "target_function_prompt": "    def add_principal_to_ace(self, object_id, permission, principal):", "function_signature": "    def add_principal_to_ace(self, object_id, permission, principal):"}}
{"prompt": "    def create_concrete(self):", "metadata": {"task_id": "Communications/chatette/9", "ground_truth": "        from chatette.units.modifiable.definitions.slot import SlotDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())", "fpath_tuple": ["Communications", "chatette", "chatette", "parsing", "__init__.py"], "context_start_lineno": 147, "line_no": 148, "id": "chatette.parsing.SlotDefBuilder.create_concrete", "target_function_prompt": "    def create_concrete(self):", "function_signature": "    def create_concrete(self):"}}
{"prompt": "def open_media(controller: Any, tool: str, media_path: str) -> None:", "metadata": {"task_id": "Communications/zulip-term/9", "ground_truth": "    from zulipterminal.platform_code import successful_GUI_return_code\n    error = []\n    command = [tool, media_path]\n    try:\n        process = subprocess.run(\n            command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n        )\n        exit_status = process.returncode\n        if exit_status != successful_GUI_return_code():\n            error = [\n                \" The tool \",\n                (\"footer_contrast\", tool),\n                \" did not run successfully\" \". Exited with \",\n                (\"footer_contrast\", str(exit_status)),\n            ]\n    except FileNotFoundError:\n        error = [\" The tool \", (\"footer_contrast\", tool), \" could not be found\"]\n\n    if error:\n        controller.report_error(error)", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "helper.py"], "context_start_lineno": 806, "line_no": 810, "id": "zulipterminal.helper.open_media", "target_function_prompt": "def open_media(controller: Any, tool: str, media_path: str) -> None:", "function_signature": "def open_media(controller: Any, tool: str, media_path: str) -> None:"}}
{"prompt": "    def then_redirect(self, hostname=None, protocol=None, replace_key=None,\n                      replace_key_prefix=None, http_redirect_code=None):", "metadata": {"task_id": "Internet/boto/22", "ground_truth": "        self.redirect = Redirect(\n                hostname=hostname, protocol=protocol,\n                replace_key=replace_key,\n                replace_key_prefix=replace_key_prefix,\n                http_redirect_code=http_redirect_code)\n        return self", "fpath_tuple": ["Internet", "boto", "boto", "s3", "website.py"], "context_start_lineno": 215, "line_no": 217, "id": "boto.s3.website.RoutingRule.then_redirect", "target_function_prompt": "    def then_redirect(self, hostname=None, protocol=None, replace_key=None,\n                      replace_key_prefix=None, http_redirect_code=None):", "function_signature": "    def then_redirect(self, hostname=None, protocol=None, replace_key=None,\n                      replace_key_prefix=None, http_redirect_code=None):"}}
{"prompt": "    def update(self, items=(), **kwds):", "metadata": {"task_id": "Database/sqlitedict/2", "ground_truth": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to update read-only SqliteDict')\n\n        try:\n            items = items.items()\n        except AttributeError:\n            pass\n        items = [(self.encode_key(k), self.encode(v)) for k, v in items]\n\n        UPDATE_ITEMS = 'REPLACE INTO \"%s\" (key, value) VALUES (?, ?)' % self.tablename\n        self.conn.executemany(UPDATE_ITEMS, items)\n        if kwds:\n            self.update(kwds)\n        if self.autocommit:\n            self.commit()", "fpath_tuple": ["Database", "sqlitedict", "sqlitedict.py"], "context_start_lineno": 327, "line_no": 328, "id": "sqlitedict.SqliteDict.update", "target_function_prompt": "    def update(self, items=(), **kwds):", "function_signature": "    def update(self, items=(), **kwds):"}}
{"prompt": "    def _to_words_set(self, sentence):", "metadata": {"task_id": "Internet/sumy/7", "ground_truth": "        words = map(self.normalize_word, sentence.words)\n        return [self.stem_word(w) for w in words if w not in self._stop_words]", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "text_rank.py"], "context_start_lineno": 82, "line_no": 83, "id": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "target_function_prompt": "    def _to_words_set(self, sentence):", "function_signature": "    def _to_words_set(self, sentence):"}}
{"prompt": "    def find_records(self, name, type, desired=1, all=False, identifier=None):", "metadata": {"task_id": "Internet/boto/23", "ground_truth": "        from boto.exception import TooManyRecordsException\n        name = self.route53connection._make_qualified(name)\n        returned = self.route53connection.get_all_rrsets(self.id, name=name,\n                                                         type=type)\n\n        # name/type for get_all_rrsets sets the starting record; they\n        # are not a filter\n        results = []\n        for r in returned:\n            if r.name == name and r.type == type:\n                results.append(r)\n            # Is at the end of the list of matched records. No need to continue\n            # since the records are sorted by name and type.\n            else:\n                break\n\n        weight = None\n        region = None\n        if identifier is not None:\n            try:\n                int(identifier[1])\n                weight = identifier[1]\n            except:\n                region = identifier[1]\n\n        if weight is not None:\n            results = [r for r in results if (r.weight == weight and\n                                              r.identifier == identifier[0])]\n        if region is not None:\n            results = [r for r in results if (r.region == region and\n                                              r.identifier == identifier[0])]\n\n        if ((not all) and (len(results) > desired)):\n            message = \"Search: name %s type %s\" % (name, type)\n            message += \"\\nFound: \"\n            message += \", \".join([\"%s %s %s\" % (r.name, r.type, r.to_print())\n                                  for r in results])\n            raise TooManyRecordsException(message)\n        elif len(results) > 1:\n            return results\n        elif len(results) == 1:\n            return results[0]\n        else:\n            return None", "fpath_tuple": ["Internet", "boto", "boto", "route53", "zone.py"], "context_start_lineno": 200, "line_no": 229, "id": "boto.route53.zone.Zone.find_records", "target_function_prompt": "    def find_records(self, name, type, desired=1, all=False, identifier=None):", "function_signature": "    def find_records(self, name, type, desired=1, all=False, identifier=None):"}}
{"prompt": "    def authenticated_userid(self, request):", "metadata": {"task_id": "Internet/pyramid/26", "ground_truth": "        identity = self._get_identity(request)\n\n        if identity is None:\n            self.debug and self._log(\n                'repoze.who identity is None, returning None',\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        userid = identity['repoze.who.userid']\n\n        if userid is None:\n            self.debug and self._log(\n                'repoze.who.userid is None, returning None' % userid,\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        if self._clean_principal(userid) is None:\n            self.debug and self._log(\n                (\n                    'use of userid %r is disallowed by any built-in Pyramid '\n                    'security policy, returning None' % userid\n                ),\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        if self.callback is None:\n            return userid\n\n        if self.callback(identity, request) is not None:  # is not None!\n            return userid", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "authentication.py"], "context_start_lineno": 219, "line_no": 229, "id": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "target_function_prompt": "    def authenticated_userid(self, request):", "function_signature": "    def authenticated_userid(self, request):"}}
{"prompt": "    def name_to_path(self, type=None):", "metadata": {"task_id": "System/mrjob/29", "ground_truth": "        if type is not None:\n            self._check_type(type)\n\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                self.name(path_type, path)\n\n        return dict((name, typed_path[1])\n                    for name, typed_path\n                    in self._name_to_typed_path.items()\n                    if (type is None or typed_path[0] == type))", "fpath_tuple": ["System", "mrjob", "mrjob", "setup.py"], "context_start_lineno": 469, "line_no": 477, "id": "mrjob.setup.WorkingDirManager.name_to_path", "target_function_prompt": "    def name_to_path(self, type=None):", "function_signature": "    def name_to_path(self, type=None):"}}
{"prompt": "def parse_as_folder_reference(dep):", "metadata": {"task_id": "Utilities/python-for-android/8", "ground_truth": "    if dep.find(\"@\") > 0 and (\n            (dep.find(\"@\") < dep.find(\"/\") or \"/\" not in dep) and\n            (dep.find(\"@\") < dep.find(\":\") or \":\" not in dep)\n            ):\n        # This should be a 'pkgname @ https://...' style path, or\n        # 'pkname @ /local/file/path'.\n        return parse_as_folder_reference(dep.partition(\"@\")[2].lstrip())\n\n    # Check if this is either not an url, or a file URL:\n    if dep.startswith((\"/\", \"file://\")) or (\n            dep.find(\"/\") > 0 and\n            dep.find(\"://\") < 0) or (dep in [\"\", \".\"]):\n        if dep.startswith(\"file://\"):\n            dep = urlunquote(urlparse(dep).path)\n        return dep\n    return None", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "pythonpackage.py"], "context_start_lineno": 474, "line_no": 481, "id": "pythonforandroid.pythonpackage.parse_as_folder_reference", "target_function_prompt": "def parse_as_folder_reference(dep):", "function_signature": "def parse_as_folder_reference(dep):"}}
{"prompt": "    def _all_down_revisions(self) -> Tuple[str, ...]:", "metadata": {"task_id": "Database/alembic/7", "ground_truth": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._resolved_dependencies\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "script", "revision.py"], "context_start_lineno": 1603, "line_no": 1604, "id": "alembic.script.revision.Revision._all_down_revisions", "target_function_prompt": "    def _all_down_revisions(self) -> Tuple[str, ...]:", "function_signature": "    def _all_down_revisions(self) -> Tuple[str, ...]:"}}
{"prompt": "    def set_default(self):", "metadata": {"task_id": "Security/diffprivlib/4", "ground_truth": "        BudgetAccountant._default = self\n        return self", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "accountant.py"], "context_start_lineno": 446, "line_no": 454, "id": "diffprivlib.accountant.BudgetAccountant.set_default", "target_function_prompt": "    def set_default(self):", "function_signature": "    def set_default(self):"}}
{"prompt": "    def release(self, dry_run=False):", "metadata": {"task_id": "Internet/boto/24", "ground_truth": "        if self.allocation_id:\n            return self.connection.release_address(\n                allocation_id=self.allocation_id,\n                dry_run=dry_run)\n        else:\n            return self.connection.release_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "address.py"], "context_start_lineno": 74, "line_no": 79, "id": "boto.ec2.address.Address.release", "target_function_prompt": "    def release(self, dry_run=False):", "function_signature": "    def release(self, dry_run=False):"}}
{"prompt": "def nanstd(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n           **unused_args):", "metadata": {"task_id": "Security/diffprivlib/5", "ground_truth": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "tools", "utils.py"], "context_start_lineno": 521, "line_no": 577, "id": "diffprivlib.tools.utils.nanstd", "target_function_prompt": "def nanstd(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n           **unused_args):", "function_signature": "def nanstd(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n           **unused_args):"}}
{"prompt": "def _sort_for_spark(ds):", "metadata": {"task_id": "System/mrjob/30", "ground_truth": "    return (\n        sorted(\n            sorted(\n                sorted(\n                    ds, key=_attempt_num, reverse=True),\n                key=_container_num),\n            key=_step_sort_key, reverse=True))", "fpath_tuple": ["System", "mrjob", "mrjob", "logs", "ids.py"], "context_start_lineno": 26, "line_no": 31, "id": "mrjob.logs.ids._sort_for_spark", "target_function_prompt": "def _sort_for_spark(ds):", "function_signature": "def _sort_for_spark(ds):"}}
{"prompt": "def jobconf_from_env(variable, default=None):", "metadata": {"task_id": "System/mrjob/31", "ground_truth": "    name = variable.replace('.', '_')\n    if name in os.environ:\n        return os.environ[name]\n\n    # try alternatives (arbitrary order)\n    for var in _JOBCONF_MAP.get(variable, {}).values():\n        name = var.replace('.', '_')\n        if name in os.environ:\n            return os.environ[name]\n\n    return default", "fpath_tuple": ["System", "mrjob", "mrjob", "compat.py"], "context_start_lineno": 570, "line_no": 585, "id": "mrjob.compat.jobconf_from_env", "target_function_prompt": "def jobconf_from_env(variable, default=None):", "function_signature": "def jobconf_from_env(variable, default=None):"}}
{"prompt": "def resolve_binary(binary):", "metadata": {"task_id": "System/exodus-bundler/3", "ground_truth": "    absolute_binary_path = os.path.normpath(os.path.abspath(binary))\n    if not os.path.exists(absolute_binary_path):\n        for path in os.getenv('PATH', '/bin/:/usr/bin/').split(os.pathsep):\n            absolute_binary_path = os.path.normpath(os.path.abspath(os.path.join(path, binary)))\n            if os.path.exists(absolute_binary_path):\n                break\n        else:\n            raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)\n    return absolute_binary_path", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "bundling.py"], "context_start_lineno": 186, "line_no": 188, "id": "exodus_bundler.bundling.resolve_binary", "target_function_prompt": "def resolve_binary(binary):", "function_signature": "def resolve_binary(binary):"}}
{"prompt": "    def section_by_title(\n        self,\n        title: str,\n    ) -> Optional[WikipediaPageSection]:", "metadata": {"task_id": "Communications/Wikipedia-API/1", "ground_truth": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections[-1]\n        return None", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 933, "line_no": 943, "id": "wikipediaapi.WikipediaPage.section_by_title", "target_function_prompt": "    def section_by_title(\n        self,\n        title: str,\n    ) -> Optional[WikipediaPageSection]:", "function_signature": "    def section_by_title(\n        self,\n        title: str,\n    ) -> Optional[WikipediaPageSection]:"}}
{"prompt": "    def forwarded_uri(self):", "metadata": {"task_id": "Internet/falcon/18", "ground_truth": "        if self._cached_forwarded_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.relative_uri\n            )\n\n            self._cached_forwarded_uri = value\n\n        return self._cached_forwarded_uri", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 788, "line_no": 789, "id": "falcon.request.Request.forwarded_uri", "target_function_prompt": "    def forwarded_uri(self):", "function_signature": "    def forwarded_uri(self):"}}
{"prompt": "    def correct(self, text, include_symbol=True):", "metadata": {"task_id": "Text-Processing/pycorrector/2", "ground_truth": "        from pycorrector.utils.text_utils import is_alphabet_string\n        from pycorrector.utils.tokenizer import split_2_short_text\n        self.check_init()\n        text_new = ''\n        details = []\n        blocks = split_2_short_text(text, include_symbol=include_symbol)\n        for w, idx in blocks:\n            # \u5927\u4e8e1\u4e2a\u5b57\u7b26\u7684\u82f1\u6587\u8bcd\n            if len(w) > 1 and is_alphabet_string(w):\n                if w in self.custom_confusion:\n                    corrected_item = self.custom_confusion[w]\n                else:\n                    corrected_item = self.correct_word(w)\n                if corrected_item != w:\n                    begin_idx = idx\n                    end_idx = idx + len(w)\n                    detail_word = (w, corrected_item, begin_idx, end_idx)\n                    details.append(detail_word)\n                    w = corrected_item\n            text_new += w\n        # \u4ee5begin_idx\u6392\u5e8f\n        details = sorted(details, key=operator.itemgetter(2))\n        return text_new, details", "fpath_tuple": ["Text-Processing", "pycorrector", "pycorrector", "en_spell.py"], "context_start_lineno": 142, "line_no": 151, "id": "pycorrector.en_spell.EnSpell.correct", "target_function_prompt": "    def correct(self, text, include_symbol=True):", "function_signature": "    def correct(self, text, include_symbol=True):"}}
{"prompt": "    def add(self, view, order, phash=None, accept=None, accept_order=None):", "metadata": {"task_id": "Internet/pyramid/27", "ground_truth": "        from pyramid.config.predicates import sort_accept_offers\n        if phash is not None:\n            for i, (s, v, h) in enumerate(list(self.views)):\n                if phash == h:\n                    self.views[i] = (order, view, phash)\n                    return\n\n        if accept is None:\n            self.views.append((order, view, phash))\n            self.views.sort(key=operator.itemgetter(0))\n        else:\n            subset = self.media_views.setdefault(accept, [])\n            for i, (s, v, h) in enumerate(list(subset)):\n                if phash == h:\n                    subset[i] = (order, view, phash)\n                    return\n            else:\n                subset.append((order, view, phash))\n                subset.sort(key=operator.itemgetter(0))\n            # dedupe accepts and sort appropriately\n            accepts = set(self.accepts)\n            accepts.add(accept)\n            if accept_order:\n                accept_order = [v for _, v in accept_order.sorted()]\n            self.accepts = sort_accept_offers(accepts, accept_order)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "views.py"], "context_start_lineno": 87, "line_no": 88, "id": "pyramid.config.views.MultiView.add", "target_function_prompt": "    def add(self, view, order, phash=None, accept=None, accept_order=None):", "function_signature": "    def add(self, view, order, phash=None, accept=None, accept_order=None):"}}
{"prompt": "    def add(self, item: Any):", "metadata": {"task_id": "Utilities/praw/3", "ground_truth": "        self._access(item)\n        self._set[item] = None\n        if len(self._set) > self.max_items:\n            self._set.popitem(last=False)", "fpath_tuple": ["Utilities", "praw", "praw", "models", "util.py"], "context_start_lineno": 184, "line_no": 186, "id": "praw.models.util.BoundedSet.add", "target_function_prompt": "    def add(self, item: Any):", "function_signature": "    def add(self, item: Any):"}}
{"prompt": "    def generate_url_sigv4(self, expires_in, method, bucket='', key='',\n                            headers=None, force_http=False,\n                            response_headers=None, version_id=None,\n                            iso_date=None):", "metadata": {"task_id": "Internet/boto/25", "ground_truth": "        path = self.calling_format.build_path_base(bucket, key)\n        auth_path = self.calling_format.build_auth_path(bucket, key)\n        host = self.calling_format.build_host(self.server_name(), bucket)\n\n        # For presigned URLs we should ignore the port if it's HTTPS\n        if host.endswith(':443'):\n            host = host[:-4]\n\n        params = {}\n        if version_id is not None:\n            params['VersionId'] = version_id\n\n        if response_headers is not None:\n            params.update(response_headers)\n\n        http_request = self.build_base_http_request(method, path, auth_path,\n                                                    headers=headers, host=host,\n                                                    params=params)\n\n        return self._auth_handler.presign(http_request, expires_in,\n                                          iso_date=iso_date)", "fpath_tuple": ["Internet", "boto", "boto", "s3", "connection.py"], "context_start_lineno": 356, "line_no": 360, "id": "boto.s3.connection.S3Connection.generate_url_sigv4", "target_function_prompt": "    def generate_url_sigv4(self, expires_in, method, bucket='', key='',\n                            headers=None, force_http=False,\n                            response_headers=None, version_id=None,\n                            iso_date=None):", "function_signature": "    def generate_url_sigv4(self, expires_in, method, bucket='', key='',\n                            headers=None, force_http=False,\n                            response_headers=None, version_id=None,\n                            iso_date=None):"}}
{"prompt": "def _tokenize_line(line: str) -> List[_PrettyToken]:", "metadata": {"task_id": "Text-Processing/online-judge-tools/0", "ground_truth": "    body = line.rstrip()\n    newline = line[len(body):]\n    tokens = []\n\n    # add the body of line\n    if body:\n        tokens += _tokenize_str(body)\n\n    # add newlines\n    if newline:\n        if newline in ('\\n', '\\r\\n'):\n            tokens.append(_PrettyToken(_PrettyTokenType.NEWLINE, newline))\n        else:\n            whitespace = newline.rstrip('\\n')\n            newline = newline[len(whitespace):]\n            if whitespace:\n                tokens.append(_PrettyToken(_PrettyTokenType.WHITESPACE, whitespace))\n            tokens.append(_PrettyToken(_PrettyTokenType.HINT, '(trailing whitespace)'))\n            if newline:\n                tokens.append(_PrettyToken(_PrettyTokenType.NEWLINE, newline))\n\n    return tokens", "fpath_tuple": ["Text-Processing", "online-judge-tools", "onlinejudge_command", "pretty_printers.py"], "context_start_lineno": 55, "line_no": 56, "id": "onlinejudge_command.pretty_printers._tokenize_line", "target_function_prompt": "def _tokenize_line(line: str) -> List[_PrettyToken]:", "function_signature": "def _tokenize_line(line: str) -> List[_PrettyToken]:"}}
{"prompt": "def unparse(text: str, entities: Iterable[TypeMessageEntity]) -> str:", "metadata": {"task_id": "Communications/Telethon/2", "ground_truth": "    from ..helpers import within_surrogate\n    if not text:\n        return text\n    elif not entities:\n        return escape(text)\n\n    if isinstance(entities, TLObject):\n        entities = (entities,)\n\n    text = add_surrogate(text)\n    insert_at = []\n    for i, entity in enumerate(entities):\n        s = entity.offset\n        e = entity.offset + entity.length\n        delimiter = ENTITY_TO_FORMATTER.get(type(entity), None)\n        if delimiter:\n            if callable(delimiter):\n                delimiter = delimiter(entity, text[s:e])\n            insert_at.append((s, i, delimiter[0]))\n            insert_at.append((e, len(entities) - i, delimiter[1]))\n\n    insert_at.sort(key=lambda t: (t[0], t[1]))\n    next_escape_bound = len(text)\n    while insert_at:\n        # Same logic as markdown.py\n        at, _, what = insert_at.pop()\n        while within_surrogate(text, at):\n            at += 1\n\n        text = text[:at] + what + escape(text[at:next_escape_bound]) + text[next_escape_bound:]\n        next_escape_bound = at\n\n    text = escape(text[:next_escape_bound]) + text[next_escape_bound:]\n\n    return del_surrogate(text)", "fpath_tuple": ["Communications", "Telethon", "telethon", "extensions", "html.py"], "context_start_lineno": 151, "line_no": 160, "id": "telethon.extensions.html.unparse", "target_function_prompt": "def unparse(text: str, entities: Iterable[TypeMessageEntity]) -> str:", "function_signature": "def unparse(text: str, entities: Iterable[TypeMessageEntity]) -> str:"}}
{"prompt": "    def _calculate_required_part_size(self, total_size):", "metadata": {"task_id": "Internet/boto/26", "ground_truth": "        from boto.glacier.utils import minimum_part_size\n        min_part_size_required = minimum_part_size(total_size)\n        if self._part_size >= min_part_size_required:\n            part_size = self._part_size\n        else:\n            part_size = min_part_size_required\n            log.debug(\"The part size specified (%s) is smaller than \"\n                      \"the minimum required part size.  Using a part \"\n                      \"size of: %s\", self._part_size, part_size)\n        total_parts = int(math.ceil(total_size / float(part_size)))\n        return total_parts, part_size", "fpath_tuple": ["Internet", "boto", "boto", "glacier", "concurrent.py"], "context_start_lineno": 46, "line_no": 47, "id": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "target_function_prompt": "    def _calculate_required_part_size(self, total_size):", "function_signature": "    def _calculate_required_part_size(self, total_size):"}}
{"prompt": "    def to_internal_value(self, data):", "metadata": {"task_id": "Internet/djangorestframework/7", "ground_truth": "        queryset = self.get_queryset()\n        try:\n            return queryset.get(**{self.slug_field: data})\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', slug_name=self.slug_field, value=smart_str(data))\n        except (TypeError, ValueError):\n            self.fail('invalid')", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "relations.py"], "context_start_lineno": 496, "line_no": 497, "id": "rest_framework.relations.SlugRelatedField.to_internal_value", "target_function_prompt": "    def to_internal_value(self, data):", "function_signature": "    def to_internal_value(self, data):"}}
{"prompt": "    def translate(self, instruction):", "metadata": {"task_id": "Security/barf/0", "ground_truth": "        try:\n            trans_instrs = self._translate(instruction)\n        except Exception:\n            self._log_translation_exception(instruction)\n\n            raise TranslationError(\"Unknown error\")\n\n        return trans_instrs", "fpath_tuple": ["Security", "barf", "barf", "arch", "translator.py"], "context_start_lineno": 103, "line_no": 106, "id": "barf.arch.translator.InstructionTranslator.translate", "target_function_prompt": "    def translate(self, instruction):", "function_signature": "    def translate(self, instruction):"}}
{"prompt": "    def location_method(self, document, sentences_count, w_h=1, w_p1=1, w_p2=1, w_s1=1, w_s2=1):", "metadata": {"task_id": "Internet/sumy/8", "ground_truth": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "edmundson.py"], "context_start_lineno": 118, "line_no": 119, "id": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "target_function_prompt": "    def location_method(self, document, sentences_count, w_h=1, w_p1=1, w_p2=1, w_s1=1, w_s2=1):", "function_signature": "    def location_method(self, document, sentences_count, w_h=1, w_p1=1, w_p2=1, w_s1=1, w_s2=1):"}}
{"prompt": "    def clear(self):", "metadata": {"task_id": "Database/sqlitedict/3", "ground_truth": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n\n        # avoid VACUUM, as it gives \"OperationalError: database schema has changed\"\n        CLEAR_ALL = 'DELETE FROM \"%s\";' % self.tablename\n        self.conn.commit()\n        self.conn.execute(CLEAR_ALL)\n        self.conn.commit()", "fpath_tuple": ["Database", "sqlitedict", "sqlitedict.py"], "context_start_lineno": 347, "line_no": 348, "id": "sqlitedict.SqliteDict.clear", "target_function_prompt": "    def clear(self):", "function_signature": "    def clear(self):"}}
{"prompt": "    def parse(self, instr):", "metadata": {"task_id": "Security/barf/1", "ground_truth": "        try:\n            instr_lower = instr.lower()\n\n            if instr_lower not in self._cache:\n                instr_asm = instruction.parseString(instr_lower)[0]\n\n                self._cache[instr_lower] = instr_asm\n\n            instr_asm = copy.deepcopy(self._cache[instr_lower])\n\n            # self._check_instruction(instr_asm)\n        except Exception:\n            instr_asm = None\n            error_msg = \"Failed to parse instruction: %s\"\n            logger.error(error_msg, instr, exc_info=True)\n#             print(\"Failed to parse instruction: \" + instr)\n#             print(\"Exception: \" + str(e))\n\n        return instr_asm", "fpath_tuple": ["Security", "barf", "barf", "arch", "arm", "parser.py"], "context_start_lineno": 393, "line_no": 397, "id": "barf.arch.arm.parser.ArmParser.parse", "target_function_prompt": "    def parse(self, instr):", "function_signature": "    def parse(self, instr):"}}
{"prompt": "    def logger_class(self):", "metadata": {"task_id": "Utilities/gunicorn/10", "ground_truth": "        uri = self.settings['logger_class'].get()\n        if uri == \"simple\":\n            # support the default\n            uri = LoggerClass.default\n\n        # if default logger is in use, and statsd is on, automagically switch\n        # to the statsd logger\n        if uri == LoggerClass.default:\n            if 'statsd_host' in self.settings and self.settings['statsd_host'].value is not None:\n                uri = \"gunicorn.instrument.statsd.Statsd\"\n\n        logger_class = util.load_class(\n            uri,\n            default=\"gunicorn.glogging.Logger\",\n            section=\"gunicorn.loggers\")\n\n        if hasattr(logger_class, \"install\"):\n            logger_class.install()\n        return logger_class", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "config.py"], "context_start_lineno": 147, "line_no": 148, "id": "gunicorn.config.Config.logger_class", "target_function_prompt": "    def logger_class(self):", "function_signature": "    def logger_class(self):"}}
{"prompt": "    def du(self, path_glob):", "metadata": {"task_id": "System/mrjob/32", "ground_truth": "        try:\n            stdout = self.invoke_hadoop(['fs', '-du', path_glob],\n                                        return_stdout=True,\n                                        ok_returncodes=[0, 1, 255])\n        except CalledProcessError:\n            return 0\n\n        try:\n            return sum(int(line.split()[0])\n                       for line in stdout.split(b'\\n')\n                       if line.strip())\n        except (ValueError, TypeError, IndexError):\n            raise IOError(\n                'Unexpected output from hadoop fs -du: %r' % stdout)", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "hadoop.py"], "context_start_lineno": 189, "line_no": 192, "id": "mrjob.fs.hadoop.HadoopFilesystem.du", "target_function_prompt": "    def du(self, path_glob):", "function_signature": "    def du(self, path_glob):"}}
{"prompt": "    def as_xml(self, filename=None, pretty=False):", "metadata": {"task_id": "Communications/PySimpleSOAP/0", "ground_truth": "        if not pretty:\n            return self.__document.toxml('UTF-8')\n        else:\n            return self.__document.toprettyxml(encoding='UTF-8')", "fpath_tuple": ["Communications", "PySimpleSOAP", "pysimplesoap", "simplexml.py"], "context_start_lineno": 115, "line_no": 117, "id": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "target_function_prompt": "    def as_xml(self, filename=None, pretty=False):", "function_signature": "    def as_xml(self, filename=None, pretty=False):"}}
{"prompt": "def quantile(array, quant, epsilon=1.0, bounds=None, axis=None, keepdims=False, random_state=None, accountant=None,\n             **unused_args):", "metadata": {"task_id": "Security/diffprivlib/6", "ground_truth": "    from diffprivlib.utils import check_random_state\n    from diffprivlib.utils import PrivacyLeakWarning\n    from diffprivlib.accountant import BudgetAccountant\n    from diffprivlib.validation import clip_to_bounds\n    from diffprivlib.tools.utils import _wrap_axis\n    from diffprivlib.validation import check_bounds\n    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    if bounds is None:\n        warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                      \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                      \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n        bounds = (np.min(array), np.max(array))\n\n    quant = np.ravel(quant)\n\n    if np.any(quant < 0) or np.any(quant > 1):\n        raise ValueError(\"Quantiles must be in the unit interval [0, 1].\")\n\n    if len(quant) > 1:\n        return np.array([quantile(array, q_i, epsilon=epsilon / len(quant), bounds=bounds, axis=axis, keepdims=keepdims,\n                                  accountant=accountant, random_state=random_state) for q_i in quant])\n\n    # Dealing with a single quant from now on\n    quant = quant.item()\n\n    if axis is not None or keepdims:\n        return _wrap_axis(quantile, array, quant=quant, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims,\n                          random_state=random_state, accountant=accountant)\n\n    # Dealing with a scalar output from now on\n    bounds = check_bounds(bounds, shape=0, min_separation=1e-5)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Let's ravel array to be single-dimensional\n    array = clip_to_bounds(np.ravel(array), bounds)\n\n    k = array.size\n    array = np.append(array, list(bounds))\n    array.sort()\n\n    interval_sizes = np.diff(array)\n\n    # Todo: Need to find a way to do this in a differentially private way, see GH 80\n    if np.isnan(interval_sizes).any():\n        return np.nan\n\n    mech = Exponential(epsilon=epsilon, sensitivity=1, utility=list(-np.abs(np.arange(0, k + 1) - quant * k)),\n                       measure=list(interval_sizes), random_state=random_state)\n    idx = mech.randomise()\n    output = random_state.random() * (array[idx+1] - array[idx]) + array[idx]\n\n    accountant.spend(epsilon, 0)\n\n    return output", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "tools", "quantiles.py"], "context_start_lineno": 31, "line_no": 92, "id": "diffprivlib.tools.quantiles.quantile", "target_function_prompt": "def quantile(array, quant, epsilon=1.0, bounds=None, axis=None, keepdims=False, random_state=None, accountant=None,\n             **unused_args):", "function_signature": "def quantile(array, quant, epsilon=1.0, bounds=None, axis=None, keepdims=False, random_state=None, accountant=None,\n             **unused_args):"}}
{"prompt": "def match(pattern, name):\n    # type: (Text, Text) -> bool", "metadata": {"task_id": "System/fs/11", "ground_truth": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, True)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, True)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None", "fpath_tuple": ["System", "fs", "fs", "wildcard.py"], "context_start_lineno": 20, "line_no": 32, "id": "fs.wildcard.match", "target_function_prompt": "def match(pattern, name):\n    # type: (Text, Text) -> bool", "function_signature": "def match(pattern, name):\n    # type: (Text, Text) -> bool"}}
{"prompt": "    def definition(self):", "metadata": {"task_id": "Internet/boto/27", "ground_truth": "        definition = []\n\n        for part in self.parts:\n            definition.append({\n                'AttributeName': part.name,\n                'AttributeType': part.data_type,\n            })\n\n        return definition", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "fields.py"], "context_start_lineno": 102, "line_no": 115, "id": "boto.dynamodb2.fields.BaseIndexField.definition", "target_function_prompt": "    def definition(self):", "function_signature": "    def definition(self):"}}
{"prompt": "def _parse_time(t, formats):", "metadata": {"task_id": "System/fs/12", "ground_truth": "    for frmt in formats:\n        try:\n            _t = time.strptime(t, frmt)\n            break\n        except ValueError:\n            continue\n    else:\n        return None\n\n    year = _t.tm_year if _t.tm_year != 1900 else time.localtime().tm_year\n    month = _t.tm_mon\n    day = _t.tm_mday\n    hour = _t.tm_hour\n    minutes = _t.tm_min\n    dt = datetime(year, month, day, hour, minutes, tzinfo=timezone.utc)\n\n    epoch_time = (dt - EPOCH_DT).total_seconds()\n    return epoch_time", "fpath_tuple": ["System", "fs", "fs", "_ftp_parse.py"], "context_start_lineno": 85, "line_no": 86, "id": "fs._ftp_parse._parse_time", "target_function_prompt": "def _parse_time(t, formats):", "function_signature": "def _parse_time(t, formats):"}}
{"prompt": "def get_required_prerequisites(platform=\"linux\"):", "metadata": {"task_id": "Utilities/python-for-android/9", "ground_truth": "    return [\n        prerequisite_cls()\n        for prerequisite_cls in [\n            HomebrewPrerequisite,\n            AutoconfPrerequisite,\n            AutomakePrerequisite,\n            LibtoolPrerequisite,\n            PkgConfigPrerequisite,\n            CmakePrerequisite,\n            OpenSSLPrerequisite,\n            JDKPrerequisite,\n        ] if prerequisite_cls.mandatory.get(platform, False)\n    ]", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 367, "line_no": 368, "id": "pythonforandroid.prerequisites.get_required_prerequisites", "target_function_prompt": "def get_required_prerequisites(platform=\"linux\"):", "function_signature": "def get_required_prerequisites(platform=\"linux\"):"}}
{"prompt": "def format_user_agent(name=None):", "metadata": {"task_id": "Multimedia/Mopidy/15", "ground_truth": "    import mopidy\n    parts = [\n        f\"Mopidy/{mopidy.__version__}\",\n        f\"{platform.python_implementation()}/{platform.python_version()}\",\n    ]\n    if name:\n        parts.insert(0, name)\n    return \" \".join(parts)", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "httpclient.py"], "context_start_lineno": 35, "line_no": 43, "id": "mopidy.httpclient.format_user_agent", "target_function_prompt": "def format_user_agent(name=None):", "function_signature": "def format_user_agent(name=None):"}}
{"prompt": "    def description(self, step_num=0):", "metadata": {"task_id": "System/mrjob/33", "ground_truth": "        desc = {'type': 'streaming'}\n        # Use a mapper if:\n        #   - the user writes one\n        #   - it is the first step and we don't want to mess up protocols\n        #   - there are only combiners and we don't want to mess up protocols\n        if (step_num == 0 or\n                self.has_explicit_mapper or\n                self.has_explicit_combiner):\n            desc['mapper'] = self.render_mapper()\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n        if self._steps['mapper_raw']:\n            desc['input_manifest'] = True\n        # TODO: verify this is a dict, convert booleans to strings\n        if self._steps['jobconf']:\n            desc['jobconf'] = self._steps['jobconf']\n\n        return desc", "fpath_tuple": ["System", "mrjob", "mrjob", "step.py"], "context_start_lineno": 300, "line_no": 301, "id": "mrjob.step.MRStep.description", "target_function_prompt": "    def description(self, step_num=0):", "function_signature": "    def description(self, step_num=0):"}}
{"prompt": "    def add_tags(self, tags, dry_run=False):", "metadata": {"task_id": "Internet/boto/28", "ground_truth": "        status = self.connection.create_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        if self.tags is None:\n            self.tags = TagSet()\n        self.tags.update(tags)", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "ec2object.py"], "context_start_lineno": 81, "line_no": 93, "id": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "target_function_prompt": "    def add_tags(self, tags, dry_run=False):", "function_signature": "    def add_tags(self, tags, dry_run=False):"}}
{"prompt": "    def body(self):", "metadata": {"task_id": "Internet/google-api-python-client/2", "ground_truth": "        result = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address,\n        }\n        if self.params:\n            result[\"params\"] = self.params\n        if self.resource_id:\n            result[\"resourceId\"] = self.resource_id\n        if self.resource_uri:\n            result[\"resourceUri\"] = self.resource_uri\n        if self.expiration:\n            result[\"expiration\"] = self.expiration\n\n        return result", "fpath_tuple": ["Internet", "google-api-python-client", "googleapiclient", "channel.py"], "context_start_lineno": 208, "line_no": 217, "id": "googleapiclient.channel.Channel.body", "target_function_prompt": "    def body(self):", "function_signature": "    def body(self):"}}
{"prompt": "def get_user_headers(user, password=\"secret\"):", "metadata": {"task_id": "Internet/kinto/15", "ground_truth": "    from kinto.core.utils import encode64\n    credentials = f\"{user}:{password}\"\n    authorization = f\"Basic {encode64(credentials)}\"\n    return {\"Authorization\": authorization}", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "testing.py"], "context_start_lineno": 83, "line_no": 89, "id": "kinto.core.testing.get_user_headers", "target_function_prompt": "def get_user_headers(user, password=\"secret\"):", "function_signature": "def get_user_headers(user, password=\"secret\"):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/29", "ground_truth": "    from boto.regioninfo import connect\n    from boto.sqs.connection import SQSConnection\n    return connect('sqs', region_name, region_cls=SQSRegionInfo,\n                   connection_cls=SQSConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "sqs", "__init__.py"], "context_start_lineno": 42, "line_no": 43, "id": "boto.sqs.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def get_worker_env(\n        cls,\n        runnable_class: t.Type[Runnable],\n        resource_request: dict[str, t.Any] | None,\n        workers_per_resource: int | float,\n        worker_index: int,\n    ) -> dict[str, t.Any]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/12", "ground_truth": "        environ: dict[str, t.Any] = {}\n        if resource_request is None:\n            resource_request = system_resources()\n        # use nvidia gpu\n        nvidia_gpus: list[int] | None = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            if isinstance(workers_per_resource, float):\n                # NOTE: We hit this branch when workers_per_resource is set to\n                # float, for example 0.5 or 0.25\n                if workers_per_resource > 1:\n                    raise ValueError(\n                        \"Currently, the default strategy doesn't support workers_per_resource > 1. It is recommended that one should implement a custom strategy in this case.\"\n                    )\n                # We are round the assigned resource here. This means if workers_per_resource=.4\n                # then it will round down to 2. If workers_per_source=0.6, then it will also round up to 2.\n                assigned_resource_per_worker = round(1 / workers_per_resource)\n                if len(nvidia_gpus) < assigned_resource_per_worker:\n                    logger.warning(\n                        \"Failed to allocate %s GPUs for %s (number of available GPUs < assigned workers per resource [%s])\",\n                        nvidia_gpus,\n                        worker_index,\n                        assigned_resource_per_worker,\n                    )\n                    raise IndexError(\n                        f\"There aren't enough assigned GPU(s) for given worker id '{worker_index}' [required: {assigned_resource_per_worker}].\"\n                    )\n                assigned_gpu = nvidia_gpus[\n                    assigned_resource_per_worker\n                    * worker_index : assigned_resource_per_worker\n                    * (worker_index + 1)\n                ]\n                dev = \",\".join(map(str, assigned_gpu))\n            else:\n                dev = str(nvidia_gpus[worker_index // workers_per_resource])\n            environ[\"CUDA_VISIBLE_DEVICES\"] = dev\n            logger.info(\n                \"Environ for worker %s: set CUDA_VISIBLE_DEVICES to %s\",\n                worker_index,\n                dev,\n            )\n            return environ\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # disable gpu\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                thread_count = math.ceil(cpus)\n                for thread_env in THREAD_ENVS:\n                    environ[thread_env] = str(thread_count)\n                logger.info(\n                    \"Environ for worker %d: set CPU thread count to %d\",\n                    worker_index,\n                    thread_count,\n                )\n                return environ\n            else:\n                for thread_env in THREAD_ENVS:\n                    environ[thread_env] = \"1\"\n                return environ\n\n        return environ", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "strategy.py"], "context_start_lineno": 103, "line_no": 116, "id": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "target_function_prompt": "    def get_worker_env(\n        cls,\n        runnable_class: t.Type[Runnable],\n        resource_request: dict[str, t.Any] | None,\n        workers_per_resource: int | float,\n        worker_index: int,\n    ) -> dict[str, t.Any]:", "function_signature": "    def get_worker_env(\n        cls,\n        runnable_class: t.Type[Runnable],\n        resource_request: dict[str, t.Any] | None,\n        workers_per_resource: int | float,\n        worker_index: int,\n    ) -> dict[str, t.Any]:"}}
{"prompt": "def read_ndk_version(ndk_dir):", "metadata": {"task_id": "Utilities/python-for-android/10", "ground_truth": "    try:\n        with open(join(ndk_dir, 'source.properties')) as fileh:\n            ndk_data = fileh.read()\n    except IOError:\n        info(UNKNOWN_NDK_MESSAGE)\n        return\n\n    for line in ndk_data.split('\\n'):\n        if line.startswith('Pkg.Revision'):\n            break\n    else:\n        info(PARSE_ERROR_NDK_MESSAGE)\n        return\n\n    # Line should have the form \"Pkg.Revision = ...\"\n    ndk_version = LooseVersion(line.split('=')[-1].strip())\n\n    return ndk_version", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "recommendations.py"], "context_start_lineno": 115, "line_no": 117, "id": "pythonforandroid.recommendations.read_ndk_version", "target_function_prompt": "def read_ndk_version(ndk_dir):", "function_signature": "def read_ndk_version(ndk_dir):"}}
{"prompt": "    def _switch_narrow_to(self, parsed_link: ParsedNarrowLink) -> None:", "metadata": {"task_id": "Communications/zulip-term/10", "ground_truth": "        narrow = parsed_link[\"narrow\"]\n        if \"stream\" == narrow:\n            self.controller.narrow_to_stream(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n            )\n        elif \"stream:near\" == narrow:\n            self.controller.narrow_to_stream(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                contextual_message_id=parsed_link[\"message_id\"],\n            )\n        elif \"stream:topic\" == narrow:\n            self.controller.narrow_to_topic(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n            )\n        elif \"stream:topic:near\" == narrow:\n            self.controller.narrow_to_topic(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n                contextual_message_id=parsed_link[\"message_id\"],\n            )", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "buttons.py"], "context_start_lineno": 581, "line_no": 585, "id": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "target_function_prompt": "    def _switch_narrow_to(self, parsed_link: ParsedNarrowLink) -> None:", "function_signature": "    def _switch_narrow_to(self, parsed_link: ParsedNarrowLink) -> None:"}}
{"prompt": "    def delete(self):", "metadata": {"task_id": "Internet/kinto/16", "ground_truth": "        self._raise_400_if_invalid_id(self.object_id)\n        obj = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(obj)\n\n        # Retreive the last_modified information from a querystring if present.\n        last_modified = self.request.validated[\"querystring\"].get(\"last_modified\")\n\n        # If less or equal than current object. Ignore it.\n        if last_modified and last_modified <= obj[self.model.modified_field]:\n            last_modified = None\n\n        try:\n            deleted = self.model.delete_object(obj, last_modified=last_modified)\n        except storage_exceptions.ObjectNotFoundError:\n            # Delete might fail if the object was deleted since we\n            # fetched it from the storage (ref Kinto/kinto#1407). This\n            # is one of a larger class of issues where another request\n            # could modify the object between our fetch and our\n            # delete, which could e.g. invalidate our precondition\n            # checking. Fixing this correctly is a larger\n            # problem. However, let's punt on fixing it correctly and\n            # just handle this one important case for now (see #1557).\n            #\n            # Raise a 404 vs. a 409 or 412 because that's what we\n            # would have done if the other thread's delete had\n            # happened a little earlier. (The client doesn't need to\n            # know that we did a bunch of work fetching the existing\n            # object for nothing.)\n            raise self._404_for_object(self.object_id)\n\n        timestamp = deleted[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(deleted, action=ACTIONS.DELETE, old=obj)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "resource", "__init__.py"], "context_start_lineno": 651, "line_no": 662, "id": "kinto.core.resource.Resource.delete", "target_function_prompt": "    def delete(self):", "function_signature": "    def delete(self):"}}
{"prompt": "    def load_default(accountant):", "metadata": {"task_id": "Security/diffprivlib/7", "ground_truth": "        if accountant is None:\n            if BudgetAccountant._default is None:\n                BudgetAccountant._default = BudgetAccountant()\n\n            return BudgetAccountant._default\n\n        if not isinstance(accountant, BudgetAccountant):\n            raise TypeError(f\"Accountant must be of type BudgetAccountant, got {type(accountant)}\")\n\n        return accountant", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "accountant.py"], "context_start_lineno": 417, "line_no": 435, "id": "diffprivlib.accountant.BudgetAccountant.load_default", "target_function_prompt": "    def load_default(accountant):", "function_signature": "    def load_default(accountant):"}}
{"prompt": "    def get_plugin(self, name):", "metadata": {"task_id": "Communications/hbmqtt/1", "ground_truth": "        for p in self._plugins:\n            if p.name == name:\n                return p\n        return None", "fpath_tuple": ["Communications", "hbmqtt", "hbmqtt", "plugins", "manager.py"], "context_start_lineno": 80, "line_no": 86, "id": "hbmqtt.plugins.manager.PluginManager.get_plugin", "target_function_prompt": "    def get_plugin(self, name):", "function_signature": "    def get_plugin(self, name):"}}
{"prompt": "def rarest_window_session(\n    session: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    value_cond_param_probs: Union[StateMatrix, dict],\n    modellable_params: set,\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str,\n    end_token: str,\n    use_geo_mean: bool = False,\n) -> Tuple[List[Cmd], float]:", "metadata": {"task_id": "Security/msticpy/6", "ground_truth": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n    if len(likelihoods) == 0:\n        return [], np.nan\n    min_lik = min(likelihoods)\n    ind = likelihoods.index(min_lik)\n    return session[ind : ind + window_len], min_lik  # noqa E203", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "cmds_params_values.py"], "context_start_lineno": 544, "line_no": 608, "id": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "target_function_prompt": "def rarest_window_session(\n    session: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    value_cond_param_probs: Union[StateMatrix, dict],\n    modellable_params: set,\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str,\n    end_token: str,\n    use_geo_mean: bool = False,\n) -> Tuple[List[Cmd], float]:", "function_signature": "def rarest_window_session(\n    session: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    value_cond_param_probs: Union[StateMatrix, dict],\n    modellable_params: set,\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str,\n    end_token: str,\n    use_geo_mean: bool = False,\n) -> Tuple[List[Cmd], float]:"}}
{"prompt": "def iterate_flattened(d):", "metadata": {"task_id": "Utilities/sacred/19", "ground_truth": "    for key in sorted(d.keys()):\n        value = d[key]\n        if isinstance(value, dict) and value:\n            for k, v in iterate_flattened(d[key]):\n                yield join_paths(key, k), v\n        else:\n            yield key, value", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 441, "line_no": 447, "id": "sacred.utils.iterate_flattened", "target_function_prompt": "def iterate_flattened(d):", "function_signature": "def iterate_flattened(d):"}}
{"prompt": "    def load(cls, dirname=None, locales=None, domain=DEFAULT_DOMAIN):", "metadata": {"task_id": "Internet/pyramid/28", "ground_truth": "        if locales is not None:\n            if not isinstance(locales, (list, tuple)):\n                locales = [locales]\n            locales = [str(locale) for locale in locales]\n        if not domain:\n            domain = cls.DEFAULT_DOMAIN\n        filename = gettext.find(domain, dirname, locales)\n        if not filename:\n            return gettext.NullTranslations()\n        with open(filename, 'rb') as fp:\n            return cls(fileobj=fp, domain=domain)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "i18n.py"], "context_start_lineno": 246, "line_no": 258, "id": "pyramid.i18n.Translations.load", "target_function_prompt": "    def load(cls, dirname=None, locales=None, domain=DEFAULT_DOMAIN):", "function_signature": "    def load(cls, dirname=None, locales=None, domain=DEFAULT_DOMAIN):"}}
{"prompt": "    def clone(self, __name__=_marker, __parent__=_marker, **kw):", "metadata": {"task_id": "Internet/pyramid/29", "ground_truth": "        oldkw = self.kw.copy()\n        oldkw.update(kw)\n        inst = self.__class__(self.__name__, self.__parent__, **oldkw)\n        inst.subs = copy.deepcopy(self.subs)\n        if __name__ is not _marker:\n            inst.__name__ = __name__\n        if __parent__ is not _marker:\n            inst.__parent__ = __parent__\n        return inst", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "testing.py"], "context_start_lineno": 214, "line_no": 221, "id": "pyramid.testing.DummyResource.clone", "target_function_prompt": "    def clone(self, __name__=_marker, __parent__=_marker, **kw):", "function_signature": "    def clone(self, __name__=_marker, __parent__=_marker, **kw):"}}
{"prompt": "    def _cat_file(self, path):", "metadata": {"task_id": "System/mrjob/34", "ground_truth": "        from mrjob.cat import decompress\n        m = _SSH_URI_RE.match(path)\n        addr = m.group('hostname')\n        fs_path = m.group('filesystem_path')\n\n        p = self._ssh_launch(addr, ['cat', fs_path])\n\n        for chunk in decompress(p.stdout, fs_path):\n            yield chunk\n\n        self._ssh_finish_run(p)", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "ssh.py"], "context_start_lineno": 189, "line_no": 190, "id": "mrjob.fs.ssh.SSHFilesystem._cat_file", "target_function_prompt": "    def _cat_file(self, path):", "function_signature": "    def _cat_file(self, path):"}}
{"prompt": "    def setacl(self, folder, who, what):", "metadata": {"task_id": "Communications/IMAPClient/5", "ground_truth": "        return self._command_and_check(\n            \"setacl\", self._normalise_folder(folder), who, what, unpack=True\n        )", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1552, "line_no": 1558, "id": "imapclient.imapclient.IMAPClient.setacl", "target_function_prompt": "    def setacl(self, folder, who, what):", "function_signature": "    def setacl(self, folder, who, what):"}}
{"prompt": "    def append(self, verb):", "metadata": {"task_id": "Communications/twilio-fatisar/10", "ground_truth": "        self.nest(verb)\n        return self", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "twiml", "__init__.py"], "context_start_lineno": 73, "line_no": 81, "id": "twilio.twiml.TwiML.append", "target_function_prompt": "    def append(self, verb):", "function_signature": "    def append(self, verb):"}}
{"prompt": "    def next_available_page(self) -> int:", "metadata": {"task_id": "Database/bplustree/7", "ground_truth": "        self.last_page += 1\n        return self.last_page", "fpath_tuple": ["Database", "bplustree", "bplustree", "memory.py"], "context_start_lineno": 200, "line_no": 201, "id": "bplustree.memory.FileMemory.next_available_page", "target_function_prompt": "    def next_available_page(self) -> int:", "function_signature": "    def next_available_page(self) -> int:"}}
{"prompt": "    def named_config(self, func):", "metadata": {"task_id": "Utilities/sacred/20", "ground_truth": "        config_scope = ConfigScope(func)\n        self._add_named_config(func.__name__, config_scope)\n        return config_scope", "fpath_tuple": ["Utilities", "sacred", "sacred", "ingredient.py"], "context_start_lineno": 164, "line_no": 170, "id": "sacred.ingredient.Ingredient.named_config", "target_function_prompt": "    def named_config(self, func):", "function_signature": "    def named_config(self, func):"}}
{"prompt": "def _union_lcs(evaluated_sentences, reference_sentence):", "metadata": {"task_id": "Internet/sumy/9", "ground_truth": "    if len(evaluated_sentences) <= 0:\n        raise (ValueError(\"Collections must contain at least 1 sentence.\"))\n\n    lcs_union = set()\n    reference_words = _split_into_words([reference_sentence])\n    combined_lcs_length = 0\n    for eval_s in evaluated_sentences:\n        evaluated_words = _split_into_words([eval_s])\n        lcs = set(_recon_lcs(reference_words, evaluated_words))\n        combined_lcs_length += len(lcs)\n        lcs_union = lcs_union.union(lcs)\n\n    union_lcs_count = len(lcs_union)\n    union_lcs_value = union_lcs_count / combined_lcs_length\n    return union_lcs_value", "fpath_tuple": ["Internet", "sumy", "sumy", "evaluation", "rouge.py"], "context_start_lineno": 219, "line_no": 236, "id": "sumy.evaluation.rouge._union_lcs", "target_function_prompt": "def _union_lcs(evaluated_sentences, reference_sentence):", "function_signature": "def _union_lcs(evaluated_sentences, reference_sentence):"}}
{"prompt": "        self.content_type = content_type", "metadata": {"task_id": "Database/datasette/17", "ground_truth": "\n    async def asgi_send(self, send):\n        headers = {}\n        headers.update(self.headers)\n        headers[\"content-type\"] = self.content_type\n        raw_headers = [\n            [key.encode(\"utf-8\"), value.encode(\"utf-8\")]\n            for key, value in headers.items()\n        ]\n        for set_cookie in self._set_cookie_headers:\n            raw_headers.append([b\"set-cookie\", set_cookie.encode(\"utf-8\")])\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": raw_headers,\n            }\n        )\n        body = self.body\n        if not isinstance(body, bytes):", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "asgi.py"], "context_start_lineno": 329, "line_no": 330, "id": "datasette.utils.asgi.Response.asgi_send", "target_function_prompt": "        self.content_type = content_type", "function_signature": "        self.content_type = content_type"}}
{"prompt": "    def create_snapshot(self, description=None, dry_run=False):", "metadata": {"task_id": "Internet/boto/30", "ground_truth": "        return self.connection.create_snapshot(\n            self.id,\n            description,\n            dry_run=dry_run\n        )", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "volume.py"], "context_start_lineno": 189, "line_no": 200, "id": "boto.ec2.volume.Volume.create_snapshot", "target_function_prompt": "    def create_snapshot(self, description=None, dry_run=False):", "function_signature": "    def create_snapshot(self, description=None, dry_run=False):"}}
{"prompt": "def get_directory_modules(directory):", "metadata": {"task_id": "Security/python-taint/1", "ground_truth": "    if _local_modules and os.path.dirname(_local_modules[0][1]) == directory:\n        return _local_modules\n\n    if not os.path.isdir(directory):\n        # example/import_test_project/A.py -> example/import_test_project\n        directory = os.path.dirname(directory)\n\n    if directory == '':\n        return _local_modules\n\n    for path in os.listdir(directory):\n        if _is_python_file(path):\n            # A.py -> A\n            module_name = os.path.splitext(path)[0]\n            _local_modules.append((module_name, os.path.join(directory, path)))\n\n    return _local_modules", "fpath_tuple": ["Security", "python-taint", "pyt", "core", "project_handler.py"], "context_start_lineno": 10, "line_no": 14, "id": "pyt.core.project_handler.get_directory_modules", "target_function_prompt": "def get_directory_modules(directory):", "function_signature": "def get_directory_modules(directory):"}}
{"prompt": "def get_default_providers_for_netcode(netcode=None):", "metadata": {"task_id": "Security/pycoin/18", "ground_truth": "    if netcode is None:\n        netcode = get_current_netcode()\n    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    if netcode not in THREAD_LOCALS.providers:\n        THREAD_LOCALS.providers[netcode] = providers_for_netcode_from_env(netcode)\n    return THREAD_LOCALS.providers[netcode]", "fpath_tuple": ["Security", "pycoin", "pycoin", "services", "providers.py"], "context_start_lineno": 134, "line_no": 135, "id": "pycoin.services.providers.get_default_providers_for_netcode", "target_function_prompt": "def get_default_providers_for_netcode(netcode=None):", "function_signature": "def get_default_providers_for_netcode(netcode=None):"}}
{"prompt": "    def run(self):", "metadata": {"task_id": "Scientific-Engineering/csvkit/2", "ground_truth": "        if 'f' not in self.override_flags:\n            self.input_file = self._open_input_file(self.args.input_path)\n\n        try:\n            with warnings.catch_warnings():\n                if getattr(self.args, 'no_header_row', None):\n                    warnings.filterwarnings(action='ignore', message='Column names not specified', module='agate')\n\n                self.main()\n        finally:\n            if 'f' not in self.override_flags:\n                self.input_file.close()", "fpath_tuple": ["Scientific-Engineering", "csvkit", "csvkit", "cli.py"], "context_start_lineno": 105, "line_no": 110, "id": "csvkit.cli.CSVKitUtility.run", "target_function_prompt": "    def run(self):", "function_signature": "    def run(self):"}}
{"prompt": "    def unread(self, data):", "metadata": {"task_id": "Utilities/gunicorn/11", "ground_truth": "        self.buf.seek(0, os.SEEK_END)\n        self.buf.write(data)", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "http", "unreader.py"], "context_start_lineno": 51, "line_no": 52, "id": "gunicorn.http.unreader.Unreader.unread", "target_function_prompt": "    def unread(self, data):", "function_signature": "    def unread(self, data):"}}
{"prompt": "    def _proc_folder_list(self, folder_data):\n        # Filter out empty strings and None's.\n        # This also deals with the special case of - no 'untagged'\n        # responses (ie, no folders). This comes back as [None].", "metadata": {"task_id": "Communications/IMAPClient/6", "ground_truth": "        from .util import chunk\n        folder_data = [item for item in folder_data if item not in (b\"\", None)]\n\n        ret = []\n        parsed = parse_response(folder_data)\n        for flags, delim, name in chunk(parsed, size=3):\n            if isinstance(name, int):\n                # Some IMAP implementations return integer folder names\n                # with quotes. These get parsed to ints so convert them\n                # back to strings.\n                name = str(name)\n            elif self.folder_encode:\n                name = decode_utf7(name)\n\n            ret.append((flags, delim, name))\n        return ret", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 746, "line_no": 750, "id": "imapclient.imapclient.IMAPClient._proc_folder_list", "target_function_prompt": "    def _proc_folder_list(self, folder_data):\n        # Filter out empty strings and None's.\n        # This also deals with the special case of - no 'untagged'\n        # responses (ie, no folders). This comes back as [None].", "function_signature": "    def _proc_folder_list(self, folder_data):\n        # Filter out empty strings and None's.\n        # This also deals with the special case of - no 'untagged'\n        # responses (ie, no folders). This comes back as [None]."}}
{"prompt": "    def _process_finished_callbacks(self):", "metadata": {"task_id": "Internet/pyramid/30", "ground_truth": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "request.py"], "context_start_lineno": 130, "line_no": 131, "id": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "target_function_prompt": "    def _process_finished_callbacks(self):", "function_signature": "    def _process_finished_callbacks(self):"}}
{"prompt": "        await send({\"type\": \"http.response.body\", \"body\": body})\n\n    def set_cookie(\n        self,\n        key,\n        value=\"\",\n        max_age=None,\n        expires=None,\n        path=\"/\",\n        domain=None,\n        secure=False,\n        httponly=False,", "metadata": {"task_id": "Database/datasette/18", "ground_truth": "        samesite=\"lax\",\n    ):\n        assert samesite in SAMESITE_VALUES, \"samesite should be one of {}\".format(\n            SAMESITE_VALUES\n        )\n        cookie = SimpleCookie()\n        cookie[key] = value\n        for prop_name, prop_value in (\n            (\"max_age\", max_age),\n            (\"expires\", expires),\n            (\"path\", path),\n            (\"domain\", domain),\n            (\"samesite\", samesite),\n        ):\n            if prop_value is not None:\n                cookie[key][prop_name.replace(\"_\", \"-\")] = prop_value\n        for prop_name, prop_value in ((\"secure\", secure), (\"httponly\", httponly)):\n            if prop_value:", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "asgi.py"], "context_start_lineno": 351, "line_no": 363, "id": "datasette.utils.asgi.Response.set_cookie", "target_function_prompt": "        await send({\"type\": \"http.response.body\", \"body\": body})\n\n    def set_cookie(\n        self,\n        key,\n        value=\"\",\n        max_age=None,\n        expires=None,\n        path=\"/\",\n        domain=None,\n        secure=False,\n        httponly=False,", "function_signature": "        await send({\"type\": \"http.response.body\", \"body\": body})\n\n    def set_cookie(\n        self,\n        key,\n        value=\"\",\n        max_age=None,\n        expires=None,\n        path=\"/\",\n        domain=None,\n        secure=False,\n        httponly=False,"}}
{"prompt": "def _tokenize_file_content_without_snipping(content: bytes) -> List[_PrettyToken]:", "metadata": {"task_id": "Text-Processing/online-judge-tools/1", "ground_truth": "    tokens, text = _decode_with_recovery(content)\n    for line in text.splitlines(keepends=True):\n        tokens += _tokenize_line(line)\n    tokens = _warn_if_empty(tokens)\n    return tokens", "fpath_tuple": ["Text-Processing", "online-judge-tools", "onlinejudge_command", "pretty_printers.py"], "context_start_lineno": 220, "line_no": 221, "id": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "target_function_prompt": "def _tokenize_file_content_without_snipping(content: bytes) -> List[_PrettyToken]:", "function_signature": "def _tokenize_file_content_without_snipping(content: bytes) -> List[_PrettyToken]:"}}
{"prompt": "    def unauthenticated_userid(self, request):", "metadata": {"task_id": "Internet/pyramid/31", "ground_truth": "        result = self.cookie.identify(request)\n        if result:\n            return result['userid']", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "authentication.py"], "context_start_lineno": 632, "line_no": 634, "id": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "target_function_prompt": "    def unauthenticated_userid(self, request):", "function_signature": "    def unauthenticated_userid(self, request):"}}
{"prompt": "    def exists(self, path_glob):", "metadata": {"task_id": "System/mrjob/35", "ground_truth": "        path_glob = _from_file_uri(path_glob)\n        return bool(glob.glob(path_glob))", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "local.py"], "context_start_lineno": 58, "line_no": 59, "id": "mrjob.fs.local.LocalFilesystem.exists", "target_function_prompt": "    def exists(self, path_glob):", "function_signature": "    def exists(self, path_glob):"}}
{"prompt": "    def result_to_console_output(cls, result: \"CertificateInfoScanResult\") -> List[str]:", "metadata": {"task_id": "System/sslyze/3", "ground_truth": "        result_as_txt = [cls._format_title(\"Certificates Information\")]\n\n        # SNI\n        server_name_indication = result.hostname_used_for_server_name_indication\n        result_as_txt.append(cls._format_field(\"Hostname sent for SNI:\", server_name_indication))\n\n        # Display each certificate deployment\n        result_as_txt.append(\n            cls._format_field(\"Number of certificates detected:\", str(len(result.certificate_deployments)))\n        )\n        for index, cert_deployment in enumerate(result.certificate_deployments):\n            result_as_txt.append(\"\\n\")\n            result_as_txt.extend(cls._cert_deployment_to_console_output(index, cert_deployment))\n\n        return result_as_txt", "fpath_tuple": ["System", "sslyze", "sslyze", "plugins", "certificate_info", "_cli_connector.py"], "context_start_lineno": 68, "line_no": 69, "id": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "target_function_prompt": "    def result_to_console_output(cls, result: \"CertificateInfoScanResult\") -> List[str]:", "function_signature": "    def result_to_console_output(cls, result: \"CertificateInfoScanResult\") -> List[str]:"}}
{"prompt": "    def index(self, val):", "metadata": {"task_id": "Utilities/boltons/31", "ground_truth": "        try:\n            return self._get_apparent_index(self.item_index_map[val])\n        except KeyError:\n            cn = self.__class__.__name__\n            raise ValueError('%r is not in %s' % (val, cn))", "fpath_tuple": ["Utilities", "boltons", "boltons", "setutils.py"], "context_start_lineno": 465, "line_no": 467, "id": "boltons.setutils.IndexedSet.index", "target_function_prompt": "    def index(self, val):", "function_signature": "    def index(self, val):"}}
{"prompt": "def calculate_luhn(partial_number: float) -> int:", "metadata": {"task_id": "Software-Development/Faker/14", "ground_truth": "    check_digit = luhn_checksum(int(partial_number) * 10)\n    return check_digit if check_digit == 0 else 10 - check_digit", "fpath_tuple": ["Software-Development", "Faker", "faker", "utils", "checksums.py"], "context_start_lineno": 17, "line_no": 21, "id": "faker.utils.checksums.calculate_luhn", "target_function_prompt": "def calculate_luhn(partial_number: float) -> int:", "function_signature": "def calculate_luhn(partial_number: float) -> int:"}}
{"prompt": "def compare_tokens(options, db):", "metadata": {"task_id": "Security/capirca/4", "ground_truth": "  t1, t2 = options.cmp\n  d1 = db.GetNet(t1)\n  d2 = db.GetNet(t2)\n  union = list(set(d1 + d2))\n  meta = (t1, t2, union)\n  results = []\n  for el in set(d1 + d2):\n    el = nacaddr.IP(el)\n    if el in d1 and el in d2:\n      results.append(str(el))\n    elif el in d1:\n      results.append(str(el))\n    elif el in d2:\n      results.append(str(el))\n  return meta, results", "fpath_tuple": ["Security", "capirca", "tools", "cgrep.py"], "context_start_lineno": 396, "line_no": 408, "id": "tools.cgrep.compare_tokens", "target_function_prompt": "def compare_tokens(options, db):", "function_signature": "def compare_tokens(options, db):"}}
{"prompt": "def app_strip_relative_path(requests_pathname, path):", "metadata": {"task_id": "Software-Development/dash/5", "ground_truth": "    if path is None:\n        return None\n    if (\n        requests_pathname != \"/\" and not path.startswith(requests_pathname.rstrip(\"/\"))\n    ) or (requests_pathname == \"/\" and not path.startswith(\"/\")):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            Paths that aren't prefixed with requests_pathname_prefix are not supported.\n            You supplied: {path} and requests_pathname_prefix was {requests_pathname}\n            \"\"\"\n        )\n    if requests_pathname != \"/\" and path.startswith(requests_pathname.rstrip(\"/\")):\n        path = path.replace(\n            # handle the case where the path might be `/my-dash-app`\n            # but the requests_pathname_prefix is `/my-dash-app/`\n            requests_pathname.rstrip(\"/\"),\n            \"\",\n            1,\n        )\n    return path.strip(\"/\")", "fpath_tuple": ["Software-Development", "dash", "dash", "_get_paths.py"], "context_start_lineno": 130, "line_no": 131, "id": "dash._get_paths.app_strip_relative_path", "target_function_prompt": "def app_strip_relative_path(requests_pathname, path):", "function_signature": "def app_strip_relative_path(requests_pathname, path):"}}
{"prompt": "    def principals_allowed_by_permission(self, context, permission):", "metadata": {"task_id": "Internet/pyramid/32", "ground_truth": "        return self.helper.principals_allowed_by_permission(\n            context, permission\n        )", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "authorization.py"], "context_start_lineno": 76, "line_no": 81, "id": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "target_function_prompt": "    def principals_allowed_by_permission(self, context, permission):", "function_signature": "    def principals_allowed_by_permission(self, context, permission):"}}
{"prompt": "    def darwin_pkg_config_location(self):", "metadata": {"task_id": "Utilities/python-for-android/11", "ground_truth": "        return os.path.join(\n            self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name),\n            \"lib/pkgconfig\",\n        )", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 276, "line_no": 277, "id": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "target_function_prompt": "    def darwin_pkg_config_location(self):", "function_signature": "    def darwin_pkg_config_location(self):"}}
{"prompt": "def _compare_server_default(\n    autogen_context: AutogenContext,\n    alter_column_op: AlterColumnOp,\n    schema: Optional[str],\n    tname: Union[quoted_name, str],\n    cname: Union[quoted_name, str],\n    conn_col: Column[Any],\n    metadata_col: Column[Any],\n) -> Optional[bool]:", "metadata": {"task_id": "Database/alembic/8", "ground_truth": "    metadata_default = metadata_col.server_default\n    conn_col_default = conn_col.server_default\n    if conn_col_default is None and metadata_default is None:\n        return False\n\n    if sqla_compat._server_default_is_computed(metadata_default):\n        # return False in case of a computed column as the server\n        # default. Note that DDL for adding or removing \"GENERATED AS\" from\n        # an existing column is not currently known for any backend.\n        # Once SQLAlchemy can reflect \"GENERATED\" as the \"computed\" element,\n        # we would also want to ignore and/or warn for changes vs. the\n        # metadata (or support backend specific DDL if applicable).\n        if not sqla_compat.has_computed_reflection:\n            return False\n\n        else:\n            return (\n                _compare_computed_default(  # type:ignore[func-returns-value]\n                    autogen_context,\n                    alter_column_op,\n                    schema,\n                    tname,\n                    cname,\n                    conn_col,\n                    metadata_col,\n                )\n            )\n    if sqla_compat._server_default_is_computed(conn_col_default):\n        _warn_computed_not_supported(tname, cname)\n        return False\n\n    if sqla_compat._server_default_is_identity(\n        metadata_default, conn_col_default\n    ):\n        alter_column_op.existing_server_default = conn_col_default\n        diff, is_alter = _compare_identity_default(\n            autogen_context,\n            alter_column_op,\n            schema,\n            tname,\n            cname,\n            conn_col,\n            metadata_col,\n        )\n        if is_alter:\n            alter_column_op.modify_server_default = metadata_default\n            if diff:\n                log.info(\n                    \"Detected server default on column '%s.%s': \"\n                    \"identity options attributes %s\",\n                    tname,\n                    cname,\n                    sorted(diff),\n                )\n    else:\n        rendered_metadata_default = _render_server_default_for_compare(\n            metadata_default, autogen_context\n        )\n\n        rendered_conn_default = (\n            cast(Any, conn_col_default).arg.text if conn_col_default else None\n        )\n\n        alter_column_op.existing_server_default = conn_col_default\n\n        is_diff = autogen_context.migration_context._compare_server_default(\n            conn_col,\n            metadata_col,\n            rendered_metadata_default,\n            rendered_conn_default,\n        )\n        if is_diff:\n            alter_column_op.modify_server_default = metadata_default\n            log.info(\"Detected server default on column '%s.%s'\", tname, cname)\n\n    return None", "fpath_tuple": ["Database", "alembic", "alembic", "autogenerate", "compare.py"], "context_start_lineno": 1124, "line_no": 1133, "id": "alembic.autogenerate.compare._compare_server_default", "target_function_prompt": "def _compare_server_default(\n    autogen_context: AutogenContext,\n    alter_column_op: AlterColumnOp,\n    schema: Optional[str],\n    tname: Union[quoted_name, str],\n    cname: Union[quoted_name, str],\n    conn_col: Column[Any],\n    metadata_col: Column[Any],\n) -> Optional[bool]:", "function_signature": "def _compare_server_default(\n    autogen_context: AutogenContext,\n    alter_column_op: AlterColumnOp,\n    schema: Optional[str],\n    tname: Union[quoted_name, str],\n    cname: Union[quoted_name, str],\n    conn_col: Column[Any],\n    metadata_col: Column[Any],\n) -> Optional[bool]:"}}
{"prompt": "def makepatch(original, modified):", "metadata": {"task_id": "Internet/google-api-python-client/3", "ground_truth": "    patch = {}\n    for key, original_value in original.items():\n        modified_value = modified.get(key, None)\n        if modified_value is None:\n            # Use None to signal that the element is deleted\n            patch[key] = None\n        elif original_value != modified_value:\n            if type(original_value) == type({}):\n                # Recursively descend objects\n                patch[key] = makepatch(original_value, modified_value)\n            else:\n                # In the case of simple types or arrays we just replace\n                patch[key] = modified_value\n        else:\n            # Don't add anything to patch if there's no change\n            pass\n    for key in modified:\n        if key not in original:\n            patch[key] = modified[key]\n\n    return patch", "fpath_tuple": ["Internet", "google-api-python-client", "googleapiclient", "model.py"], "context_start_lineno": 368, "line_no": 389, "id": "googleapiclient.model.makepatch", "target_function_prompt": "def makepatch(original, modified):", "function_signature": "def makepatch(original, modified):"}}
{"prompt": "    def batch_get(self, keys, consistent=False, attributes=None):", "metadata": {"task_id": "Internet/boto/31", "ground_truth": "        from boto.dynamodb2.results import BatchGetResultSet\n        results = BatchGetResultSet(keys=keys, max_batch_get=self.max_batch_get)\n        results.to_call(self._batch_get, consistent=consistent, attributes=attributes)\n        return results", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 1503, "line_no": 1543, "id": "boto.dynamodb2.table.Table.batch_get", "target_function_prompt": "    def batch_get(self, keys, consistent=False, attributes=None):", "function_signature": "    def batch_get(self, keys, consistent=False, attributes=None):"}}
{"prompt": "    def current_literal(self) -> Optional[bytes]:", "metadata": {"task_id": "Communications/IMAPClient/7", "ground_truth": "        if TYPE_CHECKING:\n            assert self.lex.current_source is not None\n        return self.lex.current_source.literal", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "response_lexer.py"], "context_start_lineno": 40, "line_no": 41, "id": "imapclient.response_lexer.TokenSource.current_literal", "target_function_prompt": "    def current_literal(self) -> Optional[bytes]:", "function_signature": "    def current_literal(self) -> Optional[bytes]:"}}
{"prompt": "def percentile(array, percent, epsilon=1.0, bounds=None, axis=None, keepdims=False, random_state=None, accountant=None,\n               **unused_args):", "metadata": {"task_id": "Security/diffprivlib/8", "ground_truth": "    warn_unused_args(unused_args)\n\n    quant = np.asarray(percent) / 100\n\n    if np.any(quant < 0) or np.any(quant > 1):\n        raise ValueError(\"Percentiles must be between 0 and 100 inclusive\")\n\n    return quantile(array, quant, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims,\n                    random_state=random_state, accountant=accountant)", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "tools", "quantiles.py"], "context_start_lineno": 153, "line_no": 210, "id": "diffprivlib.tools.quantiles.percentile", "target_function_prompt": "def percentile(array, percent, epsilon=1.0, bounds=None, axis=None, keepdims=False, random_state=None, accountant=None,\n               **unused_args):", "function_signature": "def percentile(array, percent, epsilon=1.0, bounds=None, axis=None, keepdims=False, random_state=None, accountant=None,\n               **unused_args):"}}
{"prompt": "    def increment_counter(self, group, counter, amount=1):", "metadata": {"task_id": "System/mrjob/36", "ground_truth": "        from mrjob.py2 import integer_types\n        if not isinstance(amount, integer_types):\n            raise TypeError('amount must be an integer, not %r' % (amount,))\n\n        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(group, string_types):\n            group = str(group)\n        if not isinstance(counter, string_types):\n            counter = str(counter)\n\n        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with the next best thing: semicolons!\n        #\n        # The relevant Hadoop code is incrCounter(), here:\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\n        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 547, "line_no": 561, "id": "mrjob.job.MRJob.increment_counter", "target_function_prompt": "    def increment_counter(self, group, counter, amount=1):", "function_signature": "    def increment_counter(self, group, counter, amount=1):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/32", "ground_truth": "    from boto.regioninfo import connect\n    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    return connect('cognito-sync', region_name,\n                   connection_cls=CognitoSyncConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "cognito", "sync", "__init__.py"], "context_start_lineno": 37, "line_no": 38, "id": "boto.cognito.sync.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "def parse(lines):", "metadata": {"task_id": "System/fs/13", "ground_truth": "    info = []\n    for line in lines:\n        if not line.strip():\n            continue\n        raw_info = parse_line(line)\n        if raw_info is not None:\n            info.append(raw_info)\n    return info", "fpath_tuple": ["System", "fs", "fs", "_ftp_parse.py"], "context_start_lineno": 66, "line_no": 67, "id": "fs._ftp_parse.parse", "target_function_prompt": "def parse(lines):", "function_signature": "def parse(lines):"}}
{"prompt": "    def to_constraint(self) -> Constraint:", "metadata": {"task_id": "Database/alembic/9", "ground_truth": "        if self._reverse is not None:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint_table = sqla_compat._table_for_constraint(constraint)\n            constraint_table.name = self.table_name\n            constraint_table.schema = self.schema\n\n            return constraint\n        else:\n            raise ValueError(\n                \"constraint cannot be produced; \"\n                \"original constraint is not present\"\n            )", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 175, "line_no": 176, "id": "alembic.operations.ops.DropConstraintOp.to_constraint", "target_function_prompt": "    def to_constraint(self) -> Constraint:", "function_signature": "    def to_constraint(self) -> Constraint:"}}
{"prompt": "def parse(url, unquote_password=False):", "metadata": {"task_id": "Software-Development/peewee/3", "ground_truth": "    parsed = urlparse(url)\n    return parseresult_to_dict(parsed, unquote_password)", "fpath_tuple": ["Software-Development", "peewee", "playhouse", "db_url.py"], "context_start_lineno": 86, "line_no": 87, "id": "playhouse.db_url.parse", "target_function_prompt": "def parse(url, unquote_password=False):", "function_signature": "def parse(url, unquote_password=False):"}}
{"prompt": "def pages_folder_config(name, pages_folder, use_pages):", "metadata": {"task_id": "Software-Development/dash/6", "ground_truth": "    if not pages_folder:\n        return None\n    is_custom_folder = str(pages_folder) != \"pages\"\n    pages_folder_path = os.path.join(flask.helpers.get_root_path(name), pages_folder)\n    if (use_pages or is_custom_folder) and not os.path.isdir(pages_folder_path):\n        error_msg = f\"\"\"\n        A folder called `{pages_folder}` does not exist. If a folder for pages is not\n        required in your application, set `pages_folder=\"\"`. For example:\n        `app = Dash(__name__,  pages_folder=\"\")`\n        \"\"\"\n        raise exceptions.InvalidConfig(error_msg)\n    return pages_folder_path", "fpath_tuple": ["Software-Development", "dash", "dash", "_configs.py"], "context_start_lineno": 124, "line_no": 125, "id": "dash._configs.pages_folder_config", "target_function_prompt": "def pages_folder_config(name, pages_folder, use_pages):", "function_signature": "def pages_folder_config(name, pages_folder, use_pages):"}}
{"prompt": "def format_criteria_date(dt: datetime) -> bytes:", "metadata": {"task_id": "Communications/IMAPClient/8", "ground_truth": "    out = \"%02d-%s-%d\" % (dt.day, _SHORT_MONTHS[dt.month], dt.year)\n    return out.encode(\"ascii\")", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "datetime_util.py"], "context_start_lineno": 66, "line_no": 68, "id": "imapclient.datetime_util.format_criteria_date", "target_function_prompt": "def format_criteria_date(dt: datetime) -> bytes:", "function_signature": "def format_criteria_date(dt: datetime) -> bytes:"}}
{"prompt": "def fixed2csv(f, schema, output=None, skip_lines=0, **kwargs):", "metadata": {"task_id": "Scientific-Engineering/csvkit/3", "ground_truth": "    streaming = bool(output)\n\n    if not streaming:\n        output = StringIO()\n\n    try:\n        encoding = kwargs['encoding']\n    except KeyError:\n        encoding = None\n\n    if isinstance(skip_lines, int):\n        while skip_lines > 0:\n            f.readline()\n            skip_lines -= 1\n    else:\n        raise ValueError('skip_lines argument must be an int')\n\n    writer = agate.csv.writer(output)\n\n    reader = FixedWidthReader(f, schema, encoding=encoding)\n    writer.writerows(reader)\n\n    if not streaming:\n        data = output.getvalue()\n        output.close()\n\n        return data\n\n    # Return empty string when streaming\n    return ''", "fpath_tuple": ["Scientific-Engineering", "csvkit", "csvkit", "convert", "fixed.py"], "context_start_lineno": 9, "line_no": 29, "id": "csvkit.convert.fixed.fixed2csv", "target_function_prompt": "def fixed2csv(f, schema, output=None, skip_lines=0, **kwargs):", "function_signature": "def fixed2csv(f, schema, output=None, skip_lines=0, **kwargs):"}}
{"prompt": "    def convert(self, value):", "metadata": {"task_id": "Internet/falcon/19", "ground_truth": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None", "fpath_tuple": ["Internet", "falcon", "falcon", "routing", "converters.py"], "context_start_lineno": 110, "line_no": 111, "id": "falcon.routing.converters.DateTimeConverter.convert", "target_function_prompt": "    def convert(self, value):", "function_signature": "    def convert(self, value):"}}
{"prompt": "def multi_replace(text, sub_map, **kwargs):", "metadata": {"task_id": "Utilities/boltons/32", "ground_truth": "    m = MultiReplace(sub_map, **kwargs)\n    return m.sub(text)", "fpath_tuple": ["Utilities", "boltons", "boltons", "strutils.py"], "context_start_lineno": 1265, "line_no": 1278, "id": "boltons.strutils.multi_replace", "target_function_prompt": "def multi_replace(text, sub_map, **kwargs):", "function_signature": "def multi_replace(text, sub_map, **kwargs):"}}
{"prompt": "def create_markdown(escape: bool=True, hard_wrap: bool=False, renderer='html', plugins=None) -> Markdown:", "metadata": {"task_id": "Text-Processing/mistune/3", "ground_truth": "    if renderer == 'ast':\n        # explicit and more similar to 2.x's API\n        renderer = None\n    elif renderer == 'html':\n        renderer = HTMLRenderer(escape=escape)\n\n    inline = InlineParser(hard_wrap=hard_wrap)\n    if plugins is not None:\n        plugins = [import_plugin(n) for n in plugins]\n    return Markdown(renderer=renderer, inline=inline, plugins=plugins)", "fpath_tuple": ["Text-Processing", "mistune", "src", "mistune", "__init__.py"], "context_start_lineno": 19, "line_no": 36, "id": "mistune.create_markdown", "target_function_prompt": "def create_markdown(escape: bool=True, hard_wrap: bool=False, renderer='html', plugins=None) -> Markdown:", "function_signature": "def create_markdown(escape: bool=True, hard_wrap: bool=False, renderer='html', plugins=None) -> Markdown:"}}
{"prompt": "def _matches_after_expansion(string_to_check: str, string_to_check_against: str,\n                             condition_keys: Optional[CaseInsensitiveDict] = None) -> bool:", "metadata": {"task_id": "Security/principalmapper/0", "ground_truth": "    copy_string = string_to_check_against\n\n    if condition_keys is not None:\n        for k, v in condition_keys.items():\n            if isinstance(v, list):\n                v = str(v)  # TODO: how would a multi-valued context value be handled in resource fields?\n            full_key = '${' + k + '}'\n            copy_string = copy_string.replace(full_key, v)\n\n    pattern = _compose_pattern(copy_string)\n    return pattern.match(string_to_check) is not None", "fpath_tuple": ["Security", "principalmapper", "principalmapper", "querying", "local_policy_simulation.py"], "context_start_lineno": 907, "line_no": 916, "id": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "target_function_prompt": "def _matches_after_expansion(string_to_check: str, string_to_check_against: str,\n                             condition_keys: Optional[CaseInsensitiveDict] = None) -> bool:", "function_signature": "def _matches_after_expansion(string_to_check: str, string_to_check_against: str,\n                             condition_keys: Optional[CaseInsensitiveDict] = None) -> bool:"}}
{"prompt": "def uses_yarn(version):", "metadata": {"task_id": "System/mrjob/37", "ground_truth": "    return (version_gte(version, '2') or\n            version_gte(version, '0.23') and not version_gte(version, '1'))", "fpath_tuple": ["System", "mrjob", "mrjob", "compat.py"], "context_start_lineno": 717, "line_no": 720, "id": "mrjob.compat.uses_yarn", "target_function_prompt": "def uses_yarn(version):", "function_signature": "def uses_yarn(version):"}}
{"prompt": "    def add_database(self, db, name=None, route=None):", "metadata": {"task_id": "Database/datasette/19", "ground_truth": "        new_databases = self.databases.copy()\n        if name is None:\n            # Pick a unique name for this database\n            suggestion = db.suggest_name()\n            name = suggestion\n        else:\n            suggestion = name\n        i = 2\n        while name in self.databases:\n            name = \"{}_{}\".format(suggestion, i)\n            i += 1\n        db.name = name\n        db.route = route or name\n        new_databases[name] = db\n        # don't mutate! that causes race conditions with live import\n        self.databases = new_databases\n        return db", "fpath_tuple": ["Database", "datasette", "datasette", "app.py"], "context_start_lineno": 414, "line_no": 415, "id": "datasette.app.Datasette.add_database", "target_function_prompt": "    def add_database(self, db, name=None, route=None):", "function_signature": "    def add_database(self, db, name=None, route=None):"}}
{"prompt": "def expand_mixed(df: pd.DataFrame, types: Any = None) -> pd.DataFrame:", "metadata": {"task_id": "Software-Development/pandas-profiling/0", "ground_truth": "    if types is None:\n        types = [list, dict, tuple]\n\n    for column_name in df.columns:\n        # All\n        non_nested_enumeration = (\n            df[column_name]\n            .dropna()\n            .map(lambda x: type(x) in types and not any(type(y) in types for y in x))\n        )\n\n        if non_nested_enumeration.all():\n            # Expand and prefix\n            expanded = pd.DataFrame(df[column_name].dropna().tolist())\n            expanded = expanded.add_prefix(column_name + \"_\")\n\n            # Add recursion\n            expanded = expand_mixed(expanded)\n\n            # Drop the expanded\n            df.drop(columns=[column_name], inplace=True)\n\n            df = pd.concat([df, expanded], axis=1)\n    return df", "fpath_tuple": ["Software-Development", "pandas-profiling", "src", "ydata_profiling", "utils", "dataframe.py"], "context_start_lineno": 143, "line_no": 157, "id": "ydata_profiling.utils.dataframe.expand_mixed", "target_function_prompt": "def expand_mixed(df: pd.DataFrame, types: Any = None) -> pd.DataFrame:", "function_signature": "def expand_mixed(df: pd.DataFrame, types: Any = None) -> pd.DataFrame:"}}
{"prompt": "    def to_internal_value(self, data):\n        # We're lenient with allowing basic numerics to be coerced into strings,\n        # but other types should fail. Eg. unclear if booleans should represent as `true` or `True`,\n        # and composites such as lists are likely user error.", "metadata": {"task_id": "Internet/djangorestframework/8", "ground_truth": "        if isinstance(data, bool) or not isinstance(data, (str, int, float,)):\n            self.fail('invalid')\n        value = str(data)\n        return value.strip() if self.trim_whitespace else value", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "fields.py"], "context_start_lineno": 752, "line_no": 756, "id": "rest_framework.fields.CharField.to_internal_value", "target_function_prompt": "    def to_internal_value(self, data):\n        # We're lenient with allowing basic numerics to be coerced into strings,\n        # but other types should fail. Eg. unclear if booleans should represent as `true` or `True`,\n        # and composites such as lists are likely user error.", "function_signature": "    def to_internal_value(self, data):\n        # We're lenient with allowing basic numerics to be coerced into strings,\n        # but other types should fail. Eg. unclear if booleans should represent as `true` or `True`,\n        # and composites such as lists are likely user error."}}
{"prompt": "    def get_class(self, import_module: bool = True) -> t.Type[T]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/13", "ground_truth": "        if self._runtime_class is None:\n            try:\n                m = sys.modules[self.module]\n            except KeyError:\n                if import_module:\n                    import importlib\n\n                    m = importlib.import_module(self.module)\n                else:\n                    raise ValueError(f\"Module {self.module} not imported\")\n\n            self._runtime_class = t.cast(\"t.Type[T]\", getattr(m, self.qualname))\n\n        return self._runtime_class", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "types.py"], "context_start_lineno": 183, "line_no": 184, "id": "bentoml._internal.types.LazyType.get_class", "target_function_prompt": "    def get_class(self, import_module: bool = True) -> t.Type[T]:", "function_signature": "    def get_class(self, import_module: bool = True) -> t.Type[T]:"}}
{"prompt": "def compute_hashes_from_fileobj(fileobj, chunk_size=1024 * 1024):", "metadata": {"task_id": "Internet/boto/33", "ground_truth": "    if six.PY3 and hasattr(fileobj, 'mode') and 'b' not in fileobj.mode:\n        raise ValueError('File-like object must be opened in binary mode!')\n\n    linear_hash = hashlib.sha256()\n    chunks = []\n    chunk = fileobj.read(chunk_size)\n    while chunk:\n        # It's possible to get a file-like object that has no mode (checked\n        # above) and returns something other than bytes (e.g. str). So here\n        # we try to catch that and encode to bytes.\n        if not isinstance(chunk, bytes):\n            chunk = chunk.encode(getattr(fileobj, 'encoding', '') or 'utf-8')\n        linear_hash.update(chunk)\n        chunks.append(hashlib.sha256(chunk).digest())\n        chunk = fileobj.read(chunk_size)\n    if not chunks:\n        chunks = [hashlib.sha256(b'').digest()]\n    return linear_hash.hexdigest(), bytes_to_hex(tree_hash(chunks))", "fpath_tuple": ["Internet", "boto", "boto", "glacier", "utils.py"], "context_start_lineno": 109, "line_no": 127, "id": "boto.glacier.utils.compute_hashes_from_fileobj", "target_function_prompt": "def compute_hashes_from_fileobj(fileobj, chunk_size=1024 * 1024):", "function_signature": "def compute_hashes_from_fileobj(fileobj, chunk_size=1024 * 1024):"}}
{"prompt": "    async def render_template(\n        self, templates, context=None, request=None, view_name=None\n    ):", "metadata": {"task_id": "Database/datasette/20", "ground_truth": "        from .utils import display_actor\n        from .utils import format_bytes\n        if not self._startup_invoked:\n            raise Exception(\"render_template() called before await ds.invoke_startup()\")\n        context = context or {}\n        if isinstance(templates, Template):\n            template = templates\n        else:\n            if isinstance(templates, str):\n                templates = [templates]\n            template = self.jinja_env.select_template(templates)\n        body_scripts = []\n        # pylint: disable=no-member\n        for extra_script in pm.hook.extra_body_script(\n            template=template.name,\n            database=context.get(\"database\"),\n            table=context.get(\"table\"),\n            columns=context.get(\"columns\"),\n            view_name=view_name,\n            request=request,\n            datasette=self,\n        ):\n            extra_script = await await_me_maybe(extra_script)\n            if isinstance(extra_script, dict):\n                script = extra_script[\"script\"]\n                module = bool(extra_script.get(\"module\"))\n            else:\n                script = extra_script\n                module = False\n            body_scripts.append({\"script\": Markup(script), \"module\": module})\n\n        extra_template_vars = {}\n        # pylint: disable=no-member\n        for extra_vars in pm.hook.extra_template_vars(\n            template=template.name,\n            database=context.get(\"database\"),\n            table=context.get(\"table\"),\n            columns=context.get(\"columns\"),\n            view_name=view_name,\n            request=request,\n            datasette=self,\n        ):\n            extra_vars = await await_me_maybe(extra_vars)\n            assert isinstance(extra_vars, dict), \"extra_vars is of type {}\".format(\n                type(extra_vars)\n            )\n            extra_template_vars.update(extra_vars)\n\n        async def menu_links():\n            links = []\n            for hook in pm.hook.menu_links(\n                datasette=self,\n                actor=request.actor if request else None,\n                request=request or None,\n            ):\n                extra_links = await await_me_maybe(hook)\n                if extra_links:\n                    links.extend(extra_links)\n            return links\n\n        template_context = {\n            **context,\n            **{\n                \"request\": request,\n                \"crumb_items\": self._crumb_items,\n                \"urls\": self.urls,\n                \"actor\": request.actor if request else None,\n                \"menu_links\": menu_links,\n                \"display_actor\": display_actor,\n                \"show_logout\": request is not None\n                and \"ds_actor\" in request.cookies\n                and request.actor,\n                \"app_css_hash\": self.app_css_hash(),\n                \"zip\": zip,\n                \"body_scripts\": body_scripts,\n                \"format_bytes\": format_bytes,\n                \"show_messages\": lambda: self._show_messages(request),\n                \"extra_css_urls\": await self._asset_urls(\n                    \"extra_css_urls\", template, context, request, view_name\n                ),\n                \"extra_js_urls\": await self._asset_urls(\n                    \"extra_js_urls\", template, context, request, view_name\n                ),\n                \"base_url\": self.setting(\"base_url\"),\n                \"csrftoken\": request.scope[\"csrftoken\"] if request else lambda: \"\",\n            },\n            **extra_template_vars,\n        }\n        if request and request.args.get(\"_context\") and self.setting(\"template_debug\"):\n            return \"<pre>{}</pre>\".format(\n                escape(json.dumps(template_context, default=repr, indent=4))\n            )\n\n        return await template.render_async(template_context)", "fpath_tuple": ["Database", "datasette", "datasette", "app.py"], "context_start_lineno": 987, "line_no": 990, "id": "datasette.app.Datasette.render_template", "target_function_prompt": "    async def render_template(\n        self, templates, context=None, request=None, view_name=None\n    ):", "function_signature": "    async def render_template(\n        self, templates, context=None, request=None, view_name=None\n    ):"}}
{"prompt": "    def parse(self, instr):", "metadata": {"task_id": "Security/barf/2", "ground_truth": "        try:\n            instr_lower = instr.lower()\n\n            if instr_lower not in self._cache:\n                instr_asm = instruction.parseString(instr_lower)[0]\n\n                self._cache[instr_lower] = instr_asm\n\n            instr_asm = copy.deepcopy(self._cache[instr_lower])\n\n            # self._check_instruction(instr_asm)\n        except Exception:\n            instr_asm = None\n\n            error_msg = \"Failed to parse instruction: %s\"\n\n            logger.error(error_msg, instr, exc_info=True)\n\n        return instr_asm", "fpath_tuple": ["Security", "barf", "barf", "arch", "x86", "parser.py"], "context_start_lineno": 290, "line_no": 293, "id": "barf.arch.x86.parser.X86Parser.parse", "target_function_prompt": "    def parse(self, instr):", "function_signature": "    def parse(self, instr):"}}
{"prompt": "    def write_ns(self, payload):", "metadata": {"task_id": "Utilities/boltons/33", "ground_truth": "        size = len(payload)\n        if size > self.maxsize:\n            raise NetstringMessageTooLong(size, self.maxsize)\n        data = str(size).encode('ascii') + b':' + payload + b','\n        self.bsock.send(data)", "fpath_tuple": ["Utilities", "boltons", "boltons", "socketutils.py"], "context_start_lineno": 700, "line_no": 701, "id": "boltons.socketutils.NetstringSocket.write_ns", "target_function_prompt": "    def write_ns(self, payload):", "function_signature": "    def write_ns(self, payload):"}}
{"prompt": "    def hash(self):", "metadata": {"task_id": "System/exodus-bundler/4", "ground_truth": "        with open(self.path, 'rb') as f:\n            return hashlib.sha256(f.read()).hexdigest()", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "bundling.py"], "context_start_lineno": 648, "line_no": 650, "id": "exodus_bundler.bundling.File.hash", "target_function_prompt": "    def hash(self):", "function_signature": "    def hash(self):"}}
{"prompt": "    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> DropTableOp:", "metadata": {"task_id": "Database/alembic/10", "ground_truth": "        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw={\n                \"comment\": table.comment,\n                \"info\": dict(table.info),\n                \"prefixes\": list(table._prefixes),\n                **table.kwargs,\n            },\n            _reverse=CreateTableOp.from_table(\n                table, _namespace_metadata=_namespace_metadata\n            ),\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 1332, "line_no": 1335, "id": "alembic.operations.ops.DropTableOp.from_table", "target_function_prompt": "    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> DropTableOp:", "function_signature": "    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> DropTableOp:"}}
{"prompt": "def generate_lorem_ipsum(\n    n: int = 5, html: bool = True, min: int = 20, max: int = 100\n) -> str:", "metadata": {"task_id": "Internet/Jinja2/8", "ground_truth": "    from .constants import LOREM_IPSUM_WORDS\n\n    words = LOREM_IPSUM_WORDS.split()\n    result = []\n\n    for _ in range(n):\n        next_capitalized = True\n        last_comma = last_fullstop = 0\n        word = None\n        last = None\n        p = []\n\n        # each paragraph contains out of 20 to 100 words.\n        for idx, _ in enumerate(range(randrange(min, max))):\n            while True:\n                word = choice(words)\n                if word != last:\n                    last = word\n                    break\n            if next_capitalized:\n                word = word.capitalize()\n                next_capitalized = False\n            # add commas\n            if idx - randrange(3, 8) > last_comma:\n                last_comma = idx\n                last_fullstop += 2\n                word += \",\"\n            # add end of sentences\n            if idx - randrange(10, 20) > last_fullstop:\n                last_comma = last_fullstop = idx\n                word += \".\"\n                next_capitalized = True\n            p.append(word)\n\n        # ensure that the paragraph ends with a dot.\n        p_str = \" \".join(p)\n\n        if p_str.endswith(\",\"):\n            p_str = p_str[:-1] + \".\"\n        elif not p_str.endswith(\".\"):\n            p_str += \".\"\n\n        result.append(p_str)\n\n    if not html:\n        return \"\\n\\n\".join(result)\n    return markupsafe.Markup(\n        \"\\n\".join(f\"<p>{markupsafe.escape(x)}</p>\" for x in result)\n    )", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "utils.py"], "context_start_lineno": 341, "line_no": 345, "id": "jinja2.utils.generate_lorem_ipsum", "target_function_prompt": "def generate_lorem_ipsum(\n    n: int = 5, html: bool = True, min: int = 20, max: int = 100\n) -> str:", "function_signature": "def generate_lorem_ipsum(\n    n: int = 5, html: bool = True, min: int = 20, max: int = 100\n) -> str:"}}
{"prompt": "    def add_file(self, path, entry_point=None):", "metadata": {"task_id": "System/exodus-bundler/5", "ground_truth": "        try:\n            file = self.file_factory(path, entry_point=entry_point, chroot=self.chroot)\n        except UnexpectedDirectoryError:\n            assert entry_point is None, \"Directories can't have entry points.\"\n            for root, directories, files in os.walk(path):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    self.add_file(file_path)\n            return\n\n        self.files.add(file)\n        if file.elf:\n            if file.elf.linker_file:\n                self.linker_files.add(file.elf.linker_file)\n                self.files |= file.elf.dependencies\n            else:\n                # Manually set the linker if there isn't one in the program header,\n                # and we've only seen one in all of the files that have been added.\n                if len(self.linker_files) == 1:\n                    [file.elf.linker_file] = self.linker_files\n                    self.files |= file.elf.dependencies\n                    # We definitely don't want a launcher for this file, so clear the linker.\n                    file.elf.linker_file = None\n                else:\n                    logger.warning((\n                        'An ELF binary without a suitable linker candidate was encountered. '\n                        'Either no linker was found or there are multiple conflicting linkers.'\n                    ))\n\n        return file", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "bundling.py"], "context_start_lineno": 717, "line_no": 733, "id": "exodus_bundler.bundling.Bundle.add_file", "target_function_prompt": "    def add_file(self, path, entry_point=None):", "function_signature": "    def add_file(self, path, entry_point=None):"}}
{"prompt": "def rsync(src, dest, flags=[\"-ax\", \"--delete\"]):", "metadata": {"task_id": "System/pyinfra/5", "ground_truth": "    show_rsync_warning()\n\n    try:\n        host.check_can_rsync()\n    except NotImplementedError as e:\n        raise OperationError(*e.args)\n\n    yield RsyncCommand(src, dest, flags)", "fpath_tuple": ["System", "pyinfra", "pyinfra", "operations", "files.py"], "context_start_lineno": 669, "line_no": 684, "id": "pyinfra.operations.files.rsync", "target_function_prompt": "def rsync(src, dest, flags=[\"-ax\", \"--delete\"]):", "function_signature": "def rsync(src, dest, flags=[\"-ax\", \"--delete\"]):"}}
{"prompt": "    def darwin_checker(self):", "metadata": {"task_id": "Utilities/python-for-android/12", "ground_truth": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 324, "line_no": 325, "id": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "target_function_prompt": "    def darwin_checker(self):", "function_signature": "    def darwin_checker(self):"}}
{"prompt": "def js_to_py_type(type_object, is_flow_type=False, indent_num=0):", "metadata": {"task_id": "Software-Development/dash/7", "ground_truth": "    js_type_name = type_object[\"name\"]\n    js_to_py_types = (\n        map_js_to_py_types_flow_types(type_object=type_object)\n        if is_flow_type\n        else map_js_to_py_types_prop_types(\n            type_object=type_object, indent_num=indent_num\n        )\n    )\n\n    if (\n        \"computed\" in type_object\n        and type_object[\"computed\"]\n        or type_object.get(\"type\", \"\") == \"function\"\n    ):\n        return \"\"\n    if js_type_name in js_to_py_types:\n        if js_type_name == \"signature\":  # This is a Flow object w/ signature\n            return js_to_py_types[js_type_name](indent_num)\n        # All other types\n        return js_to_py_types[js_type_name]()\n    return \"\"", "fpath_tuple": ["Software-Development", "dash", "dash", "development", "_py_components_generation.py"], "context_start_lineno": 654, "line_no": 670, "id": "dash.development._py_components_generation.js_to_py_type", "target_function_prompt": "def js_to_py_type(type_object, is_flow_type=False, indent_num=0):", "function_signature": "def js_to_py_type(type_object, is_flow_type=False, indent_num=0):"}}
{"prompt": "    def from_hertz(self, hertz, standard_pitch=440):", "metadata": {"task_id": "Multimedia/mingus/17", "ground_truth": "        value = (\n            log((float(hertz) * 1024) / standard_pitch, 2) + 1.0 / 24\n        ) * 12 + 9  # notes.note_to_int(\"A\")\n        self.name = notes.int_to_note(int(value) % 12)\n        self.octave = int(value / 12) - 6\n        return self", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "note.py"], "context_start_lineno": 235, "line_no": 241, "id": "mingus.containers.note.Note.from_hertz", "target_function_prompt": "    def from_hertz(self, hertz, standard_pitch=440):", "function_signature": "    def from_hertz(self, hertz, standard_pitch=440):"}}
{"prompt": "    def validate(self, key: SSHKey, client_host: str, client_addr: str,\n                 cert_principals: Optional[Sequence[str]] = None,\n                 ca: bool = False) -> Optional[Mapping[str, object]]:", "metadata": {"task_id": "Security/asyncssh/3", "ground_truth": "        for entry in self._ca_entries if ca else self._user_entries:\n            if (entry.key == key and\n                    entry.match_options(client_host, client_addr,\n                                        cert_principals)):\n                return entry.options\n\n        return None", "fpath_tuple": ["Security", "asyncssh", "asyncssh", "auth_keys.py"], "context_start_lineno": 277, "line_no": 282, "id": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "target_function_prompt": "    def validate(self, key: SSHKey, client_host: str, client_addr: str,\n                 cert_principals: Optional[Sequence[str]] = None,\n                 ca: bool = False) -> Optional[Mapping[str, object]]:", "function_signature": "    def validate(self, key: SSHKey, client_host: str, client_addr: str,\n                 cert_principals: Optional[Sequence[str]] = None,\n                 ca: bool = False) -> Optional[Mapping[str, object]]:"}}
{"prompt": "    def to_representation(self, value):", "metadata": {"task_id": "Internet/djangorestframework/9", "ground_truth": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value.pk)\n        return value.pk", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "relations.py"], "context_start_lineno": 307, "line_no": 308, "id": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "target_function_prompt": "    def to_representation(self, value):", "function_signature": "    def to_representation(self, value):"}}
{"prompt": "    def merge(files, accumulate=True):", "metadata": {"task_id": "System/prometheus-client/1", "ground_truth": "        metrics = MultiProcessCollector._read_metrics(files)\n        return MultiProcessCollector._accumulate_metrics(metrics, accumulate)", "fpath_tuple": ["System", "prometheus-client", "prometheus_client", "multiprocess.py"], "context_start_lineno": 35, "line_no": 42, "id": "prometheus_client.multiprocess.MultiProcessCollector.merge", "target_function_prompt": "    def merge(files, accumulate=True):", "function_signature": "    def merge(files, accumulate=True):"}}
{"prompt": "    def find(self, start_address, end_address, byte_depth=20, instrs_depth=2):", "metadata": {"task_id": "Security/barf/3", "ground_truth": "        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        if self._architecture == ARCH_X86:\n            candidates = self._find_x86_candidates(start_address, end_address)\n        elif self._architecture == ARCH_ARM:\n            candidates = self._find_arm_candidates(start_address, end_address)\n        else:\n            raise Exception(\"Architecture not supported.\")\n\n        # Sort and return.\n        return sorted(candidates, key=lambda g: g.address)", "fpath_tuple": ["Security", "barf", "barf", "analysis", "gadgets", "finder.py"], "context_start_lineno": 73, "line_no": 76, "id": "barf.analysis.gadgets.finder.GadgetFinder.find", "target_function_prompt": "    def find(self, start_address, end_address, byte_depth=20, instrs_depth=2):", "function_signature": "    def find(self, start_address, end_address, byte_depth=20, instrs_depth=2):"}}
{"prompt": "    def get_note_names(self):", "metadata": {"task_id": "Multimedia/mingus/18", "ground_truth": "        res = []\n        for n in self.notes:\n            if n.name not in res:\n                res.append(n.name)\n        return res", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "note_container.py"], "context_start_lineno": 292, "line_no": 297, "id": "mingus.containers.note_container.NoteContainer.get_note_names", "target_function_prompt": "    def get_note_names(self):", "function_signature": "    def get_note_names(self):"}}
{"prompt": "    def darwin_installer(self):", "metadata": {"task_id": "Utilities/python-for-android/13", "ground_truth": "        info(\"Installing cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 362, "line_no": 363, "id": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "target_function_prompt": "    def darwin_installer(self):", "function_signature": "    def darwin_installer(self):"}}
{"prompt": "    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> t.Tuple[\"ext.NpNDArray\", list[int]]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/14", "ground_truth": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 334, "line_no": 339, "id": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "target_function_prompt": "    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> t.Tuple[\"ext.NpNDArray\", list[int]]:", "function_signature": "    def from_batch_payloads(\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> t.Tuple[\"ext.NpNDArray\", list[int]]:"}}
{"prompt": "    def sections_by_title(\n        self,\n        title: str,\n    ) -> List[WikipediaPageSection]:", "metadata": {"task_id": "Communications/Wikipedia-API/2", "ground_truth": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return []\n        return sections", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 950, "line_no": 960, "id": "wikipediaapi.WikipediaPage.sections_by_title", "target_function_prompt": "    def sections_by_title(\n        self,\n        title: str,\n    ) -> List[WikipediaPageSection]:", "function_signature": "    def sections_by_title(\n        self,\n        title: str,\n    ) -> List[WikipediaPageSection]:"}}
{"prompt": "def ensure_version(config: Config, sql: bool = False) -> None:", "metadata": {"task_id": "Database/alembic/11", "ground_truth": "    script = ScriptDirectory.from_config(config)\n\n    def do_ensure_version(rev, context):\n        context._ensure_version_table()\n        return []\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=do_ensure_version,\n        as_sql=sql,\n    ):\n        script.run_env()", "fpath_tuple": ["Database", "alembic", "alembic", "command.py"], "context_start_lineno": 720, "line_no": 731, "id": "alembic.command.ensure_version", "target_function_prompt": "def ensure_version(config: Config, sql: bool = False) -> None:", "function_signature": "def ensure_version(config: Config, sql: bool = False) -> None:"}}
{"prompt": "def capture_engine_context_buffer(**kw):", "metadata": {"task_id": "Database/alembic/12", "ground_truth": "    from .env import _sqlite_file_db\n    from sqlalchemy import event\n\n    buf = io.StringIO()\n\n    eng = _sqlite_file_db()\n\n    conn = eng.connect()\n\n    @event.listens_for(conn, \"before_cursor_execute\")\n    def bce(conn, cursor, statement, parameters, context, executemany):\n        buf.write(statement + \"\\n\")\n\n    kw.update({\"connection\": conn})\n    conf = EnvironmentContext.configure\n\n    def configure(*arg, **opt):\n        opt.update(**kw)\n        return conf(*arg, **opt)\n\n    with mock.patch.object(EnvironmentContext, \"configure\", configure):\n        yield buf", "fpath_tuple": ["Database", "alembic", "alembic", "testing", "fixtures.py"], "context_start_lineno": 105, "line_no": 106, "id": "alembic.testing.fixtures.capture_engine_context_buffer", "target_function_prompt": "def capture_engine_context_buffer(**kw):", "function_signature": "def capture_engine_context_buffer(**kw):"}}
{"prompt": "def parse_subject_alternative_name_extension(certificate: Certificate) -> SubjectAlternativeNameExtension:", "metadata": {"task_id": "System/sslyze/4", "ground_truth": "    try:\n        san_ext = certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n        san_ext_value = cast(SubjectAlternativeName, san_ext.value)\n    except ExtensionNotFound:\n        return SubjectAlternativeNameExtension(dns_names=[], ip_addresses=[])\n    except DuplicateExtension:\n        # Fix for https://github.com/nabla-c0d3/sslyze/issues/420\n        # Not sure how browsers behave in this case but having a duplicate extension makes the certificate invalid\n        # so we just return no SANs (likely to make hostname validation fail, which is fine)\n        return SubjectAlternativeNameExtension(dns_names=[], ip_addresses=[])\n\n    dns_names = []\n    ip_addresses = []\n    for san_value in san_ext_value:\n        if isinstance(san_value, IPAddress):\n            ip_addresses.append(str(san_value.value))\n        elif isinstance(san_value, DNSName):\n            dns_names.append(san_value.value)\n        else:\n            pass\n\n    return SubjectAlternativeNameExtension(dns_names=dns_names, ip_addresses=ip_addresses)", "fpath_tuple": ["System", "sslyze", "sslyze", "plugins", "certificate_info", "_certificate_utils.py"], "context_start_lineno": 24, "line_no": 25, "id": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "target_function_prompt": "def parse_subject_alternative_name_extension(certificate: Certificate) -> SubjectAlternativeNameExtension:", "function_signature": "def parse_subject_alternative_name_extension(certificate: Certificate) -> SubjectAlternativeNameExtension:"}}
{"prompt": "    def render(self, *args: t.Any, **kwargs: t.Any) -> str:", "metadata": {"task_id": "Internet/Jinja2/9", "ground_truth": "        if self.environment.is_async:\n            import asyncio\n\n            close = False\n\n            try:\n                loop = asyncio.get_running_loop()\n            except RuntimeError:\n                loop = asyncio.new_event_loop()\n                close = True\n\n            try:\n                return loop.run_until_complete(self.render_async(*args, **kwargs))\n            finally:\n                if close:\n                    loop.close()\n\n        ctx = self.new_context(dict(*args, **kwargs))\n\n        try:\n            return self.environment.concat(self.root_render_func(ctx))  # type: ignore\n        except Exception:\n            self.environment.handle_exception()", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "environment.py"], "context_start_lineno": 1268, "line_no": 1278, "id": "jinja2.environment.Template.render", "target_function_prompt": "    def render(self, *args: t.Any, **kwargs: t.Any) -> str:", "function_signature": "    def render(self, *args: t.Any, **kwargs: t.Any) -> str:"}}
{"prompt": "    def pop_default():", "metadata": {"task_id": "Security/diffprivlib/9", "ground_truth": "        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "accountant.py"], "context_start_lineno": 458, "line_no": 467, "id": "diffprivlib.accountant.BudgetAccountant.pop_default", "target_function_prompt": "    def pop_default():", "function_signature": "    def pop_default():"}}
{"prompt": "    def registerHandler(self, *arg, **kw):", "metadata": {"task_id": "Internet/pyramid/33", "ground_truth": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "registry.py"], "context_start_lineno": 94, "line_no": 95, "id": "pyramid.registry.Registry.registerHandler", "target_function_prompt": "    def registerHandler(self, *arg, **kw):", "function_signature": "    def registerHandler(self, *arg, **kw):"}}
{"prompt": "    def find_redirection_file_path(tokens):", "metadata": {"task_id": "Communications/chatette/10", "ground_truth": "        if len(tokens) < 2:\n            return None\n        if tokens[-2] == REDIRECTION_APPEND_SYM:\n            return (RedirectionType.append, tokens[-1])\n        if tokens[-2] == REDIRECTION_SYM:\n            return (RedirectionType.truncate, tokens[-1])\n        if (\n            tokens[-1] == REDIRECTION_APPEND_SYM\n            or tokens[-1] == REDIRECTION_SYM\n        ):\n            return (RedirectionType.quiet, None)\n        return None", "fpath_tuple": ["Communications", "chatette", "chatette", "cli", "interactive_commands", "command_strategy.py"], "context_start_lineno": 94, "line_no": 107, "id": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "target_function_prompt": "    def find_redirection_file_path(tokens):", "function_signature": "    def find_redirection_file_path(tokens):"}}
{"prompt": "def label_contains(\n    node,\n    triggers\n):", "metadata": {"task_id": "Security/python-taint/2", "ground_truth": "    for trigger in triggers:\n        if trigger.trigger_word in node.label:\n            yield TriggerNode(trigger, node)", "fpath_tuple": ["Security", "python-taint", "pyt", "vulnerabilities", "vulnerabilities.py"], "context_start_lineno": 150, "line_no": 164, "id": "pyt.vulnerabilities.vulnerabilities.label_contains", "target_function_prompt": "def label_contains(\n    node,\n    triggers\n):", "function_signature": "def label_contains(\n    node,\n    triggers\n):"}}
{"prompt": "def _split_key_indexes(key):", "metadata": {"task_id": "Text-Processing/python-benedict/1", "ground_truth": "    if \"[\" in key and key.endswith(\"]\"):\n        keys = []\n        while True:\n            matches = re.findall(KEY_INDEX_RE, key)\n            if matches:\n                key = re.sub(KEY_INDEX_RE, \"\", key)\n                index = int(matches[0])\n                keys.insert(0, index)\n                # keys.insert(0, { keylist_util.INDEX_KEY:index })\n                continue\n            keys.insert(0, key)\n            break\n        return keys\n    return [key]", "fpath_tuple": ["Text-Processing", "python-benedict", "benedict", "dicts", "keypath", "keypath_util.py"], "context_start_lineno": 36, "line_no": 41, "id": "benedict.dicts.keypath.keypath_util._split_key_indexes", "target_function_prompt": "def _split_key_indexes(key):", "function_signature": "def _split_key_indexes(key):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/34", "ground_truth": "    from boto.regioninfo import connect\n    from boto.support.layer1 import SupportConnection\n    return connect('support', region_name,\n                   connection_cls=SupportConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "support", "__init__.py"], "context_start_lineno": 37, "line_no": 38, "id": "boto.support.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "def decode(s: Union[bytes, str]) -> str:", "metadata": {"task_id": "Communications/IMAPClient/9", "ground_truth": "    if not isinstance(s, bytes):\n        return s\n\n    res = []\n    # Store base64 substring that will be decoded once stepping on end shift character\n    b64_buffer = bytearray()\n    for c in s:\n        # Shift character without anything in buffer -> starts storing base64 substring\n        if c == AMPERSAND_ORD and not b64_buffer:\n            b64_buffer.append(c)\n        # End shift char. -> append the decoded buffer to the result and reset it\n        elif c == DASH_ORD and b64_buffer:\n            # Special case &-, representing \"&\" escaped\n            if len(b64_buffer) == 1:\n                res.append(\"&\")\n            else:\n                res.append(base64_utf7_decode(b64_buffer[1:]))\n            b64_buffer = bytearray()\n        # Still buffering between the shift character and the shift back to ASCII\n        elif b64_buffer:\n            b64_buffer.append(c)\n        # No buffer initialized yet, should be an ASCII printable char\n        else:\n            res.append(chr(c))\n\n    # Decode the remaining buffer if any\n    if b64_buffer:\n        res.append(base64_utf7_decode(b64_buffer[1:]))\n\n    return \"\".join(res)", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imap_utf7.py"], "context_start_lineno": 61, "line_no": 68, "id": "imapclient.imap_utf7.decode", "target_function_prompt": "def decode(s: Union[bytes, str]) -> str:", "function_signature": "def decode(s: Union[bytes, str]) -> str:"}}
{"prompt": "    def folder_status(self, folder, what=None):", "metadata": {"task_id": "Communications/IMAPClient/10", "ground_truth": "        if what is None:\n            what = (\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\")\n        else:\n            what = normalise_text_list(what)\n        what_ = \"(%s)\" % (\" \".join(what))\n\n        fname = self._normalise_folder(folder)\n        data = self._command_and_check(\"status\", fname, what_)\n        response = parse_response(data)\n        status_items = response[-1]\n        return dict(as_pairs(status_items))", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1001, "line_no": 1011, "id": "imapclient.imapclient.IMAPClient.folder_status", "target_function_prompt": "    def folder_status(self, folder, what=None):", "function_signature": "    def folder_status(self, folder, what=None):"}}
{"prompt": "    def get_package_name(self):", "metadata": {"task_id": "Internet/pyramid/34", "ground_truth": "        if self.package is CALLER_PACKAGE:\n            package_name = caller_package().__name__\n        else:\n            package_name = self.package.__name__\n        return package_name", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "path.py"], "context_start_lineno": 105, "line_no": 106, "id": "pyramid.path.Resolver.get_package_name", "target_function_prompt": "    def get_package_name(self):", "function_signature": "    def get_package_name(self):"}}
{"prompt": "    def create(\n        cls,\n        name: Tag | str,\n        *,\n        module: str,\n        api_version: str,\n        signatures: ModelSignaturesType,\n        labels: dict[str, str] | None = None,\n        options: ModelOptions | None = None,\n        custom_objects: dict[str, t.Any] | None = None,\n        metadata: dict[str, t.Any] | None = None,\n        context: ModelContext,\n    ) -> Model:", "metadata": {"task_id": "Scientific-Engineering/bentoml/15", "ground_truth": "        tag = Tag.from_taglike(name)\n        if tag.version is None:\n            tag = tag.make_new_version()\n        labels = {} if labels is None else labels\n        metadata = {} if metadata is None else metadata\n        options = ModelOptions() if options is None else options\n\n        model_fs = fs.open_fs(f\"temp://bentoml_model_{tag.name}\")\n\n        return cls(\n            tag,\n            model_fs,\n            ModelInfo(\n                tag=tag,\n                module=module,\n                api_version=api_version,\n                signatures=signatures,\n                labels=labels,\n                options=options,\n                metadata=metadata,\n                context=context,\n            ),\n            custom_objects=custom_objects,\n            _internal=True,\n        )", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "models", "model.py"], "context_start_lineno": 137, "line_no": 173, "id": "bentoml._internal.models.model.Model.create", "target_function_prompt": "    def create(\n        cls,\n        name: Tag | str,\n        *,\n        module: str,\n        api_version: str,\n        signatures: ModelSignaturesType,\n        labels: dict[str, str] | None = None,\n        options: ModelOptions | None = None,\n        custom_objects: dict[str, t.Any] | None = None,\n        metadata: dict[str, t.Any] | None = None,\n        context: ModelContext,\n    ) -> Model:", "function_signature": "    def create(\n        cls,\n        name: Tag | str,\n        *,\n        module: str,\n        api_version: str,\n        signatures: ModelSignaturesType,\n        labels: dict[str, str] | None = None,\n        options: ModelOptions | None = None,\n        custom_objects: dict[str, t.Any] | None = None,\n        metadata: dict[str, t.Any] | None = None,\n        context: ModelContext,\n    ) -> Model:"}}
{"prompt": "    def unselect_folder(self):", "metadata": {"task_id": "Communications/IMAPClient/11", "ground_truth": "        logger.debug(\"< UNSELECT\")\n        # IMAP4 class has no `unselect` method so we can't use `_command_and_check` there\n        _typ, data = self._imap._simple_command(\"UNSELECT\")\n        return data[0]", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 823, "line_no": 831, "id": "imapclient.imapclient.IMAPClient.unselect_folder", "target_function_prompt": "    def unselect_folder(self):", "function_signature": "    def unselect_folder(self):"}}
{"prompt": "def jobconf_from_dict(jobconf, name, default=None):", "metadata": {"task_id": "System/mrjob/38", "ground_truth": "    if name in jobconf:\n        return jobconf[name]\n\n    # try alternatives (arbitrary order)\n    for alternative in _JOBCONF_MAP.get(name, {}).values():\n        if alternative in jobconf:\n            return jobconf[alternative]\n\n    return default", "fpath_tuple": ["System", "mrjob", "mrjob", "compat.py"], "context_start_lineno": 598, "line_no": 611, "id": "mrjob.compat.jobconf_from_dict", "target_function_prompt": "def jobconf_from_dict(jobconf, name, default=None):", "function_signature": "def jobconf_from_dict(jobconf, name, default=None):"}}
{"prompt": "    def _get_mapper(self, registry):", "metadata": {"task_id": "Internet/pyramid/35", "ground_truth": "        from pyramid.config import Configurator\n\n        config = Configurator(registry=registry)\n        return config.get_routes_mapper()", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "scripts", "proutes.py"], "context_start_lineno": 305, "line_no": 306, "id": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "target_function_prompt": "    def _get_mapper(self, registry):", "function_signature": "    def _get_mapper(self, registry):"}}
{"prompt": "def extract_tables(sql):", "metadata": {"task_id": "Database/mssql-cli/6", "ground_truth": "    parsed = sqlparse.parse(sql)\n    if not parsed:\n        return ()\n\n    # INSERT statements must stop looking for tables at the sign of first\n    # Punctuation. eg: INSERT INTO abc (col1, col2) VALUES (1, 2)\n    # abc is the table name, but if we don't stop at the first lparen, then\n    # we'll identify abc, col1 and col2 as table names.\n    insert_stmt = parsed[0].token_first().value.lower() == 'insert'\n    stream = extract_from_part(parsed[0], stop_at_punctuation=insert_stmt)\n\n    # Kludge: sqlparse mistakenly identifies insert statements as\n    # function calls due to the parenthesized column list, e.g. interprets\n    # \"insert into foo (bar, baz)\" as a function call to foo with arguments\n    # (bar, baz). So don't allow any identifiers in insert statements\n    # to have is_function=True\n    identifiers = extract_table_identifiers(stream,\n                                            allow_functions=not insert_stmt)\n    # In the case 'sche.<cursor>', we get an empty TableReference; remove that\n    return tuple(i for i in identifiers if i.name)", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "packages", "parseutils", "tables.py"], "context_start_lineno": 120, "line_no": 126, "id": "mssqlcli.packages.parseutils.tables.extract_tables", "target_function_prompt": "def extract_tables(sql):", "function_signature": "def extract_tables(sql):"}}
{"prompt": "def check_instance(arg, cls, msg=\"Expected a {name} instance, not {arg!r}\"):", "metadata": {"task_id": "Multimedia/Mopidy/16", "ground_truth": "    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "internal", "validation.py"], "context_start_lineno": 63, "line_no": 64, "id": "mopidy.internal.validation.check_instance", "target_function_prompt": "def check_instance(arg, cls, msg=\"Expected a {name} instance, not {arg!r}\"):", "function_signature": "def check_instance(arg, cls, msg=\"Expected a {name} instance, not {arg!r}\"):"}}
{"prompt": "    def get_gmail_labels(self, messages):", "metadata": {"task_id": "Communications/IMAPClient/12", "ground_truth": "        response = self.fetch(messages, [b\"X-GM-LABELS\"])\n        response = self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n        return {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1273, "line_no": 1283, "id": "imapclient.imapclient.IMAPClient.get_gmail_labels", "target_function_prompt": "    def get_gmail_labels(self, messages):", "function_signature": "    def get_gmail_labels(self, messages):"}}
{"prompt": "def validate_sql_select(sql):", "metadata": {"task_id": "Database/datasette/21", "ground_truth": "    sql = \"\\n\".join(\n        line for line in sql.split(\"\\n\") if not line.strip().startswith(\"--\")\n    )\n    sql = sql.strip().lower()\n    if not any(r.match(sql) for r in allowed_sql_res):\n        raise InvalidSql(\"Statement must be a SELECT\")\n    for r, msg in disallawed_sql_res:\n        if r.search(sql):\n            raise InvalidSql(msg)", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 255, "line_no": 256, "id": "datasette.utils.validate_sql_select", "target_function_prompt": "def validate_sql_select(sql):", "function_signature": "def validate_sql_select(sql):"}}
{"prompt": "    def when(cls, key_prefix=None, http_error_code=None):", "metadata": {"task_id": "Internet/boto/35", "ground_truth": "        return cls(Condition(key_prefix=key_prefix,\n                             http_error_code=http_error_code), None)", "fpath_tuple": ["Internet", "boto", "boto", "s3", "website.py"], "context_start_lineno": 211, "line_no": 212, "id": "boto.s3.website.RoutingRule.when", "target_function_prompt": "    def when(cls, key_prefix=None, http_error_code=None):", "function_signature": "    def when(cls, key_prefix=None, http_error_code=None):"}}
{"prompt": "    def from_progression_shorthand(self, shorthand, key=\"C\"):", "metadata": {"task_id": "Multimedia/mingus/19", "ground_truth": "        from mingus.core import progressions\n        self.empty()\n        chords = progressions.to_chords(shorthand, key)\n        # warning Throw error, not a valid shorthand\n\n        if chords == []:\n            return False\n        notes = chords[0]\n        self.add_notes(notes)\n        return self", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "note_container.py"], "context_start_lineno": 154, "line_no": 164, "id": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "target_function_prompt": "    def from_progression_shorthand(self, shorthand, key=\"C\"):", "function_signature": "    def from_progression_shorthand(self, shorthand, key=\"C\"):"}}
{"prompt": "def _merge_and_sort_errors(errors, attempt_to_container_id=None):", "metadata": {"task_id": "System/mrjob/39", "ground_truth": "    from .ids import _time_sort_key\n    attempt_to_container_id = attempt_to_container_id or {}\n\n    key_to_error = {}\n\n    for error in errors:\n        # merge by container_id if we know it\n        container_id = error.get('container_id') or (\n            attempt_to_container_id.get(error.get('attempt_id')))\n\n        if container_id:\n            key = ('container_id', container_id)\n        else:\n            key = ('time_key', _time_sort_key(error))\n\n        key_to_error.setdefault(key, {})\n        # assume redundant fields will match\n        key_to_error[key].update(error)\n\n    # wrap sort key to prioritize task errors. See #1429\n    def sort_key(key_and_error):\n        key, error = key_and_error\n\n        # key[0] is 'container_id' or 'time_key'\n        return (bool(error.get('task_error')),\n                key[0] == 'container_id',\n                key[1:])\n\n    return [error for _, error in\n            sorted(key_to_error.items(), key=sort_key, reverse=True)]", "fpath_tuple": ["System", "mrjob", "mrjob", "logs", "errors.py"], "context_start_lineno": 100, "line_no": 106, "id": "mrjob.logs.errors._merge_and_sort_errors", "target_function_prompt": "def _merge_and_sort_errors(errors, attempt_to_container_id=None):", "function_signature": "def _merge_and_sort_errors(errors, attempt_to_container_id=None):"}}
{"prompt": "    def get_csrf_token(self):", "metadata": {"task_id": "Internet/pyramid/36", "ground_truth": "        token = self.get('_csrft_', None)\n        if token is None:\n            token = self.new_csrf_token()\n        return token", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "testing.py"], "context_start_lineno": 264, "line_no": 265, "id": "pyramid.testing.DummySession.get_csrf_token", "target_function_prompt": "    def get_csrf_token(self):", "function_signature": "    def get_csrf_token(self):"}}
{"prompt": "    def close(self):", "metadata": {"task_id": "Database/mssql-cli/7", "ground_truth": "        try:\n            self.stream.close()\n        except AttributeError:\n            pass", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "jsonrpc", "jsonrpcclient.py"], "context_start_lineno": 435, "line_no": 439, "id": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "target_function_prompt": "    def close(self):", "function_signature": "    def close(self):"}}
{"prompt": "def minimum_part_size(size_in_bytes, default_part_size=DEFAULT_PART_SIZE):", "metadata": {"task_id": "Internet/boto/36", "ground_truth": "    part_size = _MEGABYTE\n    if (default_part_size * MAXIMUM_NUMBER_OF_PARTS) < size_in_bytes:\n        if size_in_bytes > (4096 * _MEGABYTE * 10000):\n            raise ValueError(\"File size too large: %s\" % size_in_bytes)\n        min_part_size = size_in_bytes / 10000\n        power = 3\n        while part_size < min_part_size:\n            part_size = math.ldexp(_MEGABYTE, power)\n            power += 1\n        part_size = int(part_size)\n    else:\n        part_size = default_part_size\n    return part_size", "fpath_tuple": ["Internet", "boto", "boto", "glacier", "utils.py"], "context_start_lineno": 33, "line_no": 58, "id": "boto.glacier.utils.minimum_part_size", "target_function_prompt": "def minimum_part_size(size_in_bytes, default_part_size=DEFAULT_PART_SIZE):", "function_signature": "def minimum_part_size(size_in_bytes, default_part_size=DEFAULT_PART_SIZE):"}}
{"prompt": "    def serialize(cls, value, *args, **kwargs):", "metadata": {"task_id": "Text-Processing/rows/5", "ground_truth": "        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(cls.OUTPUT_FORMAT))", "fpath_tuple": ["Text-Processing", "rows", "rows", "fields.py"], "context_start_lineno": 359, "line_no": 360, "id": "rows.fields.DateField.serialize", "target_function_prompt": "    def serialize(cls, value, *args, **kwargs):", "function_signature": "    def serialize(cls, value, *args, **kwargs):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/37", "ground_truth": "    from boto.regioninfo import connect\n    from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    return connect('cloudsearchdomain', region_name,\n                   connection_cls=CloudSearchDomainConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "cloudsearchdomain", "__init__.py"], "context_start_lineno": 37, "line_no": 38, "id": "boto.cloudsearchdomain.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "def register_default_actions():", "metadata": {"task_id": "Scientific-Engineering/lux/0", "ground_truth": "    import lux\n    from lux.action.custom import custom\n    from lux.action.correlation import correlation\n    from lux.action.univariate import univariate\n    from lux.action.enhance import enhance\n    from lux.action.filter import add_filter\n    from lux.action.generalize import generalize\n    from lux.action.temporal import temporal\n\n    # display conditions for default actions\n    no_vis = lambda ldf: (ldf.current_vis is None) or (\n        ldf.current_vis is not None and len(ldf.current_vis) == 0\n    )\n    one_current_vis = lambda ldf: ldf.current_vis is not None and len(ldf.current_vis) == 1\n    multiple_current_vis = lambda ldf: ldf.current_vis is not None and len(ldf.current_vis) > 1\n\n    # globally register default actions\n    lux.config.register_action(\"correlation\", correlation, no_vis)\n    lux.config.register_action(\"distribution\", univariate, no_vis, \"quantitative\")\n    lux.config.register_action(\"occurrence\", univariate, no_vis, \"nominal\")\n    lux.config.register_action(\"temporal\", temporal, no_vis)\n    lux.config.register_action(\"geographical\", univariate, no_vis, \"geographical\")\n\n    lux.config.register_action(\"Enhance\", enhance, one_current_vis)\n    lux.config.register_action(\"Filter\", add_filter, one_current_vis)\n    lux.config.register_action(\"Generalize\", generalize, one_current_vis)\n\n    lux.config.register_action(\"Custom\", custom, multiple_current_vis)", "fpath_tuple": ["Scientific-Engineering", "lux", "lux", "action", "default.py"], "context_start_lineno": 0, "line_no": 1, "id": "lux.action.default.register_default_actions", "target_function_prompt": "def register_default_actions():", "function_signature": "def register_default_actions():"}}
{"prompt": "    def pop(self, *a):", "metadata": {"task_id": "Utilities/boltons/34", "ground_truth": "        lists = self.lists\n        if len(lists) == 1 and not a:\n            return self.lists[0].pop()\n        index = a and a[0]\n        if index == () or index is None or index == -1:\n            ret = lists[-1].pop()\n            if len(lists) > 1 and not lists[-1]:\n                lists.pop()\n        else:\n            list_idx, rel_idx = self._translate_index(index)\n            if list_idx is None:\n                raise IndexError()\n            ret = lists[list_idx].pop(rel_idx)\n            self._balance_list(list_idx)\n        return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "listutils.py"], "context_start_lineno": 161, "line_no": 162, "id": "boltons.listutils.BarrelList.pop", "target_function_prompt": "    def pop(self, *a):", "function_signature": "    def pop(self, *a):"}}
{"prompt": "def expand_env_var_in_values(d: t.MutableMapping[str, t.Any]) -> None:", "metadata": {"task_id": "Scientific-Engineering/bentoml/16", "ground_truth": "    for k, v in d.items():\n        if isinstance(v, t.MutableMapping):\n            expand_env_var_in_values(v)\n        elif isinstance(v, str):\n            d[k] = expand_env_var(v)\n        elif isinstance(v, t.Sequence):\n            d[k] = [expand_env_var(i) for i in v]", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "configuration", "helpers.py"], "context_start_lineno": 185, "line_no": 186, "id": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "target_function_prompt": "def expand_env_var_in_values(d: t.MutableMapping[str, t.Any]) -> None:", "function_signature": "def expand_env_var_in_values(d: t.MutableMapping[str, t.Any]) -> None:"}}
{"prompt": "def get_inner_text(text, entities):", "metadata": {"task_id": "Communications/Telethon/3", "ground_truth": "    text = add_surrogate(text)\n    result = []\n    for e in entities:\n        start = e.offset\n        end = e.offset + e.length\n        result.append(del_surrogate(text[start:end]))\n\n    return result", "fpath_tuple": ["Communications", "Telethon", "telethon", "utils.py"], "context_start_lineno": 936, "line_no": 945, "id": "telethon.utils.get_inner_text", "target_function_prompt": "def get_inner_text(text, entities):", "function_signature": "def get_inner_text(text, entities):"}}
{"prompt": "    async def suggest(self):", "metadata": {"task_id": "Database/datasette/22", "ground_truth": "        columns = await self.get_columns(self.sql, self.params)\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for column in columns:\n            if column in already_enabled:\n                continue\n            # Is every value in this column either null or a JSON array?\n            suggested_facet_sql = \"\"\"\n                select distinct json_type({column})\n                from ({sql})\n                where {column} is not null and {column} != ''\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql\n            )\n            try:\n                results = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                    log_sql_errors=False,\n                )\n                types = tuple(r[0] for r in results.rows)\n                if types in ((\"array\",), (\"array\", None)):\n                    # Now check that first 100 arrays contain only strings\n                    first_100 = [\n                        v[0]\n                        for v in await self.ds.execute(\n                            self.database,\n                            (\n                                \"select {column} from ({sql}) \"\n                                \"where {column} is not null \"\n                                \"and {column} != '' \"\n                                \"and json_array_length({column}) > 0 \"\n                                \"limit 100\"\n                            ).format(column=escape_sqlite(column), sql=self.sql),\n                            self.params,\n                            truncate=False,\n                            custom_time_limit=self.ds.setting(\n                                \"facet_suggest_time_limit_ms\"\n                            ),\n                            log_sql_errors=False,\n                        )\n                    ]\n                    if first_100 and all(\n                        self._is_json_array_of_strings(r) for r in first_100\n                    ):\n                        suggested_facets.append(\n                            {\n                                \"name\": column,\n                                \"type\": \"array\",\n                                \"toggle_url\": self.ds.absolute_url(\n                                    self.request,\n                                    self.ds.urls.path(\n                                        path_with_added_args(\n                                            self.request, {\"_facet_array\": column}\n                                        )\n                                    ),\n                                ),\n                            }\n                        )\n            except (QueryInterrupted, sqlite3.OperationalError):\n                continue\n        return suggested_facets", "fpath_tuple": ["Database", "datasette", "datasette", "facets.py"], "context_start_lineno": 302, "line_no": 303, "id": "datasette.facets.ArrayFacet.suggest", "target_function_prompt": "    async def suggest(self):", "function_signature": "    async def suggest(self):"}}
{"prompt": "def route_main():", "metadata": {"task_id": "Internet/falcon/20", "ground_truth": "    print(\n        'The \"falcon-print-routes\" command is deprecated. '\n        'Please use \"falcon-inspect-app\"'\n    )\n    main()", "fpath_tuple": ["Internet", "falcon", "falcon", "cmd", "inspect_app.py"], "context_start_lineno": 88, "line_no": 89, "id": "falcon.cmd.inspect_app.route_main", "target_function_prompt": "def route_main():", "function_signature": "def route_main():"}}
{"prompt": "def translation(domain=DOMAIN, localedir=LOCALE_DIR, languages=None):", "metadata": {"task_id": "Database/mssql-cli/8", "ground_truth": "    languages = languages if (not languages is None) else LANGUAGES\n    return gettext.translation(domain, localedir, languages, fallback=True)", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "localized_strings.py"], "context_start_lineno": 9, "line_no": 10, "id": "mssqlcli.localized_strings.translation", "target_function_prompt": "def translation(domain=DOMAIN, localedir=LOCALE_DIR, languages=None):", "function_signature": "def translation(domain=DOMAIN, localedir=LOCALE_DIR, languages=None):"}}
{"prompt": "    def yaml_out(self) -> str:", "metadata": {"task_id": "Utilities/jc/3", "ground_truth": "        try:\n            from ruamel.yaml import YAML, representer\n            YAML_INSTALLED = True\n        except Exception:\n            YAML_INSTALLED = False\n\n        if YAML_INSTALLED:\n            y_string_buf = io.BytesIO()\n\n            # monkey patch to disable plugins since we don't use them and in\n            # ruamel.yaml versions prior to 0.17.0 the use of __file__ in the\n            # plugin code is incompatible with the pyoxidizer packager\n            YAML.official_plug_ins = lambda a: []  # type: ignore\n\n            # monkey patch to disable aliases\n            representer.RoundTripRepresenter.ignore_aliases = lambda x, y: True  # type: ignore\n\n            yaml = YAML()\n            yaml.default_flow_style = False\n            yaml.explicit_start = True  # type: ignore\n            yaml.allow_unicode = not self.ascii_only\n            yaml.encoding = 'utf-8'\n            yaml.dump(self.data_out, y_string_buf)\n            y_string = y_string_buf.getvalue().decode('utf-8')[:-1]\n\n            if not self.mono:\n                class JcStyle(Style):\n                    styles: CustomColorType = self.custom_colors\n\n                return str(highlight(y_string, YamlLexer(), Terminal256Formatter(style=JcStyle))[0:-1])\n\n            return y_string\n\n        utils.warning_message(['YAML Library not installed. Reverting to JSON output.'])\n        return self.json_out()", "fpath_tuple": ["Utilities", "jc", "jc", "cli.py"], "context_start_lineno": 339, "line_no": 345, "id": "jc.cli.JcCli.yaml_out", "target_function_prompt": "    def yaml_out(self) -> str:", "function_signature": "    def yaml_out(self) -> str:"}}
{"prompt": "    def deserialize(self, cstruct=colander.null):", "metadata": {"task_id": "Internet/kinto/17", "ground_truth": "        from kinto.core.utils import merge_dicts\n        if cstruct is not colander.null:\n            defaults = cstruct.get(\"defaults\")\n            requests = cstruct.get(\"requests\")\n            if isinstance(defaults, dict) and isinstance(requests, list):\n                for request in requests:\n                    if isinstance(request, dict):\n                        merge_dicts(request, defaults)\n        return super().deserialize(cstruct)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "views", "batch.py"], "context_start_lineno": 58, "line_no": 60, "id": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "target_function_prompt": "    def deserialize(self, cstruct=colander.null):", "function_signature": "    def deserialize(self, cstruct=colander.null):"}}
{"prompt": "    async def ensure_permissions(\n        self,\n        actor: dict,\n        permissions: Sequence[Union[Tuple[str, Union[str, Tuple[str, str]]], str]],\n    ):", "metadata": {"task_id": "Database/datasette/23", "ground_truth": "        assert actor is None or isinstance(actor, dict), \"actor must be None or a dict\"\n        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            elif isinstance(permission, (tuple, list)) and len(permission) == 2:\n                action, resource = permission\n            else:\n                assert (\n                    False\n                ), \"permission should be string or tuple of two items: {}\".format(\n                    repr(permission)\n                )\n            ok = await self.permission_allowed(\n                actor,\n                action,\n                resource=resource,\n                default=None,\n            )\n            if ok is not None:\n                if ok:\n                    return\n                else:\n                    raise Forbidden(action)", "fpath_tuple": ["Database", "datasette", "datasette", "app.py"], "context_start_lineno": 690, "line_no": 700, "id": "datasette.app.Datasette.ensure_permissions", "target_function_prompt": "    async def ensure_permissions(\n        self,\n        actor: dict,\n        permissions: Sequence[Union[Tuple[str, Union[str, Tuple[str, str]]], str]],\n    ):", "function_signature": "    async def ensure_permissions(\n        self,\n        actor: dict,\n        permissions: Sequence[Union[Tuple[str, Union[str, Tuple[str, str]]], str]],\n    ):"}}
{"prompt": "    def describe(self):", "metadata": {"task_id": "Internet/boto/38", "ground_truth": "        result = self.connection.describe_table(self.table_name)\n\n        # Blindly update throughput, since what's on DynamoDB's end is likely\n        # more correct.\n        raw_throughput = result['Table']['ProvisionedThroughput']\n        self.throughput['read'] = int(raw_throughput['ReadCapacityUnits'])\n        self.throughput['write'] = int(raw_throughput['WriteCapacityUnits'])\n\n        if not self.schema:\n            # Since we have the data, build the schema.\n            raw_schema = result['Table'].get('KeySchema', [])\n            raw_attributes = result['Table'].get('AttributeDefinitions', [])\n            self.schema = self._introspect_schema(raw_schema, raw_attributes)\n\n        if not self.indexes:\n            # Build the index information as well.\n            raw_indexes = result['Table'].get('LocalSecondaryIndexes', [])\n            self.indexes = self._introspect_indexes(raw_indexes)\n\n        # Build the global index information as well.\n        raw_global_indexes = result['Table'].get('GlobalSecondaryIndexes', [])\n        self.global_indexes = self._introspect_global_indexes(raw_global_indexes)\n\n        # This is leaky.\n        return result", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 331, "line_no": 354, "id": "boto.dynamodb2.table.Table.describe", "target_function_prompt": "    def describe(self):", "function_signature": "    def describe(self):"}}
{"prompt": "def find_nested_value(d, path, default=None):", "metadata": {"task_id": "Internet/kinto/18", "ground_truth": "    if path in d:\n        return d.get(path)\n\n    # the challenge is to identify what is the root key, as dict keys may\n    # contain dot characters themselves\n    parts = path.split(\".\")\n\n    # build a list of all possible root keys from all the path parts\n    candidates = [\".\".join(parts[: i + 1]) for i in range(len(parts))]\n\n    # we start with the longest candidate paths as they're most likely to be the\n    # ones we want if they match\n    root = next((key for key in reversed(candidates) if key in d), None)\n\n    # if no valid root candidates were found, the path is invalid; abandon\n    if root is None or not isinstance(d.get(root), dict):\n        return default\n\n    # we have our root key, extract the new subpath and recur\n    subpath = path.replace(root + \".\", \"\", 1)\n    return find_nested_value(d.get(root), subpath, default=default)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "utils.py"], "context_start_lineno": 196, "line_no": 203, "id": "kinto.core.utils.find_nested_value", "target_function_prompt": "def find_nested_value(d, path, default=None):", "function_signature": "def find_nested_value(d, path, default=None):"}}
{"prompt": "    def from_constraint(cls, constraint: Constraint) -> DropConstraintOp:", "metadata": {"task_id": "Database/alembic/13", "ground_truth": "        types = {\n            \"unique_constraint\": \"unique\",\n            \"foreign_key_constraint\": \"foreignkey\",\n            \"primary_key_constraint\": \"primary\",\n            \"check_constraint\": \"check\",\n            \"column_check_constraint\": \"check\",\n            \"table_or_column_check_constraint\": \"check\",\n        }\n\n        constraint_table = sqla_compat._table_for_constraint(constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(constraint.name),\n            constraint_table.name,\n            schema=constraint_table.schema,\n            type_=types.get(constraint.__visit_name__),\n            _reverse=AddConstraintOp.from_constraint(constraint),\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 156, "line_no": 157, "id": "alembic.operations.ops.DropConstraintOp.from_constraint", "target_function_prompt": "    def from_constraint(cls, constraint: Constraint) -> DropConstraintOp:", "function_signature": "    def from_constraint(cls, constraint: Constraint) -> DropConstraintOp:"}}
{"prompt": "    def read_response(self):", "metadata": {"task_id": "Database/mssql-cli/9", "ground_truth": "        content = ['']\n        try:\n            while (not self.needs_more_data or self.read_next_chunk()):\n                # We should have all the data we need to form a message in the buffer.\n                # If we need more data to form the next message, this flag will\n                # be reset by a attempt to form a header or content.\n                self.needs_more_data = False\n                # If we can't read a header, read the next chunk.\n                if self.read_state is ReadState.Header and not self.try_read_headers():\n                    self.needs_more_data = True\n                    continue\n                # If we read the header, try the content. If that fails, read\n                # the next chunk.\n                if self.read_state is ReadState.Content and not self.try_read_content(\n                        content):\n                    self.needs_more_data = True\n                    continue\n                # We have the  content\n                break\n\n            # Resize buffer and remove bytes we have read\n            self.trim_buffer_and_resize(self.read_offset)\n            return json.loads(content[0])\n        except ValueError as ex:\n            # response has invalid json object.\n            logger.debug(u'JSON RPC Reader on read_response() encountered exception: %s', ex)\n            raise", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "jsonrpc", "jsonrpcclient.py"], "context_start_lineno": 260, "line_no": 269, "id": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "target_function_prompt": "    def read_response(self):", "function_signature": "    def read_response(self):"}}
{"prompt": "def crack_bip32(bip32_pub_node, secret_exponent, path):", "metadata": {"task_id": "Security/pycoin/19", "ground_truth": "    paths = path.split(\"/\")\n    while len(paths):\n        path = int(paths.pop())\n        secret_exponent = ascend_bip32(bip32_pub_node.subkey_for_path(\"/\".join(paths)), secret_exponent, path)\n    return bip32_pub_node.__class__(\n        bip32_pub_node._chain_code, bip32_pub_node._depth, bip32_pub_node._parent_fingerprint,\n        bip32_pub_node._child_index, secret_exponent=secret_exponent)", "fpath_tuple": ["Security", "pycoin", "pycoin", "crack", "bip32.py"], "context_start_lineno": 21, "line_no": 22, "id": "pycoin.crack.bip32.crack_bip32", "target_function_prompt": "def crack_bip32(bip32_pub_node, secret_exponent, path):", "function_signature": "def crack_bip32(bip32_pub_node, secret_exponent, path):"}}
{"prompt": "    def deserialize(cls, value, *args, **kwargs):", "metadata": {"task_id": "Text-Processing/rows/6", "ground_truth": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)", "fpath_tuple": ["Text-Processing", "rows", "rows", "fields.py"], "context_start_lineno": 469, "line_no": 470, "id": "rows.fields.JSONField.deserialize", "target_function_prompt": "    def deserialize(cls, value, *args, **kwargs):", "function_signature": "    def deserialize(cls, value, *args, **kwargs):"}}
{"prompt": "    def schema(self):", "metadata": {"task_id": "Internet/boto/39", "ground_truth": "        key_schema = []\n\n        for part in self.parts:\n            key_schema.append(part.schema())\n\n        return {\n            'IndexName': self.name,\n            'KeySchema': key_schema,\n            'Projection': {\n                'ProjectionType': self.projection_type,\n            }\n        }", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "fields.py"], "context_start_lineno": 125, "line_no": 146, "id": "boto.dynamodb2.fields.BaseIndexField.schema", "target_function_prompt": "    def schema(self):", "function_signature": "    def schema(self):"}}
{"prompt": "    def create_config(cls, cfgfile, nick, twtfile, twturl, disclose_identity, add_news):", "metadata": {"task_id": "Communications/twtxt/1", "ground_truth": "        cfgfile_dir = os.path.dirname(cfgfile)\n        if not os.path.exists(cfgfile_dir):\n            os.makedirs(cfgfile_dir)\n\n        cfg = configparser.ConfigParser()\n\n        cfg.add_section(\"twtxt\")\n        cfg.set(\"twtxt\", \"nick\", nick)\n        cfg.set(\"twtxt\", \"twtfile\", twtfile)\n        cfg.set(\"twtxt\", \"twturl\", twturl)\n        cfg.set(\"twtxt\", \"disclose_identity\", str(disclose_identity))\n        cfg.set(\"twtxt\", \"character_limit\", \"140\")\n        cfg.set(\"twtxt\", \"character_warning\", \"140\")\n\n        cfg.add_section(\"following\")\n        if add_news:\n            cfg.set(\"following\", \"twtxt\", \"https://buckket.org/twtxt_news.txt\")\n\n        conf = cls(cfgfile, cfg)\n        conf.write_config()\n        return conf", "fpath_tuple": ["Communications", "twtxt", "twtxt", "config.py"], "context_start_lineno": 62, "line_no": 72, "id": "twtxt.config.Config.create_config", "target_function_prompt": "    def create_config(cls, cfgfile, nick, twtfile, twturl, disclose_identity, add_news):", "function_signature": "    def create_config(cls, cfgfile, nick, twtfile, twturl, disclose_identity, add_news):"}}
{"prompt": "def print_exception(etype, value, tb, limit=None, file=None):", "metadata": {"task_id": "Utilities/boltons/35", "ground_truth": "    if file is None:\n        file = sys.stderr\n    if tb:\n        tbi = TracebackInfo.from_traceback(tb, limit)\n        print(str(tbi), end='', file=file)\n\n    for line in format_exception_only(etype, value):\n        print(line, end='', file=file)", "fpath_tuple": ["Utilities", "boltons", "boltons", "tbutils.py"], "context_start_lineno": 650, "line_no": 662, "id": "boltons.tbutils.print_exception", "target_function_prompt": "def print_exception(etype, value, tb, limit=None, file=None):", "function_signature": "def print_exception(etype, value, tb, limit=None, file=None):"}}
{"prompt": "    def read(self, size=None):", "metadata": {"task_id": "Utilities/gunicorn/12", "ground_truth": "        size = self.getsize(size)\n        if size == 0:\n            return b\"\"\n\n        if size < self.buf.tell():\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while size > self.buf.tell():\n            data = self.reader.read(1024)\n            if not data:\n                break\n            self.buf.write(data)\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "http", "body.py"], "context_start_lineno": 201, "line_no": 202, "id": "gunicorn.http.body.Body.read", "target_function_prompt": "    def read(self, size=None):", "function_signature": "    def read(self, size=None):"}}
{"prompt": "    def writelines(self, lines):", "metadata": {"task_id": "Utilities/boltons/36", "ground_truth": "        self._checkClosed()\n        for line in lines:\n            self.write(line)", "fpath_tuple": ["Utilities", "boltons", "boltons", "ioutils.py"], "context_start_lineno": 113, "line_no": 119, "id": "boltons.ioutils.SpooledIOBase.writelines", "target_function_prompt": "    def writelines(self, lines):", "function_signature": "    def writelines(self, lines):"}}
{"prompt": "def get_cached_reset_password(username, registry):", "metadata": {"task_id": "Internet/kinto/19", "ground_truth": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    cache_result = cache.get(cache_key)\n    return cache_result", "fpath_tuple": ["Internet", "kinto", "kinto", "plugins", "accounts", "utils.py"], "context_start_lineno": 54, "line_no": 56, "id": "kinto.plugins.accounts.utils.get_cached_reset_password", "target_function_prompt": "def get_cached_reset_password(username, registry):", "function_signature": "def get_cached_reset_password(username, registry):"}}
{"prompt": "    def subdomain(self):\n        # PERF(kgriffs): .partition is slightly faster than .split", "metadata": {"task_id": "Internet/falcon/21", "ground_truth": "        subdomain, sep, remainder = self.host.partition('.')\n        return subdomain if sep else None", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 871, "line_no": 873, "id": "falcon.request.Request.subdomain", "target_function_prompt": "    def subdomain(self):\n        # PERF(kgriffs): .partition is slightly faster than .split", "function_signature": "    def subdomain(self):\n        # PERF(kgriffs): .partition is slightly faster than .split"}}
{"prompt": "    def index(\n        self,\n        name: Optional[str],\n        tablename: Optional[str],\n        columns: Sequence[Union[str, TextClause, ColumnElement[Any]]],\n        schema: Optional[str] = None,\n        **kw,\n    ) -> Index:", "metadata": {"task_id": "Database/alembic/14", "ground_truth": "        t = sa_schema.Table(\n            tablename or \"no_table\",\n            self.metadata(),\n            schema=schema,\n        )\n        kw[\"_table\"] = t\n        idx = sa_schema.Index(\n            name,\n            *[util.sqla_compat._textual_index_column(t, n) for n in columns],\n            **kw,\n        )\n        return idx", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "schemaobj.py"], "context_start_lineno": 240, "line_no": 248, "id": "alembic.operations.schemaobj.SchemaObjects.index", "target_function_prompt": "    def index(\n        self,\n        name: Optional[str],\n        tablename: Optional[str],\n        columns: Sequence[Union[str, TextClause, ColumnElement[Any]]],\n        schema: Optional[str] = None,\n        **kw,\n    ) -> Index:", "function_signature": "    def index(\n        self,\n        name: Optional[str],\n        tablename: Optional[str],\n        columns: Sequence[Union[str, TextClause, ColumnElement[Any]]],\n        schema: Optional[str] = None,\n        **kw,\n    ) -> Index:"}}
{"prompt": "def prefixed_collapsible_map(m, prefix):", "metadata": {"task_id": "Communications/twilio-fatisar/11", "ground_truth": "    if m == values.unset:\n        return {}\n\n    def flatten_dict(d, result=None, prv_keys=None):\n        if result is None:\n            result = {}\n\n        if prv_keys is None:\n            prv_keys = []\n\n        for k, v in d.items():\n            if isinstance(v, dict):\n                flatten_dict(v, result, prv_keys + [k])\n            else:\n                result[\".\".join(prv_keys + [k])] = v\n\n        return result\n\n    if isinstance(m, dict):\n        flattened = flatten_dict(m)\n        return {\"{}.{}\".format(prefix, k): v for k, v in flattened.items()}\n\n    return {}", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "base", "serialize.py"], "context_start_lineno": 34, "line_no": 38, "id": "twilio.base.serialize.prefixed_collapsible_map", "target_function_prompt": "def prefixed_collapsible_map(m, prefix):", "function_signature": "def prefixed_collapsible_map(m, prefix):"}}
{"prompt": "def on_account_activated(event):", "metadata": {"task_id": "Internet/kinto/20", "ground_truth": "    request = event.request\n    settings = request.registry.settings\n    if not settings.get(\"account_validation\", False):\n        return\n\n    for impacted_object in event.impacted_objects:\n        old_account = impacted_object[\"old\"]\n        account = impacted_object[\"new\"]\n        if old_account.get(\"validated\", True) or not account.get(\"validated\", False):\n            # It's not an account activation, bail.\n            continue\n\n        # Send a confirmation email.\n        Emailer(request, account).send_confirmation()", "fpath_tuple": ["Internet", "kinto", "kinto", "plugins", "accounts", "views", "validation.py"], "context_start_lineno": 120, "line_no": 121, "id": "kinto.plugins.accounts.views.validation.on_account_activated", "target_function_prompt": "def on_account_activated(event):", "function_signature": "def on_account_activated(event):"}}
{"prompt": "    async def read_until(self, delimiter, size=-1, consume_delimiter=False):", "metadata": {"task_id": "Internet/falcon/22", "ground_truth": "        result = await self._read_from(\n            self._iter_delimited(delimiter, size_hint=size or 0), size\n        )\n\n        if consume_delimiter:\n            await self._consume_delimiter(delimiter)\n\n        return result", "fpath_tuple": ["Internet", "falcon", "falcon", "asgi", "reader.py"], "context_start_lineno": 288, "line_no": 289, "id": "falcon.asgi.reader.BufferedReader.read_until", "target_function_prompt": "    async def read_until(self, delimiter, size=-1, consume_delimiter=False):", "function_signature": "    async def read_until(self, delimiter, size=-1, consume_delimiter=False):"}}
{"prompt": "    def _to_box_autocomplete(self, text: str, state: Optional[int]) -> Optional[str]:", "metadata": {"task_id": "Communications/zulip-term/11", "ground_truth": "        users_list = self.view.users\n        recipients = text.rsplit(\",\", 1)\n\n        # Use the most recent recipient for autocomplete.\n        previous_recipients = f\"{recipients[0]}, \" if len(recipients) > 1 else \"\"\n        latest_text = recipients[-1].strip()\n\n        matching_users = [\n            user for user in users_list if match_user_name_and_email(user, latest_text)\n        ]\n\n        # Append the potential autocompleted recipients to the string\n        # containing the previous recipients.\n        updated_recipients = [\n            f\"{previous_recipients}{user['full_name']} <{user['email']}>\"\n            for user in matching_users\n        ]\n\n        user_names = [user[\"full_name\"] for user in matching_users]\n\n        return self._process_typeaheads(updated_recipients, state, user_names)", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "boxes.py"], "context_start_lineno": 430, "line_no": 431, "id": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "target_function_prompt": "    def _to_box_autocomplete(self, text: str, state: Optional[int]) -> Optional[str]:", "function_signature": "    def _to_box_autocomplete(self, text: str, state: Optional[int]) -> Optional[str]:"}}
{"prompt": "    def relative_uri(self):", "metadata": {"task_id": "Internet/falcon/23", "ground_truth": "        if self._cached_relative_uri is None:\n            if self.query_string:\n                self._cached_relative_uri = (\n                    self.app + self.path + '?' + self.query_string\n                )\n            else:\n                self._cached_relative_uri = self.app + self.path\n\n        return self._cached_relative_uri", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 802, "line_no": 803, "id": "falcon.request.Request.relative_uri", "target_function_prompt": "    def relative_uri(self):", "function_signature": "    def relative_uri(self):"}}
{"prompt": "    def start(self) -> None:", "metadata": {"task_id": "System/viztracer/0", "ground_truth": "        self.enable = True\n        self.parsed = False\n        if self.log_print:\n            self.overload_print()\n        if self.include_files is not None and self.exclude_files is not None:\n            raise Exception(\"include_files and exclude_files can't be both specified!\")\n        self.config()\n        self._tracer.start()", "fpath_tuple": ["System", "viztracer", "src", "viztracer", "tracer.py"], "context_start_lineno": 243, "line_no": 244, "id": "viztracer.tracer._VizTracer.start", "target_function_prompt": "    def start(self) -> None:", "function_signature": "    def start(self) -> None:"}}
{"prompt": "    def get_tags(self, headers=None):", "metadata": {"task_id": "Internet/boto/40", "ground_truth": "        from boto.s3.tagging import Tags\n        response = self.get_xml_tags(headers)\n        tags = Tags()\n        h = handler.XmlHandler(tags, self)\n        if not isinstance(response, bytes):\n            response = response.encode('utf-8')\n        xml.sax.parseString(response, h)\n        return tags", "fpath_tuple": ["Internet", "boto", "boto", "s3", "bucket.py"], "context_start_lineno": 1828, "line_no": 1829, "id": "boto.s3.bucket.Bucket.get_tags", "target_function_prompt": "    def get_tags(self, headers=None):", "function_signature": "    def get_tags(self, headers=None):"}}
{"prompt": "    def scan(\n        self,\n        package=None,\n        categories=('pyramid',),\n        onerror=None,\n        ignore=None,\n        **kw,\n    ):", "metadata": {"task_id": "Internet/pyramid/37", "ground_truth": "        package = self.maybe_dotted(package)\n        if package is None:  # pragma: no cover\n            package = caller_package()\n\n        ctorkw = {'config': self}\n        ctorkw.update(kw)\n\n        scanner = self.venusian.Scanner(**ctorkw)\n\n        scanner.scan(\n            package, categories=categories, onerror=onerror, ignore=ignore\n        )", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "__init__.py"], "context_start_lineno": 798, "line_no": 867, "id": "pyramid.config.Configurator.scan", "target_function_prompt": "    def scan(\n        self,\n        package=None,\n        categories=('pyramid',),\n        onerror=None,\n        ignore=None,\n        **kw,\n    ):", "function_signature": "    def scan(\n        self,\n        package=None,\n        categories=('pyramid',),\n        onerror=None,\n        ignore=None,\n        **kw,\n    ):"}}
{"prompt": "def get(\n    src,\n    dest,\n    add_deploy_dir=True,\n    create_local_dir=False,\n    force=False,\n):", "metadata": {"task_id": "System/pyinfra/6", "ground_truth": "    if add_deploy_dir and state.cwd:\n        dest = os.path.join(state.cwd, dest)\n\n    if create_local_dir:\n        local_pathname = os.path.dirname(dest)\n        if not os.path.exists(local_pathname):\n            os.makedirs(local_pathname)\n\n    remote_file = host.get_fact(File, path=src)\n\n    # No remote file, so assume exists and download it \"blind\"\n    if not remote_file or force:\n        yield FileDownloadCommand(src, dest, remote_temp_filename=state.get_temp_filename(dest))\n\n    # No local file, so always download\n    elif not os.path.exists(dest):\n        yield FileDownloadCommand(src, dest, remote_temp_filename=state.get_temp_filename(dest))\n\n    # Remote file exists - check if it matches our local\n    else:\n        local_sum = get_file_sha1(dest)\n        remote_sum = host.get_fact(Sha1File, path=src)\n\n        # Check sha1sum, upload if needed\n        if local_sum != remote_sum:\n            yield FileDownloadCommand(src, dest, remote_temp_filename=state.get_temp_filename(dest))", "fpath_tuple": ["System", "pyinfra", "pyinfra", "operations", "files.py"], "context_start_lineno": 716, "line_no": 747, "id": "pyinfra.operations.files.get", "target_function_prompt": "def get(\n    src,\n    dest,\n    add_deploy_dir=True,\n    create_local_dir=False,\n    force=False,\n):", "function_signature": "def get(\n    src,\n    dest,\n    add_deploy_dir=True,\n    create_local_dir=False,\n    force=False,\n):"}}
{"prompt": "    def render(self) -> str:", "metadata": {"task_id": "Software-Development/pandas-profiling/1", "ground_truth": "        from ydata_profiling.report.presentation.flavours.html import templates\n        return templates.template(\"diagram.html\").render(**self.content)", "fpath_tuple": ["Software-Development", "pandas-profiling", "src", "ydata_profiling", "report", "presentation", "flavours", "html", "image.py"], "context_start_lineno": 5, "line_no": 6, "id": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "target_function_prompt": "    def render(self) -> str:", "function_signature": "    def render(self) -> str:"}}
{"prompt": "    def enable(self, *capabilities):", "metadata": {"task_id": "Communications/IMAPClient/13", "ground_truth": "        if self._imap.state != \"AUTH\":\n            raise exceptions.IllegalStateError(\n                \"ENABLE command illegal in state %s\" % self._imap.state\n            )\n\n        resp = self._raw_command_untagged(\n            b\"ENABLE\",\n            [to_bytes(c) for c in capabilities],\n            uid=False,\n            response_name=\"ENABLED\",\n            unpack=True,\n        )\n        if not resp:\n            return []\n        return resp.split()", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 536, "line_no": 552, "id": "imapclient.imapclient.IMAPClient.enable", "target_function_prompt": "    def enable(self, *capabilities):", "function_signature": "    def enable(self, *capabilities):"}}
{"prompt": "    def check(self, epsilon, delta):", "metadata": {"task_id": "Security/diffprivlib/10", "ground_truth": "        from diffprivlib.utils import BudgetError\n        check_epsilon_delta(epsilon, delta)\n        if self.epsilon == float(\"inf\") and self.delta == 1:\n            return True\n\n        if 0 < epsilon < self.__min_epsilon:\n            raise ValueError(f\"Epsilon must be at least {self.__min_epsilon} if non-zero, got {epsilon}.\")\n\n        spent_budget = self.spent_budget + [(epsilon, delta)]\n\n        if Budget(self.epsilon, self.delta) >= self.total(spent_budget=spent_budget):\n            return True\n\n        raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\n                          f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "accountant.py"], "context_start_lineno": 274, "line_no": 296, "id": "diffprivlib.accountant.BudgetAccountant.check", "target_function_prompt": "    def check(self, epsilon, delta):", "function_signature": "    def check(self, epsilon, delta):"}}
{"prompt": "def _normalise_search_criteria(criteria, charset=None):", "metadata": {"task_id": "Communications/IMAPClient/14", "ground_truth": "    from .datetime_util import format_criteria_date\n    if not criteria:\n        raise exceptions.InvalidCriteriaError(\"no criteria specified\")\n    if not charset:\n        charset = \"us-ascii\"\n\n    if isinstance(criteria, (str, bytes)):\n        return [to_bytes(criteria, charset)]\n\n    out = []\n    for item in criteria:\n        if isinstance(item, int):\n            out.append(str(item).encode(\"ascii\"))\n        elif isinstance(item, (datetime, date)):\n            out.append(format_criteria_date(item))\n        elif isinstance(item, (list, tuple)):\n            # Process nested criteria list and wrap in parens.\n            inner = _normalise_search_criteria(item)\n            inner[0] = b\"(\" + inner[0]\n            inner[-1] = inner[-1] + b\")\"\n            out.extend(inner)  # flatten\n        else:\n            out.append(_quoted.maybe(to_bytes(item, charset)))\n    return out", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1825, "line_no": 1826, "id": "imapclient.imapclient._normalise_search_criteria", "target_function_prompt": "def _normalise_search_criteria(criteria, charset=None):", "function_signature": "def _normalise_search_criteria(criteria, charset=None):"}}
{"prompt": "def connect_all(state: \"State\"):", "metadata": {"task_id": "System/pyinfra/7", "ground_truth": "    from pyinfra.progress import progress_spinner\n    hosts = [\n        host\n        for host in state.inventory\n        if state.is_host_in_limit(host)  # these are the hosts to activate (\"initially connect to\")\n    ]\n\n    greenlet_to_host = {state.pool.spawn(host.connect): host for host in hosts}\n\n    with progress_spinner(greenlet_to_host.values()) as progress:\n        for greenlet in gevent.iwait(greenlet_to_host.keys()):\n            host = greenlet_to_host[greenlet]\n            progress(host)\n\n    # Get/set the results\n    failed_hosts = set()\n\n    for greenlet, host in greenlet_to_host.items():\n        # Raise any unexpected exception\n        greenlet.get()\n\n        if host.connected:\n            state.activate_host(host)\n        else:\n            failed_hosts.add(host)\n\n    # Remove those that failed, triggering FAIL_PERCENT check\n    state.fail_hosts(failed_hosts, activated_count=len(hosts))", "fpath_tuple": ["System", "pyinfra", "pyinfra", "api", "connect.py"], "context_start_lineno": 10, "line_no": 18, "id": "pyinfra.api.connect.connect_all", "target_function_prompt": "def connect_all(state: \"State\"):", "function_signature": "def connect_all(state: \"State\"):"}}
{"prompt": "def get_key_signature(key=\"C\"):", "metadata": {"task_id": "Multimedia/mingus/20", "ground_truth": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    for couple in keys:\n        if key in couple:\n            accidentals = keys.index(couple) - 7\n            return accidentals", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "keys.py"], "context_start_lineno": 78, "line_no": 84, "id": "mingus.core.keys.get_key_signature", "target_function_prompt": "def get_key_signature(key=\"C\"):", "function_signature": "def get_key_signature(key=\"C\"):"}}
{"prompt": "    def __repr__(self):", "metadata": {"task_id": "System/mrjob/40", "ground_truth": "        return '%s(%s)' % (\n            self.__class__.__name__,\n            ', '.join(('%s=%r' % (k, getattr(self, k))\n                       for k in self._FIELDS\n                       if getattr(self, k) is not None)))", "fpath_tuple": ["System", "mrjob", "mrjob", "step.py"], "context_start_lineno": 153, "line_no": 154, "id": "mrjob.step.StepFailedException.__repr__", "target_function_prompt": "    def __repr__(self):", "function_signature": "    def __repr__(self):"}}
{"prompt": "def _incremental_mean_and_var(X, epsilon, bounds, last_mean, last_variance, last_sample_count, random_state=None):\n    # Initialising new accountant, as budget is tracked in main class. Subject to review in line with GH issue #21", "metadata": {"task_id": "Security/diffprivlib/11", "ground_truth": "    temp_acc = BudgetAccountant()\n\n    # old = stats until now\n    # new = the current increment\n    # updated = the aggregated stats\n    last_sum = last_mean * last_sample_count\n\n    new_mean = nanmean(X, epsilon=epsilon, axis=0, bounds=bounds, random_state=random_state, accountant=temp_acc)\n    new_sample_count = np.sum(~np.isnan(X), axis=0)\n    new_sum = new_mean * new_sample_count\n    updated_sample_count = last_sample_count + new_sample_count\n\n    updated_mean = (last_sum + new_sum) / updated_sample_count\n\n    if last_variance is None:\n        updated_variance = None\n    else:\n        new_unnormalized_variance = nanvar(X, epsilon=epsilon, axis=0, bounds=bounds, random_state=random_state,\n                                           accountant=temp_acc) * new_sample_count\n        last_unnormalized_variance = last_variance * last_sample_count\n\n        with np.errstate(divide='ignore', invalid='ignore'):\n            last_over_new_count = last_sample_count / new_sample_count\n            updated_unnormalized_variance = (\n                last_unnormalized_variance + new_unnormalized_variance +\n                last_over_new_count / updated_sample_count *\n                (last_sum / last_over_new_count - new_sum) ** 2)\n\n        zeros = last_sample_count == 0\n        updated_unnormalized_variance[zeros] = new_unnormalized_variance[zeros]\n        updated_variance = updated_unnormalized_variance / updated_sample_count\n\n    return updated_mean, updated_variance, updated_sample_count", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "models", "standard_scaler.py"], "context_start_lineno": 57, "line_no": 59, "id": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "target_function_prompt": "def _incremental_mean_and_var(X, epsilon, bounds, last_mean, last_variance, last_sample_count, random_state=None):\n    # Initialising new accountant, as budget is tracked in main class. Subject to review in line with GH issue #21", "function_signature": "def _incremental_mean_and_var(X, epsilon, bounds, last_mean, last_variance, last_sample_count, random_state=None):\n    # Initialising new accountant, as budget is tracked in main class. Subject to review in line with GH issue #21"}}
{"prompt": "def combine_path_lists(*path_seqs):", "metadata": {"task_id": "System/mrjob/41", "ground_truth": "    results = []\n\n    for path in combine_lists(*path_seqs):\n        expanded = expand_path(path)\n        # if we can't expand a glob, leave as-is (maybe it refers to\n        # S3 or HDFS)\n        paths = sorted(glob.glob(expanded)) or [expanded]\n\n        results.extend(paths)\n\n    return results", "fpath_tuple": ["System", "mrjob", "mrjob", "conf.py"], "context_start_lineno": 524, "line_no": 531, "id": "mrjob.conf.combine_path_lists", "target_function_prompt": "def combine_path_lists(*path_seqs):", "function_signature": "def combine_path_lists(*path_seqs):"}}
{"prompt": "    def connect(\n        self,\n        name,\n        pattern,\n        factory=None,\n        predicates=(),\n        pregenerator=None,\n        static=False,\n    ):", "metadata": {"task_id": "Internet/pyramid/38", "ground_truth": "        if name in self.routes:\n            oldroute = self.routes[name]\n            if oldroute in self.routelist:\n                self.routelist.remove(oldroute)\n\n        route = Route(name, pattern, factory, predicates, pregenerator)\n        if not static:\n            self.routelist.append(route)\n        else:\n            self.static_routes.append(route)\n\n        self.routes[name] = route\n        return route", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "urldispatch.py"], "context_start_lineno": 45, "line_no": 54, "id": "pyramid.urldispatch.RoutesMapper.connect", "target_function_prompt": "    def connect(\n        self,\n        name,\n        pattern,\n        factory=None,\n        predicates=(),\n        pregenerator=None,\n        static=False,\n    ):", "function_signature": "    def connect(\n        self,\n        name,\n        pattern,\n        factory=None,\n        predicates=(),\n        pregenerator=None,\n        static=False,\n    ):"}}
{"prompt": "def _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result):", "metadata": {"task_id": "Text-Processing/feedparser/2", "ground_truth": "    from . import http\n    if hasattr(url_file_stream_or_string, 'read'):\n        return url_file_stream_or_string.read()\n\n    if isinstance(url_file_stream_or_string, str) \\\n       and urllib.parse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp', 'file', 'feed'):\n        return http.get(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result)\n\n    # try to open with native open function (if url_file_stream_or_string is a filename)\n    try:\n        with open(url_file_stream_or_string, 'rb') as f:\n            data = f.read()\n    except (IOError, UnicodeEncodeError, TypeError, ValueError):\n        # if url_file_stream_or_string is a str object that\n        # cannot be converted to the encoding returned by\n        # sys.getfilesystemencoding(), a UnicodeEncodeError\n        # will be thrown\n        # If url_file_stream_or_string is a string that contains NULL\n        # (such as an XML document encoded in UTF-32), TypeError will\n        # be thrown.\n        pass\n    else:\n        return data\n\n    # treat url_file_stream_or_string as string\n    if not isinstance(url_file_stream_or_string, bytes):\n        return url_file_stream_or_string.encode('utf-8')\n    return url_file_stream_or_string", "fpath_tuple": ["Text-Processing", "feedparser", "feedparser", "api.py"], "context_start_lineno": 75, "line_no": 109, "id": "feedparser.api._open_resource", "target_function_prompt": "def _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result):", "function_signature": "def _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result):"}}
{"prompt": "def compare_metadata(context: MigrationContext, metadata: MetaData) -> Any:", "metadata": {"task_id": "Database/alembic/15", "ground_truth": "    migration_script = produce_migrations(context, metadata)\n    return migration_script.upgrade_ops.as_diffs()", "fpath_tuple": ["Database", "alembic", "alembic", "autogenerate", "api.py"], "context_start_lineno": 44, "line_no": 165, "id": "alembic.autogenerate.api.compare_metadata", "target_function_prompt": "def compare_metadata(context: MigrationContext, metadata: MetaData) -> Any:", "function_signature": "def compare_metadata(context: MigrationContext, metadata: MetaData) -> Any:"}}
{"prompt": "    def dependencies(self):", "metadata": {"task_id": "System/exodus-bundler/6", "ground_truth": "        all_dependencies = set()\n        unprocessed_dependencies = set(self.direct_dependencies)\n        while len(unprocessed_dependencies):\n            all_dependencies |= unprocessed_dependencies\n            new_dependencies = set()\n            for dependency in unprocessed_dependencies:\n                if dependency.elf:\n                    new_dependencies |= set(\n                        dependency.elf.find_direct_dependencies(self.linker_file))\n            unprocessed_dependencies = new_dependencies - all_dependencies\n        return all_dependencies", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "bundling.py"], "context_start_lineno": 398, "line_no": 400, "id": "exodus_bundler.bundling.Elf.dependencies", "target_function_prompt": "    def dependencies(self):", "function_signature": "    def dependencies(self):"}}
{"prompt": "    def render_to_response(self, value, system_values, request=None):", "metadata": {"task_id": "Internet/pyramid/39", "ground_truth": "        result = self.render(value, system_values, request=request)\n        return self._make_response(result, request)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "renderers.py"], "context_start_lineno": 465, "line_no": 466, "id": "pyramid.renderers.RendererHelper.render_to_response", "target_function_prompt": "    def render_to_response(self, value, system_values, request=None):", "function_signature": "    def render_to_response(self, value, system_values, request=None):"}}
{"prompt": "    def delete_item(self, expected=None, conditional_operator=None, **kwargs):", "metadata": {"task_id": "Internet/boto/41", "ground_truth": "        expected = self._build_filters(expected, using=FILTER_OPERATORS)\n        raw_key = self._encode_keys(kwargs)\n\n        try:\n            self.connection.delete_item(self.table_name, raw_key,\n                                        expected=expected,\n                                        conditional_operator=conditional_operator)\n        except exceptions.ConditionalCheckFailedException:\n            return False\n\n        return True", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 854, "line_no": 906, "id": "boto.dynamodb2.table.Table.delete_item", "target_function_prompt": "    def delete_item(self, expected=None, conditional_operator=None, **kwargs):", "function_signature": "    def delete_item(self, expected=None, conditional_operator=None, **kwargs):"}}
{"prompt": "    def save_state(self, name, state):", "metadata": {"task_id": "Security/threatingestor/0", "ground_truth": "        logger.debug(f\"Updating state for '{name}' to '{state}'\")\n        self.cursor.execute('INSERT OR REPLACE INTO states (name, state) values (?, ?)', (name, state))\n        self.conn.commit()", "fpath_tuple": ["Security", "threatingestor", "threatingestor", "state.py"], "context_start_lineno": 23, "line_no": 25, "id": "threatingestor.state.State.save_state", "target_function_prompt": "    def save_state(self, name, state):", "function_signature": "    def save_state(self, name, state):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/42", "ground_truth": "    from boto.regioninfo import connect\n    from boto.awslambda.layer1 import AWSLambdaConnection\n    return connect('awslambda', region_name,\n                   connection_cls=AWSLambdaConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "awslambda", "__init__.py"], "context_start_lineno": 36, "line_no": 37, "id": "boto.awslambda.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def shutdown(self):", "metadata": {"task_id": "Database/mssql-cli/10", "ground_truth": "        self.cancel = True\n        # Enqueue None to optimistically unblock background threads so\n        # they can check for the cancellation flag.\n        self.request_queue.put(None)\n\n        # Wait for request thread to finish with a timeout in seconds.\n        self.request_thread.join(1)\n\n        # close the underlying writer.\n        self.writer.close()\n        logger.info('Shutting down Json rpc client.')", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "jsonrpc", "jsonrpcclient.py"], "context_start_lineno": 168, "line_no": 172, "id": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "target_function_prompt": "    def shutdown(self):", "function_signature": "    def shutdown(self):"}}
{"prompt": "    def dump(self) -> bytearray:", "metadata": {"task_id": "Database/bplustree/8", "ground_truth": "        data = bytearray()\n        for record in self.entries:\n            data.extend(record.dump())\n\n        used_page_length = len(data) + 4\n        assert 0 <= used_page_length < self._tree_conf.page_size\n        next_page = 0 if self.next_page is None else self.next_page\n        header = (\n            self._node_type_int.to_bytes(1, ENDIAN) +\n            used_page_length.to_bytes(3, ENDIAN) +\n            next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n\n        data = bytearray(header) + data\n\n        padding = self._tree_conf.page_size - len(data)\n        assert padding >= 0\n        data.extend(bytearray(padding))\n        assert len(data) == self._tree_conf.page_size\n\n        return data", "fpath_tuple": ["Database", "bplustree", "bplustree", "node.py"], "context_start_lineno": 49, "line_no": 50, "id": "bplustree.node.Node.dump", "target_function_prompt": "    def dump(self) -> bytearray:", "function_signature": "    def dump(self) -> bytearray:"}}
{"prompt": "def spatial_match(password, _graphs=GRAPHS, _ranked_dictionaries=RANKED_DICTIONARIES):", "metadata": {"task_id": "Security/zxcvbn-python/6", "ground_truth": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_match_helper(password, graph, graph_name))\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "matching.py"], "context_start_lineno": 301, "line_no": 302, "id": "zxcvbn.matching.spatial_match", "target_function_prompt": "def spatial_match(password, _graphs=GRAPHS, _ranked_dictionaries=RANKED_DICTIONARIES):", "function_signature": "def spatial_match(password, _graphs=GRAPHS, _ranked_dictionaries=RANKED_DICTIONARIES):"}}
{"prompt": "def _certificate_matches_hostname(certificate: Certificate, server_hostname: str) -> bool:", "metadata": {"task_id": "System/sslyze/5", "ground_truth": "    from sslyze.plugins.certificate_info._certificate_utils import get_common_names\n    from sslyze.plugins.certificate_info._certificate_utils import parse_subject_alternative_name_extension\n    try:\n        cert_subject = certificate.subject\n    except ValueError:\n        # Cryptography could not parse the certificate https://github.com/nabla-c0d3/sslyze/issues/495\n        return False\n\n    subj_alt_name_ext = parse_subject_alternative_name_extension(certificate)\n    subj_alt_name_as_list = [(\"DNS\", name) for name in subj_alt_name_ext.dns_names]\n    subj_alt_name_as_list.extend([(\"IP Address\", ip) for ip in subj_alt_name_ext.ip_addresses])\n\n    certificate_names = {\n        \"subject\": (tuple([(\"commonName\", name) for name in get_common_names(cert_subject)]),),\n        \"subjectAltName\": tuple(subj_alt_name_as_list),\n    }\n    # CertificateError is raised on failure\n    try:\n        match_hostname(certificate_names, server_hostname)  # type: ignore\n        return True\n    except CertificateError:\n        return False", "fpath_tuple": ["System", "sslyze", "sslyze", "plugins", "certificate_info", "_cert_chain_analyzer.py"], "context_start_lineno": 273, "line_no": 276, "id": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "target_function_prompt": "def _certificate_matches_hostname(certificate: Certificate, server_hostname: str) -> bool:", "function_signature": "def _certificate_matches_hostname(certificate: Certificate, server_hostname: str) -> bool:"}}
{"prompt": "    def elements(self):", "metadata": {"task_id": "Utilities/boltons/37", "ground_truth": "        repeaters = itertools.starmap(itertools.repeat, self.iteritems())\n        return itertools.chain.from_iterable(repeaters)", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 725, "line_no": 729, "id": "boltons.cacheutils.ThresholdCounter.elements", "target_function_prompt": "    def elements(self):", "function_signature": "    def elements(self):"}}
{"prompt": "    def get_package(self):", "metadata": {"task_id": "Internet/pyramid/40", "ground_truth": "        if self.package is CALLER_PACKAGE:\n            package = caller_package()\n        else:\n            package = self.package\n        return package", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "path.py"], "context_start_lineno": 112, "line_no": 113, "id": "pyramid.path.Resolver.get_package", "target_function_prompt": "    def get_package(self):", "function_signature": "    def get_package(self):"}}
{"prompt": "def detect_elf_binary(filename):", "metadata": {"task_id": "System/exodus-bundler/7", "ground_truth": "    if not os.path.exists(filename):\n        raise MissingFileError('The \"%s\" file was not found.' % filename)\n\n    with open(filename, 'rb') as f:\n        first_four_bytes = f.read(4)\n\n    return first_four_bytes == b'\\x7fELF'", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "bundling.py"], "context_start_lineno": 155, "line_no": 157, "id": "exodus_bundler.bundling.detect_elf_binary", "target_function_prompt": "def detect_elf_binary(filename):", "function_signature": "def detect_elf_binary(filename):"}}
{"prompt": "    def enqueue(\n        self,\n        name=None,\n        action=None,\n        max_queue_size=None,\n        method=None,\n        wait_url=None,\n        wait_url_method=None,\n        workflow_sid=None,\n        **kwargs\n    ):", "metadata": {"task_id": "Communications/twilio-fatisar/12", "ground_truth": "        return self.nest(\n            Enqueue(\n                name=name,\n                action=action,\n                max_queue_size=max_queue_size,\n                method=method,\n                wait_url=wait_url,\n                wait_url_method=wait_url_method,\n                workflow_sid=workflow_sid,\n                **kwargs\n            )\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "twiml", "voice_response.py"], "context_start_lineno": 113, "line_no": 138, "id": "twilio.twiml.voice_response.VoiceResponse.enqueue", "target_function_prompt": "    def enqueue(\n        self,\n        name=None,\n        action=None,\n        max_queue_size=None,\n        method=None,\n        wait_url=None,\n        wait_url_method=None,\n        workflow_sid=None,\n        **kwargs\n    ):", "function_signature": "    def enqueue(\n        self,\n        name=None,\n        action=None,\n        max_queue_size=None,\n        method=None,\n        wait_url=None,\n        wait_url_method=None,\n        workflow_sid=None,\n        **kwargs\n    ):"}}
{"prompt": "    def _parse_narrow_link(cls, link: str) -> ParsedNarrowLink:", "metadata": {"task_id": "Communications/zulip-term/12", "ground_truth": "        fragments = urlparse(link.rstrip(\"/\")).fragment.split(\"/\")\n        len_fragments = len(fragments)\n        parsed_link = ParsedNarrowLink()\n\n        if len_fragments == 3 and fragments[1] == \"stream\":\n            stream_data = cls._decode_stream_data(fragments[2])\n            parsed_link = dict(narrow=\"stream\", stream=stream_data)\n\n        elif (\n            len_fragments == 5 and fragments[1] == \"stream\" and fragments[3] == \"topic\"\n        ):\n            stream_data = cls._decode_stream_data(fragments[2])\n            topic_name = hash_util_decode(fragments[4])\n            parsed_link = dict(\n                narrow=\"stream:topic\", stream=stream_data, topic_name=topic_name\n            )\n\n        elif len_fragments == 5 and fragments[1] == \"stream\" and fragments[3] == \"near\":\n            stream_data = cls._decode_stream_data(fragments[2])\n            message_id = cls._decode_message_id(fragments[4])\n            parsed_link = dict(\n                narrow=\"stream:near\", stream=stream_data, message_id=message_id\n            )\n\n        elif (\n            len_fragments == 7\n            and fragments[1] == \"stream\"\n            and fragments[3] == \"topic\"\n            and fragments[5] == \"near\"\n        ):\n            stream_data = cls._decode_stream_data(fragments[2])\n            topic_name = hash_util_decode(fragments[4])\n            message_id = cls._decode_message_id(fragments[6])\n            parsed_link = dict(\n                narrow=\"stream:topic:near\",\n                stream=stream_data,\n                topic_name=topic_name,\n                message_id=message_id,\n            )\n\n        return parsed_link", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "buttons.py"], "context_start_lineno": 464, "line_no": 478, "id": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "target_function_prompt": "    def _parse_narrow_link(cls, link: str) -> ParsedNarrowLink:", "function_signature": "    def _parse_narrow_link(cls, link: str) -> ParsedNarrowLink:"}}
{"prompt": "    def response(self):", "metadata": {"task_id": "Internet/pyramid/41", "ground_truth": "        from pyramid.response import _get_response_factory\n        f = _get_response_factory(self.registry)\n        return f(self)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "testing.py"], "context_start_lineno": 397, "line_no": 398, "id": "pyramid.testing.DummyRequest.response", "target_function_prompt": "    def response(self):", "function_signature": "    def response(self):"}}
{"prompt": "def get_bib_abbrv_obj():", "metadata": {"task_id": "Database/awesome-autodl/1", "ground_truth": "    from awesome_autodl.data_cls import BibAbbreviations\n\n    xfile = str(get_bib_abbrv_file())\n    return BibAbbreviations(xfile)", "fpath_tuple": ["Database", "awesome-autodl", "awesome_autodl", "__init__.py"], "context_start_lineno": 78, "line_no": 79, "id": "awesome_autodl.get_bib_abbrv_obj", "target_function_prompt": "def get_bib_abbrv_obj():", "function_signature": "def get_bib_abbrv_obj():"}}
{"prompt": "def get_config_path(module_id: ModuleID = None, ext: str = 'yaml') -> Path:", "metadata": {"task_id": "Communications/ehforwarderbot/0", "ground_truth": "    if module_id:\n        config_path = get_data_path(module_id)\n    else:\n        profile = coordinator.profile\n        config_path = get_base_path() / 'profiles' / profile\n    if not config_path.exists():\n        config_path.mkdir(parents=True)\n    return config_path / \"config.{}\".format(ext)", "fpath_tuple": ["Communications", "ehforwarderbot", "ehforwarderbot", "utils.py"], "context_start_lineno": 87, "line_no": 102, "id": "ehforwarderbot.utils.get_config_path", "target_function_prompt": "def get_config_path(module_id: ModuleID = None, ext: str = 'yaml') -> Path:", "function_signature": "def get_config_path(module_id: ModuleID = None, ext: str = 'yaml') -> Path:"}}
{"prompt": "def linear_buckets(start: float, step: float, end: float) -> tuple[float, ...]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/17", "ground_truth": "    assert start > 0.0\n    assert start < end\n    assert step > 0.0\n\n    bound = start\n    buckets: list[float] = []\n    while bound < end:\n        buckets.append(bound)\n        bound += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "utils", "metrics.py"], "context_start_lineno": 58, "line_no": 66, "id": "bentoml._internal.utils.metrics.linear_buckets", "target_function_prompt": "def linear_buckets(start: float, step: float, end: float) -> tuple[float, ...]:", "function_signature": "def linear_buckets(start: float, step: float, end: float) -> tuple[float, ...]:"}}
{"prompt": "    def new_csrf_token(self, request):", "metadata": {"task_id": "Internet/pyramid/42", "ground_truth": "        token = self._token_factory()\n        request.cookies[self.cookie_name] = token\n\n        def set_cookie(request, response):\n            self.cookie_profile.set_cookies(response, token)\n\n        request.add_response_callback(set_cookie)\n        return token", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "csrf.py"], "context_start_lineno": 135, "line_no": 137, "id": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "target_function_prompt": "    def new_csrf_token(self, request):", "function_signature": "    def new_csrf_token(self, request):"}}
{"prompt": "    def __getattr__(self, name):\n        # allow directive extension names to work", "metadata": {"task_id": "Internet/pyramid/43", "ground_truth": "        from pyramid.config.actions import action_method\n        directives = getattr(self.registry, '_directives', {})\n        c = directives.get(name)\n        if c is None:\n            raise AttributeError(name)\n        c, action_wrap = c\n        if action_wrap:\n            c = action_method(c)\n        # Create a bound method (works on both Py2 and Py3)\n        # http://stackoverflow.com/a/1015405/209039\n        m = c.__get__(self, self.__class__)\n        return m", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "__init__.py"], "context_start_lineno": 697, "line_no": 699, "id": "pyramid.config.Configurator.__getattr__", "target_function_prompt": "    def __getattr__(self, name):\n        # allow directive extension names to work", "function_signature": "    def __getattr__(self, name):\n        # allow directive extension names to work"}}
{"prompt": "def recursepath(path, reverse=False):\n    # type: (Text, bool) -> List[Text]", "metadata": {"task_id": "System/fs/14", "ground_truth": "    if path in \"/\":\n        return [\"/\"]\n\n    path = abspath(normpath(path)) + \"/\"\n\n    paths = [\"/\"]\n    find = path.find\n    append = paths.append\n    pos = 1\n    len_path = len(path)\n\n    while pos < len_path:\n        pos = find(\"/\", pos)\n        append(path[:pos])\n        pos += 1\n\n    if reverse:\n        return paths[::-1]\n    return paths", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 115, "line_no": 132, "id": "fs.path.recursepath", "target_function_prompt": "def recursepath(path, reverse=False):\n    # type: (Text, bool) -> List[Text]", "function_signature": "def recursepath(path, reverse=False):\n    # type: (Text, bool) -> List[Text]"}}
{"prompt": "    def create_concrete(self):", "metadata": {"task_id": "Communications/chatette/11", "ground_truth": "        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )", "fpath_tuple": ["Communications", "chatette", "chatette", "parsing", "__init__.py"], "context_start_lineno": 161, "line_no": 162, "id": "chatette.parsing.IntentDefBuilder.create_concrete", "target_function_prompt": "    def create_concrete(self):", "function_signature": "    def create_concrete(self):"}}
{"prompt": "def _get_word_ngrams(n, sentences):", "metadata": {"task_id": "Internet/sumy/10", "ground_truth": "    assert (len(sentences) > 0)\n    assert (n > 0)\n\n    words = set()\n    for sentence in sentences:\n        words.update(_get_ngrams(n, _split_into_words([sentence])))\n\n    return words", "fpath_tuple": ["Internet", "sumy", "sumy", "evaluation", "rouge.py"], "context_start_lineno": 26, "line_no": 27, "id": "sumy.evaluation.rouge._get_word_ngrams", "target_function_prompt": "def _get_word_ngrams(n, sentences):", "function_signature": "def _get_word_ngrams(n, sentences):"}}
{"prompt": "    def new_key(self, key_name=None):", "metadata": {"task_id": "Internet/boto/43", "ground_truth": "        if not key_name:\n            raise ValueError('Empty key names are not allowed')\n        return self.key_class(self, key_name)", "fpath_tuple": ["Internet", "boto", "boto", "s3", "bucket.py"], "context_start_lineno": 613, "line_no": 623, "id": "boto.s3.bucket.Bucket.new_key", "target_function_prompt": "    def new_key(self, key_name=None):", "function_signature": "    def new_key(self, key_name=None):"}}
{"prompt": "def host_info_getter(func, name=None):", "metadata": {"task_id": "Utilities/sacred/21", "ground_truth": "    warnings.warn(\n        \"The host_info_getter is deprecated. \"\n        \"Please use the `additional_host_info` argument\"\n        \" in the Experiment constructor.\",\n        DeprecationWarning,\n    )\n    name = name or func.__name__\n    host_info_gatherers[name] = func\n    return func", "fpath_tuple": ["Utilities", "sacred", "sacred", "host_info.py"], "context_start_lineno": 83, "line_no": 106, "id": "sacred.host_info.host_info_getter", "target_function_prompt": "def host_info_getter(func, name=None):", "function_signature": "def host_info_getter(func, name=None):"}}
{"prompt": "    def correct_word(self, word):", "metadata": {"task_id": "Text-Processing/pycorrector/3", "ground_truth": "        self.check_init()\n        candi_prob = {i: self.probability(i) for i in self.candidates(word)}\n        sort_candi_prob = sorted(candi_prob.items(), key=operator.itemgetter(1))\n        return sort_candi_prob[-1][0]", "fpath_tuple": ["Text-Processing", "pycorrector", "pycorrector", "en_spell.py"], "context_start_lineno": 98, "line_no": 105, "id": "pycorrector.en_spell.EnSpell.correct_word", "target_function_prompt": "    def correct_word(self, word):", "function_signature": "    def correct_word(self, word):"}}
{"prompt": "    def gather_named_configs(\n        self,\n    ) -> Generator[Tuple[str, Union[ConfigScope, ConfigDict, str]], None, None]:", "metadata": {"task_id": "Utilities/sacred/22", "ground_truth": "        for ingredient, _ in self.traverse_ingredients():\n            for config_name, config in ingredient.named_configs.items():\n                config_name = join_paths(ingredient.path, config_name)\n                config_name = self.post_process_name(config_name, ingredient)\n                yield config_name, config", "fpath_tuple": ["Utilities", "sacred", "sacred", "ingredient.py"], "context_start_lineno": 315, "line_no": 327, "id": "sacred.ingredient.Ingredient.gather_named_configs", "target_function_prompt": "    def gather_named_configs(\n        self,\n    ) -> Generator[Tuple[str, Union[ConfigScope, ConfigDict, str]], None, None]:", "function_signature": "    def gather_named_configs(\n        self,\n    ) -> Generator[Tuple[str, Union[ConfigScope, ConfigDict, str]], None, None]:"}}
{"prompt": "    def add(self, key, val):", "metadata": {"task_id": "Utilities/boltons/38", "ground_truth": "        if key not in self.data:\n            self.data[key] = set()\n        self.data[key].add(val)\n        if val not in self.inv.data:\n            self.inv.data[val] = set()\n        self.inv.data[val].add(key)", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 966, "line_no": 967, "id": "boltons.dictutils.ManyToMany.add", "target_function_prompt": "    def add(self, key, val):", "function_signature": "    def add(self, key, val):"}}
{"prompt": "def parse_datetime(value):", "metadata": {"task_id": "Communications/hl7/0", "ground_truth": "    if not value:\n        return None\n\n    # Split off optional timezone\n    dt_match = DTM_TZ_RE.match(value)\n    if not dt_match:\n        raise ValueError(\"Malformed HL7 datetime {0}\".format(value))\n    dtm = dt_match.group(1)\n    tzh = dt_match.group(2)\n    tzm = dt_match.group(3)\n    if tzh and tzm:\n        minutes = int(tzh) * 60\n        minutes += math.copysign(int(tzm), minutes)\n        tzinfo = _UTCOffset(minutes)\n    else:\n        tzinfo = None\n\n    precision = len(dtm)\n\n    if precision >= 4:\n        year = int(dtm[0:4])\n    else:\n        raise ValueError(\"Malformed HL7 datetime {0}\".format(value))\n\n    if precision >= 6:\n        month = int(dtm[4:6])\n    else:\n        month = 1\n\n    if precision >= 8:\n        day = int(dtm[6:8])\n    else:\n        day = 1\n\n    if precision >= 10:\n        hour = int(dtm[8:10])\n    else:\n        hour = 0\n\n    if precision >= 12:\n        minute = int(dtm[10:12])\n    else:\n        minute = 0\n\n    if precision >= 14:\n        delta = datetime.timedelta(seconds=float(dtm[12:]))\n        second = delta.seconds\n        microsecond = delta.microseconds\n    else:\n        second = 0\n        microsecond = 0\n\n    return datetime.datetime(\n        year, month, day, hour, minute, second, microsecond, tzinfo=tzinfo\n    )", "fpath_tuple": ["Communications", "hl7", "hl7", "datatypes.py"], "context_start_lineno": 28, "line_no": 36, "id": "hl7.datatypes.parse_datetime", "target_function_prompt": "def parse_datetime(value):", "function_signature": "def parse_datetime(value):"}}
{"prompt": "    def add_spendable(self, spendable):", "metadata": {"task_id": "Security/pycoin/20", "ground_truth": "        item_bytes = spendable.tx_hash + struct.pack(\"<L\", spendable.tx_out_index)\n        self.add_item(item_bytes)", "fpath_tuple": ["Security", "pycoin", "pycoin", "bloomfilter.py"], "context_start_lineno": 47, "line_no": 48, "id": "pycoin.bloomfilter.BloomFilter.add_spendable", "target_function_prompt": "    def add_spendable(self, spendable):", "function_signature": "    def add_spendable(self, spendable):"}}
{"prompt": "    def setdefault(self, key, default=None):", "metadata": {"task_id": "Utilities/boltons/39", "ground_truth": "        with self._lock:\n            try:\n                return self[key]\n            except KeyError:\n                self.soft_miss_count += 1\n                self[key] = default\n                return default", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 294, "line_no": 295, "id": "boltons.cacheutils.LRI.setdefault", "target_function_prompt": "    def setdefault(self, key, default=None):", "function_signature": "    def setdefault(self, key, default=None):"}}
{"prompt": "def http_error(\n    httpexception, errno=None, code=None, error=None, message=None, info=None, details=None\n):", "metadata": {"task_id": "Internet/kinto/21", "ground_truth": "    errno = errno or ERRORS.UNDEFINED\n\n    if isinstance(errno, Enum):\n        errno = errno.value\n\n    body = {\n        \"code\": code or httpexception.code,\n        \"errno\": errno,\n        \"error\": error or httpexception.title,\n        \"message\": message,\n        \"info\": info,\n        \"details\": details or colander.drop,\n    }\n\n    response = httpexception\n    response.errno = errno\n    response.json = ErrorSchema().deserialize(body)\n    response.content_type = \"application/json\"\n    return response", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "errors.py"], "context_start_lineno": 88, "line_no": 103, "id": "kinto.core.errors.http_error", "target_function_prompt": "def http_error(\n    httpexception, errno=None, code=None, error=None, message=None, info=None, details=None\n):", "function_signature": "def http_error(\n    httpexception, errno=None, code=None, error=None, message=None, info=None, details=None\n):"}}
{"prompt": "def relative_major(key):", "metadata": {"task_id": "Multimedia/mingus/21", "ground_truth": "    for couple in keys:\n        if key == couple[1]:\n            return couple[0]\n    raise NoteFormatError(\"'%s' is not a minor key\" % key)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "keys.py"], "context_start_lineno": 144, "line_no": 151, "id": "mingus.core.keys.relative_major", "target_function_prompt": "def relative_major(key):", "function_signature": "def relative_major(key):"}}
{"prompt": "def convert_to_nested_dict(dotted_dict):", "metadata": {"task_id": "Utilities/sacred/23", "ground_truth": "    nested_dict = {}\n    for k, v in iterate_flattened(dotted_dict):\n        set_by_dotted_path(nested_dict, k, v)\n    return nested_dict", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 536, "line_no": 538, "id": "sacred.utils.convert_to_nested_dict", "target_function_prompt": "def convert_to_nested_dict(dotted_dict):", "function_signature": "def convert_to_nested_dict(dotted_dict):"}}
{"prompt": "    def with_package(self, package):", "metadata": {"task_id": "Internet/pyramid/44", "ground_truth": "        configurator = self.__class__(\n            registry=self.registry,\n            package=package,\n            root_package=self.root_package,\n            autocommit=self.autocommit,\n            route_prefix=self.route_prefix,\n            introspection=self.introspection,\n        )\n        configurator.basepath = self.basepath\n        configurator.includepath = self.includepath\n        configurator.info = self.info\n        return configurator", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "__init__.py"], "context_start_lineno": 712, "line_no": 716, "id": "pyramid.config.Configurator.with_package", "target_function_prompt": "    def with_package(self, package):", "function_signature": "    def with_package(self, package):"}}
{"prompt": "    def ls(self, path_glob):", "metadata": {"task_id": "System/mrjob/42", "ground_truth": "        m = _SSH_URI_RE.match(path_glob)\n        addr = m.group('hostname')\n        path_to_ls = m.group('filesystem_path')\n\n        p = self._ssh_launch(\n            addr, ['find', '-L', path_to_ls, '-type', 'f'])\n\n        for line in p.stdout:\n            path = to_unicode(line).rstrip('\\n')\n            yield 'ssh://%s%s' % (addr, path)\n\n        self._ssh_finish_run(p)", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "ssh.py"], "context_start_lineno": 172, "line_no": 173, "id": "mrjob.fs.ssh.SSHFilesystem.ls", "target_function_prompt": "    def ls(self, path_glob):", "function_signature": "    def ls(self, path_glob):"}}
{"prompt": "def dump_mrjob_conf(conf, f):", "metadata": {"task_id": "System/mrjob/43", "ground_truth": "    if yaml:\n        _dump_yaml_with_clear_tags(conf, f, default_flow_style=False)\n    else:\n        json.dump(conf, f, indent=2)\n    f.flush()", "fpath_tuple": ["System", "mrjob", "mrjob", "conf.py"], "context_start_lineno": 345, "line_no": 360, "id": "mrjob.conf.dump_mrjob_conf", "target_function_prompt": "def dump_mrjob_conf(conf, f):", "function_signature": "def dump_mrjob_conf(conf, f):"}}
{"prompt": "    def load_bytecode(self, bucket: Bucket) -> None:", "metadata": {"task_id": "Internet/Jinja2/10", "ground_truth": "        try:\n            code = self.client.get(self.prefix + bucket.key)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise\n        else:\n            bucket.bytecode_from_string(code)", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "bccache.py"], "context_start_lineno": 385, "line_no": 386, "id": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "target_function_prompt": "    def load_bytecode(self, bucket: Bucket) -> None:", "function_signature": "    def load_bytecode(self, bucket: Bucket) -> None:"}}
{"prompt": "    def fields(self):", "metadata": {"task_id": "Internet/djangorestframework/10", "ground_truth": "        from rest_framework.utils.serializer_helpers import BindingDict\n        fields = BindingDict(self)\n        for key, value in self.get_fields().items():\n            fields[key] = value\n        return fields", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "serializers.py"], "context_start_lineno": 344, "line_no": 351, "id": "rest_framework.serializers.Serializer.fields", "target_function_prompt": "    def fields(self):", "function_signature": "    def fields(self):"}}
{"prompt": "    def get_flags(self, messages):", "metadata": {"task_id": "Communications/IMAPClient/15", "ground_truth": "        response = self.fetch(messages, [\"FLAGS\"])\n        return self._filter_fetch_dict(response, b\"FLAGS\")", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1231, "line_no": 1238, "id": "imapclient.imapclient.IMAPClient.get_flags", "target_function_prompt": "    def get_flags(self, messages):", "function_signature": "    def get_flags(self, messages):"}}
{"prompt": "    def post_refresh_callback(self, authorizer):", "metadata": {"task_id": "Utilities/praw/4", "ground_truth": "        with open(self._filename, \"w\") as fp:\n            fp.write(authorizer.refresh_token)", "fpath_tuple": ["Utilities", "praw", "praw", "util", "token_manager.py"], "context_start_lineno": 87, "line_no": 89, "id": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "target_function_prompt": "    def post_refresh_callback(self, authorizer):", "function_signature": "    def post_refresh_callback(self, authorizer):"}}
{"prompt": "def make_grouping_by_index(schema, flat_values):", "metadata": {"task_id": "Software-Development/dash/8", "ground_truth": "    def _perform_make_grouping_like(value, next_values):\n        if isinstance(value, (tuple, list)):\n            return list(\n                _perform_make_grouping_like(el, next_values)\n                for i, el in enumerate(value)\n            )\n\n        if isinstance(value, dict):\n            return {\n                k: _perform_make_grouping_like(v, next_values)\n                for i, (k, v) in enumerate(value.items())\n            }\n\n        return next_values.pop(0)\n\n    if not isinstance(flat_values, list):\n        raise ValueError(\n            \"The flat_values argument must be a list. \"\n            f\"Received value of type {type(flat_values)}\"\n        )\n\n    expected_length = len(flatten_grouping(schema))\n    if len(flat_values) != expected_length:\n        raise ValueError(\n            f\"The specified grouping pattern requires {expected_length} \"\n            f\"elements but received {len(flat_values)}\\n\"\n            f\"    Grouping pattern: {repr(schema)}\\n\"\n            f\"    Values: {flat_values}\"\n        )\n\n    return _perform_make_grouping_like(schema, list(flat_values))", "fpath_tuple": ["Software-Development", "dash", "dash", "_grouping.py"], "context_start_lineno": 67, "line_no": 79, "id": "dash._grouping.make_grouping_by_index", "target_function_prompt": "def make_grouping_by_index(schema, flat_values):", "function_signature": "def make_grouping_by_index(schema, flat_values):"}}
{"prompt": "def operation_definition(servicename, operationname):", "metadata": {"task_id": "Security/trailscraper/3", "ground_truth": "    with open(service_definition_file(servicename), encoding=\"UTF-8\") as definition_file:\n        service_definition = json.loads(definition_file.read())\n        return service_definition['operations'][operationname]", "fpath_tuple": ["Security", "trailscraper", "trailscraper", "boto_service_definitions.py"], "context_start_lineno": 31, "line_no": 33, "id": "trailscraper.boto_service_definitions.operation_definition", "target_function_prompt": "def operation_definition(servicename, operationname):", "function_signature": "def operation_definition(servicename, operationname):"}}
{"prompt": "", "metadata": {"task_id": "Database/datasette/24", "ground_truth": "    @classmethod\n    def text(cls, body, status=200, headers=None):\n        return cls(\n            str(body),\n            status=status,\n            headers=headers,", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "asgi.py"], "context_start_lineno": 392, "line_no": 393, "id": "datasette.utils.asgi.Response.text", "target_function_prompt": "", "function_signature": ""}}
{"prompt": "def _write_config_file(text):", "metadata": {"task_id": "Database/alembic/16", "ground_truth": "    cfg = _testing_config()\n    with open(cfg.config_file_name, \"w\") as f:\n        f.write(text)\n    return cfg", "fpath_tuple": ["Database", "alembic", "alembic", "testing", "env.py"], "context_start_lineno": 240, "line_no": 241, "id": "alembic.testing.env._write_config_file", "target_function_prompt": "def _write_config_file(text):", "function_signature": "def _write_config_file(text):"}}
{"prompt": "def regex_match(password, _regexen=REGEXEN, _ranked_dictionaries=RANKED_DICTIONARIES):", "metadata": {"task_id": "Security/zxcvbn-python/7", "ground_truth": "    matches = []\n    for name, regex in _regexen.items():\n        for rx_match in regex.finditer(password):\n            matches.append({\n                'pattern': 'regex',\n                'token': rx_match.group(0),\n                'i': rx_match.start(),\n                'j': rx_match.end()-1,\n                'regex_name': name,\n                'regex_match': rx_match,\n            })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "matching.py"], "context_start_lineno": 443, "line_no": 444, "id": "zxcvbn.matching.regex_match", "target_function_prompt": "def regex_match(password, _regexen=REGEXEN, _ranked_dictionaries=RANKED_DICTIONARIES):", "function_signature": "def regex_match(password, _regexen=REGEXEN, _ranked_dictionaries=RANKED_DICTIONARIES):"}}
{"prompt": "    def from_chord_shorthand(self, shorthand):", "metadata": {"task_id": "Multimedia/mingus/22", "ground_truth": "        self.empty()\n        self.add_notes(chords.from_shorthand(shorthand))\n        return self", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "note_container.py"], "context_start_lineno": 111, "line_no": 121, "id": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "target_function_prompt": "    def from_chord_shorthand(self, shorthand):", "function_signature": "    def from_chord_shorthand(self, shorthand):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/44", "ground_truth": "    from boto.regioninfo import connect\n    return connect('cloudwatch', region_name,\n                   connection_cls=CloudWatchConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "cloudwatch", "__init__.py"], "context_start_lineno": 47, "line_no": 58, "id": "boto.ec2.cloudwatch.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def get_bootstrap_from_recipes(cls, recipes, ctx):", "metadata": {"task_id": "Utilities/python-for-android/14", "ground_truth": "        known_web_packages = {\"flask\"}  # to pick webview over service_only\n        recipes_with_deps_lists = expand_dependencies(recipes, ctx)\n        acceptable_bootstraps = cls.get_usable_bootstraps_for_recipes(\n            recipes, ctx\n        )\n\n        def have_dependency_in_recipes(dep):\n            for dep_list in recipes_with_deps_lists:\n                if dep in dep_list:\n                    return True\n            return False\n\n        # Special rule: return SDL2 bootstrap if there's an sdl2 dep:\n        if (have_dependency_in_recipes(\"sdl2\") and\n                \"sdl2\" in [b.name for b in acceptable_bootstraps]\n                ):\n            info('Using sdl2 bootstrap since it is in dependencies')\n            return cls.get_bootstrap(\"sdl2\", ctx)\n\n        # Special rule: return \"webview\" if we depend on common web recipe:\n        for possible_web_dep in known_web_packages:\n            if have_dependency_in_recipes(possible_web_dep):\n                # We have a web package dep!\n                if \"webview\" in [b.name for b in acceptable_bootstraps]:\n                    info('Using webview bootstrap since common web packages '\n                         'were found {}'.format(\n                             known_web_packages.intersection(recipes)\n                         ))\n                    return cls.get_bootstrap(\"webview\", ctx)\n\n        prioritized_acceptable_bootstraps = sorted(\n            list(acceptable_bootstraps),\n            key=functools.cmp_to_key(_cmp_bootstraps_by_priority)\n        )\n\n        if prioritized_acceptable_bootstraps:\n            info('Using the highest ranked/first of these: {}'\n                 .format(prioritized_acceptable_bootstraps[0].name))\n            return prioritized_acceptable_bootstraps[0]\n        return None", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "bootstrap.py"], "context_start_lineno": 250, "line_no": 255, "id": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "target_function_prompt": "    def get_bootstrap_from_recipes(cls, recipes, ctx):", "function_signature": "    def get_bootstrap_from_recipes(cls, recipes, ctx):"}}
{"prompt": "def translate_jobconf_dict(jobconf, hadoop_version=None):", "metadata": {"task_id": "System/mrjob/44", "ground_truth": "    translated_jobconf = jobconf.copy()\n    translation_warnings = {}\n\n    for variable, value in jobconf.items():\n        if hadoop_version:\n            variants = [translate_jobconf(variable, hadoop_version)]\n        else:\n            variants = translate_jobconf_for_all_versions(variable)\n\n        for variant in variants:\n            if variant in jobconf:\n                # this happens if variant == variable or\n                # if the variant was in jobconf to start with\n                continue\n\n            translated_jobconf[variant] = value\n\n            if hadoop_version:\n                translation_warnings[variable] = variant\n\n    if translation_warnings:\n        log.warning(\"Detected hadoop configuration property names that\"\n                    \" do not match hadoop version %s:\"\n                    \"\\nThe have been translated as follows\\n %s\",\n                    hadoop_version,\n                    '\\n'.join([\n                        \"%s: %s\" % (variable, variant) for variable, variant\n                        in sorted(translation_warnings.items())]))\n\n    return translated_jobconf", "fpath_tuple": ["System", "mrjob", "mrjob", "compat.py"], "context_start_lineno": 676, "line_no": 685, "id": "mrjob.compat.translate_jobconf_dict", "target_function_prompt": "def translate_jobconf_dict(jobconf, hadoop_version=None):", "function_signature": "def translate_jobconf_dict(jobconf, hadoop_version=None):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/45", "ground_truth": "    from boto.regioninfo import connect\n    from boto.configservice.layer1 import ConfigServiceConnection\n    return connect('configservice', region_name,\n                   connection_cls=ConfigServiceConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "configservice", "__init__.py"], "context_start_lineno": 37, "line_no": 38, "id": "boto.configservice.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def add(self, name, val, after=None, before=None):", "metadata": {"task_id": "Internet/pyramid/45", "ground_truth": "        if name in self.names:\n            self.remove(name)\n        self.names.append(name)\n        self.name2val[name] = val\n        if after is None and before is None:\n            before = self.default_before\n            after = self.default_after\n        if after is not None:\n            if not is_nonstr_iter(after):\n                after = (after,)\n            self.name2after[name] = after\n            self.order += [(u, name) for u in after]\n            self.req_after.add(name)\n        if before is not None:\n            if not is_nonstr_iter(before):\n                before = (before,)\n            self.name2before[name] = before\n            self.order += [(name, o) for o in before]\n            self.req_before.add(name)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "util.py"], "context_start_lineno": 461, "line_no": 480, "id": "pyramid.util.TopologicalSorter.add", "target_function_prompt": "    def add(self, name, val, after=None, before=None):", "function_signature": "    def add(self, name, val, after=None, before=None):"}}
{"prompt": "    def from_index(cls, index: Index) -> CreateIndexOp:", "metadata": {"task_id": "Database/alembic/17", "ground_truth": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            sqla_compat._get_index_expressions(index),\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.kwargs,\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 896, "line_no": 897, "id": "alembic.operations.ops.CreateIndexOp.from_index", "target_function_prompt": "    def from_index(cls, index: Index) -> CreateIndexOp:", "function_signature": "    def from_index(cls, index: Index) -> CreateIndexOp:"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/46", "ground_truth": "    from boto.regioninfo import connect\n    return connect('ses', region_name, connection_cls=SESConnection,\n                   **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "ses", "__init__.py"], "context_start_lineno": 37, "line_no": 49, "id": "boto.ses.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def get_table_name(self, postfix, old=False):", "metadata": {"task_id": "Utilities/stellar/1", "ground_truth": "        if not self.snapshot:\n            raise Exception('Table name requires snapshot')\n        if not self.snapshot.hash:\n            raise Exception('Snapshot hash is empty.')\n\n        if old:\n            return 'stellar_%s_%s_%s' % (\n                self.table_name,\n                self.snapshot.hash,\n                postfix\n            )\n        else:\n            return 'stellar_%s' % hashlib.md5(\n                ('%s|%s|%s' % (\n                    self.table_name,\n                    self.snapshot.hash,\n                    postfix\n                )).encode('utf-8')\n            ).hexdigest()[0:16]", "fpath_tuple": ["Utilities", "stellar", "stellar", "models.py"], "context_start_lineno": 46, "line_no": 47, "id": "stellar.models.Table.get_table_name", "target_function_prompt": "    def get_table_name(self, postfix, old=False):", "function_signature": "    def get_table_name(self, postfix, old=False):"}}
{"prompt": "    def add_member(self, name: str, uid: ChatID, alias: Optional[str] = None,\n                   id: ChatID = ChatID(\"\"),\n                   vendor_specific: Dict[str, Any] = None, description: str = \"\",\n                   middleware: Optional[Middleware] = None) -> ChatMember:", "metadata": {"task_id": "Communications/ehforwarderbot/1", "ground_truth": "        if id:\n            warnings.warn(\"`id` argument is deprecated, use `uid` instead.\", DeprecationWarning)\n            uid = uid or id\n        member = ChatMember(self, name=name, alias=alias, uid=uid,\n                            vendor_specific=vendor_specific, description=description,\n                            middleware=middleware)\n        self.members.append(member)\n        return member", "fpath_tuple": ["Communications", "ehforwarderbot", "ehforwarderbot", "chat.py"], "context_start_lineno": 501, "line_no": 532, "id": "ehforwarderbot.chat.Chat.add_member", "target_function_prompt": "    def add_member(self, name: str, uid: ChatID, alias: Optional[str] = None,\n                   id: ChatID = ChatID(\"\"),\n                   vendor_specific: Dict[str, Any] = None, description: str = \"\",\n                   middleware: Optional[Middleware] = None) -> ChatMember:", "function_signature": "    def add_member(self, name: str, uid: ChatID, alias: Optional[str] = None,\n                   id: ChatID = ChatID(\"\"),\n                   vendor_specific: Dict[str, Any] = None, description: str = \"\",\n                   middleware: Optional[Middleware] = None) -> ChatMember:"}}
{"prompt": "    def _topological_sort(\n        self,\n        revisions: Collection[Revision],\n        heads: Any,\n    ) -> List[str]:", "metadata": {"task_id": "Database/alembic/18", "ground_truth": "        id_to_rev = self._revision_map\n\n        def get_ancestors(rev_id):\n            return {\n                r.revision\n                for r in self._get_ancestor_nodes([id_to_rev[rev_id]])\n            }\n\n        todo = {d.revision for d in revisions}\n\n        # Use revision map (ordered dict) key order to pre-sort.\n        inserted_order = list(self._revision_map)\n\n        current_heads = list(\n            sorted(\n                {d.revision for d in heads if d.revision in todo},\n                key=inserted_order.index,\n            )\n        )\n        ancestors_by_idx = [get_ancestors(rev_id) for rev_id in current_heads]\n\n        output = []\n\n        current_candidate_idx = 0\n        while current_heads:\n            candidate = current_heads[current_candidate_idx]\n\n            for check_head_index, ancestors in enumerate(ancestors_by_idx):\n                # scan all the heads.  see if we can continue walking\n                # down the current branch indicated by current_candidate_idx.\n                if (\n                    check_head_index != current_candidate_idx\n                    and candidate in ancestors\n                ):\n                    current_candidate_idx = check_head_index\n                    # nope, another head is dependent on us, they have\n                    # to be traversed first\n                    break\n            else:\n                # yup, we can emit\n                if candidate in todo:\n                    output.append(candidate)\n                    todo.remove(candidate)\n\n                # now update the heads with our ancestors.\n\n                candidate_rev = id_to_rev[candidate]\n                assert candidate_rev is not None\n\n                heads_to_add = [\n                    r\n                    for r in candidate_rev._normalized_down_revisions\n                    if r in todo and r not in current_heads\n                ]\n\n                if not heads_to_add:\n                    # no ancestors, so remove this head from the list\n                    del current_heads[current_candidate_idx]\n                    del ancestors_by_idx[current_candidate_idx]\n                    current_candidate_idx = max(current_candidate_idx - 1, 0)\n                else:\n                    if (\n                        not candidate_rev._normalized_resolved_dependencies\n                        and len(candidate_rev._versioned_down_revisions) == 1\n                    ):\n                        current_heads[current_candidate_idx] = heads_to_add[0]\n\n                        # for plain movement down a revision line without\n                        # any mergepoints, branchpoints, or deps, we\n                        # can update the ancestors collection directly\n                        # by popping out the candidate we just emitted\n                        ancestors_by_idx[current_candidate_idx].discard(\n                            candidate\n                        )\n\n                    else:\n                        # otherwise recalculate it again, things get\n                        # complicated otherwise.  This can possibly be\n                        # improved to not run the whole ancestor thing\n                        # each time but it was getting complicated\n                        current_heads[current_candidate_idx] = heads_to_add[0]\n                        current_heads.extend(heads_to_add[1:])\n                        ancestors_by_idx[\n                            current_candidate_idx\n                        ] = get_ancestors(heads_to_add[0])\n                        ancestors_by_idx.extend(\n                            get_ancestors(head) for head in heads_to_add[1:]\n                        )\n\n        assert not todo\n        return output", "fpath_tuple": ["Database", "alembic", "alembic", "script", "revision.py"], "context_start_lineno": 911, "line_no": 923, "id": "alembic.script.revision.RevisionMap._topological_sort", "target_function_prompt": "    def _topological_sort(\n        self,\n        revisions: Collection[Revision],\n        heads: Any,\n    ) -> List[str]:", "function_signature": "    def _topological_sort(\n        self,\n        revisions: Collection[Revision],\n        heads: Any,\n    ) -> List[str]:"}}
{"prompt": "    def error_message(self) -> str:", "metadata": {"task_id": "Utilities/praw/5", "ground_truth": "        error_str = self.error_type\n        if self.message:\n            error_str += f\": {self.message!r}\"\n        if self.field:\n            error_str += f\" on field {self.field!r}\"\n        return error_str", "fpath_tuple": ["Utilities", "praw", "praw", "exceptions.py"], "context_start_lineno": 23, "line_no": 25, "id": "praw.exceptions.RedditErrorItem.error_message", "target_function_prompt": "    def error_message(self) -> str:", "function_signature": "    def error_message(self) -> str:"}}
{"prompt": "def score_sessions(\n    data: pd.DataFrame, session_column: str, window_length: int\n) -> pd.DataFrame:", "metadata": {"task_id": "Security/msticpy/7", "ground_truth": "    from .model import Model\n    from ...common.exceptions import MsticpyException\n    if not isinstance(data, pd.DataFrame):\n        raise MsticpyException(\"`data` should be a pandas dataframe\")\n    if session_column not in data.columns:\n        raise MsticpyException(f'\"{session_column}\" should be a column in the `data`')\n\n    sessions_df = data.copy()\n    sessions = sessions_df[session_column].values.tolist()\n\n    model = Model(sessions=sessions)\n    model.train()\n    model.compute_rarest_windows(\n        window_len=window_length, use_geo_mean=False, use_start_end_tokens=True\n    )\n\n    sessions_df[\n        f\"rarest_window{window_length}_likelihood\"\n    ] = model.rare_window_likelihoods[window_length]\n    sessions_df[f\"rarest_window{window_length}\"] = model.rare_windows[window_length]\n\n    return sessions_df", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "anomalous.py"], "context_start_lineno": 18, "line_no": 56, "id": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "target_function_prompt": "def score_sessions(\n    data: pd.DataFrame, session_column: str, window_length: int\n) -> pd.DataFrame:", "function_signature": "def score_sessions(\n    data: pd.DataFrame, session_column: str, window_length: int\n) -> pd.DataFrame:"}}
{"prompt": "def to_unicode(object):", "metadata": {"task_id": "Internet/sumy/11", "ground_truth": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        # try decode instance to unicode\n        return instance_to_unicode(object)", "fpath_tuple": ["Internet", "sumy", "sumy", "_compat.py"], "context_start_lineno": 59, "line_no": 60, "id": "sumy._compat.to_unicode", "target_function_prompt": "def to_unicode(object):", "function_signature": "def to_unicode(object):"}}
{"prompt": "def crossval(clf=None, X=None, y=None, folds=10, n=5, path=None):", "metadata": {"task_id": "Utilities/whereami/0", "ground_truth": "    if X is None or y is None:\n        X, y = get_train_data(path)\n    if len(X) < folds:\n        raise ValueError('There are not enough samples ({}). Need at least {}.'.format(len(X), folds))\n    clf = clf or get_model(path)\n    tot = 0\n    print(\"KFold folds={}, running {} times\".format(folds, n))\n    for i in range(n):\n        res = cross_val_score(clf, X, y, cv=folds).mean()\n        tot += res\n        print(\"{}/{}: {}\".format(i + 1, n, res))\n    print(\"-------- total --------\")\n    print(tot / n)\n    return tot / n", "fpath_tuple": ["Utilities", "whereami", "whereami", "predict.py"], "context_start_lineno": 24, "line_no": 25, "id": "whereami.predict.crossval", "target_function_prompt": "def crossval(clf=None, X=None, y=None, folds=10, n=5, path=None):", "function_signature": "def crossval(clf=None, X=None, y=None, folds=10, n=5, path=None):"}}
{"prompt": "def find_available_locales(providers: List[str]) -> List[str]:", "metadata": {"task_id": "Software-Development/Faker/15", "ground_truth": "    available_locales = set()\n\n    for provider_path in providers:\n        provider_module = import_module(provider_path)\n        if getattr(provider_module, \"localized\", False):\n            langs = list_module(provider_module)\n            available_locales.update(langs)\n    return sorted(available_locales)", "fpath_tuple": ["Software-Development", "Faker", "faker", "utils", "loading.py"], "context_start_lineno": 40, "line_no": 41, "id": "faker.utils.loading.find_available_locales", "target_function_prompt": "def find_available_locales(providers: List[str]) -> List[str]:", "function_signature": "def find_available_locales(providers: List[str]) -> List[str]:"}}
{"prompt": "def filter_size_required(element_count, false_positive_probability):\n    # The size S of the filter in bytes is given by\n    # (-1 / pow(log(2), 2) * N * log(P)) / 8\n    # Of course you must ensure it does not go over the maximum size\n    # (36,000: selected as it represents a filter of 20,000 items with false\n    # positive rate of < 0.1% or 10,000 items and a false positive rate of < 0.0001%).", "metadata": {"task_id": "Security/pycoin/21", "ground_truth": "    lfpp = math.log(false_positive_probability)\n    return min(36000, int(((-1 / pow(LOG_2, 2) * element_count * lfpp)+7) // 8))", "fpath_tuple": ["Security", "pycoin", "pycoin", "bloomfilter.py"], "context_start_lineno": 9, "line_no": 15, "id": "pycoin.bloomfilter.filter_size_required", "target_function_prompt": "def filter_size_required(element_count, false_positive_probability):\n    # The size S of the filter in bytes is given by\n    # (-1 / pow(log(2), 2) * N * log(P)) / 8\n    # Of course you must ensure it does not go over the maximum size\n    # (36,000: selected as it represents a filter of 20,000 items with false\n    # positive rate of < 0.1% or 10,000 items and a false positive rate of < 0.0001%).", "function_signature": "def filter_size_required(element_count, false_positive_probability):\n    # The size S of the filter in bytes is given by\n    # (-1 / pow(log(2), 2) * N * log(P)) / 8\n    # Of course you must ensure it does not go over the maximum size\n    # (36,000: selected as it represents a filter of 20,000 items with false\n    # positive rate of < 0.1% or 10,000 items and a false positive rate of < 0.0001%)."}}
{"prompt": "def iterate_flattened_separately(dictionary, manually_sorted_keys=None):", "metadata": {"task_id": "Utilities/sacred/24", "ground_truth": "    manually_sorted_keys = manually_sorted_keys or []\n\n    def get_order(key_and_value):\n        key, value = key_and_value\n        if key in manually_sorted_keys:\n            return 0, manually_sorted_keys.index(key)\n        elif not is_non_empty_dict(value):\n            return 1, key\n        else:\n            return 2, key\n\n    for key, value in sorted(dictionary.items(), key=get_order):\n        if is_non_empty_dict(value):\n            yield key, PATHCHANGE\n            for k, val in iterate_flattened_separately(value, manually_sorted_keys):\n                yield join_paths(key, k), val\n        else:\n            yield key, value", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 409, "line_no": 417, "id": "sacred.utils.iterate_flattened_separately", "target_function_prompt": "def iterate_flattened_separately(dictionary, manually_sorted_keys=None):", "function_signature": "def iterate_flattened_separately(dictionary, manually_sorted_keys=None):"}}
{"prompt": "    def _args_for_streaming_step(self, step_num):", "metadata": {"task_id": "System/mrjob/45", "ground_truth": "        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n        if not hadoop_streaming_jar:\n            raise Exception('no Hadoop streaming jar')\n\n        return (self.get_hadoop_bin() + ['jar', hadoop_streaming_jar] +\n                self._hadoop_streaming_jar_args(step_num))", "fpath_tuple": ["System", "mrjob", "mrjob", "hadoop.py"], "context_start_lineno": 478, "line_no": 479, "id": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "target_function_prompt": "    def _args_for_streaming_step(self, step_num):", "function_signature": "    def _args_for_streaming_step(self, step_num):"}}
{"prompt": "def is_perfect_consonant(note1, note2, include_fourths=True):", "metadata": {"task_id": "Multimedia/mingus/23", "ground_truth": "    dhalf = measure(note1, note2)\n    return dhalf in [0, 7] or include_fourths and dhalf == 5", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "intervals.py"], "context_start_lineno": 506, "line_no": 515, "id": "mingus.core.intervals.is_perfect_consonant", "target_function_prompt": "def is_perfect_consonant(note1, note2, include_fourths=True):", "function_signature": "def is_perfect_consonant(note1, note2, include_fourths=True):"}}
{"prompt": "    def get(self):", "metadata": {"task_id": "Internet/pyramid/46", "ground_truth": "        try:\n            return self.stack[-1]\n        except IndexError:\n            return self.default()", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "threadlocal.py"], "context_start_lineno": 24, "line_no": 25, "id": "pyramid.threadlocal.ThreadLocalManager.get", "target_function_prompt": "    def get(self):", "function_signature": "    def get(self):"}}
{"prompt": "    def read(self, size=-1):", "metadata": {"task_id": "Security/oletools/0", "ground_truth": "        if self.handle is None:\n            raise IOError('read on closed handle')\n        if self.pos >= self.size:\n            # print('ZipSubFile: read fake at end')\n            return b''   # fake being at the end, even if we are not\n        data = self.handle.read(size)\n        self.pos += len(data)\n        # print('ZipSubFile: read {} bytes, pos now {}'.format(size, self.pos))\n        return data", "fpath_tuple": ["Security", "oletools", "oletools", "ooxml.py"], "context_start_lineno": 311, "line_no": 317, "id": "oletools.ooxml.ZipSubFile.read", "target_function_prompt": "    def read(self, size=-1):", "function_signature": "    def read(self, size=-1):"}}
{"prompt": "def instance_uri_registry(registry, resource_name, **params):", "metadata": {"task_id": "Internet/kinto/22", "ground_truth": "    request = Request.blank(path=\"\")\n    request.registry = registry\n    return instance_uri(request, resource_name, **params)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "utils.py"], "context_start_lineno": 494, "line_no": 500, "id": "kinto.core.utils.instance_uri_registry", "target_function_prompt": "def instance_uri_registry(registry, resource_name, **params):", "function_signature": "def instance_uri_registry(registry, resource_name, **params):"}}
{"prompt": "def main(args: Optional[List[Any]] = None) -> None:", "metadata": {"task_id": "Software-Development/pandas-profiling/2", "ground_truth": "    from ydata_profiling.utils.dataframe import read_pandas\n    parsed_args = parse_args(args)\n    kwargs = vars(parsed_args)\n\n    input_file = Path(kwargs.pop(\"input_file\"))\n    output_file = kwargs.pop(\"output_file\")\n    if output_file is None:\n        output_file = str(input_file.with_suffix(\".html\"))\n\n    silent = kwargs.pop(\"silent\")\n\n    # read the DataFrame\n    df = read_pandas(input_file)\n\n    # Generate the profiling report\n    p = ProfileReport(\n        df,\n        **kwargs,\n    )\n    p.to_file(Path(output_file), silent=silent)", "fpath_tuple": ["Software-Development", "pandas-profiling", "src", "ydata_profiling", "controller", "console.py"], "context_start_lineno": 98, "line_no": 106, "id": "ydata_profiling.controller.console.main", "target_function_prompt": "def main(args: Optional[List[Any]] = None) -> None:", "function_signature": "def main(args: Optional[List[Any]] = None) -> None:"}}
{"prompt": "def _match_task_log_path(path, application_id=None, job_id=None):", "metadata": {"task_id": "System/mrjob/46", "ground_truth": "    from .ids import _to_job_id\n    m = _PRE_YARN_TASK_LOG_PATH_RE.match(path)\n    if m:\n        if job_id and job_id != _to_job_id(m.group('attempt_id')):\n            return None  # matches, but wrong job_id\n\n        return dict(\n            attempt_id=m.group('attempt_id'),\n            log_type=m.group('log_type'))\n\n    m = _YARN_TASK_LOG_PATH_RE.match(path)\n    if m:\n        if application_id and application_id != m.group('application_id'):\n            return None  # matches, but wrong application_id\n\n        return dict(\n            application_id=m.group('application_id'),\n            container_id=m.group('container_id'),\n            log_type=m.group('log_type'))\n\n    return None", "fpath_tuple": ["System", "mrjob", "mrjob", "logs", "task.py"], "context_start_lineno": 218, "line_no": 229, "id": "mrjob.logs.task._match_task_log_path", "target_function_prompt": "def _match_task_log_path(path, application_id=None, job_id=None):", "function_signature": "def _match_task_log_path(path, application_id=None, job_id=None):"}}
{"prompt": "    def _extract_sublist(self, listing):", "metadata": {"task_id": "Utilities/praw/6", "ground_truth": "        from .listing import ModNoteListing\n        if isinstance(listing, list):\n            return listing[1]  # for submission duplicates\n        elif isinstance(listing, dict):\n            classes = [FlairListing, ModNoteListing]\n\n            for listing_type in classes:\n                if listing_type.CHILD_ATTRIBUTE in listing:\n                    return listing_type(self._reddit, listing)\n            else:\n                raise ValueError(\n                    \"The generator returned a dictionary PRAW didn't recognize.\"\n                    \" File a bug report at PRAW.\"\n                )\n        return listing", "fpath_tuple": ["Utilities", "praw", "praw", "models", "listing", "generator.py"], "context_start_lineno": 68, "line_no": 69, "id": "praw.models.listing.generator.ListingGenerator._extract_sublist", "target_function_prompt": "    def _extract_sublist(self, listing):", "function_signature": "    def _extract_sublist(self, listing):"}}
{"prompt": "def complement(wrapped):", "metadata": {"task_id": "Utilities/boltons/40", "ground_truth": "    if type(wrapped) is _ComplementSet:\n        return wrapped.complemented()\n    if type(wrapped) is frozenset:\n        return _ComplementSet(excluded=wrapped)\n    return _ComplementSet(excluded=set(wrapped))", "fpath_tuple": ["Utilities", "boltons", "boltons", "setutils.py"], "context_start_lineno": 474, "line_no": 554, "id": "boltons.setutils.complement", "target_function_prompt": "def complement(wrapped):", "function_signature": "def complement(wrapped):"}}
{"prompt": "    def remote_addr(self):", "metadata": {"task_id": "Internet/falcon/24", "ground_truth": "        try:\n            value = self.env['REMOTE_ADDR']\n        except KeyError:\n            value = '127.0.0.1'\n\n        return value", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 949, "line_no": 950, "id": "falcon.request.Request.remote_addr", "target_function_prompt": "    def remote_addr(self):", "function_signature": "    def remote_addr(self):"}}
{"prompt": "    def idle_done(self):", "metadata": {"task_id": "Communications/IMAPClient/16", "ground_truth": "        logger.debug(\"< DONE\")\n        self._imap.send(b\"DONE\\r\\n\")\n        return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 983, "line_no": 997, "id": "imapclient.imapclient.IMAPClient.idle_done", "target_function_prompt": "    def idle_done(self):", "function_signature": "    def idle_done(self):"}}
{"prompt": "    def root(self):", "metadata": {"task_id": "Internet/djangorestframework/11", "ground_truth": "        root = self\n        while root.parent is not None:\n            root = root.parent\n        return root", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "fields.py"], "context_start_lineno": 612, "line_no": 616, "id": "rest_framework.fields.Field.root", "target_function_prompt": "    def root(self):", "function_signature": "    def root(self):"}}
{"prompt": "def get_dropbox_folder_location():", "metadata": {"task_id": "Utilities/mackup/2", "ground_truth": "    host_db_path = os.path.join(os.environ[\"HOME\"], \".dropbox/host.db\")\n    try:\n        with open(host_db_path, \"r\") as f_hostdb:\n            data = f_hostdb.read().split()\n    except IOError:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Dropbox install\"))\n    dropbox_home = base64.b64decode(data[1]).decode()\n\n    return dropbox_home", "fpath_tuple": ["Utilities", "mackup", "mackup", "utils.py"], "context_start_lineno": 195, "line_no": 202, "id": "mackup.utils.get_dropbox_folder_location", "target_function_prompt": "def get_dropbox_folder_location():", "function_signature": "def get_dropbox_folder_location():"}}
{"prompt": "    def classify(self, gadget):", "metadata": {"task_id": "Security/barf/4", "ground_truth": "        typed_gadgets = []\n\n        for g_type, g_classifier in self._classifiers.items():\n            try:\n                typed_gadgets += self._classify(gadget, g_classifier, g_type, self._emu_iters)\n            except:\n                import traceback\n\n                print(\"[-] Error classifying gadgets :\")\n                print(gadget)\n                print(\"\")\n                print(traceback.format_exc())\n\n        # Sort and return.\n        return sorted(typed_gadgets, key=lambda g: str(g))", "fpath_tuple": ["Security", "barf", "barf", "analysis", "gadgets", "classifier.py"], "context_start_lineno": 103, "line_no": 106, "id": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "target_function_prompt": "    def classify(self, gadget):", "function_signature": "    def classify(self, gadget):"}}
{"prompt": "def group_diff(options, db):", "metadata": {"task_id": "Security/capirca/5", "ground_truth": "  nested_rvals = []\n  for ip in options.gmp:\n    nested_rvals.append(get_ip_parents(ip, db))\n  # get just the list of groups, stripping out the networks.\n  group1 = [x[0] for x in nested_rvals[0]]\n  group2 = [x[0] for x in nested_rvals[1]]\n  common = sorted(list(set(group1) & set(group2)))\n  diff1 = sorted(list(set(group1) - set(group2)))\n  diff2 = sorted(list(set(group2) - set(group1)))\n  return common, diff1, diff2", "fpath_tuple": ["Security", "capirca", "tools", "cgrep.py"], "context_start_lineno": 300, "line_no": 311, "id": "tools.cgrep.group_diff", "target_function_prompt": "def group_diff(options, db):", "function_signature": "def group_diff(options, db):"}}
{"prompt": "def _latex_item_to_string(item, *, escape=False, as_content=False):", "metadata": {"task_id": "Text-Processing/PyLaTeX/1", "ground_truth": "    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content()\n        else:\n            return item.dumps()\n    elif not isinstance(item, str):\n        item = str(item)\n\n    if escape:\n        item = escape_latex(item)\n\n    return item", "fpath_tuple": ["Text-Processing", "PyLaTeX", "pylatex", "utils.py"], "context_start_lineno": 203, "line_no": 222, "id": "pylatex.utils._latex_item_to_string", "target_function_prompt": "def _latex_item_to_string(item, *, escape=False, as_content=False):", "function_signature": "def _latex_item_to_string(item, *, escape=False, as_content=False):"}}
{"prompt": "    def _build_modifiers_repr(self):", "metadata": {"task_id": "Communications/chatette/12", "ground_truth": "        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers", "fpath_tuple": ["Communications", "chatette", "chatette", "parsing", "__init__.py"], "context_start_lineno": 124, "line_no": 125, "id": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "target_function_prompt": "    def _build_modifiers_repr(self):", "function_signature": "    def _build_modifiers_repr(self):"}}
{"prompt": "def extract_regex(\n    regex: Union[str, Pattern[str]], text: str, replace_entities: bool = True\n) -> List[str]:", "metadata": {"task_id": "Text-Processing/parsel/2", "ground_truth": "    if isinstance(regex, str):\n        regex = re.compile(regex, re.UNICODE)\n\n    if \"extract\" in regex.groupindex:\n        # named group\n        try:\n            extracted = cast(Match[str], regex.search(text)).group(\"extract\")\n        except AttributeError:\n            strings = []\n        else:\n            strings = [extracted] if extracted is not None else []\n    else:\n        # full regex or numbered groups\n        strings = regex.findall(text)\n\n    strings = flatten(strings)\n    if not replace_entities:\n        return strings\n    return [w3lib_replace_entities(s, keep=[\"lt\", \"amp\"]) for s in strings]", "fpath_tuple": ["Text-Processing", "parsel", "parsel", "utils.py"], "context_start_lineno": 57, "line_no": 65, "id": "parsel.utils.extract_regex", "target_function_prompt": "def extract_regex(\n    regex: Union[str, Pattern[str]], text: str, replace_entities: bool = True\n) -> List[str]:", "function_signature": "def extract_regex(\n    regex: Union[str, Pattern[str]], text: str, replace_entities: bool = True\n) -> List[str]:"}}
{"prompt": "    def from_int(self, integer):", "metadata": {"task_id": "Multimedia/mingus/24", "ground_truth": "        self.name = notes.int_to_note(integer % 12)\n        self.octave = integer // 12\n        return self", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "note.py"], "context_start_lineno": 201, "line_no": 210, "id": "mingus.containers.note.Note.from_int", "target_function_prompt": "    def from_int(self, integer):", "function_signature": "    def from_int(self, integer):"}}
{"prompt": "    def convert(self, value):", "metadata": {"task_id": "Internet/falcon/25", "ground_truth": "        if self._num_digits is not None and len(value) != self._num_digits:\n            return None\n\n        # NOTE(kgriffs): int() will accept numbers with preceding or\n        # trailing whitespace, so we need to do our own check. Using\n        # strip() is faster than either a regex or a series of or'd\n        # membership checks via \"in\", esp. as the length of contiguous\n        # numbers in the value grows.\n        if value.strip() != value:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            return None\n\n        if self._min is not None and value < self._min:\n            return None\n\n        if self._max is not None and value > self._max:\n            return None\n\n        return value", "fpath_tuple": ["Internet", "falcon", "falcon", "routing", "converters.py"], "context_start_lineno": 68, "line_no": 69, "id": "falcon.routing.converters.IntConverter.convert", "target_function_prompt": "    def convert(self, value):", "function_signature": "    def convert(self, value):"}}
{"prompt": "def call_with_supported_arguments(fn, **kwargs):", "metadata": {"task_id": "Database/datasette/25", "ground_truth": "    call_with = _gather_arguments(fn, kwargs)\n    return fn(*call_with)", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 1002, "line_no": 1003, "id": "datasette.utils.call_with_supported_arguments", "target_function_prompt": "def call_with_supported_arguments(fn, **kwargs):", "function_signature": "def call_with_supported_arguments(fn, **kwargs):"}}
{"prompt": "    def fmt_logline(msg, detail=None, hint=None, structured=None):", "metadata": {"task_id": "System/wal-e/7", "ground_truth": "        msg_parts = ['MSG: ' + msg]\n\n        if detail is not None:\n            msg_parts.append('DETAIL: ' + detail)\n        if hint is not None:\n            msg_parts.append('HINT: ' + hint)\n\n        # Initialize a fresh dictionary if structured is not passed,\n        # because keyword arguments are not re-evaluated when calling\n        # the function and it's okay for callees to mutate their\n        # passed dictionary.\n        if structured is None:\n            structured = {}\n\n        msg_parts.append('STRUCTURED: ' +\n                         WalELogger._fmt_structured(structured))\n\n        return '\\n'.join(msg_parts)", "fpath_tuple": ["System", "wal-e", "wal_e", "log_help.py"], "context_start_lineno": 160, "line_no": 161, "id": "wal_e.log_help.WalELogger.fmt_logline", "target_function_prompt": "    def fmt_logline(msg, detail=None, hint=None, structured=None):", "function_signature": "    def fmt_logline(msg, detail=None, hint=None, structured=None):"}}
{"prompt": "    def from_jwk(jwk: str | JWKDict) -> bytes:", "metadata": {"task_id": "Utilities/PyJWT/2", "ground_truth": "        try:\n            if isinstance(jwk, str):\n                obj: JWKDict = json.loads(jwk)\n            elif isinstance(jwk, dict):\n                obj = jwk\n            else:\n                raise ValueError\n        except ValueError:\n            raise InvalidKeyError(\"Key is not valid JSON\")\n\n        if obj.get(\"kty\") != \"oct\":\n            raise InvalidKeyError(\"Not an HMAC key\")\n\n        return base64url_decode(obj[\"k\"])", "fpath_tuple": ["Utilities", "PyJWT", "jwt", "algorithms.py"], "context_start_lineno": 289, "line_no": 290, "id": "jwt.algorithms.HMACAlgorithm.from_jwk", "target_function_prompt": "    def from_jwk(jwk: str | JWKDict) -> bytes:", "function_signature": "    def from_jwk(jwk: str | JWKDict) -> bytes:"}}
{"prompt": "    def apply(self, target):", "metadata": {"task_id": "Internet/pyramid/47", "ground_truth": "        if self.properties:\n            self.apply_properties(target, self.properties)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "util.py"], "context_start_lineno": 183, "line_no": 185, "id": "pyramid.util.InstancePropertyHelper.apply", "target_function_prompt": "    def apply(self, target):", "function_signature": "    def apply(self, target):"}}
{"prompt": "    def categorymembers(self) -> PagesDict:", "metadata": {"task_id": "Communications/Wikipedia-API/3", "ground_truth": "        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 1046, "line_no": 1057, "id": "wikipediaapi.WikipediaPage.categorymembers", "target_function_prompt": "    def categorymembers(self) -> PagesDict:", "function_signature": "    def categorymembers(self) -> PagesDict:"}}
{"prompt": "def add_query_param(request, key, val):", "metadata": {"task_id": "Internet/djangorestframework/12", "ground_truth": "    from rest_framework.utils.urls import replace_query_param\n    iri = request.get_full_path()\n    uri = iri_to_uri(iri)\n    return escape(replace_query_param(uri, key, val))", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "templatetags", "rest_framework.py"], "context_start_lineno": 147, "line_no": 151, "id": "rest_framework.templatetags.rest_framework.add_query_param", "target_function_prompt": "def add_query_param(request, key, val):", "function_signature": "def add_query_param(request, key, val):"}}
{"prompt": "def dictionary_guesses(match):\n    # keep these as properties for display purposes", "metadata": {"task_id": "Security/zxcvbn-python/8", "ground_truth": "    match['base_guesses'] = match['rank']\n    match['uppercase_variations'] = uppercase_variations(match)\n    match['l33t_variations'] = l33t_variations(match)\n    reversed_variations = match.get('reversed', False) and 2 or 1\n\n    return match['base_guesses'] * match['uppercase_variations'] * \\\n        match['l33t_variations'] * reversed_variations", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "scoring.py"], "context_start_lineno": 262, "line_no": 264, "id": "zxcvbn.scoring.dictionary_guesses", "target_function_prompt": "def dictionary_guesses(match):\n    # keep these as properties for display purposes", "function_signature": "def dictionary_guesses(match):\n    # keep these as properties for display purposes"}}
{"prompt": "    def execute(self):\n        # MRJob does Hadoop Streaming stuff, or defers to its superclass\n        # (MRJobLauncher) if not otherwise instructed", "metadata": {"task_id": "System/mrjob/47", "ground_truth": "        if self.options.run_mapper:\n            self.run_mapper(self.options.step_num)\n\n        elif self.options.run_combiner:\n            self.run_combiner(self.options.step_num)\n\n        elif self.options.run_reducer:\n            self.run_reducer(self.options.step_num)\n\n        elif self.options.run_spark:\n            self.run_spark(self.options.step_num)\n\n        else:\n            self.run_job()", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 675, "line_no": 678, "id": "mrjob.job.MRJob.execute", "target_function_prompt": "    def execute(self):\n        # MRJob does Hadoop Streaming stuff, or defers to its superclass\n        # (MRJobLauncher) if not otherwise instructed", "function_signature": "    def execute(self):\n        # MRJob does Hadoop Streaming stuff, or defers to its superclass\n        # (MRJobLauncher) if not otherwise instructed"}}
{"prompt": "def inspect_app(app: App) -> 'AppInfo':", "metadata": {"task_id": "Internet/falcon/26", "ground_truth": "    routes = inspect_routes(app)\n    static = inspect_static_routes(app)\n    sinks = inspect_sinks(app)\n    error_handlers = inspect_error_handlers(app)\n    middleware = inspect_middleware(app)\n    return AppInfo(routes, middleware, static, sinks, error_handlers, app._ASGI)", "fpath_tuple": ["Internet", "falcon", "falcon", "inspect.py"], "context_start_lineno": 28, "line_no": 40, "id": "falcon.inspect.inspect_app", "target_function_prompt": "def inspect_app(app: App) -> 'AppInfo':", "function_signature": "def inspect_app(app: App) -> 'AppInfo':"}}
{"prompt": "def _recon_lcs(x, y):", "metadata": {"task_id": "Internet/sumy/12", "ground_truth": "    table = _lcs(x, y)\n\n    def _recon(i, j):\n        if i == 0 or j == 0:\n            return []\n        elif x[i - 1] == y[j - 1]:\n            return _recon(i - 1, j - 1) + [(x[i - 1], i)]\n        elif table[i - 1, j] > table[i, j - 1]:\n            return _recon(i - 1, j)\n        else:\n            return _recon(i, j - 1)\n\n    i, j = _get_index_of_lcs(x, y)\n    recon_tuple = tuple(map(lambda r: r[0], _recon(i, j)))\n    return recon_tuple", "fpath_tuple": ["Internet", "sumy", "sumy", "evaluation", "rouge.py"], "context_start_lineno": 80, "line_no": 89, "id": "sumy.evaluation.rouge._recon_lcs", "target_function_prompt": "def _recon_lcs(x, y):", "function_signature": "def _recon_lcs(x, y):"}}
{"prompt": "def escape_css_string(s):", "metadata": {"task_id": "Database/datasette/26", "ground_truth": "    return _css_re.sub(\n        lambda m: \"\\\\\" + (f\"{ord(m.group()):X}\".zfill(6)),\n        s.replace(\"\\r\\n\", \"\\n\"),\n    )", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 337, "line_no": 338, "id": "datasette.utils.escape_css_string", "target_function_prompt": "def escape_css_string(s):", "function_signature": "def escape_css_string(s):"}}
{"prompt": "    def definition(self):", "metadata": {"task_id": "Internet/boto/47", "ground_truth": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.data_type,\n        }", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "fields.py"], "context_start_lineno": 26, "line_no": 39, "id": "boto.dynamodb2.fields.BaseSchemaField.definition", "target_function_prompt": "    def definition(self):", "function_signature": "    def definition(self):"}}
{"prompt": "    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:", "metadata": {"task_id": "Database/alembic/19", "ground_truth": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        # need a dummy column name here since SQLAlchemy\n        # 0.7.6 and further raises on Index with no columns\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self._reverse.columns if self._reverse else [\"x\"],\n            schema=self.schema,\n            **self.kw,\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 1059, "line_no": 1062, "id": "alembic.operations.ops.DropIndexOp.to_index", "target_function_prompt": "    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:", "function_signature": "    def to_index(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Index:"}}
{"prompt": "    def check_end(self) -> None:", "metadata": {"task_id": "Security/asyncssh/4", "ground_truth": "        if self:\n            raise PacketDecodeError('Unexpected data at end of packet')", "fpath_tuple": ["Security", "asyncssh", "asyncssh", "packet.py"], "context_start_lineno": 102, "line_no": 105, "id": "asyncssh.packet.SSHPacket.check_end", "target_function_prompt": "    def check_end(self) -> None:", "function_signature": "    def check_end(self) -> None:"}}
{"prompt": "    def set_property(cls, target, callable, name=None, reify=False):", "metadata": {"task_id": "Internet/pyramid/48", "ground_truth": "        prop = cls.make_property(callable, name=name, reify=reify)\n        cls.apply_properties(target, [prop])", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "util.py"], "context_start_lineno": 169, "line_no": 171, "id": "pyramid.util.InstancePropertyHelper.set_property", "target_function_prompt": "    def set_property(cls, target, callable, name=None, reify=False):", "function_signature": "    def set_property(cls, target, callable, name=None, reify=False):"}}
{"prompt": "    def deserialize(self, value):", "metadata": {"task_id": "Multimedia/Mopidy/17", "ground_truth": "        value = decode(value)\n        validators.validate_choice(value.lower(), log.COLORS)\n        return value.lower()", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 338, "line_no": 339, "id": "mopidy.config.types.LogColor.deserialize", "target_function_prompt": "    def deserialize(self, value):", "function_signature": "    def deserialize(self, value):"}}
{"prompt": "    def from_fs(cls: t.Type[Model], item_fs: FS) -> Model:", "metadata": {"task_id": "Scientific-Engineering/bentoml/18", "ground_truth": "        try:\n            with item_fs.open(MODEL_YAML_FILENAME, \"r\") as model_yaml:\n                info = ModelInfo.from_yaml_file(model_yaml)\n        except fs.errors.ResourceNotFound:\n            raise BentoMLException(\n                f\"Failed to load bento model because it does not contain a '{MODEL_YAML_FILENAME}'\"\n            )\n\n        res = Model(tag=info.tag, model_fs=item_fs, info=info, _internal=True)\n        try:\n            res.validate()\n        except BentoMLException as e:\n            raise BentoMLException(f\"Failed to load {res!s}: {e}\") from None\n\n        return res", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "models", "model.py"], "context_start_lineno": 218, "line_no": 219, "id": "bentoml._internal.models.model.Model.from_fs", "target_function_prompt": "    def from_fs(cls: t.Type[Model], item_fs: FS) -> Model:", "function_signature": "    def from_fs(cls: t.Type[Model], item_fs: FS) -> Model:"}}
{"prompt": "def minor_third(note):", "metadata": {"task_id": "Multimedia/mingus/25", "ground_truth": "    trd = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, trd, 3)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "intervals.py"], "context_start_lineno": 177, "line_no": 178, "id": "mingus.core.intervals.minor_third", "target_function_prompt": "def minor_third(note):", "function_signature": "def minor_third(note):"}}
{"prompt": "    def get_response(self, request_id=0, owner_uri=0):", "metadata": {"task_id": "Database/mssql-cli/11", "ground_truth": "        if request_id in self.response_map:\n            if not self.response_map[request_id].empty():\n                return self.response_map[request_id].get()\n\n        if owner_uri in self.response_map:\n            if not self.response_map[owner_uri].empty():\n                return self.response_map[owner_uri].get()\n\n        if not self.response_map[0].empty():\n            return self.response_map[0].get()\n\n        if not self.exception_queue.empty():\n            raise self.exception_queue.get()\n\n        return None", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "jsonrpc", "jsonrpcclient.py"], "context_start_lineno": 67, "line_no": 71, "id": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "target_function_prompt": "    def get_response(self, request_id=0, owner_uri=0):", "function_signature": "    def get_response(self, request_id=0, owner_uri=0):"}}
{"prompt": "    def unique_constraint(\n        self,\n        name: Optional[sqla_compat._ConstraintNameDefined],\n        source: str,\n        local_cols: Sequence[str],\n        schema: Optional[str] = None,\n        **kw,\n    ) -> UniqueConstraint:", "metadata": {"task_id": "Database/alembic/20", "ground_truth": "        t = sa_schema.Table(\n            source,\n            self.metadata(),\n            *[sa_schema.Column(n, NULLTYPE) for n in local_cols],\n            schema=schema,\n        )\n        kw[\"name\"] = name\n        uq = sa_schema.UniqueConstraint(*[t.c[n] for n in local_cols], **kw)\n        # TODO: need event tests to ensure the event\n        # is fired off here\n        t.append_constraint(uq)\n        return uq", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "schemaobj.py"], "context_start_lineno": 120, "line_no": 128, "id": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "target_function_prompt": "    def unique_constraint(\n        self,\n        name: Optional[sqla_compat._ConstraintNameDefined],\n        source: str,\n        local_cols: Sequence[str],\n        schema: Optional[str] = None,\n        **kw,\n    ) -> UniqueConstraint:", "function_signature": "    def unique_constraint(\n        self,\n        name: Optional[sqla_compat._ConstraintNameDefined],\n        source: str,\n        local_cols: Sequence[str],\n        schema: Optional[str] = None,\n        **kw,\n    ) -> UniqueConstraint:"}}
{"prompt": "def validate_grouping(grouping, schema, full_schema=None, path=()):", "metadata": {"task_id": "Software-Development/dash/9", "ground_truth": "    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, (tuple, list)):\n        SchemaTypeValidationError.check(grouping, full_schema, path, (tuple, list))\n        SchemaLengthValidationError.check(grouping, full_schema, path, len(schema))\n\n        for i, (g, s) in enumerate(zip(grouping, schema)):\n            validate_grouping(g, s, full_schema=full_schema, path=path + (i,))\n    elif isinstance(schema, dict):\n        SchemaTypeValidationError.check(grouping, full_schema, path, dict)\n        SchemaKeysValidationError.check(grouping, full_schema, path, set(schema))\n\n        for k in schema:\n            validate_grouping(\n                grouping[k], schema[k], full_schema=full_schema, path=path + (k,)\n            )\n    else:\n        pass", "fpath_tuple": ["Software-Development", "dash", "dash", "_grouping.py"], "context_start_lineno": 200, "line_no": 205, "id": "dash._grouping.validate_grouping", "target_function_prompt": "def validate_grouping(grouping, schema, full_schema=None, path=()):", "function_signature": "def validate_grouping(grouping, schema, full_schema=None, path=()):"}}
{"prompt": "    def send(self, data):", "metadata": {"task_id": "Communications/hl7/1", "ground_truth": "        self.socket.send(data)\n        # wait for the ACK/NACK\n        return self.socket.recv(RECV_BUFFER)", "fpath_tuple": ["Communications", "hl7", "hl7", "client.py"], "context_start_lineno": 81, "line_no": 86, "id": "hl7.client.MLLPClient.send", "target_function_prompt": "    def send(self, data):", "function_signature": "    def send(self, data):"}}
{"prompt": "def map_grouping(fn, grouping):", "metadata": {"task_id": "Software-Development/dash/10", "ground_truth": "    if isinstance(grouping, (tuple, list)):\n        return [map_grouping(fn, g) for g in grouping]\n\n    if isinstance(grouping, dict):\n        return AttributeDict({k: map_grouping(fn, g) for k, g in grouping.items()})\n\n    return fn(grouping)", "fpath_tuple": ["Software-Development", "dash", "dash", "_grouping.py"], "context_start_lineno": 112, "line_no": 122, "id": "dash._grouping.map_grouping", "target_function_prompt": "def map_grouping(fn, grouping):", "function_signature": "def map_grouping(fn, grouping):"}}
{"prompt": "    def summarize(\n        self, config: Settings, series: pd.Series, dtype: Type[VisionsBaseType]\n    ) -> dict:", "metadata": {"task_id": "Software-Development/pandas-profiling/3", "ground_truth": "        _, _, summary = self.handle(str(dtype), config, series, {\"type\": str(dtype)})\n        return summary", "fpath_tuple": ["Software-Development", "pandas-profiling", "src", "ydata_profiling", "model", "summarizer.py"], "context_start_lineno": 33, "line_no": 41, "id": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "target_function_prompt": "    def summarize(\n        self, config: Settings, series: pd.Series, dtype: Type[VisionsBaseType]\n    ) -> dict:", "function_signature": "    def summarize(\n        self, config: Settings, series: pd.Series, dtype: Type[VisionsBaseType]\n    ) -> dict:"}}
{"prompt": "def merge(\n    config: Config,\n    revisions: _RevIdType,\n    message: Optional[str] = None,\n    branch_label: Optional[_RevIdType] = None,\n    rev_id: Optional[str] = None,\n) -> Optional[Script]:", "metadata": {"task_id": "Database/alembic/21", "ground_truth": "    script = ScriptDirectory.from_config(config)\n    template_args = {\n        \"config\": config  # Let templates use config for\n        # e.g. multiple databases\n    }\n\n    environment = util.asbool(config.get_main_option(\"revision_environment\"))\n\n    if environment:\n\n        def nothing(rev, context):\n            return []\n\n        with EnvironmentContext(\n            config,\n            script,\n            fn=nothing,\n            as_sql=False,\n            template_args=template_args,\n        ):\n            script.run_env()\n\n    return script.generate_revision(\n        rev_id or util.rev_id(),\n        message,\n        refresh=True,\n        head=revisions,\n        branch_labels=branch_label,\n        **template_args,  # type:ignore[arg-type]\n    )", "fpath_tuple": ["Database", "alembic", "alembic", "command.py"], "context_start_lineno": 301, "line_no": 325, "id": "alembic.command.merge", "target_function_prompt": "def merge(\n    config: Config,\n    revisions: _RevIdType,\n    message: Optional[str] = None,\n    branch_label: Optional[_RevIdType] = None,\n    rev_id: Optional[str] = None,\n) -> Optional[Script]:", "function_signature": "def merge(\n    config: Config,\n    revisions: _RevIdType,\n    message: Optional[str] = None,\n    branch_label: Optional[_RevIdType] = None,\n    rev_id: Optional[str] = None,\n) -> Optional[Script]:"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/48", "ground_truth": "    from boto.regioninfo import connect\n    from boto.logs.layer1 import CloudWatchLogsConnection\n    return connect('logs', region_name,\n                   connection_cls=CloudWatchLogsConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "logs", "__init__.py"], "context_start_lineno": 37, "line_no": 38, "id": "boto.logs.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def make_names_data(inventory_filename: Optional[str] = None):", "metadata": {"task_id": "System/pyinfra/8", "ground_truth": "        show_warning()\n\n        if not inventory_filename:\n            raise InventoryError(\"No Ansible inventory filename provided!\")\n\n        if not path.exists(inventory_filename):\n            raise InventoryError(\n                (\"Could not find Ansible inventory file: {0}\").format(inventory_filename),\n            )\n\n        return parse_inventory(inventory_filename)", "fpath_tuple": ["System", "pyinfra", "pyinfra", "connectors", "ansible.py"], "context_start_lineno": 43, "line_no": 44, "id": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "target_function_prompt": "    def make_names_data(inventory_filename: Optional[str] = None):", "function_signature": "    def make_names_data(inventory_filename: Optional[str] = None):"}}
{"prompt": "def put(\n    src,\n    dest,\n    user=None,\n    group=None,\n    mode=None,\n    add_deploy_dir=True,\n    create_remote_dir=True,\n    force=False,\n    assume_exists=False,\n):", "metadata": {"task_id": "System/pyinfra/9", "ground_truth": "    if hasattr(src, \"read\"):\n        local_file = src\n        local_sum = get_file_sha1(src)\n\n    # Assume string filename\n    else:\n        # Add deploy directory?\n        if add_deploy_dir and state.cwd:\n            src = os.path.join(state.cwd, src)\n\n        local_file = src\n\n        if os.path.isfile(local_file):\n            local_sum = get_file_sha1(local_file)\n        elif assume_exists:\n            local_sum = None\n        else:\n            raise IOError(\"No such file: {0}\".format(local_file))\n\n    if mode is True:\n        if os.path.isfile(local_file):\n            mode = get_path_permissions_mode(local_file)\n        else:\n            logger.warning(\n                (\"No local file exists to get permissions from with `mode=True` ({0})\").format(\n                    get_call_location(),\n                ),\n            )\n    else:\n        mode = ensure_mode_int(mode)\n\n    remote_file = host.get_fact(File, path=dest)\n\n    if not remote_file and host.get_fact(Directory, path=dest):\n        dest = unix_path_join(dest, os.path.basename(src))\n        remote_file = host.get_fact(File, path=dest)\n\n    if create_remote_dir:\n        yield from _create_remote_dir(state, host, dest, user, group)\n\n    # No remote file, always upload and user/group/mode if supplied\n    if not remote_file or force:\n        yield FileUploadCommand(\n            local_file,\n            dest,\n            remote_temp_filename=state.get_temp_filename(dest),\n        )\n\n        if user or group:\n            yield file_utils.chown(dest, user, group)\n\n        if mode:\n            yield file_utils.chmod(dest, mode)\n\n    # File exists, check sum and check user/group/mode if supplied\n    else:\n        remote_sum = host.get_fact(Sha1File, path=dest)\n\n        # Check sha1sum, upload if needed\n        if local_sum != remote_sum:\n            yield FileUploadCommand(\n                local_file,\n                dest,\n                remote_temp_filename=state.get_temp_filename(dest),\n            )\n\n            if user or group:\n                yield file_utils.chown(dest, user, group)\n\n            if mode:\n                yield file_utils.chmod(dest, mode)\n\n        else:\n            changed = False\n\n            # Check mode\n            if mode and remote_file[\"mode\"] != mode:\n                yield file_utils.chmod(dest, mode)\n                changed = True\n\n            # Check user/group\n            if (user and remote_file[\"user\"] != user) or (group and remote_file[\"group\"] != group):\n                yield file_utils.chown(dest, user, group)\n                changed = True\n\n            if not changed:\n                host.noop(\"file {0} is already uploaded\".format(dest))", "fpath_tuple": ["System", "pyinfra", "pyinfra", "operations", "files.py"], "context_start_lineno": 781, "line_no": 841, "id": "pyinfra.operations.files.put", "target_function_prompt": "def put(\n    src,\n    dest,\n    user=None,\n    group=None,\n    mode=None,\n    add_deploy_dir=True,\n    create_remote_dir=True,\n    force=False,\n    assume_exists=False,\n):", "function_signature": "def put(\n    src,\n    dest,\n    user=None,\n    group=None,\n    mode=None,\n    add_deploy_dir=True,\n    create_remote_dir=True,\n    force=False,\n    assume_exists=False,\n):"}}
{"prompt": "    def run_job(self):", "metadata": {"task_id": "System/mrjob/48", "ground_truth": "        from mrjob.step import StepFailedException\n        log_stream = codecs.getwriter('utf_8')(self.stderr)\n\n        self.set_up_logging(quiet=self.options.quiet,\n                            verbose=self.options.verbose,\n                            stream=log_stream)\n\n        with self.make_runner() as runner:\n            try:\n                runner.run()\n            except StepFailedException as e:\n                # no need for a runner stacktrace if step failed; runners will\n                # log more useful information anyway\n                log.error(str(e))\n                sys.exit(1)\n\n            if self._should_cat_output():\n                for chunk in runner.cat_output():\n                    self.stdout.write(chunk)\n                self.stdout.flush()", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 619, "line_no": 629, "id": "mrjob.job.MRJob.run_job", "target_function_prompt": "    def run_job(self):", "function_signature": "    def run_job(self):"}}
{"prompt": "    def expose_authentication_method(cls, method_name, definition):", "metadata": {"task_id": "Internet/kinto/23", "ground_truth": "        cls.security_definitions[method_name] = definition\n        cls.security_roles[method_name] = definition.get(\"scopes\", {}).keys()", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "openapi.py"], "context_start_lineno": 26, "line_no": 49, "id": "kinto.core.openapi.OpenAPI.expose_authentication_method", "target_function_prompt": "    def expose_authentication_method(cls, method_name, definition):", "function_signature": "    def expose_authentication_method(cls, method_name, definition):"}}
{"prompt": "    def create_or_migrate_schema(self, dry_run=False):", "metadata": {"task_id": "Internet/kinto/24", "ground_truth": "        version = self.get_installed_version()\n        if not version:\n            self.create_schema(dry_run)\n            return\n\n        logger.info(f\"Detected PostgreSQL {self.name} schema version {version}.\")\n        if version == self.schema_version:\n            logger.info(f\"PostgreSQL {self.name} schema is up-to-date.\")\n            return\n\n        self.migrate_schema(version, dry_run)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "storage", "postgresql", "migrator.py"], "context_start_lineno": 41, "line_no": 43, "id": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "target_function_prompt": "    def create_or_migrate_schema(self, dry_run=False):", "function_signature": "    def create_or_migrate_schema(self, dry_run=False):"}}
{"prompt": "def cast_to_unicode(anything):", "metadata": {"task_id": "Communications/chatette/13", "ground_truth": "    if sys.version_info[0] == 3:\n        return anything\n\n    if isinstance(anything, str):\n        return unicode(anything, \"utf-8\")\n    if isinstance(anything, dict):\n        cast_dict = dict()\n        for key in anything:\n            cast_key = cast_to_unicode(key)\n            cast_value = cast_to_unicode(anything[key])\n            cast_dict[cast_key] = cast_value\n        return cast_dict\n    if isinstance(anything, list):\n        cast_list = []\n        for e in anything:\n            cast_list.append(cast_to_unicode(e))\n        return cast_list\n    return anything", "fpath_tuple": ["Communications", "chatette", "chatette", "utils.py"], "context_start_lineno": 59, "line_no": 66, "id": "chatette.utils.cast_to_unicode", "target_function_prompt": "def cast_to_unicode(anything):", "function_signature": "def cast_to_unicode(anything):"}}
{"prompt": "    def write_value(self, key, value, timestamp):", "metadata": {"task_id": "System/prometheus-client/2", "ground_truth": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)", "fpath_tuple": ["System", "prometheus-client", "prometheus_client", "mmap_dict.py"], "context_start_lineno": 126, "line_no": 127, "id": "prometheus_client.mmap_dict.MmapedDict.write_value", "target_function_prompt": "    def write_value(self, key, value, timestamp):", "function_signature": "    def write_value(self, key, value, timestamp):"}}
{"prompt": "def normpath(path):\n    # type: (Text) -> Text", "metadata": {"task_id": "System/fs/15", "ground_truth": "    from .errors import IllegalBackReference\n    if path in \"/\":\n        return path\n\n    # An early out if there is no need to normalize this path\n    if not _requires_normalization(path):\n        return path.rstrip(\"/\")\n\n    prefix = \"/\" if path.startswith(\"/\") else \"\"\n    components = []  # type: List[Text]\n    try:\n        for component in path.split(\"/\"):\n            if component in \"..\":  # True for '..', '.', and ''\n                if component == \"..\":\n                    components.pop()\n            else:\n                components.append(component)\n    except IndexError:\n        # FIXME (@althonos): should be raised from the IndexError\n        raise IllegalBackReference(path)\n    return prefix + \"/\".join(components)", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 49, "line_no": 71, "id": "fs.path.normpath", "target_function_prompt": "def normpath(path):\n    # type: (Text) -> Text", "function_signature": "def normpath(path):\n    # type: (Text) -> Text"}}
{"prompt": "    def forget(self, request):", "metadata": {"task_id": "Internet/pyramid/49", "ground_truth": "        identifier = self._get_identifier(request)\n        if identifier is None:\n            return []\n        identity = self._get_identity(request)\n        return identifier.forget(request.environ, identity)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "authentication.py"], "context_start_lineno": 362, "line_no": 369, "id": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "target_function_prompt": "    def forget(self, request):", "function_signature": "    def forget(self, request):"}}
{"prompt": "def to_bytes(object):", "metadata": {"task_id": "Internet/sumy/13", "ground_truth": "    if isinstance(object, bytes):\n        return object\n    elif isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n    else:\n        # try encode instance to bytes\n        return instance_to_bytes(object)", "fpath_tuple": ["Internet", "sumy", "sumy", "_compat.py"], "context_start_lineno": 49, "line_no": 50, "id": "sumy._compat.to_bytes", "target_function_prompt": "def to_bytes(object):", "function_signature": "def to_bytes(object):"}}
{"prompt": "    def allow_update_activities(self):", "metadata": {"task_id": "Communications/twilio-fatisar/13", "ground_truth": "        post_filter = {\"ActivitySid\": {\"required\": True}}\n        self._make_policy(self.resource_url, \"POST\", True, post_filter=post_filter)", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "jwt", "taskrouter", "capabilities.py"], "context_start_lineno": 58, "line_no": 59, "id": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "target_function_prompt": "    def allow_update_activities(self):", "function_signature": "    def allow_update_activities(self):"}}
{"prompt": "def parse_s3_uri(uri):", "metadata": {"task_id": "System/mrjob/49", "ground_truth": "    components = urlparse(uri)\n    if (components.scheme not in ('s3', 's3n', 's3a') or\n        '/' not in components.path):  # noqa\n\n        raise ValueError('Invalid S3 URI: %s' % uri)\n\n    return components.netloc, components.path[1:]", "fpath_tuple": ["System", "mrjob", "mrjob", "parse.py"], "context_start_lineno": 51, "line_no": 59, "id": "mrjob.parse.parse_s3_uri", "target_function_prompt": "def parse_s3_uri(uri):", "function_signature": "def parse_s3_uri(uri):"}}
{"prompt": "    def text(self) -> str:", "metadata": {"task_id": "Communications/Wikipedia-API/4", "ground_truth": "        txt = self.summary\n        if len(txt) > 0:\n            txt += \"\\n\\n\"\n        for sec in self.sections:\n            txt += sec.full_text(level=2)\n        return txt.strip()", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 968, "line_no": 974, "id": "wikipediaapi.WikipediaPage.text", "target_function_prompt": "    def text(self) -> str:", "function_signature": "    def text(self) -> str:"}}
{"prompt": "    def abspath(self):", "metadata": {"task_id": "Internet/pyramid/50", "ground_truth": "        return os.path.abspath(\n            self.pkg_resources.resource_filename(self.pkg_name, self.path)\n        )", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "path.py"], "context_start_lineno": 374, "line_no": 375, "id": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "target_function_prompt": "    def abspath(self):", "function_signature": "    def abspath(self):"}}
{"prompt": "    def most_common(self, n=None):", "metadata": {"task_id": "Utilities/boltons/41", "ground_truth": "        if n <= 0:\n            return []\n        ret = sorted(self.iteritems(), key=lambda x: x[1], reverse=True)\n        if n is None or n >= len(ret):\n            return ret\n        return ret[:n]", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 732, "line_no": 736, "id": "boltons.cacheutils.ThresholdCounter.most_common", "target_function_prompt": "    def most_common(self, n=None):", "function_signature": "    def most_common(self, n=None):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/49", "ground_truth": "    from boto.regioninfo import connect\n    from boto.route53.domains.layer1 import Route53DomainsConnection\n    return connect('route53domains', region_name,\n                   connection_cls=Route53DomainsConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "route53", "domains", "__init__.py"], "context_start_lineno": 36, "line_no": 37, "id": "boto.route53.domains.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def deserialize(self, value):", "metadata": {"task_id": "Multimedia/Mopidy/18", "ground_truth": "        raw_value = decode(value).strip()\n        validators.validate_required(raw_value, self._required)\n        if not raw_value:\n            return None\n\n        if self._separator in raw_value:\n            values = raw_value.split(self._separator, 1)\n        elif self._optional_pair:\n            values = (raw_value, raw_value)\n        else:\n            raise ValueError(\n                f\"Config value must include {self._separator!r} separator: {raw_value}\"\n            )\n\n        return (\n            self._subtypes[0].deserialize(encode(values[0])),\n            self._subtypes[1].deserialize(encode(values[1])),\n        )", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 240, "line_no": 241, "id": "mopidy.config.types.Pair.deserialize", "target_function_prompt": "    def deserialize(self, value):", "function_signature": "    def deserialize(self, value):"}}
{"prompt": "    def __repr__(self):", "metadata": {"task_id": "Internet/pyramid/51", "ground_truth": "        return '<%s instance at %s with msg %r>' % (\n            self.__class__.__name__,\n            id(self),\n            self.msg,\n        )", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "security.py"], "context_start_lineno": 179, "line_no": 180, "id": "pyramid.security.PermitsResult.__repr__", "target_function_prompt": "    def __repr__(self):", "function_signature": "    def __repr__(self):"}}
{"prompt": "async def derive_named_parameters(db, sql):", "metadata": {"task_id": "Database/datasette/27", "ground_truth": "    explain = \"explain {}\".format(sql.strip().rstrip(\";\"))\n    possible_params = _re_named_parameter.findall(sql)\n    try:\n        results = await db.execute(explain, {p: None for p in possible_params})\n        return [row[\"p4\"].lstrip(\":\") for row in results if row[\"opcode\"] == \"Variable\"]\n    except sqlite3.DatabaseError:\n        return possible_params", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 1124, "line_no": 1125, "id": "datasette.utils.derive_named_parameters", "target_function_prompt": "async def derive_named_parameters(db, sql):", "function_signature": "async def derive_named_parameters(db, sql):"}}
{"prompt": "def service_definition_file(servicename):", "metadata": {"task_id": "Security/trailscraper/4", "ground_truth": "    boto_service_definition_files()\n    service_definitions_for_service = fnmatch.filter(boto_service_definition_files(),\n                                                     \"**/\" + servicename + \"/*/service-*.json\")\n\n    service_definitions_for_service.sort()\n\n    return service_definitions_for_service[-1]", "fpath_tuple": ["Security", "trailscraper", "trailscraper", "boto_service_definitions.py"], "context_start_lineno": 19, "line_no": 22, "id": "trailscraper.boto_service_definitions.service_definition_file", "target_function_prompt": "def service_definition_file(servicename):", "function_signature": "def service_definition_file(servicename):"}}
{"prompt": "    def to_payload(\n        cls,\n        batch: ext.PdDataFrame | ext.PdSeries,\n        batch_dim: int,\n    ) -> Payload:", "metadata": {"task_id": "Scientific-Engineering/bentoml/19", "ground_truth": "        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        if isinstance(batch, pd.Series):\n            batch = pd.DataFrame([batch])\n\n        meta: dict[str, bool | int | float | str | list[int]] = {\"format\": \"pickle5\"}\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n\n        if indices:\n            meta[\"with_buffer\"] = True\n            data = concat_buffer_bs\n            meta[\"pickle_bytes_str\"] = base64.b64encode(bs).decode(\"ascii\")\n            meta[\"indices\"] = indices\n        else:\n            meta[\"with_buffer\"] = False\n            data = bs\n\n        return cls.create_payload(\n            data,\n            batch.shape[0],\n            meta=meta,\n        )", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 380, "line_no": 385, "id": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "target_function_prompt": "    def to_payload(\n        cls,\n        batch: ext.PdDataFrame | ext.PdSeries,\n        batch_dim: int,\n    ) -> Payload:", "function_signature": "    def to_payload(\n        cls,\n        batch: ext.PdDataFrame | ext.PdSeries,\n        batch_dim: int,\n    ) -> Payload:"}}
{"prompt": "    def __call_permissive__(self, context, request):", "metadata": {"task_id": "Internet/pyramid/52", "ground_truth": "        view = self.match(context, request)\n        view = getattr(view, '__call_permissive__', view)\n        return view(context, request)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "views.py"], "context_start_lineno": 137, "line_no": 138, "id": "pyramid.config.views.MultiView.__call_permissive__", "target_function_prompt": "    def __call_permissive__(self, context, request):", "function_signature": "    def __call_permissive__(self, context, request):"}}
{"prompt": "    def get_object_permission_principals(self, object_id, permission):", "metadata": {"task_id": "Internet/kinto/25", "ground_truth": "        permission_key = f\"permission:{object_id}:{permission}\"\n        members = self._store.get(permission_key, set())\n        return members", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "permission", "memory.py"], "context_start_lineno": 89, "line_no": 90, "id": "kinto.core.permission.memory.Permission.get_object_permission_principals", "target_function_prompt": "    def get_object_permission_principals(self, object_id, permission):", "function_signature": "    def get_object_permission_principals(self, object_id, permission):"}}
{"prompt": "def parse_doc_filename(input_uri):", "metadata": {"task_id": "System/mrjob/50", "ground_truth": "    from mrjob.util import file_ext\n    name_with_ext = posixpath.basename(input_uri)\n    name = name_with_ext[:-len(file_ext(name_with_ext))]\n\n    parts = name.split('-')\n\n    doc_id = parts[0]\n    cats = {}\n\n    for part in parts[1:]:\n        if part.startswith('not_'):\n            cats[part[4:]] = False\n        else:\n            cats[part] = True\n\n    return dict(id=doc_id, cats=cats)", "fpath_tuple": ["System", "mrjob", "mrjob", "examples", "mr_text_classifier.py"], "context_start_lineno": 64, "line_no": 69, "id": "mrjob.examples.mr_text_classifier.parse_doc_filename", "target_function_prompt": "def parse_doc_filename(input_uri):", "function_signature": "def parse_doc_filename(input_uri):"}}
{"prompt": "def colorize(string, color):", "metadata": {"task_id": "Utilities/pymusic-dl/0", "ground_truth": "    string = str(string)\n    if color not in colors:\n        return string\n    if platform.system() == \"Windows\":\n        return string\n    return colors[color] + string + \"\\033[0m\"", "fpath_tuple": ["Utilities", "pymusic-dl", "music_dl", "utils.py"], "context_start_lineno": 30, "line_no": 31, "id": "music_dl.utils.colorize", "target_function_prompt": "def colorize(string, color):", "function_signature": "def colorize(string, color):"}}
{"prompt": "def _parse_step_syslog(lines):", "metadata": {"task_id": "System/mrjob/51", "ground_truth": "    return _parse_step_syslog_from_log4j_records(\n        _parse_hadoop_log4j_records(lines))", "fpath_tuple": ["System", "mrjob", "mrjob", "logs", "step.py"], "context_start_lineno": 251, "line_no": 270, "id": "mrjob.logs.step._parse_step_syslog", "target_function_prompt": "def _parse_step_syslog(lines):", "function_signature": "def _parse_step_syslog(lines):"}}
{"prompt": "def safeeval(expr, globals=None, locals=None):", "metadata": {"task_id": "System/mrjob/52", "ground_truth": "    from mrjob.py2 import PY2\n    safe_globals = {\n        'False': False,\n        'None': None,\n        'True': True,\n        '__builtin__': None,\n        '__builtins__': None,\n        'set': set\n    }\n\n    # xrange is range in Python 3\n    if PY2:\n        safe_globals['xrange'] = xrange\n    else:\n        safe_globals['range'] = range\n\n    # PyPy needs special magic\n    def open(*args, **kwargs):\n        raise NameError(\"name 'open' is not defined\")\n    safe_globals['open'] = open\n\n    # add the user-specified global variables\n    if globals:\n        safe_globals.update(globals)\n\n    return eval(expr, safe_globals, locals)", "fpath_tuple": ["System", "mrjob", "mrjob", "util.py"], "context_start_lineno": 133, "line_no": 141, "id": "mrjob.util.safeeval", "target_function_prompt": "def safeeval(expr, globals=None, locals=None):", "function_signature": "def safeeval(expr, globals=None, locals=None):"}}
{"prompt": "    def to_dict(self):", "metadata": {"task_id": "Internet/boto/50", "ground_truth": "        d = {}\n        for batch in self:\n            b = batch.to_dict()\n            if b['Keys']:\n                d[batch.table.name] = b\n        return d", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb", "batch.py"], "context_start_lineno": 205, "line_no": 209, "id": "boto.dynamodb.batch.BatchList.to_dict", "target_function_prompt": "    def to_dict(self):", "function_signature": "    def to_dict(self):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/51", "ground_truth": "    from boto.regioninfo import connect\n    from boto.machinelearning.layer1 import MachineLearningConnection\n    return connect('machinelearning', region_name,\n                   connection_cls=MachineLearningConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "machinelearning", "__init__.py"], "context_start_lineno": 38, "line_no": 39, "id": "boto.machinelearning.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def from_interval_shorthand(self, startnote, shorthand, up=True):", "metadata": {"task_id": "Multimedia/mingus/26", "ground_truth": "        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        n = Note(startnote.name, startnote.octave, startnote.dynamics)\n        n.transpose(shorthand, up)\n        self.add_notes([startnote, n])\n        return self", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "note_container.py"], "context_start_lineno": 129, "line_no": 142, "id": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "target_function_prompt": "    def from_interval_shorthand(self, startnote, shorthand, up=True):", "function_signature": "    def from_interval_shorthand(self, startnote, shorthand, up=True):"}}
{"prompt": "    def manifest(self):", "metadata": {"task_id": "Internet/pyramid/53", "ground_truth": "        if self.reload:\n            if not self.exists(self.manifest_path):\n                return {}\n            mtime = self.getmtime(self.manifest_path)\n            if self._mtime is None or mtime > self._mtime:\n                self._manifest = self.get_manifest()\n                self._mtime = mtime\n        return self._manifest", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "static.py"], "context_start_lineno": 409, "line_no": 411, "id": "pyramid.static.ManifestCacheBuster.manifest", "target_function_prompt": "    def manifest(self):", "function_signature": "    def manifest(self):"}}
{"prompt": "    def find_ref(self, name: str) -> t.Optional[str]:", "metadata": {"task_id": "Internet/Jinja2/11", "ground_truth": "        if name in self.refs:\n            return self.refs[name]\n\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n\n        return None", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "idtracking.py"], "context_start_lineno": 67, "line_no": 68, "id": "jinja2.idtracking.Symbols.find_ref", "target_function_prompt": "    def find_ref(self, name: str) -> t.Optional[str]:", "function_signature": "    def find_ref(self, name: str) -> t.Optional[str]:"}}
{"prompt": "    async def invoke_startup(self):\n        # This must be called for Datasette to be in a usable state", "metadata": {"task_id": "Database/datasette/28", "ground_truth": "        if self._startup_invoked:\n            return\n        for hook in pm.hook.prepare_jinja2_environment(\n            env=self.jinja_env, datasette=self\n        ):\n            await await_me_maybe(hook)\n        for hook in pm.hook.startup(datasette=self):\n            await await_me_maybe(hook)\n        self._startup_invoked = True", "fpath_tuple": ["Database", "datasette", "datasette", "app.py"], "context_start_lineno": 385, "line_no": 387, "id": "datasette.app.Datasette.invoke_startup", "target_function_prompt": "    async def invoke_startup(self):\n        # This must be called for Datasette to be in a usable state", "function_signature": "    async def invoke_startup(self):\n        # This must be called for Datasette to be in a usable state"}}
{"prompt": "    def update(self, E, **F):\n        # E and F are throwback names to the dict() __doc__", "metadata": {"task_id": "Utilities/boltons/42", "ground_truth": "        with self._lock:\n            if E is self:\n                return\n            setitem = self.__setitem__\n            if callable(getattr(E, 'keys', None)):\n                for k in E.keys():\n                    setitem(k, E[k])\n            else:\n                for k, v in E:\n                    setitem(k, v)\n            for k in F:\n                setitem(k, F[k])\n            return", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 303, "line_no": 305, "id": "boltons.cacheutils.LRI.update", "target_function_prompt": "    def update(self, E, **F):\n        # E and F are throwback names to the dict() __doc__", "function_signature": "    def update(self, E, **F):\n        # E and F are throwback names to the dict() __doc__"}}
{"prompt": "    def sip(\n        self,\n        sip_url,\n        username=None,\n        password=None,\n        url=None,\n        method=None,\n        status_callback_event=None,\n        status_callback=None,\n        status_callback_method=None,\n        machine_detection=None,\n        amd_status_callback_method=None,\n        amd_status_callback=None,\n        machine_detection_timeout=None,\n        machine_detection_speech_threshold=None,\n        machine_detection_speech_end_threshold=None,\n        machine_detection_silence_timeout=None,\n        **kwargs\n    ):", "metadata": {"task_id": "Communications/twilio-fatisar/14", "ground_truth": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=password,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                machine_detection=machine_detection,\n                amd_status_callback_method=amd_status_callback_method,\n                amd_status_callback=amd_status_callback,\n                machine_detection_timeout=machine_detection_timeout,\n                machine_detection_speech_threshold=machine_detection_speech_threshold,\n                machine_detection_speech_end_threshold=machine_detection_speech_end_threshold,\n                machine_detection_silence_timeout=machine_detection_silence_timeout,\n                **kwargs\n            )\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "twiml", "voice_response.py"], "context_start_lineno": 2193, "line_no": 2234, "id": "twilio.twiml.voice_response.Dial.sip", "target_function_prompt": "    def sip(\n        self,\n        sip_url,\n        username=None,\n        password=None,\n        url=None,\n        method=None,\n        status_callback_event=None,\n        status_callback=None,\n        status_callback_method=None,\n        machine_detection=None,\n        amd_status_callback_method=None,\n        amd_status_callback=None,\n        machine_detection_timeout=None,\n        machine_detection_speech_threshold=None,\n        machine_detection_speech_end_threshold=None,\n        machine_detection_silence_timeout=None,\n        **kwargs\n    ):", "function_signature": "    def sip(\n        self,\n        sip_url,\n        username=None,\n        password=None,\n        url=None,\n        method=None,\n        status_callback_event=None,\n        status_callback=None,\n        status_callback_method=None,\n        machine_detection=None,\n        amd_status_callback_method=None,\n        amd_status_callback=None,\n        machine_detection_timeout=None,\n        machine_detection_speech_threshold=None,\n        machine_detection_speech_end_threshold=None,\n        machine_detection_silence_timeout=None,\n        **kwargs\n    ):"}}
{"prompt": "def relativefrom(base, path):\n    # type: (Text, Text) -> Text", "metadata": {"task_id": "System/fs/16", "ground_truth": "    base_parts = list(iteratepath(base))\n    path_parts = list(iteratepath(path))\n\n    common = 0\n    for component_a, component_b in zip(base_parts, path_parts):\n        if component_a != component_b:\n            break\n        common += 1\n\n    return \"/\".join([\"..\"] * (len(base_parts) - common) + path_parts[common:])", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 542, "line_no": 559, "id": "fs.path.relativefrom", "target_function_prompt": "def relativefrom(base, path):\n    # type: (Text, Text) -> Text", "function_signature": "def relativefrom(base, path):\n    # type: (Text, Text) -> Text"}}
{"prompt": "    def darwin_checker(self):", "metadata": {"task_id": "Utilities/python-for-android/15", "ground_truth": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 308, "line_no": 309, "id": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "target_function_prompt": "    def darwin_checker(self):", "function_signature": "    def darwin_checker(self):"}}
{"prompt": "def resolve_file_path(path, search_environment_path=False):", "metadata": {"task_id": "System/exodus-bundler/8", "ground_truth": "    if search_environment_path:\n        path = resolve_binary(path)\n    if not os.path.exists(path):\n        raise MissingFileError('The \"%s\" file was not found.' % path)\n    if os.path.isdir(path):\n        raise UnexpectedDirectoryError('\"%s\" is a directory, not a file.' % path)\n    return os.path.normpath(os.path.abspath(path))", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "bundling.py"], "context_start_lineno": 199, "line_no": 209, "id": "exodus_bundler.bundling.resolve_file_path", "target_function_prompt": "def resolve_file_path(path, search_environment_path=False):", "function_signature": "def resolve_file_path(path, search_environment_path=False):"}}
{"prompt": "def iteratepath(path):\n    # type: (Text) -> List[Text]", "metadata": {"task_id": "System/fs/17", "ground_truth": "    path = relpath(normpath(path))\n    if not path:\n        return []\n    return path.split(\"/\")", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 94, "line_no": 109, "id": "fs.path.iteratepath", "target_function_prompt": "def iteratepath(path):\n    # type: (Text) -> List[Text]", "function_signature": "def iteratepath(path):\n    # type: (Text) -> List[Text]"}}
{"prompt": "def is_ppt(filename):", "metadata": {"task_id": "Security/oletools/1", "ground_truth": "    have_current_user = False\n    have_user_edit = False\n    have_persist_dir = False\n    have_document_container = False\n    ppt_file = None\n    try:\n        ppt_file = PptFile(filename)\n        for stream in ppt_file.iter_streams():\n            if stream.name == 'Current User':\n                for record in stream.iter_records():\n                    if isinstance(record, PptRecordCurrentUser):\n                        have_current_user = True\n                        if have_current_user and have_user_edit and \\\n                                have_persist_dir and have_document_container:\n                            return True\n            elif stream.name == 'PowerPoint Document':\n                for record in stream.iter_records():\n                    if record.type == 0x0ff5:     # UserEditAtom\n                        have_user_edit = True\n                    elif record.type == 0x1772:   # PersistDirectoryAtom\n                        have_persist_dir = True\n                    elif record.type == 0x03e8:   # DocumentContainer\n                        have_document_container = True\n                    else:\n                        continue\n                    if have_current_user and have_user_edit and \\\n                            have_persist_dir and have_document_container:\n                        return True\n            else:   # ignore other streams/storages since they are optional\n                continue\n    except Exception as exc:\n        logging.debug('Ignoring exception in is_ppt, assume is not ppt',\n                      exc_info=True)\n    finally:\n        if ppt_file is not None:\n            ppt_file.close()\n    return False", "fpath_tuple": ["Security", "oletools", "oletools", "ppt_record_parser.py"], "context_start_lineno": 142, "line_no": 157, "id": "oletools.ppt_record_parser.is_ppt", "target_function_prompt": "def is_ppt(filename):", "function_signature": "def is_ppt(filename):"}}
{"prompt": "    def get_all_vpc_peering_connections(self, vpc_peering_connection_ids=None, \n                                        filters=None, dry_run=False):", "metadata": {"task_id": "Internet/boto/52", "ground_truth": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids, 'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, dict(filters))\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params, [('item', VpcPeeringConnection)])", "fpath_tuple": ["Internet", "boto", "boto", "vpc", "__init__.py"], "context_start_lineno": 1551, "line_no": 1590, "id": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "target_function_prompt": "    def get_all_vpc_peering_connections(self, vpc_peering_connection_ids=None, \n                                        filters=None, dry_run=False):", "function_signature": "    def get_all_vpc_peering_connections(self, vpc_peering_connection_ids=None, \n                                        filters=None, dry_run=False):"}}
{"prompt": "    def llvm_prebuilt_dir(self):", "metadata": {"task_id": "Utilities/python-for-android/16", "ground_truth": "        return os.path.join(\n            self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag\n        )", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "androidndk.py"], "context_start_lineno": 23, "line_no": 24, "id": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "target_function_prompt": "    def llvm_prebuilt_dir(self):", "function_signature": "    def llvm_prebuilt_dir(self):"}}
{"prompt": "    def precedence(self):", "metadata": {"task_id": "Internet/djangorestframework/13", "ground_truth": "        if self.main_type == '*':\n            return 0\n        elif self.sub_type == '*':\n            return 1\n        elif not self.params or list(self.params) == ['q']:\n            return 2\n        return 3", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "utils", "mediatypes.py"], "context_start_lineno": 64, "line_no": 68, "id": "rest_framework.utils.mediatypes._MediaType.precedence", "target_function_prompt": "    def precedence(self):", "function_signature": "    def precedence(self):"}}
{"prompt": "    def plot(self, data=None, **kwargs):", "metadata": {"task_id": "Multimedia/hypertools/9", "ground_truth": "        from .plot.plot import plot as plotter\n\n        if data is None:\n            d = copy.copy(self.data)\n            transform = copy.copy(self.xform_data)\n            if any([k in kwargs for k in ['reduce', 'align', 'normalize',\n                                          'semantic', 'vectorizer', 'corpus']]):\n                d = copy.copy(self.data)\n                transform = None\n        else:\n            d = data\n            transform = None\n\n        # get kwargs and update with new kwargs\n        new_kwargs = copy.copy(self.kwargs)\n        update_kwargs = dict(transform=transform, reduce=self.reduce,\n                       align=self.align, normalize=self.normalize,\n                       semantic=self.semantic, vectorizer=self.vectorizer,\n                       corpus=self.corpus)\n        new_kwargs.update(update_kwargs)\n        for key in kwargs:\n            new_kwargs.update({key : kwargs[key]})\n        return plotter(d, **new_kwargs)", "fpath_tuple": ["Multimedia", "hypertools", "hypertools", "datageometry.py"], "context_start_lineno": 148, "line_no": 170, "id": "hypertools.datageometry.DataGeometry.plot", "target_function_prompt": "    def plot(self, data=None, **kwargs):", "function_signature": "    def plot(self, data=None, **kwargs):"}}
{"prompt": "    def to_diff_tuple(self) -> Any:", "metadata": {"task_id": "Database/alembic/22", "ground_truth": "        col_diff = []\n        schema, tname, cname = self.schema, self.table_name, self.column_name\n\n        if self.modify_type is not None:\n            col_diff.append(\n                (\n                    \"modify_type\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_type,\n                    self.modify_type,\n                )\n            )\n\n        if self.modify_nullable is not None:\n            col_diff.append(\n                (\n                    \"modify_nullable\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_nullable,\n                    self.modify_nullable,\n                )\n            )\n\n        if self.modify_server_default is not False:\n            col_diff.append(\n                (\n                    \"modify_default\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_comment\": self.existing_comment,\n                    },\n                    self.existing_server_default,\n                    self.modify_server_default,\n                )\n            )\n\n        if self.modify_comment is not False:\n            col_diff.append(\n                (\n                    \"modify_comment\",\n                    schema,\n                    tname,\n                    cname,\n                    {\n                        \"existing_nullable\": self.existing_nullable,\n                        \"existing_type\": self.existing_type,\n                        \"existing_server_default\": (\n                            self.existing_server_default\n                        ),\n                    },\n                    self.existing_comment,\n                    self.modify_comment,\n                )\n            )\n\n        return col_diff", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 1681, "line_no": 1682, "id": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "target_function_prompt": "    def to_diff_tuple(self) -> Any:", "function_signature": "    def to_diff_tuple(self) -> Any:"}}
{"prompt": "    def get_data_dir(cls, config: Config) -> Path:", "metadata": {"task_id": "Multimedia/Mopidy/19", "ground_truth": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "ext.py"], "context_start_lineno": 108, "line_no": 116, "id": "mopidy.ext.Extension.get_data_dir", "target_function_prompt": "    def get_data_dir(cls, config: Config) -> Path:", "function_signature": "    def get_data_dir(cls, config: Config) -> Path:"}}
{"prompt": "def unidecode(txt: str) -> str:", "metadata": {"task_id": "Software-Development/Faker/16", "ground_truth": "    chars = \"\"\n    for ch in txt:\n        codepoint = ord(ch)\n\n        try:\n            chars += codes[codepoint]\n        except IndexError:\n            pass\n    return chars", "fpath_tuple": ["Software-Development", "Faker", "faker", "decode", "__init__.py"], "context_start_lineno": 3, "line_no": 4, "id": "faker.decode.unidecode", "target_function_prompt": "def unidecode(txt: str) -> str:", "function_signature": "def unidecode(txt: str) -> str:"}}
{"prompt": "def is_valid_ip(arg):", "metadata": {"task_id": "Security/capirca/6", "ground_truth": "  try:\n    nacaddr.IP(arg)\n  except:\n    raise argparse.ArgumentTypeError('%s is an invalid ip address' % arg)\n  return arg", "fpath_tuple": ["Security", "capirca", "tools", "cgrep.py"], "context_start_lineno": 50, "line_no": 62, "id": "tools.cgrep.is_valid_ip", "target_function_prompt": "def is_valid_ip(arg):", "function_signature": "def is_valid_ip(arg):"}}
{"prompt": "    def __str__(self):", "metadata": {"task_id": "Internet/djangorestframework/14", "ground_truth": "        ret = \"%s/%s\" % (self.main_type, self.sub_type)\n        for key, val in self.params.items():\n            ret += \"; %s=%s\" % (key, val)\n        return ret", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "utils", "mediatypes.py"], "context_start_lineno": 76, "line_no": 77, "id": "rest_framework.utils.mediatypes._MediaType.__str__", "target_function_prompt": "    def __str__(self):", "function_signature": "    def __str__(self):"}}
{"prompt": "def get_resource(\n    resources: dict[str, t.Any], resource_kind: str, validate: bool = True\n) -> t.Any:", "metadata": {"task_id": "Scientific-Engineering/bentoml/20", "ground_truth": "    if resource_kind not in _RESOURCE_REGISTRY:\n        raise BentoMLConfigException(f\"Unknown resource kind '{resource_kind}'.\")\n\n    resource: t.Type[Resource[t.Any]] = _RESOURCE_REGISTRY[resource_kind]\n\n    if resource_kind in resources:\n        if resources[resource_kind] == \"system\":\n            return resource.from_system()\n        else:\n            res = resource.from_spec(resources[resource_kind])\n            if validate:\n                resource.validate(res)\n            return res\n    else:\n        return None", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "resource.py"], "context_start_lineno": 22, "line_no": 25, "id": "bentoml._internal.resource.get_resource", "target_function_prompt": "def get_resource(\n    resources: dict[str, t.Any], resource_kind: str, validate: bool = True\n) -> t.Any:", "function_signature": "def get_resource(\n    resources: dict[str, t.Any], resource_kind: str, validate: bool = True\n) -> t.Any:"}}
{"prompt": "def extract_basic_authorization(headers):", "metadata": {"task_id": "Internet/Authlib/5", "ground_truth": "    auth = headers.get('Authorization')\n    if not auth or ' ' not in auth:\n        return None, None\n\n    auth_type, auth_token = auth.split(None, 1)\n    if auth_type.lower() != 'basic':\n        return None, None\n\n    try:\n        query = to_unicode(base64.b64decode(auth_token))\n    except (binascii.Error, TypeError):\n        return None, None\n    if ':' in query:\n        username, password = query.split(':', 1)\n        return username, password\n    return query, None", "fpath_tuple": ["Internet", "Authlib", "authlib", "oauth2", "rfc6749", "util.py"], "context_start_lineno": 23, "line_no": 24, "id": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "target_function_prompt": "def extract_basic_authorization(headers):", "function_signature": "def extract_basic_authorization(headers):"}}
{"prompt": "    def options(self):", "metadata": {"task_id": "Communications/twtxt/2", "ground_truth": "        try:\n            return dict(self.cfg.items(\"twtxt\"))\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n            return {}", "fpath_tuple": ["Communications", "twtxt", "twtxt", "config.py"], "context_start_lineno": 113, "line_no": 115, "id": "twtxt.config.Config.options", "target_function_prompt": "    def options(self):", "function_signature": "    def options(self):"}}
{"prompt": "def substitute_major_for_minor(progression, substitute_index, ignore_suffix=False):", "metadata": {"task_id": "Multimedia/mingus/27", "ground_truth": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, 5)\n        a = interval_diff(roman, n, 9) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "progressions.py"], "context_start_lineno": 328, "line_no": 340, "id": "mingus.core.progressions.substitute_major_for_minor", "target_function_prompt": "def substitute_major_for_minor(progression, substitute_index, ignore_suffix=False):", "function_signature": "def substitute_major_for_minor(progression, substitute_index, ignore_suffix=False):"}}
{"prompt": "def find_available_providers(modules: List[ModuleType]) -> List[str]:", "metadata": {"task_id": "Software-Development/Faker/17", "ground_truth": "    available_providers = set()\n    for providers_mod in modules:\n        if providers_mod.__package__:\n            providers = [\n                \".\".join([providers_mod.__package__, mod]) for mod in list_module(providers_mod) if mod != \"__pycache__\"\n            ]\n            available_providers.update(providers)\n    return sorted(available_providers)", "fpath_tuple": ["Software-Development", "Faker", "faker", "utils", "loading.py"], "context_start_lineno": 51, "line_no": 52, "id": "faker.utils.loading.find_available_providers", "target_function_prompt": "def find_available_providers(modules: List[ModuleType]) -> List[str]:", "function_signature": "def find_available_providers(modules: List[ModuleType]) -> List[str]:"}}
{"prompt": "    def extend(self, data):", "metadata": {"task_id": "Utilities/boltons/43", "ground_truth": "        if not data:\n            return\n        self._data.extend(data)\n        self._set_width()\n        self._fill()", "fpath_tuple": ["Utilities", "boltons", "boltons", "tableutils.py"], "context_start_lineno": 291, "line_no": 295, "id": "boltons.tableutils.Table.extend", "target_function_prompt": "    def extend(self, data):", "function_signature": "    def extend(self, data):"}}
{"prompt": "    def page(\n        self,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        unquote: bool = False,\n    ) -> \"WikipediaPage\":", "metadata": {"task_id": "Communications/Wikipedia-API/5", "ground_truth": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 198, "line_no": 231, "id": "wikipediaapi.Wikipedia.page", "target_function_prompt": "    def page(\n        self,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        unquote: bool = False,\n    ) -> \"WikipediaPage\":", "function_signature": "    def page(\n        self,\n        title: str,\n        ns: WikiNamespace = Namespace.MAIN,\n        unquote: bool = False,\n    ) -> \"WikipediaPage\":"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/53", "ground_truth": "    from boto.regioninfo import connect\n    from boto.glacier.layer2 import Layer2\n    return connect('glacier', region_name, connection_cls=Layer2, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "glacier", "__init__.py"], "context_start_lineno": 38, "line_no": 39, "id": "boto.glacier.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def print_stdout(self, text: str, *arg) -> None:", "metadata": {"task_id": "Database/alembic/23", "ground_truth": "        if arg:\n            output = str(text) % arg\n        else:\n            output = str(text)\n\n        util.write_outstream(self.stdout, output, \"\\n\", **self.messaging_opts)", "fpath_tuple": ["Database", "alembic", "alembic", "config.py"], "context_start_lineno": 161, "line_no": 177, "id": "alembic.config.Config.print_stdout", "target_function_prompt": "    def print_stdout(self, text: str, *arg) -> None:", "function_signature": "    def print_stdout(self, text: str, *arg) -> None:"}}
{"prompt": "    def get_header(self, name, required=False, default=None):", "metadata": {"task_id": "Internet/falcon/27", "ground_truth": "        wsgi_name = name.upper().replace('-', '_')\n\n        # Use try..except to optimize for the header existing in most cases\n        try:\n            # Don't take the time to cache beforehand, using HTTP naming.\n            # This will be faster, assuming that most headers are looked\n            # up only once, and not all headers will be requested.\n            return self.env['HTTP_' + wsgi_name]\n\n        except KeyError:\n            # NOTE(kgriffs): There are a couple headers that do not\n            # use the HTTP prefix in the env, so try those. We expect\n            # people to usually just use the relevant helper properties\n            # to access these instead of .get_header.\n            if wsgi_name in WSGI_CONTENT_HEADERS:\n                try:\n                    return self.env[wsgi_name]\n                except KeyError:\n                    pass\n\n            if not required:\n                return default\n\n            raise errors.HTTPMissingHeader(name)", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 1123, "line_no": 1147, "id": "falcon.request.Request.get_header", "target_function_prompt": "    def get_header(self, name, required=False, default=None):", "function_signature": "    def get_header(self, name, required=False, default=None):"}}
{"prompt": "    def read(self, n=-1):", "metadata": {"task_id": "Utilities/boltons/44", "ground_truth": "        self._checkClosed()\n        ret = self.buffer.reader.read(n, n)\n        self._tell = self.tell() + len(ret)\n        return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "ioutils.py"], "context_start_lineno": 404, "line_no": 405, "id": "boltons.ioutils.SpooledStringIO.read", "target_function_prompt": "    def read(self, n=-1):", "function_signature": "    def read(self, n=-1):"}}
{"prompt": "    def remove_tags(self, tags, dry_run=False):", "metadata": {"task_id": "Internet/boto/54", "ground_truth": "        status = self.connection.delete_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        for key, value in tags.items():\n            if key in self.tags:\n                if value is None or value == self.tags[key]:\n                    del self.tags[key]", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "ec2object.py"], "context_start_lineno": 121, "line_no": 135, "id": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "target_function_prompt": "    def remove_tags(self, tags, dry_run=False):", "function_signature": "    def remove_tags(self, tags, dry_run=False):"}}
{"prompt": "    def parse(self, instrs):", "metadata": {"task_id": "Security/barf/5", "ground_truth": "        instrs_reil = []\n\n        try:\n            for instr in instrs:\n                instr_lower = instr.lower()\n\n                # If the instruction to parsed is not in the cache,\n                # parse it and add it to the cache.\n                if instr_lower not in self._cache:\n                    self._cache[instr_lower] = instruction.parseString(\n                        instr_lower)[0]\n\n                # Retrieve parsed instruction from the cache and clone\n                # it.\n                instrs_reil += [copy.deepcopy(self._cache[instr_lower])]\n        except:\n            error_msg = \"Failed to parse instruction: %s\"\n\n            logger.error(error_msg, instr, exc_info=True)\n\n        return instrs_reil", "fpath_tuple": ["Security", "barf", "barf", "core", "reil", "parser.py"], "context_start_lineno": 196, "line_no": 199, "id": "barf.core.reil.parser.ReilParser.parse", "target_function_prompt": "    def parse(self, instrs):", "function_signature": "    def parse(self, instrs):"}}
{"prompt": "def native_value(value):", "metadata": {"task_id": "Internet/kinto/26", "ground_truth": "    if isinstance(value, str):\n        try:\n            value = json.loads(value)\n        except ValueError:\n            return value\n    return value", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "utils.py"], "context_start_lineno": 117, "line_no": 123, "id": "kinto.core.utils.native_value", "target_function_prompt": "def native_value(value):", "function_signature": "def native_value(value):"}}
{"prompt": "def _no_sql_testing_config(dialect=\"postgresql\", directives=\"\"):", "metadata": {"task_id": "Database/alembic/24", "ground_truth": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    return _write_config_file(\n        \"\"\"\n[alembic]\nscript_location = %s\nsqlalchemy.url = %s://\n%s\n\n[loggers]\nkeys = root\n\n[handlers]\nkeys = console\n\n[logger_root]\nlevel = WARN\nhandlers = console\nqualname =\n\n[handler_console]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatters]\nkeys = generic\n\n[formatter_generic]\nformat = %%(levelname)-5.5s [%%(name)s] %%(message)s\ndatefmt = %%H:%%M:%%S\n\n\"\"\"\n        % (dir_, dialect, directives)\n    )", "fpath_tuple": ["Database", "alembic", "alembic", "testing", "env.py"], "context_start_lineno": 200, "line_no": 203, "id": "alembic.testing.env._no_sql_testing_config", "target_function_prompt": "def _no_sql_testing_config(dialect=\"postgresql\", directives=\"\"):", "function_signature": "def _no_sql_testing_config(dialect=\"postgresql\", directives=\"\"):"}}
{"prompt": "    def cat(self, path_glob):", "metadata": {"task_id": "System/mrjob/53", "ground_truth": "        for i, filename in enumerate(self.ls(path_glob)):\n            if i > 0:\n                yield b''  # mark end of previous file\n\n            for line in self._cat_file(filename):\n                yield line", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "base.py"], "context_start_lineno": 53, "line_no": 60, "id": "mrjob.fs.base.Filesystem.cat", "target_function_prompt": "    def cat(self, path_glob):", "function_signature": "    def cat(self, path_glob):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/55", "ground_truth": "    from boto.regioninfo import connect\n    from boto.dynamodb2.layer1 import DynamoDBConnection\n    return connect('dynamodb', region_name, connection_cls=DynamoDBConnection,\n                   **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "__init__.py"], "context_start_lineno": 38, "line_no": 39, "id": "boto.dynamodb2.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def get(self, category_name, discriminator, default=None):", "metadata": {"task_id": "Internet/pyramid/54", "ground_truth": "        category = self._categories.setdefault(category_name, {})\n        intr = category.get(discriminator, default)\n        return intr", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "registry.py"], "context_start_lineno": 130, "line_no": 131, "id": "pyramid.registry.Introspector.get", "target_function_prompt": "    def get(self, category_name, discriminator, default=None):", "function_signature": "    def get(self, category_name, discriminator, default=None):"}}
{"prompt": "def histogram_compute(\n    config: Settings,\n    finite_values: np.ndarray,\n    n_unique: int,\n    name: str = \"histogram\",\n    weights: Optional[np.ndarray] = None,\n) -> dict:", "metadata": {"task_id": "Software-Development/pandas-profiling/4", "ground_truth": "    stats = {}\n    hist_config = config.plot.histogram\n    bins_arg = \"auto\" if hist_config.bins == 0 else min(hist_config.bins, n_unique)\n    bins = np.histogram_bin_edges(finite_values, bins=bins_arg)\n    if len(bins) > hist_config.max_bins:\n        bins = np.histogram_bin_edges(finite_values, bins=hist_config.max_bins)\n        weights = weights if weights and len(weights) == hist_config.max_bins else None\n\n    stats[name] = np.histogram(\n        finite_values, bins=bins, weights=weights, density=config.plot.histogram.density\n    )\n\n    return stats", "fpath_tuple": ["Software-Development", "pandas-profiling", "src", "ydata_profiling", "model", "summary_algorithms.py"], "context_start_lineno": 28, "line_no": 35, "id": "ydata_profiling.model.summary_algorithms.histogram_compute", "target_function_prompt": "def histogram_compute(\n    config: Settings,\n    finite_values: np.ndarray,\n    n_unique: int,\n    name: str = \"histogram\",\n    weights: Optional[np.ndarray] = None,\n) -> dict:", "function_signature": "def histogram_compute(\n    config: Settings,\n    finite_values: np.ndarray,\n    n_unique: int,\n    name: str = \"histogram\",\n    weights: Optional[np.ndarray] = None,\n) -> dict:"}}
{"prompt": "def std(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n        **unused_args):", "metadata": {"task_id": "Security/diffprivlib/12", "ground_truth": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "tools", "utils.py"], "context_start_lineno": 461, "line_no": 515, "id": "diffprivlib.tools.utils.std", "target_function_prompt": "def std(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n        **unused_args):", "function_signature": "def std(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n        **unused_args):"}}
{"prompt": "    def __repr__(self, n_budget_max=5):", "metadata": {"task_id": "Security/diffprivlib/13", "ground_truth": "        params = []\n        if self.epsilon != float(\"inf\"):\n            params.append(f\"epsilon={self.epsilon}\")\n\n        if self.delta != 1:\n            params.append(f\"delta={self.delta}\")\n\n        if self.slack > 0:\n            params.append(f\"slack={self.slack}\")\n\n        if self.spent_budget:\n            if len(self.spent_budget) > n_budget_max:\n                params.append(\"spent_budget=\" + str(self.spent_budget[:n_budget_max] + [\"...\"]).replace(\"'\", \"\"))\n            else:\n                params.append(\"spent_budget=\" + str(self.spent_budget))\n\n        return \"BudgetAccountant(\" + \", \".join(params) + \")\"", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "accountant.py"], "context_start_lineno": 152, "line_no": 153, "id": "diffprivlib.accountant.BudgetAccountant.__repr__", "target_function_prompt": "    def __repr__(self, n_budget_max=5):", "function_signature": "    def __repr__(self, n_budget_max=5):"}}
{"prompt": "    def backlinks(self) -> PagesDict:", "metadata": {"task_id": "Communications/Wikipedia-API/6", "ground_truth": "        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 1014, "line_no": 1025, "id": "wikipediaapi.WikipediaPage.backlinks", "target_function_prompt": "    def backlinks(self) -> PagesDict:", "function_signature": "    def backlinks(self) -> PagesDict:"}}
{"prompt": "    def find_ancestral_path(self, h1, h2, path_cache={}):", "metadata": {"task_id": "Security/pycoin/22", "ground_truth": "        p1 = self.maximum_path(h1, path_cache)\n        p2 = self.maximum_path(h2, path_cache)\n        if p1[-1] != p2[-1]:\n            return [], []\n\n        shorter_len = min(len(p1), len(p2))\n        i1 = len(p1) - shorter_len\n        i2 = len(p2) - shorter_len\n        while 1:\n            if p1[i1] == p2[i2]:\n                return p1[:i1+1], p2[:i2+1]\n            i1 += 1\n            i2 += 1", "fpath_tuple": ["Security", "pycoin", "pycoin", "blockchain", "ChainFinder.py"], "context_start_lineno": 84, "line_no": 85, "id": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "target_function_prompt": "    def find_ancestral_path(self, h1, h2, path_cache={}):", "function_signature": "    def find_ancestral_path(self, h1, h2, path_cache={}):"}}
{"prompt": "    def noop(self):", "metadata": {"task_id": "Communications/IMAPClient/17", "ground_truth": "        tag = self._imap._command(\"NOOP\")\n        return self._consume_until_tagged_response(tag, \"NOOP\")", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 868, "line_no": 884, "id": "imapclient.imapclient.IMAPClient.noop", "target_function_prompt": "    def noop(self):", "function_signature": "    def noop(self):"}}
{"prompt": "    def starttls(self, ssl_context=None):", "metadata": {"task_id": "Communications/IMAPClient/18", "ground_truth": "        if self.ssl or self._starttls_done:\n            raise exceptions.IMAPClientAbortError(\"TLS session already established\")\n\n        typ, data = self._imap._simple_command(\"STARTTLS\")\n        self._checkok(\"starttls\", typ, data)\n\n        self._starttls_done = True\n\n        self._imap.sock = tls.wrap_socket(self._imap.sock, ssl_context, self.host)\n        self._imap.file = self._imap.sock.makefile(\"rb\")\n        return data[0]", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 360, "line_no": 377, "id": "imapclient.imapclient.IMAPClient.starttls", "target_function_prompt": "    def starttls(self, ssl_context=None):", "function_signature": "    def starttls(self, ssl_context=None):"}}
{"prompt": "    def replace(self, key, newkey):", "metadata": {"task_id": "Utilities/boltons/45", "ground_truth": "        if key not in self.data:\n            return\n        self.data[newkey] = fwdset = self.data.pop(key)\n        for val in fwdset:\n            revset = self.inv.data[val]\n            revset.remove(key)\n            revset.add(newkey)", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 982, "line_no": 986, "id": "boltons.dictutils.ManyToMany.replace", "target_function_prompt": "    def replace(self, key, newkey):", "function_signature": "    def replace(self, key, newkey):"}}
{"prompt": "def check_instances(arg, cls, msg=\"Expected a list of {name}, not {arg!r}\"):", "metadata": {"task_id": "Multimedia/Mopidy/20", "ground_truth": "    _check_iterable(arg, msg, name=cls.__name__)\n    if not all(isinstance(instance, cls) for instance in arg):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "internal", "validation.py"], "context_start_lineno": 68, "line_no": 69, "id": "mopidy.internal.validation.check_instances", "target_function_prompt": "def check_instances(arg, cls, msg=\"Expected a list of {name}, not {arg!r}\"):", "function_signature": "def check_instances(arg, cls, msg=\"Expected a list of {name}, not {arg!r}\"):"}}
{"prompt": "    def getsendbuffer(self):", "metadata": {"task_id": "Utilities/boltons/46", "ground_truth": "        with self._send_lock:\n            return b''.join(self.sbuf)", "fpath_tuple": ["Utilities", "boltons", "boltons", "socketutils.py"], "context_start_lineno": 193, "line_no": 195, "id": "boltons.socketutils.BufferedSocket.getsendbuffer", "target_function_prompt": "    def getsendbuffer(self):", "function_signature": "    def getsendbuffer(self):"}}
{"prompt": "def _parse_screen(next_lines: List[str]) -> Optional[Screen]:", "metadata": {"task_id": "Utilities/jc/4", "ground_truth": "    next_line = next_lines.pop()\n    result = re.match(_screen_pattern, next_line)\n    if not result:\n        next_lines.append(next_line)\n        return None\n\n    raw_matches = result.groupdict()\n\n    screen: Screen = {\"devices\": []}\n    for k, v in raw_matches.items():\n        screen[k] = int(v)\n\n    while next_lines:\n        device: Optional[Device] = _parse_device(next_lines)\n        if not device:\n            break\n        else:\n            screen[\"devices\"].append(device)\n\n    return screen", "fpath_tuple": ["Utilities", "jc", "jc", "parsers", "xrandr.py"], "context_start_lineno": 291, "line_no": 292, "id": "jc.parsers.xrandr._parse_screen", "target_function_prompt": "def _parse_screen(next_lines: List[str]) -> Optional[Screen]:", "function_signature": "def _parse_screen(next_lines: List[str]) -> Optional[Screen]:"}}
{"prompt": "    def _create_dictionary(self, document):", "metadata": {"task_id": "Internet/sumy/14", "ground_truth": "        words = map(self.normalize_word, document.words)\n        unique_words = frozenset(self.stem_word(w) for w in words if w not in self._stop_words)\n\n        return dict((w, i) for i, w in enumerate(unique_words))", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "lsa.py"], "context_start_lineno": 54, "line_no": 56, "id": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "target_function_prompt": "    def _create_dictionary(self, document):", "function_signature": "    def _create_dictionary(self, document):"}}
{"prompt": "    def render_view(self, request, response, view, context):", "metadata": {"task_id": "Internet/pyramid/55", "ground_truth": "        system = {\n            'view': view,\n            'renderer_name': self.name,  # b/c\n            'renderer_info': self,\n            'context': context,\n            'request': request,\n            'req': request,\n            'get_csrf_token': partial(get_csrf_token, request),\n        }\n        return self.render_to_response(response, system, request=request)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "renderers.py"], "context_start_lineno": 432, "line_no": 433, "id": "pyramid.renderers.RendererHelper.render_view", "target_function_prompt": "    def render_view(self, request, response, view, context):", "function_signature": "    def render_view(self, request, response, view, context):"}}
{"prompt": "    def from_file(cls, file):", "metadata": {"task_id": "Communications/twtxt/3", "ground_truth": "        if not os.path.exists(file):\n            raise ValueError(\"Config file not found.\")\n\n        try:\n            config_parser = configparser.ConfigParser()\n            config_parser.read(file)\n\n            configuration = cls(file, config_parser)\n            if not configuration.check_config_sanity():\n                raise ValueError(\"Error in config file.\")\n            else:\n                return configuration\n        except configparser.Error:\n            raise ValueError(\"Config file is invalid.\")", "fpath_tuple": ["Communications", "twtxt", "twtxt", "config.py"], "context_start_lineno": 35, "line_no": 40, "id": "twtxt.config.Config.from_file", "target_function_prompt": "    def from_file(cls, file):", "function_signature": "    def from_file(cls, file):"}}
{"prompt": "def snowflake_time(id: int, /) -> datetime.datetime:", "metadata": {"task_id": "Software-Development/discord-py/3", "ground_truth": "    timestamp = ((id >> 22) + DISCORD_EPOCH) / 1000\n    return datetime.datetime.fromtimestamp(timestamp, tz=datetime.timezone.utc)", "fpath_tuple": ["Software-Development", "discord-py", "discord", "utils.py"], "context_start_lineno": 374, "line_no": 390, "id": "discord.utils.snowflake_time", "target_function_prompt": "def snowflake_time(id: int, /) -> datetime.datetime:", "function_signature": "def snowflake_time(id: int, /) -> datetime.datetime:"}}
{"prompt": "def dictionary_match(password, _ranked_dictionaries=RANKED_DICTIONARIES):", "metadata": {"task_id": "Security/zxcvbn-python/9", "ground_truth": "    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(length):\n            for j in range(i, length):\n                if password_lower[i:j + 1] in ranked_dict:\n                    word = password_lower[i:j + 1]\n                    rank = ranked_dict[word]\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j + 1],\n                        'matched_word': word,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': False,\n                        'l33t': False,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "matching.py"], "context_start_lineno": 95, "line_no": 96, "id": "zxcvbn.matching.dictionary_match", "target_function_prompt": "def dictionary_match(password, _ranked_dictionaries=RANKED_DICTIONARIES):", "function_signature": "def dictionary_match(password, _ranked_dictionaries=RANKED_DICTIONARIES):"}}
{"prompt": "def get_custom_modules_path() -> Path:", "metadata": {"task_id": "Communications/ehforwarderbot/2", "ground_truth": "    channel_path = get_base_path() / \"modules\"\n    if not channel_path.exists():\n        channel_path.mkdir(parents=True)\n    return channel_path", "fpath_tuple": ["Communications", "ehforwarderbot", "ehforwarderbot", "utils.py"], "context_start_lineno": 112, "line_no": 119, "id": "ehforwarderbot.utils.get_custom_modules_path", "target_function_prompt": "def get_custom_modules_path() -> Path:", "function_signature": "def get_custom_modules_path() -> Path:"}}
{"prompt": "    def __str__(self):", "metadata": {"task_id": "System/mrjob/54", "ground_truth": "        if self.step_desc:\n            step_desc = self.step_desc\n        else:\n            if self.step_num is not None:\n                # 1-index step numbers\n                if self.last_step_num is not None:\n                    step_name = 'Steps %d-%d' % (\n                        self.step_num + 1, self.last_step_num + 1)\n                else:\n                    step_name = 'Step %d' % (self.step_num + 1)\n\n                if self.num_steps:\n                    step_desc = '%s of %d' % (step_name, self.num_steps)\n                else:\n                    step_desc = step_name\n            else:\n                step_desc = 'Step'\n\n        if self.reason:\n            return '%s failed: %s' % (step_desc, self.reason)\n        else:\n            return '%s failed' % step_desc", "fpath_tuple": ["System", "mrjob", "mrjob", "step.py"], "context_start_lineno": 127, "line_no": 130, "id": "mrjob.step.StepFailedException.__str__", "target_function_prompt": "    def __str__(self):", "function_signature": "    def __str__(self):"}}
{"prompt": "def recursive_update(d, u):", "metadata": {"task_id": "Utilities/sacred/25", "ground_truth": "    for k, v in u.items():\n        if isinstance(v, collections.abc.Mapping):\n            r = recursive_update(d.get(k, {}), v)\n            d[k] = r\n        else:\n            d[k] = u[k]\n    return d", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 391, "line_no": 400, "id": "sacred.utils.recursive_update", "target_function_prompt": "def recursive_update(d, u):", "function_signature": "def recursive_update(d, u):"}}
{"prompt": "    def forget(self, request, **kw):", "metadata": {"task_id": "Internet/pyramid/56", "ground_truth": "        if self.userid_key in request.session:\n            del request.session[self.userid_key]\n        return []", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "authentication.py"], "context_start_lineno": 1269, "line_no": 1271, "id": "pyramid.authentication.SessionAuthenticationHelper.forget", "target_function_prompt": "    def forget(self, request, **kw):", "function_signature": "    def forget(self, request, **kw):"}}
{"prompt": "    def set_quota(self, quotas):", "metadata": {"task_id": "Communications/IMAPClient/19", "ground_truth": "        if not quotas:\n            return\n\n        quota_root = None\n        set_quota_args = []\n\n        for quota in quotas:\n            if quota_root is None:\n                quota_root = quota.quota_root\n            elif quota_root != quota.quota_root:\n                raise ValueError(\"set_quota only accepts a single quota root\")\n\n            set_quota_args.append(\"{} {}\".format(quota.resource, quota.limit))\n\n        set_quota_args = \" \".join(set_quota_args)\n        args = [to_bytes(_quote(quota_root)), to_bytes(\"({})\".format(set_quota_args))]\n\n        response = self._raw_command_untagged(\n            b\"SETQUOTA\", args, uid=False, response_name=\"QUOTA\"\n        )\n        return _parse_quota(response)", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1604, "line_no": 1609, "id": "imapclient.imapclient.IMAPClient.set_quota", "target_function_prompt": "    def set_quota(self, quotas):", "function_signature": "    def set_quota(self, quotas):"}}
{"prompt": "    def check_csrf_token(self, request, supplied_token):", "metadata": {"task_id": "Internet/pyramid/57", "ground_truth": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "csrf.py"], "context_start_lineno": 85, "line_no": 87, "id": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "target_function_prompt": "    def check_csrf_token(self, request, supplied_token):", "function_signature": "    def check_csrf_token(self, request, supplied_token):"}}
{"prompt": "def write_file(filename: FilePath, data: bytes, mode: str = 'wb') -> int:", "metadata": {"task_id": "Security/asyncssh/5", "ground_truth": "    with open_file(filename, mode) as f:\n        return f.write(data)", "fpath_tuple": ["Security", "asyncssh", "asyncssh", "misc.py"], "context_start_lineno": 224, "line_no": 227, "id": "asyncssh.misc.write_file", "target_function_prompt": "def write_file(filename: FilePath, data: bytes, mode: str = 'wb') -> int:", "function_signature": "def write_file(filename: FilePath, data: bytes, mode: str = 'wb') -> int:"}}
{"prompt": "def get_matcher(patterns, case_sensitive):\n    # type: (Iterable[Text], bool) -> Callable[[Text], bool]", "metadata": {"task_id": "System/fs/18", "ground_truth": "    if not patterns:\n        return lambda name: True\n    if case_sensitive:\n        return partial(match_any, patterns)\n    else:\n        return partial(imatch_any, patterns)", "fpath_tuple": ["System", "fs", "fs", "wildcard.py"], "context_start_lineno": 100, "line_no": 123, "id": "fs.wildcard.get_matcher", "target_function_prompt": "def get_matcher(patterns, case_sensitive):\n    # type: (Iterable[Text], bool) -> Callable[[Text], bool]", "function_signature": "def get_matcher(patterns, case_sensitive):\n    # type: (Iterable[Text], bool) -> Callable[[Text], bool]"}}
{"prompt": "    def get_cache_dir(cls, config: Config) -> Path:", "metadata": {"task_id": "Multimedia/Mopidy/21", "ground_truth": "        if cls.ext_name is None:\n            raise AssertionError\n        cache_dir_path = (\n            path.expand_path(config[\"core\"][\"cache_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(cache_dir_path)\n        return cache_dir_path", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "ext.py"], "context_start_lineno": 76, "line_no": 84, "id": "mopidy.ext.Extension.get_cache_dir", "target_function_prompt": "    def get_cache_dir(cls, config: Config) -> Path:", "function_signature": "    def get_cache_dir(cls, config: Config) -> Path:"}}
{"prompt": "def saslprep(s: str) -> str:", "metadata": {"task_id": "Security/asyncssh/6", "ground_truth": "    prohibited = (stringprep.in_table_c12, stringprep.in_table_c21_c22,\n                  stringprep.in_table_c3, stringprep.in_table_c4,\n                  stringprep.in_table_c5, stringprep.in_table_c6,\n                  stringprep.in_table_c7, stringprep.in_table_c8,\n                  stringprep.in_table_c9)\n\n    return _stringprep(s, True, _map_saslprep, 'NFKC', prohibited, True)", "fpath_tuple": ["Security", "asyncssh", "asyncssh", "saslprep.py"], "context_start_lineno": 106, "line_no": 109, "id": "asyncssh.saslprep.saslprep", "target_function_prompt": "def saslprep(s: str) -> str:", "function_signature": "def saslprep(s: str) -> str:"}}
{"prompt": "    def sort(self):\n        # poor pythonist's mergesort, it's faster than sorted(self)\n        # when the lists' average length is greater than 512.", "metadata": {"task_id": "Utilities/boltons/47", "ground_truth": "        if len(self.lists) == 1:\n            self.lists[0].sort()\n        else:\n            for li in self.lists:\n                li.sort()\n            tmp_sorted = sorted(chain.from_iterable(self.lists))\n            del self.lists[:]\n            self.lists[0] = tmp_sorted\n            self._balance_list(0)", "fpath_tuple": ["Utilities", "boltons", "boltons", "listutils.py"], "context_start_lineno": 309, "line_no": 312, "id": "boltons.listutils.BarrelList.sort", "target_function_prompt": "    def sort(self):\n        # poor pythonist's mergesort, it's faster than sorted(self)\n        # when the lists' average length is greater than 512.", "function_signature": "    def sort(self):\n        # poor pythonist's mergesort, it's faster than sorted(self)\n        # when the lists' average length is greater than 512."}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/56", "ground_truth": "    from boto.regioninfo import connect\n    from boto.opsworks.layer1 import OpsWorksConnection\n    return connect('opsworks', region_name,\n                   connection_cls=OpsWorksConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "opsworks", "__init__.py"], "context_start_lineno": 38, "line_no": 39, "id": "boto.opsworks.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def insert(self, path, source):", "metadata": {"task_id": "Internet/pyramid/58", "ground_truth": "        if not path or path.endswith('/'):\n            override = DirectoryOverride(path, source)\n        else:\n            override = FileOverride(path, source)\n        self.overrides.insert(0, override)\n        return override", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "assets.py"], "context_start_lineno": 110, "line_no": 111, "id": "pyramid.config.assets.PackageOverrides.insert", "target_function_prompt": "    def insert(self, path, source):", "function_signature": "    def insert(self, path, source):"}}
{"prompt": "    def sort(self, sort_criteria, criteria=\"ALL\", charset=\"UTF-8\"):", "metadata": {"task_id": "Communications/IMAPClient/20", "ground_truth": "        args = [\n            _normalise_sort_criteria(sort_criteria),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"SORT\", args, unpack=True)\n        return [int(i) for i in ids.split()]", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1172, "line_no": 1194, "id": "imapclient.imapclient.IMAPClient.sort", "target_function_prompt": "    def sort(self, sort_criteria, criteria=\"ALL\", charset=\"UTF-8\"):", "function_signature": "    def sort(self, sort_criteria, criteria=\"ALL\", charset=\"UTF-8\"):"}}
{"prompt": "def _parse_pre_yarn_history_log(lines):", "metadata": {"task_id": "System/mrjob/55", "ground_truth": "    result = {}\n    task_to_counters = {}  # used for successful tasks in failed jobs\n\n    for record in _parse_pre_yarn_history_records(lines):\n        fields = record['fields']\n\n        # if job is successful, we get counters for the entire job at the end\n        if record['type'] == 'Job' and 'COUNTERS' in fields:\n            result['counters'] = _parse_pre_yarn_counters(fields['COUNTERS'])\n\n        # otherwise, compile counters for each successful task\n        #\n        # Note: this apparently records a higher total than the task tracker\n        # (possibly some tasks are duplicates?). Couldn't figure out the logic\n        # behind this while looking at the history file\n        elif (record['type'] == 'Task' and\n              'COUNTERS' in fields and 'TASKID' in fields):\n            task_id = fields['TASKID']\n            counters = _parse_pre_yarn_counters(fields['COUNTERS'])\n\n            task_to_counters[task_id] = counters\n\n        # only want FAILED (not KILLED) tasks with non-blank errors\n        elif (record['type'] in ('MapAttempt', 'ReduceAttempt') and\n              'TASK_ATTEMPT_ID' in fields and\n              fields.get('TASK_STATUS') == 'FAILED' and\n              fields.get('ERROR')):\n            result.setdefault('errors', [])\n            result['errors'].append(dict(\n                hadoop_error=dict(\n                    message=fields['ERROR'],\n                    start_line=record['start_line'],\n                    num_lines=record['num_lines']),\n                attempt_id=fields['TASK_ATTEMPT_ID']))\n\n    # if job failed, patch together counters from successful tasks\n    if 'counters' not in result and task_to_counters:\n        result['counters'] = _sum_counters(*task_to_counters.values())\n\n    return result", "fpath_tuple": ["System", "mrjob", "mrjob", "logs", "history.py"], "context_start_lineno": 286, "line_no": 293, "id": "mrjob.logs.history._parse_pre_yarn_history_log", "target_function_prompt": "def _parse_pre_yarn_history_log(lines):", "function_signature": "def _parse_pre_yarn_history_log(lines):"}}
{"prompt": "def collect_nodes(metadata, base=\"\", nodes=None):", "metadata": {"task_id": "Software-Development/dash/11", "ground_truth": "    nodes = nodes or []\n\n    for prop_name, value in metadata.items():\n        # Support for recursive shapes, the type is directly in the field.\n        t_value = value.get(\"type\", value)\n        p_type = t_value.get(\"name\")\n\n        if base:\n            key = f\"{base}.{prop_name}\"\n        else:\n            key = prop_name\n        if is_node(p_type):\n            nodes.append(key)\n        elif p_type == \"arrayOf\":\n            a_value = t_value.get(\"value\", t_value)\n            nodes = collect_array(a_value, key, nodes)\n        elif is_shape(p_type):\n            nodes = collect_nodes(t_value[\"value\"], key, nodes)\n        elif p_type == \"union\":\n            nodes = collect_union(t_value[\"value\"], key, nodes)\n        elif p_type == \"objectOf\":\n            o_value = t_value.get(\"value\", {})\n            nodes = collect_object(o_value, key, nodes)\n\n    return nodes", "fpath_tuple": ["Software-Development", "dash", "dash", "development", "_collect_nodes.py"], "context_start_lineno": 48, "line_no": 49, "id": "dash.development._collect_nodes.collect_nodes", "target_function_prompt": "def collect_nodes(metadata, base=\"\", nodes=None):", "function_signature": "def collect_nodes(metadata, base=\"\", nodes=None):"}}
{"prompt": "    def try_read_headers(self):", "metadata": {"task_id": "Database/mssql-cli/12", "ground_truth": "        scan_offset = self.read_offset\n        while (scan_offset + 3 < self.buffer_end_offset and\n               (self.buffer[scan_offset] != self.CR or\n                self.buffer[scan_offset + 1] != self.LF or\n                self.buffer[scan_offset + 2] != self.CR or\n                self.buffer[scan_offset + 3] != self.LF)):\n            scan_offset += 1\n\n        # if we reached the end\n        if scan_offset + 3 >= self.buffer_end_offset:\n            return False\n\n        # Split the headers by new line\n        try:\n            headers_read = self.buffer[self.read_offset:scan_offset].decode(\n                u'ascii')\n            for header in headers_read.split(u'\\n'):\n                colon_index = header.find(u':')\n\n                if colon_index == -1:\n                    logger.debug(\n                        u'JSON RPC Reader encountered missing colons in try_read_headers()')\n                    raise KeyError(\n                        u'Colon missing from Header: {}.'.format(header))\n\n                # Case insensitive.\n                header_key = header[:colon_index].lower()\n                header_value = header[colon_index + 1:]\n\n                self.headers[header_key] = header_value\n\n            # Was content-length header found?\n            if 'content-length' not in self.headers:\n                logger.debug(\n                    u'JSON RPC Reader did not find Content-Length in the headers')\n                raise LookupError(\n                    u'Content-Length was not found in headers received.')\n\n            self.expected_content_length = int(self.headers[u'content-length'])\n\n        except ValueError:\n            # Content-length contained invalid literal for int.\n            self.trim_buffer_and_resize(scan_offset + 4)\n            raise\n\n        # Pushing read pointer past the newline characters.\n        self.read_offset = scan_offset + 4\n        self.read_state = ReadState.Content\n\n        return True", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "jsonrpc", "jsonrpcclient.py"], "context_start_lineno": 333, "line_no": 344, "id": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "target_function_prompt": "    def try_read_headers(self):", "function_signature": "    def try_read_headers(self):"}}
{"prompt": "    def serialize(self, value, display=False):", "metadata": {"task_id": "Multimedia/Mopidy/22", "ground_truth": "        if value is not None and display:\n            return \"********\"\n        return super().serialize(value, display)", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 137, "line_no": 138, "id": "mopidy.config.types.Secret.serialize", "target_function_prompt": "    def serialize(self, value, display=False):", "function_signature": "    def serialize(self, value, display=False):"}}
{"prompt": "    def resend_unprocessed(self):\n        # If there are unprocessed records (for instance, the user was over\n        # their throughput limitations), iterate over them & send until they're\n        # all there.", "metadata": {"task_id": "Internet/boto/57", "ground_truth": "        boto.log.info(\n            \"Re-sending %s unprocessed items.\" % len(self._unprocessed)\n        )\n\n        while len(self._unprocessed):\n            # Again, do 25 at a time.\n            to_resend = self._unprocessed[:25]\n            # Remove them from the list.\n            self._unprocessed = self._unprocessed[25:]\n            batch_data = {\n                self.table.table_name: to_resend\n            }\n            boto.log.info(\"Sending %s items\" % len(to_resend))\n            resp = self.table.connection.batch_write_item(batch_data)\n            self.handle_unprocessed(resp)\n            boto.log.info(\n                \"%s unprocessed items left\" % len(self._unprocessed)\n            )", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 1702, "line_no": 1706, "id": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "target_function_prompt": "    def resend_unprocessed(self):\n        # If there are unprocessed records (for instance, the user was over\n        # their throughput limitations), iterate over them & send until they're\n        # all there.", "function_signature": "    def resend_unprocessed(self):\n        # If there are unprocessed records (for instance, the user was over\n        # their throughput limitations), iterate over them & send until they're\n        # all there."}}
{"prompt": "def get_bounds(locations, lonlat=False):", "metadata": {"task_id": "Scientific-Engineering/folium/1", "ground_truth": "    bounds = [[None, None], [None, None]]\n    for point in iter_coords(locations):\n        bounds = [\n            [\n                none_min(bounds[0][0], point[0]),\n                none_min(bounds[0][1], point[1]),\n            ],\n            [\n                none_max(bounds[1][0], point[0]),\n                none_max(bounds[1][1], point[1]),\n            ],\n        ]\n    if lonlat:\n        bounds = _locations_mirror(bounds)\n    return bounds", "fpath_tuple": ["Scientific-Engineering", "folium", "folium", "utilities.py"], "context_start_lineno": 381, "line_no": 387, "id": "folium.utilities.get_bounds", "target_function_prompt": "def get_bounds(locations, lonlat=False):", "function_signature": "def get_bounds(locations, lonlat=False):"}}
{"prompt": "def pandas_cramers_compute(\n    config: Settings, df: pd.DataFrame, summary: dict\n) -> Optional[pd.DataFrame]:", "metadata": {"task_id": "Software-Development/pandas-profiling/5", "ground_truth": "    threshold = config.categorical_maximum_correlation_distinct\n\n    # `index` and `columns` must not be a set since Pandas 1.5,\n    # so convert it to a list. The order of the list is arbitrary.\n    categoricals = list(\n        {\n            key\n            for key, value in summary.items()\n            if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n            and 1 < value[\"n_distinct\"] <= threshold\n        }\n    )\n\n    if len(categoricals) <= 1:\n        return None\n\n    matrix = np.zeros((len(categoricals), len(categoricals)))\n    np.fill_diagonal(matrix, 1.0)\n    correlation_matrix = pd.DataFrame(\n        matrix,\n        index=categoricals,\n        columns=categoricals,\n    )\n\n    for name1, name2 in itertools.combinations(categoricals, 2):\n        confusion_matrix = pd.crosstab(df[name1], df[name2])\n        if confusion_matrix.empty:\n            correlation_matrix.loc[name2, name1] = np.nan\n        else:\n            correlation_matrix.loc[name2, name1] = _cramers_corrected_stat(\n                confusion_matrix, correction=True\n            )\n        correlation_matrix.loc[name1, name2] = correlation_matrix.loc[name2, name1]\n    return correlation_matrix", "fpath_tuple": ["Software-Development", "pandas-profiling", "src", "ydata_profiling", "model", "pandas", "correlations_pandas.py"], "context_start_lineno": 84, "line_no": 87, "id": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "target_function_prompt": "def pandas_cramers_compute(\n    config: Settings, df: pd.DataFrame, summary: dict\n) -> Optional[pd.DataFrame]:", "function_signature": "def pandas_cramers_compute(\n    config: Settings, df: pd.DataFrame, summary: dict\n) -> Optional[pd.DataFrame]:"}}
{"prompt": "    def to_plotly_json(self):\n        # Add normal properties", "metadata": {"task_id": "Software-Development/dash/12", "ground_truth": "        props = {\n            p: getattr(self, p)\n            for p in self._prop_names  # pylint: disable=no-member\n            if hasattr(self, p)\n        }\n        # Add the wildcard properties data-* and aria-*\n        props.update(\n            {\n                k: getattr(self, k)\n                for k in self.__dict__\n                if any(\n                    k.startswith(w)\n                    # pylint:disable=no-member\n                    for w in self._valid_wildcard_attributes\n                )\n            }\n        )\n        as_json = {\n            \"props\": props,\n            \"type\": self._type,  # pylint: disable=no-member\n            \"namespace\": self._namespace,  # pylint: disable=no-member\n        }\n\n        return as_json", "fpath_tuple": ["Software-Development", "dash", "dash", "development", "base_component.py"], "context_start_lineno": 202, "line_no": 204, "id": "dash.development.base_component.Component.to_plotly_json", "target_function_prompt": "    def to_plotly_json(self):\n        # Add normal properties", "function_signature": "    def to_plotly_json(self):\n        # Add normal properties"}}
{"prompt": "def assert_imap_protocol(condition: bool, message: Optional[bytes] = None) -> None:", "metadata": {"task_id": "Communications/IMAPClient/21", "ground_truth": "    from . import exceptions\n    if not condition:\n        msg = \"Server replied with a response that violates the IMAP protocol\"\n        if message:\n            # FIXME(jlvillal): This looks wrong as it repeats `msg` twice\n            msg += \"{}: {}\".format(\n                msg, message.decode(encoding=\"ascii\", errors=\"ignore\")\n            )\n        raise exceptions.ProtocolError(msg)", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "util.py"], "context_start_lineno": 32, "line_no": 33, "id": "imapclient.util.assert_imap_protocol", "target_function_prompt": "def assert_imap_protocol(condition: bool, message: Optional[bytes] = None) -> None:", "function_signature": "def assert_imap_protocol(condition: bool, message: Optional[bytes] = None) -> None:"}}
{"prompt": "    def load_from_dir(self, from_date, to_date):", "metadata": {"task_id": "Security/trailscraper/5", "ground_truth": "        records = []\n        for logfile in self._valid_log_files():\n            if logfile.contains_events_for_timeframe(from_date, to_date):\n                records.extend(logfile.records())\n\n        return records", "fpath_tuple": ["Security", "trailscraper", "trailscraper", "record_sources", "local_directory_record_source.py"], "context_start_lineno": 36, "line_no": 38, "id": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "target_function_prompt": "    def load_from_dir(self, from_date, to_date):", "function_signature": "    def load_from_dir(self, from_date, to_date):"}}
{"prompt": "def history(\n    config: Config,\n    rev_range: Optional[str] = None,\n    verbose: bool = False,\n    indicate_current: bool = False,\n) -> None:", "metadata": {"task_id": "Database/alembic/25", "ground_truth": "    base: Optional[str]\n    head: Optional[str]\n    script = ScriptDirectory.from_config(config)\n    if rev_range is not None:\n        if \":\" not in rev_range:\n            raise util.CommandError(\n                \"History range requires [start]:[end], \" \"[start]:, or :[end]\"\n            )\n        base, head = rev_range.strip().split(\":\")\n    else:\n        base = head = None\n\n    environment = (\n        util.asbool(config.get_main_option(\"revision_environment\"))\n        or indicate_current\n    )\n\n    def _display_history(config, script, base, head, currents=()):\n        for sc in script.walk_revisions(\n            base=base or \"base\", head=head or \"heads\"\n        ):\n            if indicate_current:\n                sc._db_current_indicator = sc.revision in currents\n\n            config.print_stdout(\n                sc.cmd_format(\n                    verbose=verbose,\n                    include_branches=True,\n                    include_doc=True,\n                    include_parents=True,\n                )\n            )\n\n    def _display_history_w_current(config, script, base, head):\n        def _display_current_history(rev, context):\n            if head == \"current\":\n                _display_history(config, script, base, rev, rev)\n            elif base == \"current\":\n                _display_history(config, script, rev, head, rev)\n            else:\n                _display_history(config, script, base, head, rev)\n            return []\n\n        with EnvironmentContext(config, script, fn=_display_current_history):\n            script.run_env()\n\n    if base == \"current\" or head == \"current\" or environment:\n        _display_history_w_current(config, script, base, head)\n    else:\n        _display_history(config, script, base, head)", "fpath_tuple": ["Database", "alembic", "alembic", "command.py"], "context_start_lineno": 471, "line_no": 488, "id": "alembic.command.history", "target_function_prompt": "def history(\n    config: Config,\n    rev_range: Optional[str] = None,\n    verbose: bool = False,\n    indicate_current: bool = False,\n) -> None:", "function_signature": "def history(\n    config: Config,\n    rev_range: Optional[str] = None,\n    verbose: bool = False,\n    indicate_current: bool = False,\n) -> None:"}}
{"prompt": "    def put(self, src, path):", "metadata": {"task_id": "System/mrjob/56", "ground_truth": "        path = _from_file_uri(path)\n        shutil.copyfile(src, path)", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "local.py"], "context_start_lineno": 67, "line_no": 69, "id": "mrjob.fs.local.LocalFilesystem.put", "target_function_prompt": "    def put(self, src, path):", "function_signature": "    def put(self, src, path):"}}
{"prompt": "    def recv(self, size, flags=0, timeout=_UNSET):", "metadata": {"task_id": "Utilities/boltons/48", "ground_truth": "        with self._recv_lock:\n            if timeout is _UNSET:\n                timeout = self.timeout\n            if flags:\n                raise ValueError(\"non-zero flags not supported: %r\" % flags)\n            if len(self.rbuf) >= size:\n                data, self.rbuf = self.rbuf[:size], self.rbuf[size:]\n                return data\n            if self.rbuf:\n                ret, self.rbuf = self.rbuf, b''\n                return ret\n            self.sock.settimeout(timeout)\n            try:\n                data = self.sock.recv(self._recvsize)\n            except socket.timeout:\n                raise Timeout(timeout)  # check the rbuf attr for more\n            if len(data) > size:\n                data, self.rbuf = data[:size], data[size:]\n        return data", "fpath_tuple": ["Utilities", "boltons", "boltons", "socketutils.py"], "context_start_lineno": 198, "line_no": 218, "id": "boltons.socketutils.BufferedSocket.recv", "target_function_prompt": "    def recv(self, size, flags=0, timeout=_UNSET):", "function_signature": "    def recv(self, size, flags=0, timeout=_UNSET):"}}
{"prompt": "    def from_system(cls) -> float:", "metadata": {"task_id": "Scientific-Engineering/bentoml/21", "ground_truth": "        if psutil.POSIX:\n            return query_cgroup_cpu_count()\n        else:\n            return float(query_os_cpu_count())", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "resource.py"], "context_start_lineno": 104, "line_no": 105, "id": "bentoml._internal.resource.CpuResource.from_system", "target_function_prompt": "    def from_system(cls) -> float:", "function_signature": "    def from_system(cls) -> float:"}}
{"prompt": "    def serialize(cls, value, *args, **kwargs):", "metadata": {"task_id": "Text-Processing/rows/7", "ground_truth": "        if value is not None:\n            if not isinstance(value, six.binary_type):\n                value_error(value, cls)\n            else:\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, binascii.Error):\n                    return value\n        else:\n            return \"\"", "fpath_tuple": ["Text-Processing", "rows", "rows", "fields.py"], "context_start_lineno": 112, "line_no": 113, "id": "rows.fields.BinaryField.serialize", "target_function_prompt": "    def serialize(cls, value, *args, **kwargs):", "function_signature": "    def serialize(cls, value, *args, **kwargs):"}}
{"prompt": "    def read(self):", "metadata": {"task_id": "Security/passpie/0", "ground_truth": "        elements = []\n        for rootdir, dirs, files in os.walk(self.path):\n            filenames = [f for f in files if f.endswith(self.extension)]\n            for filename in filenames:\n                docpath = os.path.join(rootdir, filename)\n                with open(docpath) as f:\n                    elements.append(yaml.load(f.read()))\n\n        return {\"_default\":\n                {idx: elem for idx, elem in enumerate(elements, start=1)}}", "fpath_tuple": ["Security", "passpie", "passpie", "database.py"], "context_start_lineno": 32, "line_no": 33, "id": "passpie.database.PasspieStorage.read", "target_function_prompt": "    def read(self):", "function_signature": "    def read(self):"}}
{"prompt": "    def counter(self) -> Union[int, float]:", "metadata": {"task_id": "Utilities/praw/7", "ground_truth": "        max_jitter = self._base / 16.0\n        value = self._base + random.random() * max_jitter - max_jitter / 2\n        self._base = min(self._base * 2, self._max)\n        return value", "fpath_tuple": ["Utilities", "praw", "praw", "models", "util.py"], "context_start_lineno": 208, "line_no": 210, "id": "praw.models.util.ExponentialCounter.counter", "target_function_prompt": "    def counter(self) -> Union[int, float]:", "function_signature": "    def counter(self) -> Union[int, float]:"}}
{"prompt": "    def stream_box_edit_view(\n        self, stream_id: int, caption: str = \"\", title: str = \"\"\n    ) -> None:", "metadata": {"task_id": "Communications/zulip-term/13", "ground_truth": "        self.stream_write_box = urwid.Text(caption)\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        self.edit_mode_button = EditModeButton(\n            controller=self.model.controller,\n            width=20,\n        )\n        self.header_write_box.widget_list.append(self.edit_mode_button)\n\n        # Use callback to set stream marker - it shouldn't change, so don't need signal\n        self._set_stream_write_box_style(None, caption)", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "boxes.py"], "context_start_lineno": 400, "line_no": 403, "id": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "target_function_prompt": "    def stream_box_edit_view(\n        self, stream_id: int, caption: str = \"\", title: str = \"\"\n    ) -> None:", "function_signature": "    def stream_box_edit_view(\n        self, stream_id: int, caption: str = \"\", title: str = \"\"\n    ) -> None:"}}
{"prompt": "def clip_to_bounds(array, bounds):", "metadata": {"task_id": "Security/diffprivlib/14", "ground_truth": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n\n    lower, upper = check_bounds(bounds, np.size(bounds[0]), min_separation=0)\n    clipped_array = array.copy()\n\n    if np.allclose(lower, np.min(lower)) and np.allclose(upper, np.max(upper)):\n        clipped_array = np.clip(clipped_array, np.min(lower), np.max(upper))\n    else:\n        if array.ndim != 2:\n            raise ValueError(f\"For non-scalar bounds, input array must be 2-dimensional. Got {array.ndim} dimensions.\")\n\n        for feature in range(array.shape[1]):\n            clipped_array[:, feature] = np.clip(array[:, feature], lower[feature], upper[feature])\n\n    return clipped_array", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "validation.py"], "context_start_lineno": 166, "line_no": 184, "id": "diffprivlib.validation.clip_to_bounds", "target_function_prompt": "def clip_to_bounds(array, bounds):", "function_signature": "def clip_to_bounds(array, bounds):"}}
{"prompt": "def validate_metadata(metadata: MetadataDict):", "metadata": {"task_id": "Scientific-Engineering/bentoml/22", "ground_truth": "    if not isinstance(metadata, dict):\n        raise ValueError(\"metadata must be a dict!\")\n\n    for key, val in metadata.items():\n        if not isinstance(key, (str, int, float)):\n            raise ValueError(\"metadata keys must be strings\")\n\n        metadata[key] = _validate_metadata_entry(val)", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "utils", "__init__.py"], "context_start_lineno": 320, "line_no": 321, "id": "bentoml._internal.utils.validate_metadata", "target_function_prompt": "def validate_metadata(metadata: MetadataDict):", "function_signature": "def validate_metadata(metadata: MetadataDict):"}}
{"prompt": "", "metadata": {"task_id": "Database/datasette/29", "ground_truth": "    @classmethod\n    def html(cls, body, status=200, headers=None):\n        return cls(\n            body,\n            status=status,\n            headers=headers,", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "asgi.py"], "context_start_lineno": 383, "line_no": 384, "id": "datasette.utils.asgi.Response.html", "target_function_prompt": "", "function_signature": ""}}
{"prompt": "    def _TweetTextWrap(self,\n                       status,\n                       char_lim=CHARACTER_LIMIT):\n", "metadata": {"task_id": "Internet/python-twitter/0", "ground_truth": "        from twitter.twitter_utils import is_url\n        if not self._config:\n            self.GetHelpConfiguration()\n\n        tweets = []\n        line = []\n        line_length = 0\n        words = re.split(r'\\s', status)\n\n        if len(words) == 1 and not is_url(words[0]):\n            if len(words[0]) > CHARACTER_LIMIT:\n                raise TwitterError(\"Unable to split status into tweetable parts. Word was: {0}/{1}\".format(len(words[0]), char_lim))\n            else:\n                tweets.append(words[0])\n                return tweets\n\n        for word in words:\n            if len(word) > char_lim:\n                raise TwitterError(\"Unable to split status into tweetable parts. Word was: {0}/{1}\".format(len(word), char_lim))\n            new_len = line_length\n\n            if is_url(word):\n                new_len = line_length + self._config['short_url_length_https'] + 1\n            else:\n                new_len += len(word) + 1\n\n            if new_len > CHARACTER_LIMIT:\n                tweets.append(' '.join(line))\n                line = [word]\n                line_length = new_len - line_length\n            else:\n                line.append(word)\n                line_length = new_len\n\n        tweets.append(' '.join(line))\n        return tweets", "fpath_tuple": ["Internet", "python-twitter", "twitter", "api.py"], "context_start_lineno": 1430, "line_no": 1434, "id": "twitter.api.Api._TweetTextWrap", "target_function_prompt": "    def _TweetTextWrap(self,\n                       status,\n                       char_lim=CHARACTER_LIMIT):\n", "function_signature": "    def _TweetTextWrap(self,\n                       status,\n                       char_lim=CHARACTER_LIMIT):\n"}}
{"prompt": "def call(function, *args, **kwargs):", "metadata": {"task_id": "System/pyinfra/10", "ground_truth": "    from pyinfra import logger\n    from pyinfra.api.util import get_call_location\n    argspec = getfullargspec(function)\n    if \"state\" in argspec.args and \"host\" in argspec.args:\n        logger.warning(\n            \"Callback functions used in `python.call` operations no \"\n            f\"longer take `state` and `host` arguments: {get_call_location(frame_offset=3)}\",\n        )\n\n    kwargs.pop(\"state\", None)\n    kwargs.pop(\"host\", None)\n    yield FunctionCommand(function, args, kwargs)", "fpath_tuple": ["System", "pyinfra", "pyinfra", "operations", "python.py"], "context_start_lineno": 12, "line_no": 45, "id": "pyinfra.operations.python.call", "target_function_prompt": "def call(function, *args, **kwargs):", "function_signature": "def call(function, *args, **kwargs):"}}
{"prompt": "def tokenize_format_str(fstr, resolve_pos=True):", "metadata": {"task_id": "Utilities/boltons/49", "ground_truth": "    ret = []\n    if resolve_pos:\n        fstr = infer_positional_format_args(fstr)\n    formatter = Formatter()\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if lit:\n            ret.append(lit)\n        if fname is None:\n            continue\n        ret.append(BaseFormatField(fname, fspec, conv))\n    return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "formatutils.py"], "context_start_lineno": 202, "line_no": 209, "id": "boltons.formatutils.tokenize_format_str", "target_function_prompt": "def tokenize_format_str(fstr, resolve_pos=True):", "function_signature": "def tokenize_format_str(fstr, resolve_pos=True):"}}
{"prompt": "def run_ops(state: \"State\", serial: bool = False, no_wait: bool = False):", "metadata": {"task_id": "System/pyinfra/11", "ground_truth": "    from pyinfra.context import ctx_state\n    state.is_executing = True\n\n    with ctx_state.use(state):\n        # Run all ops, but server by server\n        if serial:\n            _run_serial_ops(state)\n        # Run all the ops on each server in parallel (not waiting at each operation)\n        elif no_wait:\n            _run_no_wait_ops(state)\n        # Default: run all ops in order, waiting at each for all servers to complete\n        else:\n            for op_hash in state.get_op_order():\n                _run_single_op(state, op_hash)", "fpath_tuple": ["System", "pyinfra", "pyinfra", "api", "operations.py"], "context_start_lineno": 357, "line_no": 368, "id": "pyinfra.api.operations.run_ops", "target_function_prompt": "def run_ops(state: \"State\", serial: bool = False, no_wait: bool = False):", "function_signature": "def run_ops(state: \"State\", serial: bool = False, no_wait: bool = False):"}}
{"prompt": "    def _left_record_node(self) -> Union['LonelyRootNode', 'LeafNode']:", "metadata": {"task_id": "Database/bplustree/9", "ground_truth": "        node = self._root_node\n        while not isinstance(node, (LonelyRootNode, LeafNode)):\n            node = self._mem.get_node(node.smallest_entry.before)\n        return node", "fpath_tuple": ["Database", "bplustree", "bplustree", "tree.py"], "context_start_lineno": 277, "line_no": 278, "id": "bplustree.tree.BPlusTree._left_record_node", "target_function_prompt": "    def _left_record_node(self) -> Union['LonelyRootNode', 'LeafNode']:", "function_signature": "    def _left_record_node(self) -> Union['LonelyRootNode', 'LeafNode']:"}}
{"prompt": "    def _topic_box_autocomplete(self, text: str, state: Optional[int]) -> Optional[str]:", "metadata": {"task_id": "Communications/zulip-term/14", "ground_truth": "        topic_names = self.model.topics_in_stream(self.stream_id)\n\n        topic_typeaheads = match_topics(topic_names, text)\n\n        # Typeaheads and suggestions are the same.\n        return self._process_typeaheads(topic_typeaheads, state, topic_typeaheads)", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "boxes.py"], "context_start_lineno": 453, "line_no": 454, "id": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "target_function_prompt": "    def _topic_box_autocomplete(self, text: str, state: Optional[int]) -> Optional[str]:", "function_signature": "    def _topic_box_autocomplete(self, text: str, state: Optional[int]) -> Optional[str]:"}}
{"prompt": "def extract_open_path(line):", "metadata": {"task_id": "System/exodus-bundler/9", "ground_truth": "    line = strip_pid_prefix(line)\n    for prefix in ['openat(AT_FDCWD, \"', 'open(\"']:\n        if line.startswith(prefix):\n            parts = line[len(prefix):].split('\", ')\n            if len(parts) != 2:\n                continue\n            if 'ENOENT' in parts[1]:\n                continue\n            if 'O_RDONLY' not in parts[1]:\n                continue\n            if 'O_DIRECTORY' in parts[1]:\n                continue\n            return parts[0]\n    return None", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "input_parsing.py"], "context_start_lineno": 40, "line_no": 42, "id": "exodus_bundler.input_parsing.extract_open_path", "target_function_prompt": "def extract_open_path(line):", "function_signature": "def extract_open_path(line):"}}
{"prompt": "def _detect_http_redirection(http_response: HTTPResponse, server_host_name: str, server_port: int) -> Optional[str]:", "metadata": {"task_id": "System/sslyze/6", "ground_truth": "    next_location_path = None\n    if 300 <= http_response.status < 400:\n        location_header = _extract_first_header_value(http_response, \"Location\")\n        if location_header:\n            parsed_location = urlsplit(location_header)\n            is_relative_url = False if parsed_location.hostname else True\n            if is_relative_url:\n                # Yes, to a relative URL; follow the redirection\n                next_location_path = location_header\n            else:\n                is_absolute_url_to_same_hostname = parsed_location.hostname == server_host_name\n                absolute_url_port = 443 if parsed_location.port is None else parsed_location.port\n                is_absolute_url_to_same_port = absolute_url_port == server_port\n                if is_absolute_url_to_same_hostname and is_absolute_url_to_same_port:\n                    # Yes, to an absolute URL to the same server; follow the redirection\n                    next_location_path = f\"{parsed_location.path}\"\n                    if parsed_location.query:\n                        next_location_path += f\"?{parsed_location.query}\"\n\n    return next_location_path", "fpath_tuple": ["System", "sslyze", "sslyze", "plugins", "http_headers_plugin.py"], "context_start_lineno": 269, "line_no": 271, "id": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "target_function_prompt": "def _detect_http_redirection(http_response: HTTPResponse, server_host_name: str, server_port: int) -> Optional[str]:", "function_signature": "def _detect_http_redirection(http_response: HTTPResponse, server_host_name: str, server_port: int) -> Optional[str]:"}}
{"prompt": "    def read(self, size):", "metadata": {"task_id": "Utilities/gunicorn/13", "ground_truth": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n\n        size = min(self.length, size)\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        data = self.unreader.read()\n        while data:\n            buf.write(data)\n            if buf.tell() >= size:\n                break\n            data = self.unreader.read()\n\n        buf = buf.getvalue()\n        ret, rest = buf[:size], buf[size:]\n        self.unreader.unread(rest)\n        self.length -= size\n        return ret", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "http", "body.py"], "context_start_lineno": 113, "line_no": 114, "id": "gunicorn.http.body.LengthReader.read", "target_function_prompt": "    def read(self, size):", "function_signature": "    def read(self, size):"}}
{"prompt": "    def __repr__(self):", "metadata": {"task_id": "Utilities/boltons/50", "ground_truth": "        cn = self.__class__.__name__\n        val_map = super(LRI, self).__repr__()\n        return ('%s(max_size=%r, on_miss=%r, values=%s)'\n                % (cn, self.max_size, self.on_miss, val_map))", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 332, "line_no": 333, "id": "boltons.cacheutils.LRI.__repr__", "target_function_prompt": "    def __repr__(self):", "function_signature": "    def __repr__(self):"}}
{"prompt": "    def _get_flattened_ll(self):", "metadata": {"task_id": "Utilities/boltons/51", "ground_truth": "        flattened_list = []\n        link = self._anchor\n        while True:\n            flattened_list.append((link[KEY], link[VALUE]))\n            link = link[NEXT]\n            if link is self._anchor:\n                break\n        return flattened_list", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 165, "line_no": 166, "id": "boltons.cacheutils.LRI._get_flattened_ll", "target_function_prompt": "    def _get_flattened_ll(self):", "function_signature": "    def _get_flattened_ll(self):"}}
{"prompt": "    def read(self, line):", "metadata": {"task_id": "System/mrjob/57", "ground_truth": "        raw_key, raw_value = line.split(b'\\t', 1)\n\n        if raw_key != self._last_key_encoded:\n            self._last_key_encoded = raw_key\n            self._last_key_decoded = self._loads(raw_key)\n        return (self._last_key_decoded, self._loads(raw_value))", "fpath_tuple": ["System", "mrjob", "mrjob", "protocol.py"], "context_start_lineno": 82, "line_no": 90, "id": "mrjob.protocol._KeyCachingProtocol.read", "target_function_prompt": "    def read(self, line):", "function_signature": "    def read(self, line):"}}
{"prompt": "    def _task_python_bin(self):", "metadata": {"task_id": "System/mrjob/58", "ground_truth": "        return (self._opts['task_python_bin'] or\n                self._python_bin())", "fpath_tuple": ["System", "mrjob", "mrjob", "bin.py"], "context_start_lineno": 184, "line_no": 187, "id": "mrjob.bin.MRJobBinRunner._task_python_bin", "target_function_prompt": "    def _task_python_bin(self):", "function_signature": "    def _task_python_bin(self):"}}
{"prompt": "def compute_cmds_probs(  # nosec\n    seq1_counts: Union[StateMatrix, dict],\n    seq2_counts: Union[StateMatrix, dict],\n    unk_token: str,\n) -> Tuple[StateMatrix, StateMatrix]:", "metadata": {"task_id": "Security/msticpy/8", "ground_truth": "    total_cmds = sum(seq1_counts.values())\n\n    prior_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    trans_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    # compute prior probs\n    for cmd in seq1_counts:\n        prior_probs[cmd] = seq1_counts[cmd] / total_cmds\n\n    # compute trans probs\n    for prev, currents in seq2_counts.items():\n        for current in currents:\n            trans_probs[prev][current] = seq2_counts[prev][current] / sum(\n                seq2_counts[prev].values()\n            )\n\n    prior_probs_sm = StateMatrix(states=prior_probs, unk_token=unk_token)\n    trans_probs_sm = StateMatrix(states=trans_probs, unk_token=unk_token)\n\n    return prior_probs_sm, trans_probs_sm", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "probabilities.py"], "context_start_lineno": 13, "line_no": 40, "id": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "target_function_prompt": "def compute_cmds_probs(  # nosec\n    seq1_counts: Union[StateMatrix, dict],\n    seq2_counts: Union[StateMatrix, dict],\n    unk_token: str,\n) -> Tuple[StateMatrix, StateMatrix]:", "function_signature": "def compute_cmds_probs(  # nosec\n    seq1_counts: Union[StateMatrix, dict],\n    seq2_counts: Union[StateMatrix, dict],\n    unk_token: str,\n) -> Tuple[StateMatrix, StateMatrix]:"}}
{"prompt": "def add(buffer: bytes, entropy: int) -> None:", "metadata": {"task_id": "Security/pyOpenSSL/0", "ground_truth": "    if not isinstance(buffer, bytes):\n        raise TypeError(\"buffer must be a byte string\")\n\n    if not isinstance(entropy, int):\n        raise TypeError(\"entropy must be an integer\")\n\n    _lib.RAND_add(buffer, len(buffer), entropy)", "fpath_tuple": ["Security", "pyOpenSSL", "src", "OpenSSL", "rand.py"], "context_start_lineno": 7, "line_no": 24, "id": "OpenSSL.rand.add", "target_function_prompt": "def add(buffer: bytes, entropy: int) -> None:", "function_signature": "def add(buffer: bytes, entropy: int) -> None:"}}
{"prompt": "    def is_task(self):", "metadata": {"task_id": "System/mrjob/59", "ground_truth": "        return (self.options.run_mapper or\n                self.options.run_combiner or\n                self.options.run_reducer or\n                self.options.run_spark)", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 1220, "line_no": 1226, "id": "mrjob.job.MRJob.is_task", "target_function_prompt": "    def is_task(self):", "function_signature": "    def is_task(self):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/58", "ground_truth": "    from boto.regioninfo import connect\n    from boto.cloudsearch2.layer1 import CloudSearchConnection\n    return connect('cloudsearch', region_name,\n                   connection_cls=CloudSearchConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "cloudsearch2", "__init__.py"], "context_start_lineno": 35, "line_no": 36, "id": "boto.cloudsearch2.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "def ite(size, cond, true, false):", "metadata": {"task_id": "Security/barf/6", "ground_truth": "    from barf.core.smt.smtsymbol import Bool\n    assert type(cond) is Bool\n\n    return BitVec(size, \"ite\", cond, true, false)", "fpath_tuple": ["Security", "barf", "barf", "core", "smt", "smtfunction.py"], "context_start_lineno": 58, "line_no": 59, "id": "barf.core.smt.smtfunction.ite", "target_function_prompt": "def ite(size, cond, true, false):", "function_signature": "def ite(size, cond, true, false):"}}
{"prompt": "    def message(\n        self,\n        body=None,\n        to=None,\n        from_=None,\n        action=None,\n        method=None,\n        status_callback=None,\n        **kwargs\n    ):", "metadata": {"task_id": "Communications/twilio-fatisar/15", "ground_truth": "        return self.nest(\n            Message(\n                body=body,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "twiml", "messaging_response.py"], "context_start_lineno": 20, "line_no": 43, "id": "twilio.twiml.messaging_response.MessagingResponse.message", "target_function_prompt": "    def message(\n        self,\n        body=None,\n        to=None,\n        from_=None,\n        action=None,\n        method=None,\n        status_callback=None,\n        **kwargs\n    ):", "function_signature": "    def message(\n        self,\n        body=None,\n        to=None,\n        from_=None,\n        action=None,\n        method=None,\n        status_callback=None,\n        **kwargs\n    ):"}}
{"prompt": "def do_OP_RIPEMD160(stack):", "metadata": {"task_id": "Security/pycoin/23", "ground_truth": "    from ..encoding.hash import ripemd160\n    stack.append(ripemd160(stack.pop()).digest())", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "stackops.py"], "context_start_lineno": 112, "line_no": 113, "id": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "target_function_prompt": "def do_OP_RIPEMD160(stack):", "function_signature": "def do_OP_RIPEMD160(stack):"}}
{"prompt": "    def host(self):", "metadata": {"task_id": "Internet/falcon/28", "ground_truth": "        try:\n            # NOTE(kgriffs): Prefer the host header; the web server\n            # isn't supposed to mess with it, so it should be what\n            # the client actually sent.\n            host_header = self.env['HTTP_HOST']\n            host, port = parse_host(host_header)\n        except KeyError:\n            # PERF(kgriffs): According to PEP-3333, this header\n            # will always be present.\n            host = self.env['SERVER_NAME']\n\n        return host", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 830, "line_no": 831, "id": "falcon.request.Request.host", "target_function_prompt": "    def host(self):", "function_signature": "    def host(self):"}}
{"prompt": "def find_resource(resource, path):", "metadata": {"task_id": "Internet/pyramid/59", "ground_truth": "    if isinstance(path, str):\n        path = ascii_(path)\n    D = traverse(resource, path)\n    view_name = D['view_name']\n    context = D['context']\n    if view_name:\n        raise KeyError('%r has no subelement %s' % (context, view_name))\n    return context", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "traversal.py"], "context_start_lineno": 34, "line_no": 78, "id": "pyramid.traversal.find_resource", "target_function_prompt": "def find_resource(resource, path):", "function_signature": "def find_resource(resource, path):"}}
{"prompt": "    def fs(self):", "metadata": {"task_id": "System/mrjob/60", "ground_truth": "        from mrjob.fs.local import LocalFilesystem\n        from mrjob.fs.hadoop import HadoopFilesystem\n        if self._fs is None:\n            self._fs = CompositeFilesystem()\n\n            # don't pass [] to fs; this means not to use hadoop until\n            # fs.set_hadoop_bin() is called (used for running hadoop over SSH).\n            hadoop_bin = self._opts['hadoop_bin'] or None\n\n            self._fs.add_fs('hadoop',\n                            HadoopFilesystem(hadoop_bin))\n            self._fs.add_fs('local', LocalFilesystem())\n\n        return self._fs", "fpath_tuple": ["System", "mrjob", "mrjob", "hadoop.py"], "context_start_lineno": 191, "line_no": 195, "id": "mrjob.hadoop.HadoopJobRunner.fs", "target_function_prompt": "    def fs(self):", "function_signature": "    def fs(self):"}}
{"prompt": "def get_dependency_tuple_list_for_recipe(recipe, blacklist=None):", "metadata": {"task_id": "Utilities/python-for-android/17", "ground_truth": "    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n    if recipe.depends is None:\n        dependencies = []\n    else:\n        # Turn all dependencies into tuples so that product will work\n        dependencies = fix_deplist(recipe.depends)\n\n        # Filter out blacklisted items and turn lowercase:\n        dependencies = [\n            tuple(set(deptuple) - blacklist)\n            for deptuple in dependencies\n            if tuple(set(deptuple) - blacklist)\n        ]\n    return dependencies", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "graph.py"], "context_start_lineno": 41, "line_no": 45, "id": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "target_function_prompt": "def get_dependency_tuple_list_for_recipe(recipe, blacklist=None):", "function_signature": "def get_dependency_tuple_list_for_recipe(recipe, blacklist=None):"}}
{"prompt": "    def update_global_secondary_index(self, global_indexes):", "metadata": {"task_id": "Internet/boto/59", "ground_truth": "        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n            return True\n        else:\n            msg = 'You need to provide the global indexes to ' \\\n                  'update_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 556, "line_no": 583, "id": "boto.dynamodb2.table.Table.update_global_secondary_index", "target_function_prompt": "    def update_global_secondary_index(self, global_indexes):", "function_signature": "    def update_global_secondary_index(self, global_indexes):"}}
{"prompt": "    def rate_sentences(self, document):", "metadata": {"task_id": "Internet/sumy/15", "ground_truth": "        sentences_words = [(s, self._to_words_set(s)) for s in document.sentences]\n        ratings = defaultdict(float)\n\n        for (sentence1, words1), (sentence2, words2) in combinations(sentences_words, 2):\n            rank = self._rate_sentences_edge(words1, words2)\n            ratings[sentence1] += rank\n            ratings[sentence2] += rank\n\n        return ratings", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "reduction.py"], "context_start_lineno": 29, "line_no": 30, "id": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "target_function_prompt": "    def rate_sentences(self, document):", "function_signature": "    def rate_sentences(self, document):"}}
{"prompt": "def is_valid_note(note):", "metadata": {"task_id": "Multimedia/mingus/28", "ground_truth": "    if note[0] not in _note_dict:\n        return False\n    for post in note[1:]:\n        if post != \"b\" and post != \"#\":\n            return False\n    return True", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "notes.py"], "context_start_lineno": 70, "line_no": 72, "id": "mingus.core.notes.is_valid_note", "target_function_prompt": "def is_valid_note(note):", "function_signature": "def is_valid_note(note):"}}
{"prompt": "    def popitem(self):", "metadata": {"task_id": "Utilities/boltons/52", "ground_truth": "        key, val = dict.popitem(self)\n        dict.__delitem__(self.inv, val)\n        return key, val", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 863, "line_no": 864, "id": "boltons.dictutils.OneToOne.popitem", "target_function_prompt": "    def popitem(self):", "function_signature": "    def popitem(self):"}}
{"prompt": "def get_instance_userdata(version='latest', sep=None,\n                          url='http://169.254.169.254', timeout=None, num_retries=5):", "metadata": {"task_id": "Internet/boto/60", "ground_truth": "    ud_url = _build_instance_metadata_url(url, version, 'user-data')\n    user_data = retry_url(ud_url, retry_on_404=False, num_retries=num_retries, timeout=timeout)\n    if user_data:\n        if sep:\n            l = user_data.split(sep)\n            user_data = {}\n            for nvpair in l:\n                t = nvpair.split('=')\n                user_data[t[0].strip()] = t[1].strip()\n    return user_data", "fpath_tuple": ["Internet", "boto", "boto", "utils.py"], "context_start_lineno": 429, "line_no": 431, "id": "boto.utils.get_instance_userdata", "target_function_prompt": "def get_instance_userdata(version='latest', sep=None,\n                          url='http://169.254.169.254', timeout=None, num_retries=5):", "function_signature": "def get_instance_userdata(version='latest', sep=None,\n                          url='http://169.254.169.254', timeout=None, num_retries=5):"}}
{"prompt": "    def get_quota_root(self, mailbox):", "metadata": {"task_id": "Communications/IMAPClient/22", "ground_truth": "        quota_root_rep = self._raw_command_untagged(\n            b\"GETQUOTAROOT\", to_bytes(mailbox), uid=False, response_name=\"QUOTAROOT\"\n        )\n        quota_rep = self._imap.untagged_responses.pop(\"QUOTA\", [])\n        quota_root_rep = parse_response(quota_root_rep)\n        quota_root = MailboxQuotaRoots(\n            to_unicode(quota_root_rep[0]), [to_unicode(q) for q in quota_root_rep[1:]]\n        )\n        return quota_root, _parse_quota(quota_rep)", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1583, "line_no": 1593, "id": "imapclient.imapclient.IMAPClient.get_quota_root", "target_function_prompt": "    def get_quota_root(self, mailbox):", "function_signature": "    def get_quota_root(self, mailbox):"}}
{"prompt": "def parse(trigger_word_file):", "metadata": {"task_id": "Security/python-taint/3", "ground_truth": "    with open(trigger_word_file) as fd:\n        triggers_dict = json.load(fd)\n    sources = [Source(s) for s in triggers_dict['sources']]\n    sinks = [\n        Sink.from_json(trigger, data)\n        for trigger, data in triggers_dict['sinks'].items()\n    ]\n    return Definitions(sources, sinks)", "fpath_tuple": ["Security", "python-taint", "pyt", "vulnerabilities", "trigger_definitions_parser.py"], "context_start_lineno": 68, "line_no": 74, "id": "pyt.vulnerabilities.trigger_definitions_parser.parse", "target_function_prompt": "def parse(trigger_word_file):", "function_signature": "def parse(trigger_word_file):"}}
{"prompt": "def reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):", "metadata": {"task_id": "Internet/djangorestframework/15", "ground_truth": "    scheme = getattr(request, 'versioning_scheme', None)\n    if scheme is not None:\n        try:\n            url = scheme.reverse(viewname, args, kwargs, request, format, **extra)\n        except NoReverseMatch:\n            # In case the versioning scheme reversal fails, fallback to the\n            # default implementation\n            url = _reverse(viewname, args, kwargs, request, format, **extra)\n    else:\n        url = _reverse(viewname, args, kwargs, request, format, **extra)\n\n    return preserve_builtin_query_params(url, request)", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "reverse.py"], "context_start_lineno": 31, "line_no": 37, "id": "rest_framework.reverse.reverse", "target_function_prompt": "def reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):", "function_signature": "def reverse(viewname, args=None, kwargs=None, request=None, format=None, **extra):"}}
{"prompt": "    def setup_dirs(self, storage_dir):", "metadata": {"task_id": "Utilities/python-for-android/18", "ground_truth": "        self.storage_dir = expanduser(storage_dir)\n        if ' ' in self.storage_dir:\n            raise ValueError('storage dir path cannot contain spaces, please '\n                             'specify a path with --storage-dir')\n        self.build_dir = join(self.storage_dir, 'build')\n        self.dist_dir = join(self.storage_dir, 'dists')", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "build.py"], "context_start_lineno": 140, "line_no": 143, "id": "pythonforandroid.build.Context.setup_dirs", "target_function_prompt": "    def setup_dirs(self, storage_dir):", "function_signature": "    def setup_dirs(self, storage_dir):"}}
{"prompt": "    def get_csrf_token(self, request):", "metadata": {"task_id": "Internet/pyramid/60", "ground_truth": "        bound_cookies = self.cookie_profile.bind(request)\n        token = bound_cookies.get_value()\n        if not token:\n            token = self.new_csrf_token(request)\n        return token", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "csrf.py"], "context_start_lineno": 146, "line_no": 149, "id": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "target_function_prompt": "    def get_csrf_token(self, request):", "function_signature": "    def get_csrf_token(self, request):"}}
{"prompt": "    def recv_close(self, timeout=_UNSET, maxsize=_UNSET):", "metadata": {"task_id": "Utilities/boltons/53", "ground_truth": "        with self._recv_lock:\n            if maxsize is _UNSET:\n                maxsize = self.maxsize\n            if maxsize is None:\n                maxsize = _RECV_LARGE_MAXSIZE\n            try:\n                recvd = self.recv_size(maxsize + 1, timeout)\n            except ConnectionClosed:\n                ret, self.rbuf = self.rbuf, b''\n            else:\n                # put extra received bytes (now in rbuf) after recvd\n                self.rbuf = recvd + self.rbuf\n                size_read = min(maxsize, len(self.rbuf))\n                raise MessageTooLong(size_read)  # check receive buffer\n        return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "socketutils.py"], "context_start_lineno": 261, "line_no": 269, "id": "boltons.socketutils.BufferedSocket.recv_close", "target_function_prompt": "    def recv_close(self, timeout=_UNSET, maxsize=_UNSET):", "function_signature": "    def recv_close(self, timeout=_UNSET, maxsize=_UNSET):"}}
{"prompt": "    def get_attr_by_channel(self, channel):", "metadata": {"task_id": "Scientific-Engineering/lux/1", "ground_truth": "        spec_obj = list(\n            filter(\n                lambda x: x.channel == channel and x.value == \"\" if hasattr(x, \"channel\") else False,\n                self._inferred_intent,\n            )\n        )\n        return spec_obj", "fpath_tuple": ["Scientific-Engineering", "lux", "lux", "vis", "Vis.py"], "context_start_lineno": 141, "line_no": 142, "id": "lux.vis.Vis.Vis.get_attr_by_channel", "target_function_prompt": "    def get_attr_by_channel(self, channel):", "function_signature": "    def get_attr_by_channel(self, channel):"}}
{"prompt": "def get_unused_fence(content: str) -> str:", "metadata": {"task_id": "Communications/zulip-term/15", "ground_truth": "    from zulipterminal.config.regexes import REGEX_QUOTED_FENCE_LENGTH\n    max_length_fence = 3\n\n    matches = findall(REGEX_QUOTED_FENCE_LENGTH, content, flags=MULTILINE)\n    if len(matches) != 0:\n        max_length_fence = max(max_length_fence, len(max(matches, key=len)) + 1)\n\n    return \"`\" * max_length_fence", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "helper.py"], "context_start_lineno": 710, "line_no": 716, "id": "zulipterminal.helper.get_unused_fence", "target_function_prompt": "def get_unused_fence(content: str) -> str:", "function_signature": "def get_unused_fence(content: str) -> str:"}}
{"prompt": "    def serialize(self, value, display=False):", "metadata": {"task_id": "Multimedia/Mopidy/23", "ground_truth": "        lookup = {v: k for k, v in self.levels.items()}\n        if value in lookup:\n            return encode(lookup[value])\n        return \"\"", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 371, "line_no": 372, "id": "mopidy.config.types.LogLevel.serialize", "target_function_prompt": "    def serialize(self, value, display=False):", "function_signature": "    def serialize(self, value, display=False):"}}
{"prompt": "    def chunk(self):", "metadata": {"task_id": "Utilities/gunicorn/14", "ground_truth": "        if not self.iter:\n            return b\"\"\n        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b\"\"", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "http", "unreader.py"], "context_start_lineno": 71, "line_no": 72, "id": "gunicorn.http.unreader.IterUnreader.chunk", "target_function_prompt": "    def chunk(self):", "function_signature": "    def chunk(self):"}}
{"prompt": "    def read(self, filepath, encoding='utf-8', state=None):", "metadata": {"task_id": "Text-Processing/mistune/4", "ground_truth": "        if state is None:\n            state = self.block.state_cls()\n\n        state.env['__file__'] = filepath\n        with open(filepath, 'rb') as f:\n            s = f.read()\n\n        s = s.decode(encoding)\n        return self.parse(s, state)", "fpath_tuple": ["Text-Processing", "mistune", "src", "mistune", "markdown.py"], "context_start_lineno": 95, "line_no": 96, "id": "mistune.markdown.Markdown.read", "target_function_prompt": "    def read(self, filepath, encoding='utf-8', state=None):", "function_signature": "    def read(self, filepath, encoding='utf-8', state=None):"}}
{"prompt": "    def darwin_checker(self):", "metadata": {"task_id": "Utilities/python-for-android/19", "ground_truth": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 340, "line_no": 341, "id": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "target_function_prompt": "    def darwin_checker(self):", "function_signature": "    def darwin_checker(self):"}}
{"prompt": "    def get_free_parameters(self, args, kwargs, bound=False):", "metadata": {"task_id": "Utilities/sacred/26", "ground_truth": "        expected_args = self._get_expected_args(bound)\n        return [a for a in expected_args[len(args) :] if a not in kwargs]", "fpath_tuple": ["Utilities", "sacred", "sacred", "config", "signature.py"], "context_start_lineno": 65, "line_no": 66, "id": "sacred.config.signature.Signature.get_free_parameters", "target_function_prompt": "    def get_free_parameters(self, args, kwargs, bound=False):", "function_signature": "    def get_free_parameters(self, args, kwargs, bound=False):"}}
{"prompt": "def canonicalize_color(color: str) -> str:", "metadata": {"task_id": "Communications/zulip-term/16", "ground_truth": "    from zulipterminal.config.regexes import REGEX_COLOR_6_DIGIT\n    from zulipterminal.config.regexes import REGEX_COLOR_3_DIGIT\n    if match(REGEX_COLOR_6_DIGIT, color, ASCII) is not None:\n        # '#xxxxxx' color, stored by current zulip server\n        return (color[:2] + color[3] + color[5]).lower()\n    elif match(REGEX_COLOR_3_DIGIT, color, ASCII) is not None:\n        # '#xxx' color, which may be stored by the zulip server <= 2.0.0\n        # Potentially later versions too\n        return color.lower()\n    else:\n        raise ValueError(f'Unknown format for color \"{color}\"')", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "helper.py"], "context_start_lineno": 639, "line_no": 644, "id": "zulipterminal.helper.canonicalize_color", "target_function_prompt": "def canonicalize_color(color: str) -> str:", "function_signature": "def canonicalize_color(color: str) -> str:"}}
{"prompt": "def inspect_static_routes(app: App) -> 'List[StaticRouteInfo]':", "metadata": {"task_id": "Internet/falcon/29", "ground_truth": "    routes = []\n    for sr, _, _ in app._static_routes:\n        info = StaticRouteInfo(sr._prefix, sr._directory, sr._fallback_filename)\n        routes.append(info)\n    return routes", "fpath_tuple": ["Internet", "falcon", "falcon", "inspect.py"], "context_start_lineno": 104, "line_no": 115, "id": "falcon.inspect.inspect_static_routes", "target_function_prompt": "def inspect_static_routes(app: App) -> 'List[StaticRouteInfo]':", "function_signature": "def inspect_static_routes(app: App) -> 'List[StaticRouteInfo]':"}}
{"prompt": "    def serialize(self, value, display=False):", "metadata": {"task_id": "Multimedia/Mopidy/24", "ground_truth": "        if value.lower() in log.COLORS:\n            return encode(value.lower())\n        return \"\"", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 343, "line_no": 344, "id": "mopidy.config.types.LogColor.serialize", "target_function_prompt": "    def serialize(self, value, display=False):", "function_signature": "    def serialize(self, value, display=False):"}}
{"prompt": "    def read_next_chunk(self):", "metadata": {"task_id": "Database/mssql-cli/13", "ground_truth": "        current_buffer_size = len(self.buffer)\n        if ((current_buffer_size - self.buffer_end_offset) /\n                current_buffer_size) < self.BUFFER_RESIZE_TRIGGER:\n            resized_buffer = bytearray(current_buffer_size * 2)\n            # copy current buffer content to new buffer.\n            resized_buffer[0:current_buffer_size] = self.buffer\n            # point to new buffer.\n            self.buffer = resized_buffer\n\n        # Memory view is required in order to read into a subset of a byte\n        # array\n        try:\n            length_read = self.stream.readinto(\n                memoryview(self.buffer)[self.buffer_end_offset:])\n            self.buffer_end_offset += length_read\n\n            if not length_read:\n                logger.debug(u'JSON RPC Reader reached end of stream')\n                raise EOFError(u'End of stream reached, no output.')\n\n            return True\n        except ValueError as ex:\n            logger.debug(u'JSON RPC Reader on read_next_chunk encountered exception: %s', ex)\n            # Stream was closed.\n            raise", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "jsonrpc", "jsonrpcclient.py"], "context_start_lineno": 297, "line_no": 307, "id": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "target_function_prompt": "    def read_next_chunk(self):", "function_signature": "    def read_next_chunk(self):"}}
{"prompt": "def get_func_and_args(commands):", "metadata": {"task_id": "System/pyinfra/12", "ground_truth": "    from .util import parse_cli_arg\n    operation_name = commands[0]\n\n    op = try_import_module_attribute(operation_name, prefix=\"pyinfra.operations\")\n\n    # Parse the arguments\n    operation_args = commands[1:]\n\n    if len(operation_args) == 1:\n        # Check if we're JSON (in which case we expect a list of two items:\n        # a list of args and a dict of kwargs).\n        try:\n            args, kwargs = json.loads(operation_args[0])\n            return op, (args or (), kwargs or {})\n        except ValueError:\n            pass\n\n    args = [parse_cli_arg(arg) for arg in operation_args if \"=\" not in arg]\n\n    kwargs = {\n        key: parse_cli_arg(value)\n        for key, value in [arg.split(\"=\", 1) for arg in operation_args if \"=\" in arg]\n    }\n\n    return op, (args, kwargs)", "fpath_tuple": ["System", "pyinfra", "pyinfra_cli", "commands.py"], "context_start_lineno": 10, "line_no": 11, "id": "pyinfra_cli.commands.get_func_and_args", "target_function_prompt": "def get_func_and_args(commands):", "function_signature": "def get_func_and_args(commands):"}}
{"prompt": "def combine_opts(combiners, *opts_list):", "metadata": {"task_id": "System/mrjob/61", "ground_truth": "    final_opts = {}\n\n    keys = set()\n    for opts in opts_list:\n        if isinstance(opts, ClearedValue):\n            raise TypeError\n        elif opts:\n            keys.update(opts)\n\n    for key in keys:\n        values = _resolve_clear_tags_in_list(\n            opts[key] for opts in opts_list if opts and key in opts)\n\n        combine_func = combiners.get(key) or combine_values\n        final_opts[key] = combine_func(*values)\n\n    return final_opts", "fpath_tuple": ["System", "mrjob", "mrjob", "conf.py"], "context_start_lineno": 544, "line_no": 557, "id": "mrjob.conf.combine_opts", "target_function_prompt": "def combine_opts(combiners, *opts_list):", "function_signature": "def combine_opts(combiners, *opts_list):"}}
{"prompt": "    def md5sum(self, path):", "metadata": {"task_id": "System/mrjob/62", "ground_truth": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "local.py"], "context_start_lineno": 100, "line_no": 101, "id": "mrjob.fs.local.LocalFilesystem.md5sum", "target_function_prompt": "    def md5sum(self, path):", "function_signature": "    def md5sum(self, path):"}}
{"prompt": "    def from_page_data(cls, tree_conf: TreeConf, data: bytes,\n                       page: int=None) -> 'Node':", "metadata": {"task_id": "Database/bplustree/10", "ground_truth": "        node_type_byte = data[0:NODE_TYPE_BYTES]\n        node_type_int = int.from_bytes(node_type_byte, ENDIAN)\n        if node_type_int == 1:\n            return LonelyRootNode(tree_conf, data, page)\n        elif node_type_int == 2:\n            return RootNode(tree_conf, data, page)\n        elif node_type_int == 3:\n            return InternalNode(tree_conf, data, page)\n        elif node_type_int == 4:\n            return LeafNode(tree_conf, data, page)\n        else:\n            assert False, 'No Node with type {} exists'.format(node_type_int)", "fpath_tuple": ["Database", "bplustree", "bplustree", "node.py"], "context_start_lineno": 144, "line_no": 146, "id": "bplustree.node.Node.from_page_data", "target_function_prompt": "    def from_page_data(cls, tree_conf: TreeConf, data: bytes,\n                       page: int=None) -> 'Node':", "function_signature": "    def from_page_data(cls, tree_conf: TreeConf, data: bytes,\n                       page: int=None) -> 'Node':"}}
{"prompt": "    def response(self, resp, content):", "metadata": {"task_id": "Internet/google-api-python-client/4", "ground_truth": "        from googleapiclient.errors import HttpError\n        self._log_response(resp, content)\n        # Error handling is TBD, for example, do we retry\n        # for some operation/error combinations?\n        if resp.status < 300:\n            if resp.status == 204:\n                # A 204: No Content response should be treated differently\n                # to all the other success states\n                return self.no_content_response\n            return self.deserialize(content)\n        else:\n            LOGGER.debug(\"Content from bad request was: %r\" % content)\n            raise HttpError(resp, content)", "fpath_tuple": ["Internet", "google-api-python-client", "googleapiclient", "model.py"], "context_start_lineno": 196, "line_no": 209, "id": "googleapiclient.model.BaseModel.response", "target_function_prompt": "    def response(self, resp, content):", "function_signature": "    def response(self, resp, content):"}}
{"prompt": "def parts(path):\n    # type: (Text) -> List[Text]", "metadata": {"task_id": "System/fs/19", "ground_truth": "    _path = normpath(path)\n    components = _path.strip(\"/\")\n\n    _parts = [\"/\" if _path.startswith(\"/\") else \"./\"]\n    if components:\n        _parts += components.split(\"/\")\n    return _parts", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 268, "line_no": 283, "id": "fs.path.parts", "target_function_prompt": "def parts(path):\n    # type: (Text) -> List[Text]", "function_signature": "def parts(path):\n    # type: (Text) -> List[Text]"}}
{"prompt": "def _track_serve_init(\n    svc: Service,\n    production: bool,\n    serve_kind: str,\n    from_server_api: bool,\n    serve_info: ServeInfo = Provide[BentoMLContainer.serve_info],\n):", "metadata": {"task_id": "Scientific-Engineering/bentoml/23", "ground_truth": "    if svc.bento is not None:\n        bento = svc.bento\n        event_properties = ServeInitEvent(\n            serve_id=serve_info.serve_id,\n            serve_from_bento=True,\n            serve_from_server_api=from_server_api,\n            production=production,\n            serve_kind=serve_kind,\n            bento_creation_timestamp=bento.info.creation_time,\n            num_of_models=len(bento.info.models),\n            num_of_runners=len(svc.runners),\n            num_of_apis=len(bento.info.apis),\n            model_types=[m.module for m in bento.info.models],\n            runnable_types=[r.runnable_type for r in bento.info.runners],\n            api_input_types=[api.input_type for api in bento.info.apis],\n            api_output_types=[api.output_type for api in bento.info.apis],\n        )\n    else:\n        event_properties = ServeInitEvent(\n            serve_id=serve_info.serve_id,\n            serve_from_bento=False,\n            serve_from_server_api=from_server_api,\n            production=production,\n            serve_kind=serve_kind,\n            bento_creation_timestamp=None,\n            num_of_models=len(\n                set(\n                    svc.models\n                    + [model for runner in svc.runners for model in runner.models]\n                )\n            ),\n            num_of_runners=len(svc.runners),\n            num_of_apis=len(svc.apis.keys()),\n            runnable_types=[r.runnable_class.__name__ for r in svc.runners],\n            api_input_types=[api.input.__class__.__name__ for api in svc.apis.values()],\n            api_output_types=[\n                api.output.__class__.__name__ for api in svc.apis.values()\n            ],\n        )\n\n    track(event_properties)", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "utils", "analytics", "usage_stats.py"], "context_start_lineno": 128, "line_no": 135, "id": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "target_function_prompt": "def _track_serve_init(\n    svc: Service,\n    production: bool,\n    serve_kind: str,\n    from_server_api: bool,\n    serve_info: ServeInfo = Provide[BentoMLContainer.serve_info],\n):", "function_signature": "def _track_serve_init(\n    svc: Service,\n    production: bool,\n    serve_kind: str,\n    from_server_api: bool,\n    serve_info: ServeInfo = Provide[BentoMLContainer.serve_info],\n):"}}
{"prompt": "    def transpose(self, interval, up=True):", "metadata": {"task_id": "Multimedia/mingus/29", "ground_truth": "        for cont in self.bar:\n            cont[2].transpose(interval, up)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "bar.py"], "context_start_lineno": 179, "line_no": 184, "id": "mingus.containers.bar.Bar.transpose", "target_function_prompt": "    def transpose(self, interval, up=True):", "function_signature": "    def transpose(self, interval, up=True):"}}
{"prompt": "    def add(self, intr):", "metadata": {"task_id": "Internet/pyramid/61", "ground_truth": "        category = self._categories.setdefault(intr.category_name, {})\n        category[intr.discriminator] = intr\n        category[intr.discriminator_hash] = intr\n        intr.order = self._counter\n        self._counter += 1", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "registry.py"], "context_start_lineno": 123, "line_no": 124, "id": "pyramid.registry.Introspector.add", "target_function_prompt": "    def add(self, intr):", "function_signature": "    def add(self, intr):"}}
{"prompt": "    def add(self, item):", "metadata": {"task_id": "Utilities/boltons/54", "ground_truth": "        if item not in self.item_index_map:\n            self.item_index_map[item] = len(self.item_list)\n            self.item_list.append(item)", "fpath_tuple": ["Utilities", "boltons", "boltons", "setutils.py"], "context_start_lineno": 237, "line_no": 239, "id": "boltons.setutils.IndexedSet.add", "target_function_prompt": "    def add(self, item):", "function_signature": "    def add(self, item):"}}
{"prompt": "    def typeset(self) -> Optional[VisionsTypeset]:", "metadata": {"task_id": "Software-Development/pandas-profiling/6", "ground_truth": "        from ydata_profiling.model.typeset import ProfilingTypeSet\n        if self._typeset is None:\n            self._typeset = ProfilingTypeSet(self.config, self._type_schema)\n        return self._typeset", "fpath_tuple": ["Software-Development", "pandas-profiling", "src", "ydata_profiling", "profile_report.py"], "context_start_lineno": 238, "line_no": 239, "id": "ydata_profiling.profile_report.ProfileReport.typeset", "target_function_prompt": "    def typeset(self) -> Optional[VisionsTypeset]:", "function_signature": "    def typeset(self) -> Optional[VisionsTypeset]:"}}
{"prompt": "    def __repr__(self):", "metadata": {"task_id": "Database/bplustree/11", "ground_truth": "        return '<Reference: key={} before={} after={}>'.format(\n            self.key, self.before, self.after\n        )", "fpath_tuple": ["Database", "bplustree", "bplustree", "entry.py"], "context_start_lineno": 186, "line_no": 187, "id": "bplustree.entry.Reference.__repr__", "target_function_prompt": "    def __repr__(self):", "function_signature": "    def __repr__(self):"}}
{"prompt": "    def forwarded_prefix(self):", "metadata": {"task_id": "Internet/falcon/30", "ground_truth": "        if self._cached_forwarded_prefix is None:\n            self._cached_forwarded_prefix = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.app\n            )\n\n        return self._cached_forwarded_prefix", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 821, "line_no": 822, "id": "falcon.request.Request.forwarded_prefix", "target_function_prompt": "    def forwarded_prefix(self):", "function_signature": "    def forwarded_prefix(self):"}}
{"prompt": "    def dial(\n        self,\n        number=None,\n        action=None,\n        method=None,\n        timeout=None,\n        hangup_on_star=None,\n        time_limit=None,\n        caller_id=None,\n        record=None,\n        trim=None,\n        recording_status_callback=None,\n        recording_status_callback_method=None,\n        recording_status_callback_event=None,\n        answer_on_bridge=None,\n        ring_tone=None,\n        recording_track=None,\n        sequential=None,\n        refer_url=None,\n        refer_method=None,\n        **kwargs\n    ):", "metadata": {"task_id": "Communications/twilio-fatisar/16", "ground_truth": "        return self.nest(\n            Dial(\n                number=number,\n                action=action,\n                method=method,\n                timeout=timeout,\n                hangup_on_star=hangup_on_star,\n                time_limit=time_limit,\n                caller_id=caller_id,\n                record=record,\n                trim=trim,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                answer_on_bridge=answer_on_bridge,\n                ring_tone=ring_tone,\n                recording_track=recording_track,\n                sequential=sequential,\n                refer_url=refer_url,\n                refer_method=refer_method,\n                **kwargs\n            )\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "twiml", "voice_response.py"], "context_start_lineno": 32, "line_no": 79, "id": "twilio.twiml.voice_response.VoiceResponse.dial", "target_function_prompt": "    def dial(\n        self,\n        number=None,\n        action=None,\n        method=None,\n        timeout=None,\n        hangup_on_star=None,\n        time_limit=None,\n        caller_id=None,\n        record=None,\n        trim=None,\n        recording_status_callback=None,\n        recording_status_callback_method=None,\n        recording_status_callback_event=None,\n        answer_on_bridge=None,\n        ring_tone=None,\n        recording_track=None,\n        sequential=None,\n        refer_url=None,\n        refer_method=None,\n        **kwargs\n    ):", "function_signature": "    def dial(\n        self,\n        number=None,\n        action=None,\n        method=None,\n        timeout=None,\n        hangup_on_star=None,\n        time_limit=None,\n        caller_id=None,\n        record=None,\n        trim=None,\n        recording_status_callback=None,\n        recording_status_callback_method=None,\n        recording_status_callback_event=None,\n        answer_on_bridge=None,\n        ring_tone=None,\n        recording_track=None,\n        sequential=None,\n        refer_url=None,\n        refer_method=None,\n        **kwargs\n    ):"}}
{"prompt": "    def _compute_ratings(self, sentences):", "metadata": {"task_id": "Internet/sumy/16", "ground_truth": "        word_freq = self._compute_tf(sentences)\n        ratings = {}\n\n        # make it a list so that it can be modified\n        sentences_list = list(sentences)\n\n        # get all content words once for efficiency\n        sentences_as_words = [self._get_content_words_in_sentence(s) for s in sentences]\n\n        # Removes one sentence per iteration by adding to summary\n        while len(sentences_list) > 0:\n            best_sentence_index = self._find_index_of_best_sentence(word_freq, sentences_as_words)\n            best_sentence = sentences_list.pop(best_sentence_index)\n\n            # value is the iteration in which it was removed multiplied by -1 so that the first sentences removed (the most important) have highest values\n            ratings[best_sentence] = -len(ratings)\n\n            # update probabilities\n            best_sentence_words = sentences_as_words.pop(best_sentence_index)\n            self._update_tf(word_freq, best_sentence_words)\n\n        return ratings", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "sum_basic.py"], "context_start_lineno": 97, "line_no": 98, "id": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "target_function_prompt": "    def _compute_ratings(self, sentences):", "function_signature": "    def _compute_ratings(self, sentences):"}}
{"prompt": "    def type(self):\n        # type: () -> ResourceType", "metadata": {"task_id": "System/fs/20", "ground_truth": "        from .enums import ResourceType\n        self._require_namespace(\"details\")\n        return ResourceType(self.get(\"details\", \"type\", 0))", "fpath_tuple": ["System", "fs", "fs", "info.py"], "context_start_lineno": 279, "line_no": 290, "id": "fs.info.Info.type", "target_function_prompt": "    def type(self):\n        # type: () -> ResourceType", "function_signature": "    def type(self):\n        # type: () -> ResourceType"}}
{"prompt": "def can_file_be_synced_on_current_platform(path):", "metadata": {"task_id": "Utilities/mackup/3", "ground_truth": "    can_be_synced = True\n\n    # If the given path is relative, prepend home\n    fullpath = os.path.join(os.environ[\"HOME\"], path)\n\n    # Compute the ~/Library path on macOS\n    # End it with a slash because we are looking for this specific folder and\n    # not any file/folder named LibrarySomething\n    library_path = os.path.join(os.environ[\"HOME\"], \"Library/\")\n\n    if platform.system() == constants.PLATFORM_LINUX:\n        if fullpath.startswith(library_path):\n            can_be_synced = False\n\n    return can_be_synced", "fpath_tuple": ["Utilities", "mackup", "mackup", "utils.py"], "context_start_lineno": 364, "line_no": 382, "id": "mackup.utils.can_file_be_synced_on_current_platform", "target_function_prompt": "def can_file_be_synced_on_current_platform(path):", "function_signature": "def can_file_be_synced_on_current_platform(path):"}}
{"prompt": "def check_uri(arg, msg=\"Expected a valid URI, not {arg!r}\"):", "metadata": {"task_id": "Multimedia/Mopidy/25", "ground_truth": "    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    elif urllib.parse.urlparse(arg).scheme == \"\":\n        raise exceptions.ValidationError(msg.format(arg=arg))", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "internal", "validation.py"], "context_start_lineno": 121, "line_no": 122, "id": "mopidy.internal.validation.check_uri", "target_function_prompt": "def check_uri(arg, msg=\"Expected a valid URI, not {arg!r}\"):", "function_signature": "def check_uri(arg, msg=\"Expected a valid URI, not {arg!r}\"):"}}
{"prompt": "    def seek(self, offset, whence=os.SEEK_SET):", "metadata": {"task_id": "Utilities/boltons/55", "ground_truth": "        if whence != os.SEEK_SET:\n            raise NotImplementedError(\n                'MultiFileReader.seek() only supports os.SEEK_SET')\n        if offset != 0:\n            raise NotImplementedError(\n                'MultiFileReader only supports seeking to start at this time')\n        for f in self._fileobjs:\n            f.seek(0)", "fpath_tuple": ["Utilities", "boltons", "boltons", "ioutils.py"], "context_start_lineno": 592, "line_no": 596, "id": "boltons.ioutils.MultiFileReader.seek", "target_function_prompt": "    def seek(self, offset, whence=os.SEEK_SET):", "function_signature": "    def seek(self, offset, whence=os.SEEK_SET):"}}
{"prompt": "    def pop(self, key, default=Sentinel):", "metadata": {"task_id": "Software-Development/peewee/4", "ground_truth": "        with self._database.atomic():\n            try:\n                result = self[key]\n            except KeyError:\n                if default is Sentinel:\n                    raise\n                return default\n            del self[key]\n\n        return result", "fpath_tuple": ["Software-Development", "peewee", "playhouse", "kv.py"], "context_start_lineno": 162, "line_no": 163, "id": "playhouse.kv.KeyValue.pop", "target_function_prompt": "    def pop(self, key, default=Sentinel):", "function_signature": "    def pop(self, key, default=Sentinel):"}}
{"prompt": "    def clone(self, name=None, package=None, registry=None):", "metadata": {"task_id": "Internet/pyramid/62", "ground_truth": "        if name is None:\n            name = self.name\n        if package is None:\n            package = self.package\n        if registry is None:\n            registry = self.registry\n        return self.__class__(name=name, package=package, registry=registry)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "renderers.py"], "context_start_lineno": 492, "line_no": 493, "id": "pyramid.renderers.RendererHelper.clone", "target_function_prompt": "    def clone(self, name=None, package=None, registry=None):", "function_signature": "    def clone(self, name=None, package=None, registry=None):"}}
{"prompt": "    def worker_class(self):", "metadata": {"task_id": "Utilities/gunicorn/15", "ground_truth": "        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gunicorn.workers.gthread.ThreadWorker\"\n\n        worker_class = util.load_class(uri)\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n        return worker_class", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "config.py"], "context_start_lineno": 112, "line_no": 113, "id": "gunicorn.config.Config.worker_class", "target_function_prompt": "    def worker_class(self):", "function_signature": "    def worker_class(self):"}}
{"prompt": "def chunked(src, size, count=None, **kw):", "metadata": {"task_id": "Utilities/boltons/56", "ground_truth": "    chunk_iter = chunked_iter(src, size, **kw)\n    if count is None:\n        return list(chunk_iter)\n    else:\n        return list(itertools.islice(chunk_iter, count))", "fpath_tuple": ["Utilities", "boltons", "boltons", "iterutils.py"], "context_start_lineno": 302, "line_no": 318, "id": "boltons.iterutils.chunked", "target_function_prompt": "def chunked(src, size, count=None, **kw):", "function_signature": "def chunked(src, size, count=None, **kw):"}}
{"prompt": "    def partial_save(self):", "metadata": {"task_id": "Internet/boto/61", "ground_truth": "        key = self.get_keys()\n        # Build a new dict of only the data we're changing.\n        final_data, fields = self.prepare_partial()\n\n        if not final_data:\n            return False\n\n        # Remove the key(s) from the ``final_data`` if present.\n        # They should only be present if this is a new item, in which\n        # case we shouldn't be sending as part of the data to update.\n        for fieldname, value in key.items():\n            if fieldname in final_data:\n                del final_data[fieldname]\n\n                try:\n                    # It's likely also in ``fields``, so remove it there too.\n                    fields.remove(fieldname)\n                except KeyError:\n                    pass\n\n        # Build expectations of only the fields we're planning to update.\n        expects = self.build_expects(fields=fields)\n        returned = self.table._update_item(key, final_data, expects=expects)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "items.py"], "context_start_lineno": 368, "line_no": 387, "id": "boto.dynamodb2.items.Item.partial_save", "target_function_prompt": "    def partial_save(self):", "function_signature": "    def partial_save(self):"}}
{"prompt": "    def get_formatted(self):", "metadata": {"task_id": "Utilities/boltons/57", "ground_truth": "        tb_str = self.tb_info.get_formatted()\n        return ''.join([tb_str, '%s: %s' % (self.exc_type, self.exc_msg)])", "fpath_tuple": ["Utilities", "boltons", "boltons", "tbutils.py"], "context_start_lineno": 440, "line_no": 446, "id": "boltons.tbutils.ExceptionInfo.get_formatted", "target_function_prompt": "    def get_formatted(self):", "function_signature": "    def get_formatted(self):"}}
{"prompt": "    def add_rule(self, allowed_method, allowed_origin,\n                 id=None, allowed_header=None, max_age_seconds=None,\n                 expose_header=None):", "metadata": {"task_id": "Internet/boto/62", "ground_truth": "        if not isinstance(allowed_method, (list, tuple)):\n            allowed_method = [allowed_method]\n        if not isinstance(allowed_origin, (list, tuple)):\n            allowed_origin = [allowed_origin]\n        if not isinstance(allowed_origin, (list, tuple)):\n            if allowed_origin is None:\n                allowed_origin = []\n            else:\n                allowed_origin = [allowed_origin]\n        if not isinstance(expose_header, (list, tuple)):\n            if expose_header is None:\n                expose_header = []\n            else:\n                expose_header = [expose_header]\n        rule = CORSRule(allowed_method, allowed_origin, id, allowed_header,\n                        max_age_seconds, expose_header)\n        self.append(rule)", "fpath_tuple": ["Internet", "boto", "boto", "s3", "cors.py"], "context_start_lineno": 145, "line_no": 193, "id": "boto.s3.cors.CORSConfiguration.add_rule", "target_function_prompt": "    def add_rule(self, allowed_method, allowed_origin,\n                 id=None, allowed_header=None, max_age_seconds=None,\n                 expose_header=None):", "function_signature": "    def add_rule(self, allowed_method, allowed_origin,\n                 id=None, allowed_header=None, max_age_seconds=None,\n                 expose_header=None):"}}
{"prompt": "    def new_csrf_token(self, request):", "metadata": {"task_id": "Internet/pyramid/63", "ground_truth": "        token = self._token_factory()\n        request.session[self.key] = token\n        return token", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "csrf.py"], "context_start_lineno": 71, "line_no": 73, "id": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "target_function_prompt": "    def new_csrf_token(self, request):", "function_signature": "    def new_csrf_token(self, request):"}}
{"prompt": "def _parse_model(next_lines: List[str], quiet: bool = False) -> Optional[Model]:", "metadata": {"task_id": "Utilities/jc/5", "ground_truth": "    from jc.parsers.pyedid.edid import Edid\n    from jc.parsers.pyedid.helpers.edid_helper import EdidHelper\n    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    if not re.match(_edid_head_pattern, next_line):\n        next_lines.append(next_line)\n        return None\n\n    edid_hex_value = \"\"\n\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n\n        if not result:\n            next_lines.append(next_line)\n            break\n\n        matches = result.groupdict()\n        edid_hex_value += matches[\"edid_line\"]\n\n    edid = Edid(EdidHelper.hex2bytes(edid_hex_value))\n\n    model: Model = {\n        \"name\": edid.name or \"Generic\",\n        \"product_id\": str(edid.product),\n        \"serial_number\": str(edid.serial),\n    }\n    return model", "fpath_tuple": ["Utilities", "jc", "jc", "parsers", "xrandr.py"], "context_start_lineno": 399, "line_no": 400, "id": "jc.parsers.xrandr._parse_model", "target_function_prompt": "def _parse_model(next_lines: List[str], quiet: bool = False) -> Optional[Model]:", "function_signature": "def _parse_model(next_lines: List[str], quiet: bool = False) -> Optional[Model]:"}}
{"prompt": "def spatial_guesses(match):", "metadata": {"task_id": "Security/zxcvbn-python/10", "ground_truth": "    if match['graph'] in ['qwerty', 'dvorak']:\n        s = KEYBOARD_STARTING_POSITIONS\n        d = KEYBOARD_AVERAGE_DEGREE\n    else:\n        s = KEYPAD_STARTING_POSITIONS\n        d = KEYPAD_AVERAGE_DEGREE\n    guesses = 0\n    L = len(match['token'])\n    t = match['turns']\n    # estimate the number of possible patterns w/ length L or less with t turns\n    # or less.\n    for i in range(2, L + 1):\n        possible_turns = min(t, i - 1) + 1\n        for j in range(1, possible_turns):\n            guesses += nCk(i - 1, j - 1) * s * pow(d, j)\n    # add extra guesses for shifted keys. (% instead of 5, A instead of a.)\n    # math is similar to extra guesses of l33t substitutions in dictionary\n    # matches.\n    if match['shifted_count']:\n        S = match['shifted_count']\n        U = len(match['token']) - match['shifted_count']  # unshifted count\n        if S == 0 or U == 0:\n            guesses *= 2\n        else:\n            shifted_variations = 0\n            for i in range(1, min(S, U) + 1):\n                shifted_variations += nCk(S + U, i)\n            guesses *= shifted_variations\n\n    return guesses", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "scoring.py"], "context_start_lineno": 334, "line_no": 335, "id": "zxcvbn.scoring.spatial_guesses", "target_function_prompt": "def spatial_guesses(match):", "function_signature": "def spatial_guesses(match):"}}
{"prompt": "def get_tunings(instrument=None, nr_of_strings=None, nr_of_courses=None):", "metadata": {"task_id": "Multimedia/mingus/30", "ground_truth": "    search = \"\"\n    if instrument is not None:\n        search = str.upper(instrument)\n    result = []\n    keys = list(_known.keys())\n    inkeys = search in keys\n    for x in keys:\n        if (\n            instrument is None\n            or not inkeys\n            and x.find(search) == 0\n            or inkeys\n            and search == x\n        ):\n            if nr_of_strings is None and nr_of_courses is None:\n                result += list(_known[x][1].values())\n            elif nr_of_strings is not None and nr_of_courses is None:\n                result += [\n                    y\n                    for y in six.itervalues(_known[x][1])\n                    if y.count_strings() == nr_of_strings\n                ]\n            elif nr_of_strings is None and nr_of_courses is not None:\n                result += [\n                    y\n                    for y in six.itervalues(_known[x][1])\n                    if y.count_courses() == nr_of_courses\n                ]\n            else:\n                result += [\n                    y\n                    for y in six.itervalues(_known[x][1])\n                    if y.count_strings() == nr_of_strings\n                    and y.count_courses() == nr_of_courses\n                ]\n    return result", "fpath_tuple": ["Multimedia", "mingus", "mingus", "extra", "tunings.py"], "context_start_lineno": 447, "line_no": 458, "id": "mingus.extra.tunings.get_tunings", "target_function_prompt": "def get_tunings(instrument=None, nr_of_strings=None, nr_of_courses=None):", "function_signature": "def get_tunings(instrument=None, nr_of_strings=None, nr_of_courses=None):"}}
{"prompt": "    def iteritems(self):", "metadata": {"task_id": "Utilities/boltons/58", "ground_truth": "        for key in self.data:\n            for val in self.data[key]:\n                yield key, val", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 994, "line_no": 995, "id": "boltons.dictutils.ManyToMany.iteritems", "target_function_prompt": "    def iteritems(self):", "function_signature": "    def iteritems(self):"}}
{"prompt": "    def ls(self, path_glob):", "metadata": {"task_id": "System/mrjob/63", "ground_truth": "        bare_path_glob = _from_file_uri(path_glob)\n        uri_scheme = path_glob[0:-len(bare_path_glob)]  # 'file:///' or ''\n\n        for path in glob.glob(bare_path_glob):\n            if os.path.isdir(path):\n                for dirname, _, filenames in os.walk(path, followlinks=True):\n                    for filename in filenames:\n                        yield uri_scheme + os.path.join(dirname, filename)\n            else:\n                yield uri_scheme + path", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "local.py"], "context_start_lineno": 39, "line_no": 40, "id": "mrjob.fs.local.LocalFilesystem.ls", "target_function_prompt": "    def ls(self, path_glob):", "function_signature": "    def ls(self, path_glob):"}}
{"prompt": "    def delete_item(self, **kwargs):", "metadata": {"task_id": "Internet/boto/63", "ground_truth": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 1650, "line_no": 1651, "id": "boto.dynamodb2.table.BatchTable.delete_item", "target_function_prompt": "    def delete_item(self, **kwargs):", "function_signature": "    def delete_item(self, **kwargs):"}}
{"prompt": "def iswildcard(path):\n    # type: (Text) -> bool", "metadata": {"task_id": "System/fs/21", "ground_truth": "    assert path is not None\n    return not _WILD_CHARS.isdisjoint(path)", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 574, "line_no": 591, "id": "fs.path.iswildcard", "target_function_prompt": "def iswildcard(path):\n    # type: (Text) -> bool", "function_signature": "def iswildcard(path):\n    # type: (Text) -> bool"}}
{"prompt": "    def create_concrete(self):", "metadata": {"task_id": "Communications/chatette/14", "ground_truth": "        from chatette.units.modifiable.definitions.alias import AliasDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.alias]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return AliasDefinition(self.identifier, self._build_modifiers_repr())", "fpath_tuple": ["Communications", "chatette", "chatette", "parsing", "__init__.py"], "context_start_lineno": 138, "line_no": 139, "id": "chatette.parsing.AliasDefBuilder.create_concrete", "target_function_prompt": "    def create_concrete(self):", "function_signature": "    def create_concrete(self):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/64", "ground_truth": "    from boto.regioninfo import connect\n    from boto.dynamodb.layer2 import Layer2\n    return connect('dynamodb', region_name, connection_cls=Layer2, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb", "__init__.py"], "context_start_lineno": 38, "line_no": 39, "id": "boto.dynamodb.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "def parse_implicit_response(uri, state=None):", "metadata": {"task_id": "Internet/Authlib/6", "ground_truth": "    from .errors import MissingTokenException\n    from .errors import MissingTokenTypeException\n    fragment = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(fragment, keep_blank_values=True))\n\n    if 'access_token' not in params:\n        raise MissingTokenException()\n\n    if 'token_type' not in params:\n        raise MissingTokenTypeException()\n\n    if state and params.get('state', None) != state:\n        raise MismatchingStateException()\n\n    return params", "fpath_tuple": ["Internet", "Authlib", "authlib", "oauth2", "rfc6749", "parameters.py"], "context_start_lineno": 152, "line_no": 193, "id": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "target_function_prompt": "def parse_implicit_response(uri, state=None):", "function_signature": "def parse_implicit_response(uri, state=None):"}}
{"prompt": "def include(f):", "metadata": {"task_id": "Text-Processing/dominate/3", "ground_truth": "  fl = open(f, 'r')\n  data = fl.read()\n  fl.close()\n  return raw(data)", "fpath_tuple": ["Text-Processing", "dominate", "dominate", "util.py"], "context_start_lineno": 33, "line_no": 38, "id": "dominate.util.include", "target_function_prompt": "def include(f):", "function_signature": "def include(f):"}}
{"prompt": "    def _runner_kwargs(self):", "metadata": {"task_id": "System/mrjob/64", "ground_truth": "        from mrjob.options import _RUNNER_OPTS\n        kwargs = combine_dicts(\n            self._non_option_kwargs(),\n            # don't screen out irrelevant opts (see #1898)\n            self._kwargs_from_switches(set(_RUNNER_OPTS)),\n            self._job_kwargs(),\n        )\n\n        if self._runner_class().alias in ('inline', 'spark'):\n            kwargs = dict(mrjob_cls=self.__class__, **kwargs)\n\n        # pass steps to runner (see #1845)\n        kwargs = dict(steps=self._steps_desc(), **kwargs)\n\n        return kwargs", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 724, "line_no": 727, "id": "mrjob.job.MRJob._runner_kwargs", "target_function_prompt": "    def _runner_kwargs(self):", "function_signature": "    def _runner_kwargs(self):"}}
{"prompt": "    def flush(self):", "metadata": {"task_id": "Utilities/boltons/59", "ground_truth": "        with self._send_lock:\n            self.send(b'')\n        return", "fpath_tuple": ["Utilities", "boltons", "boltons", "socketutils.py"], "context_start_lineno": 472, "line_no": 474, "id": "boltons.socketutils.BufferedSocket.flush", "target_function_prompt": "    def flush(self):", "function_signature": "    def flush(self):"}}
{"prompt": "def concat(size, *args):", "metadata": {"task_id": "Security/barf/7", "ground_truth": "    if len(args) == 1:\n        return args[0]\n\n    return BitVec(size * len(args), \"concat\", *args)", "fpath_tuple": ["Security", "barf", "barf", "core", "smt", "smtfunction.py"], "context_start_lineno": 65, "line_no": 66, "id": "barf.core.smt.smtfunction.concat", "target_function_prompt": "def concat(size, *args):", "function_signature": "def concat(size, *args):"}}
{"prompt": "    def generic_autocomplete(self, text: str, state: Optional[int]) -> Optional[str]:", "metadata": {"task_id": "Communications/zulip-term/17", "ground_truth": "        autocomplete_map = OrderedDict(\n            [\n                (\"@_\", self.autocomplete_users),\n                (\"@_**\", self.autocomplete_users),\n                (\"@\", self.autocomplete_mentions),\n                (\"@*\", self.autocomplete_groups),\n                (\"@**\", self.autocomplete_users),\n                (\"#\", self.autocomplete_streams),\n                (\"#**\", self.autocomplete_streams),\n                (\":\", self.autocomplete_emojis),\n            ]\n        )\n\n        # Look in a reverse order to find the last autocomplete prefix used in\n        # the text. For instance, if text='@#example', use '#' as the prefix.\n        # FIXME: Mentions can actually start with '#', and streams with\n        #        anything; this implementation simply chooses the right-most\n        #        match of the longest length\n        prefix_indices = {prefix: text.rfind(prefix) for prefix in autocomplete_map}\n\n        text = self.validate_and_patch_autocomplete_stream_and_topic(\n            text, autocomplete_map, prefix_indices\n        )\n\n        found_prefix_indices = {\n            prefix: index for prefix, index in prefix_indices.items() if index > -1\n        }\n        # Return text if it doesn't have any of the autocomplete prefixes.\n        if not found_prefix_indices:\n            return text\n\n        # Use latest longest matching prefix (so @_ wins vs @)\n        prefix_index = max(found_prefix_indices.values())\n        prefix = max(\n            (len(prefix), prefix)\n            for prefix, index in found_prefix_indices.items()\n            if index == prefix_index\n        )[1]\n        autocomplete_func = autocomplete_map[prefix]\n\n        # NOTE: The following block only executes if any of the autocomplete\n        # prefixes exist.\n        typeaheads, suggestions = autocomplete_func(text[prefix_index:], prefix)\n\n        typeahead = self._process_typeaheads(typeaheads, state, suggestions)\n        if typeahead:\n            typeahead = text[:prefix_index] + typeahead\n        return typeahead", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "boxes.py"], "context_start_lineno": 476, "line_no": 477, "id": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "target_function_prompt": "    def generic_autocomplete(self, text: str, state: Optional[int]) -> Optional[str]:", "function_signature": "    def generic_autocomplete(self, text: str, state: Optional[int]) -> Optional[str]:"}}
{"prompt": "    def as_hex(self):\n        # make template to pad out to number of bytes necessary to represent bits", "metadata": {"task_id": "Utilities/boltons/60", "ground_truth": "        tmpl = '%0{0}X'.format(2 * (self.len // 8 + ((self.len % 8) != 0)))\n        ret = tmpl % self.val\n        return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "mathutils.py"], "context_start_lineno": 222, "line_no": 224, "id": "boltons.mathutils.Bits.as_hex", "target_function_prompt": "    def as_hex(self):\n        # make template to pad out to number of bytes necessary to represent bits", "function_signature": "    def as_hex(self):\n        # make template to pad out to number of bytes necessary to represent bits"}}
{"prompt": "    async def facet_results(self):", "metadata": {"task_id": "Database/datasette/30", "ground_truth": "        facet_results = []\n        facets_timed_out = []\n\n        qs_pairs = self.get_querystring_pairs()\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select {col} as value, count(*) as count from (\n                    {sql}\n                )\n                where {col} is not null\n                group by {col} order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet\": column})\n                        ),\n                        \"results\": facet_results_values,\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                if self.table:\n                    # Attempt to expand foreign keys into labels\n                    values = [row[\"value\"] for row in facet_rows]\n                    expanded = await self.ds.expand_foreign_keys(\n                        self.database, self.table, column, values\n                    )\n                else:\n                    expanded = {}\n                for row in facet_rows:\n                    column_qs = column\n                    if column.startswith(\"_\"):\n                        column_qs = \"{}__exact\".format(column)\n                    selected = (column_qs, str(row[\"value\"])) in qs_pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {column_qs: str(row[\"value\"])}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {column_qs: row[\"value\"]}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": row[\"value\"],\n                            \"label\": expanded.get((column, row[\"value\"]), row[\"value\"]),\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request, self.ds.urls.path(toggle_path)\n                            ),\n                            \"selected\": selected,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out", "fpath_tuple": ["Database", "datasette", "datasette", "facets.py"], "context_start_lineno": 209, "line_no": 210, "id": "datasette.facets.ColumnFacet.facet_results", "target_function_prompt": "    async def facet_results(self):", "function_signature": "    async def facet_results(self):"}}
{"prompt": "    def add_adapter(self, type_or_iface, adapter):", "metadata": {"task_id": "Internet/pyramid/64", "ground_truth": "        self.components.registerAdapter(\n            adapter, (type_or_iface,), IJSONAdapter\n        )", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "renderers.py"], "context_start_lineno": 236, "line_no": 258, "id": "pyramid.renderers.JSON.add_adapter", "target_function_prompt": "    def add_adapter(self, type_or_iface, adapter):", "function_signature": "    def add_adapter(self, type_or_iface, adapter):"}}
{"prompt": "    def send_message(self, message):", "metadata": {"task_id": "Communications/hl7/2", "ground_truth": "        if isinstance(message, bytes):\n            # Assume we have the correct encoding\n            binary = message\n        else:\n            # Encode the unicode message into a bytestring\n            if isinstance(message, hl7.Message):\n                message = str(message)\n            binary = message.encode(self.encoding)\n\n        # wrap in MLLP message container\n        data = SB + binary + EB + CR\n        return self.send(data)", "fpath_tuple": ["Communications", "hl7", "hl7", "client.py"], "context_start_lineno": 59, "line_no": 68, "id": "hl7.client.MLLPClient.send_message", "target_function_prompt": "    def send_message(self, message):", "function_signature": "    def send_message(self, message):"}}
{"prompt": "    def to_xml(self):", "metadata": {"task_id": "Internet/boto/65", "ground_truth": "        parts = ['<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n          '<WebsiteConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">']\n        if self.suffix is not None:\n            parts.append(tag('IndexDocument', tag('Suffix', self.suffix)))\n        if self.error_key is not None:\n            parts.append(tag('ErrorDocument', tag('Key', self.error_key)))\n        if self.redirect_all_requests_to is not None:\n            parts.append(self.redirect_all_requests_to.to_xml())\n        if self.routing_rules:\n            parts.append(self.routing_rules.to_xml())\n        parts.append('</WebsiteConfiguration>')\n        return ''.join(parts)", "fpath_tuple": ["Internet", "boto", "boto", "s3", "website.py"], "context_start_lineno": 76, "line_no": 77, "id": "boto.s3.website.WebsiteConfiguration.to_xml", "target_function_prompt": "    def to_xml(self):", "function_signature": "    def to_xml(self):"}}
{"prompt": "    def assertLoopErrorHandlerCalled(self, msg_re: str):", "metadata": {"task_id": "Database/asyncpg/0", "ground_truth": "        contexts = []\n\n        def handler(loop, ctx):\n            contexts.append(ctx)\n\n        old_handler = self.loop.get_exception_handler()\n        self.loop.set_exception_handler(handler)\n        try:\n            yield\n\n            for ctx in contexts:\n                msg = ctx.get('message')\n                if msg and re.search(msg_re, msg):\n                    return\n\n            raise AssertionError(\n                'no message matching {!r} was logged with '\n                'loop.call_exception_handler()'.format(msg_re))\n\n        finally:\n            self.loop.set_exception_handler(old_handler)", "fpath_tuple": ["Database", "asyncpg", "asyncpg", "_testbase", "__init__.py"], "context_start_lineno": 143, "line_no": 144, "id": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "target_function_prompt": "    def assertLoopErrorHandlerCalled(self, msg_re: str):", "function_signature": "    def assertLoopErrorHandlerCalled(self, msg_re: str):"}}
{"prompt": "def env_file_fixture(txt):", "metadata": {"task_id": "Database/alembic/26", "ground_truth": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    txt = (\n        \"\"\"\nfrom alembic import context\n\nconfig = context.config\n\"\"\"\n        + txt\n    )\n\n    path = os.path.join(dir_, \"env.py\")\n    pyc_path = util.pyc_file_from_path(path)\n    if pyc_path:\n        os.unlink(pyc_path)\n\n    with open(path, \"w\") as f:\n        f.write(txt)", "fpath_tuple": ["Database", "alembic", "alembic", "testing", "env.py"], "context_start_lineno": 71, "line_no": 72, "id": "alembic.testing.env.env_file_fixture", "target_function_prompt": "def env_file_fixture(txt):", "function_signature": "def env_file_fixture(txt):"}}
{"prompt": "    def create(cls, mod):", "metadata": {"task_id": "Utilities/sacred/27", "ground_truth": "        if not cls.modname_to_dist:\n            # some packagenames don't match the module names (e.g. PyYAML)\n            # so we set up a dict to map from module name to package name\n            for dist in pkg_resources.working_set:\n                try:\n                    toplevel_names = dist._get_metadata(\"top_level.txt\")\n                    for tln in toplevel_names:\n                        cls.modname_to_dist[tln] = dist.project_name, dist.version\n                except Exception:\n                    pass\n\n        name, version = cls.modname_to_dist.get(mod.__name__, (mod.__name__, None))\n\n        return PackageDependency(name, version)", "fpath_tuple": ["Utilities", "sacred", "sacred", "dependencies.py"], "context_start_lineno": 522, "line_no": 523, "id": "sacred.dependencies.PackageDependency.create", "target_function_prompt": "    def create(cls, mod):", "function_signature": "    def create(cls, mod):"}}
{"prompt": "    def _introspect_indexes(self, raw_indexes):", "metadata": {"task_id": "Internet/boto/66", "ground_truth": "        return self._introspect_all_indexes(\n            raw_indexes, self._PROJECTION_TYPE_TO_INDEX.get('local_indexes'))", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 314, "line_no": 319, "id": "boto.dynamodb2.table.Table._introspect_indexes", "target_function_prompt": "    def _introspect_indexes(self, raw_indexes):", "function_signature": "    def _introspect_indexes(self, raw_indexes):"}}
{"prompt": "    def pre_refresh_callback(self, authorizer):", "metadata": {"task_id": "Utilities/praw/8", "ground_truth": "        if authorizer.refresh_token is None:\n            with open(self._filename) as fp:\n                authorizer.refresh_token = fp.read().strip()", "fpath_tuple": ["Utilities", "praw", "praw", "util", "token_manager.py"], "context_start_lineno": 92, "line_no": 94, "id": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "target_function_prompt": "    def pre_refresh_callback(self, authorizer):", "function_signature": "    def pre_refresh_callback(self, authorizer):"}}
{"prompt": "    def tell(self):", "metadata": {"task_id": "Utilities/boltons/61", "ground_truth": "        self._checkClosed()\n        return self._tell", "fpath_tuple": ["Utilities", "boltons", "boltons", "ioutils.py"], "context_start_lineno": 507, "line_no": 509, "id": "boltons.ioutils.SpooledStringIO.tell", "target_function_prompt": "    def tell(self):", "function_signature": "    def tell(self):"}}
{"prompt": "    def say(self, message=None, voice=None, loop=None, language=None, **kwargs):", "metadata": {"task_id": "Communications/twilio-fatisar/17", "ground_truth": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "twiml", "voice_response.py"], "context_start_lineno": 374, "line_no": 386, "id": "twilio.twiml.voice_response.VoiceResponse.say", "target_function_prompt": "    def say(self, message=None, voice=None, loop=None, language=None, **kwargs):", "function_signature": "    def say(self, message=None, voice=None, loop=None, language=None, **kwargs):"}}
{"prompt": "def ensure_tuple(x):", "metadata": {"task_id": "Software-Development/PySnooper/2", "ground_truth": "    from .pycompat import string_types\n    if isinstance(x, collections_abc.Iterable) and \\\n                                               not isinstance(x, string_types):\n        return tuple(x)\n    else:\n        return (x,)", "fpath_tuple": ["Software-Development", "PySnooper", "pysnooper", "utils.py"], "context_start_lineno": 89, "line_no": 90, "id": "pysnooper.utils.ensure_tuple", "target_function_prompt": "def ensure_tuple(x):", "function_signature": "def ensure_tuple(x):"}}
{"prompt": "    def to_jwt(self, ttl=None):", "metadata": {"task_id": "Communications/twilio-fatisar/18", "ground_truth": "        if not self.secret_key:\n            raise ValueError(\"JWT does not have a signing key configured.\")\n\n        headers = self.headers.copy()\n\n        payload = self.payload.copy()\n        if ttl:\n            payload[\"exp\"] = int(time.time()) + ttl\n\n        return jwt_lib.encode(\n            payload, self.secret_key, algorithm=self.algorithm, headers=headers\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "jwt", "__init__.py"], "context_start_lineno": 102, "line_no": 109, "id": "twilio.jwt.Jwt.to_jwt", "target_function_prompt": "    def to_jwt(self, ttl=None):", "function_signature": "    def to_jwt(self, ttl=None):"}}
{"prompt": "def nanmean(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n            **unused_args):", "metadata": {"task_id": "Security/diffprivlib/15", "ground_truth": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "tools", "utils.py"], "context_start_lineno": 210, "line_no": 264, "id": "diffprivlib.tools.utils.nanmean", "target_function_prompt": "def nanmean(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n            **unused_args):", "function_signature": "def nanmean(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n            **unused_args):"}}
{"prompt": "def parse(html: str) -> Tuple[str, List[TypeMessageEntity]]:", "metadata": {"task_id": "Communications/Telethon/4", "ground_truth": "    from ..helpers import strip_text\n    if not html:\n        return html, []\n\n    parser = HTMLToTelegramParser()\n    parser.feed(add_surrogate(html))\n    text = strip_text(parser.text, parser.entities)\n    return del_surrogate(text), parser.entities", "fpath_tuple": ["Communications", "Telethon", "telethon", "extensions", "html.py"], "context_start_lineno": 112, "line_no": 120, "id": "telethon.extensions.html.parse", "target_function_prompt": "def parse(html: str) -> Tuple[str, List[TypeMessageEntity]]:", "function_signature": "def parse(html: str) -> Tuple[str, List[TypeMessageEntity]]:"}}
{"prompt": "    def cue_method(self, document, sentences_count, bonus_word_value=1, stigma_word_value=1):", "metadata": {"task_id": "Internet/sumy/17", "ground_truth": "        summarization_method = self._build_cue_method_instance()\n        return summarization_method(document, sentences_count, bonus_word_value,\n            stigma_word_value)", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "edmundson.py"], "context_start_lineno": 89, "line_no": 90, "id": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "target_function_prompt": "    def cue_method(self, document, sentences_count, bonus_word_value=1, stigma_word_value=1):", "function_signature": "    def cue_method(self, document, sentences_count, bonus_word_value=1, stigma_word_value=1):"}}
{"prompt": "def minor_seventh(note):", "metadata": {"task_id": "Multimedia/mingus/31", "ground_truth": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 10)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "intervals.py"], "context_start_lineno": 225, "line_no": 226, "id": "mingus.core.intervals.minor_seventh", "target_function_prompt": "def minor_seventh(note):", "function_signature": "def minor_seventh(note):"}}
{"prompt": "    def get_revisions(\n        self, id_: Optional[_GetRevArg]\n    ) -> Tuple[Optional[_RevisionOrBase], ...]:", "metadata": {"task_id": "Database/alembic/27", "ground_truth": "        if isinstance(id_, (list, tuple, set, frozenset)):\n            return sum([self.get_revisions(id_elem) for id_elem in id_], ())\n        else:\n            resolved_id, branch_label = self._resolve_revision_number(id_)\n            if len(resolved_id) == 1:\n                try:\n                    rint = int(resolved_id[0])\n                    if rint < 0:\n                        # branch@-n -> walk down from heads\n                        select_heads = self.get_revisions(\"heads\")\n                        if branch_label is not None:\n                            select_heads = tuple(\n                                head\n                                for head in select_heads\n                                if branch_label\n                                in is_revision(head).branch_labels\n                            )\n                        return tuple(\n                            self._walk(head, steps=rint)\n                            for head in select_heads\n                        )\n                except ValueError:\n                    # couldn't resolve as integer\n                    pass\n            return tuple(\n                self._revision_for_ident(rev_id, branch_label)\n                for rev_id in resolved_id\n            )", "fpath_tuple": ["Database", "alembic", "alembic", "script", "revision.py"], "context_start_lineno": 508, "line_no": 528, "id": "alembic.script.revision.RevisionMap.get_revisions", "target_function_prompt": "    def get_revisions(\n        self, id_: Optional[_GetRevArg]\n    ) -> Tuple[Optional[_RevisionOrBase], ...]:", "function_signature": "    def get_revisions(\n        self, id_: Optional[_GetRevArg]\n    ) -> Tuple[Optional[_RevisionOrBase], ...]:"}}
{"prompt": "    def close(self):", "metadata": {"task_id": "Utilities/boltons/62", "ground_truth": "        with self._recv_lock:\n            with self._send_lock:\n                self.rbuf = b''\n                self.rbuf_unconsumed = self.rbuf\n                self.sbuf[:] = []\n                self.sock.close()\n        return", "fpath_tuple": ["Utilities", "boltons", "boltons", "socketutils.py"], "context_start_lineno": 560, "line_no": 568, "id": "boltons.socketutils.BufferedSocket.close", "target_function_prompt": "    def close(self):", "function_signature": "    def close(self):"}}
{"prompt": "def _render_unique_constraint(\n    constraint: UniqueConstraint,\n    autogen_context: AutogenContext,\n    namespace_metadata: Optional[MetaData],\n) -> str:", "metadata": {"task_id": "Database/alembic/28", "ground_truth": "    rendered = _user_defined_render(\"unique\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    return _uq_constraint(constraint, autogen_context, False)", "fpath_tuple": ["Database", "alembic", "alembic", "autogenerate", "render.py"], "context_start_lineno": 1018, "line_no": 1023, "id": "alembic.autogenerate.render._render_unique_constraint", "target_function_prompt": "def _render_unique_constraint(\n    constraint: UniqueConstraint,\n    autogen_context: AutogenContext,\n    namespace_metadata: Optional[MetaData],\n) -> str:", "function_signature": "def _render_unique_constraint(\n    constraint: UniqueConstraint,\n    autogen_context: AutogenContext,\n    namespace_metadata: Optional[MetaData],\n) -> str:"}}
{"prompt": "    def assert_(self, **kw):", "metadata": {"task_id": "Internet/pyramid/65", "ground_truth": "        for k, v in kw.items():\n            myval = self._received.get(k, _marker)\n            if myval is _marker:\n                myval = self._implementation._received.get(k, _marker)\n                if myval is _marker:\n                    raise AssertionError(\n                        'A value for key \"%s\" was not passed to the renderer'\n                        % k\n                    )\n\n            if myval != v:\n                raise AssertionError(\n                    '\\nasserted value for %s: %r\\nactual value: %r'\n                    % (k, v, myval)\n                )\n        return True", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "testing.py"], "context_start_lineno": 115, "line_no": 123, "id": "pyramid.testing.DummyTemplateRenderer.assert_", "target_function_prompt": "    def assert_(self, **kw):", "function_signature": "    def assert_(self, **kw):"}}
{"prompt": "    def int_from_script_bytes(class_, s, require_minimal=False):", "metadata": {"task_id": "Security/pycoin/24", "ground_truth": "        from pycoin.coins.SolutionChecker import ScriptError\n        from . import errno\n        if len(s) == 0:\n            return 0\n        s = bytearray(s)\n        s.reverse()\n        i = s[0]\n        v = i & 0x7f\n        if require_minimal:\n            if v == 0:\n                if len(s) <= 1 or ((s[1] & 0x80) == 0):\n                    raise ScriptError(\"non-minimally encoded\", errno.UNKNOWN_ERROR)\n        is_negative = ((i & 0x80) > 0)\n        for b in s[1:]:\n            v <<= 8\n            v += b\n        if is_negative:\n            v = -v\n        return v", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "IntStreamer.py"], "context_start_lineno": 8, "line_no": 9, "id": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "target_function_prompt": "    def int_from_script_bytes(class_, s, require_minimal=False):", "function_signature": "    def int_from_script_bytes(class_, s, require_minimal=False):"}}
{"prompt": "    def getacl(self, folder):", "metadata": {"task_id": "Communications/IMAPClient/23", "ground_truth": "        from . import response_lexer\n        data = self._command_and_check(\"getacl\", self._normalise_folder(folder))\n        parts = list(response_lexer.TokenSource(data))\n        parts = parts[1:]  # First item is folder name\n        return [(parts[i], parts[i + 1]) for i in range(0, len(parts), 2)]", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1541, "line_no": 1545, "id": "imapclient.imapclient.IMAPClient.getacl", "target_function_prompt": "    def getacl(self, folder):", "function_signature": "    def getacl(self, folder):"}}
{"prompt": "def _build_urllib2_request(url, agent, accept_header, etag, modified, referrer, auth, request_headers):", "metadata": {"task_id": "Text-Processing/feedparser/3", "ground_truth": "    request = urllib.request.Request(url)\n    request.add_header('User-Agent', agent)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if isinstance(modified, str):\n        modified = _parse_date(modified)\n    elif isinstance(modified, datetime.datetime):\n        modified = modified.utctimetuple()\n    if modified:\n        # format into an RFC 1123-compliant timestamp. We can't use\n        # time.strftime() since the %a and %b directives can be affected\n        # by the current locale, but RFC 2616 states that dates must be\n        # in English.\n        short_weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n        request.add_header('If-Modified-Since', '%s, %02d %s %04d %02d:%02d:%02d GMT' % (short_weekdays[modified[6]], modified[2], months[modified[1] - 1], modified[0], modified[3], modified[4], modified[5]))\n    if referrer:\n        request.add_header('Referer', referrer)\n    request.add_header('Accept-encoding', 'gzip, deflate')\n    if auth:\n        request.add_header('Authorization', 'Basic %s' % auth)\n    if accept_header:\n        request.add_header('Accept', accept_header)\n    # use this for whatever -- cookies, special headers, etc\n    # [('Cookie','Something'),('x-special-header','Another Value')]\n    for header_name, header_value in request_headers.items():\n        request.add_header(header_name, header_value)\n    request.add_header('A-IM', 'feed')  # RFC 3229 support\n    return request", "fpath_tuple": ["Text-Processing", "feedparser", "feedparser", "http.py"], "context_start_lineno": 91, "line_no": 92, "id": "feedparser.http._build_urllib2_request", "target_function_prompt": "def _build_urllib2_request(url, agent, accept_header, etag, modified, referrer, auth, request_headers):", "function_signature": "def _build_urllib2_request(url, agent, accept_header, etag, modified, referrer, auth, request_headers):"}}
{"prompt": "def repeat_match(password, _ranked_dictionaries=RANKED_DICTIONARIES):", "metadata": {"task_id": "Security/zxcvbn-python/11", "ground_truth": "    from zxcvbn.scoring import most_guessable_match_sequence\n    matches = []\n    greedy = re.compile(r'(.+)\\1+')\n    lazy = re.compile(r'(.+?)\\1+')\n    lazy_anchored = re.compile(r'^(.+?)\\1+$')\n    last_index = 0\n    while last_index < len(password):\n        greedy_match = greedy.search(password, pos=last_index)\n        lazy_match = lazy.search(password, pos=last_index)\n\n        if not greedy_match:\n            break\n\n        if len(greedy_match.group(0)) > len(lazy_match.group(0)):\n            # greedy beats lazy for 'aabaab'\n            #   greedy: [aabaab, aab]\n            #   lazy:   [aa,     a]\n            match = greedy_match\n            # greedy's repeated string might itself be repeated, eg.\n            # aabaab in aabaabaabaab.\n            # run an anchored lazy match on greedy's repeated string\n            # to find the shortest repeated string\n            base_token = lazy_anchored.search(match.group(0)).group(1)\n        else:\n            match = lazy_match\n            base_token = match.group(1)\n\n        i, j = match.span()[0], match.span()[1] - 1\n\n        # recursively match and score the base string\n        base_analysis = most_guessable_match_sequence(\n            base_token,\n            omnimatch(base_token)\n        )\n        base_matches = base_analysis['sequence']\n        base_guesses = base_analysis['guesses']\n        matches.append({\n            'pattern': 'repeat',\n            'i': i,\n            'j': j,\n            'token': match.group(0),\n            'base_token': base_token,\n            'base_guesses': base_guesses,\n            'base_matches': base_matches,\n            'repeat_count': len(match.group(0)) / len(base_token),\n        })\n        last_index = j + 1\n\n    return matches", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "matching.py"], "context_start_lineno": 249, "line_no": 250, "id": "zxcvbn.matching.repeat_match", "target_function_prompt": "def repeat_match(password, _ranked_dictionaries=RANKED_DICTIONARIES):", "function_signature": "def repeat_match(password, _ranked_dictionaries=RANKED_DICTIONARIES):"}}
{"prompt": "    def idle_check(self, timeout=None):", "metadata": {"task_id": "Communications/IMAPClient/24", "ground_truth": "        sock = self.socket()\n\n        # make the socket non-blocking so the timeout can be\n        # implemented for this call\n        sock.settimeout(None)\n        sock.setblocking(0)\n\n        if POLL_SUPPORT:\n            poll_func = self._poll_socket\n        else:\n            poll_func = self._select_poll_socket\n\n        try:\n            resps = []\n            events = poll_func(sock, timeout)\n            if events:\n                while True:\n                    try:\n                        line = self._imap._get_line()\n                    except (socket.timeout, socket.error):\n                        break\n                    except IMAPClient.AbortError:\n                        # An imaplib.IMAP4.abort with \"EOF\" is raised\n                        # under Python 3\n                        err = sys.exc_info()[1]\n                        if \"EOF\" in err.args[0]:\n                            break\n                        raise\n                    else:\n                        resps.append(_parse_untagged_response(line))\n            return resps\n        finally:\n            sock.setblocking(1)\n            self._set_read_timeout()", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 929, "line_no": 947, "id": "imapclient.imapclient.IMAPClient.idle_check", "target_function_prompt": "    def idle_check(self, timeout=None):", "function_signature": "    def idle_check(self, timeout=None):"}}
{"prompt": "def IP(ip, comment='', token='', strict=True):", "metadata": {"task_id": "Security/capirca/7", "ground_truth": "  if isinstance(ip, ipaddress._BaseNetwork):  # pylint disable=protected-access\n    imprecise_ip = ip\n  else:\n    imprecise_ip = ipaddress.ip_network(ip, strict=strict)\n  if imprecise_ip.version == 4:\n    return IPv4(ip, comment, token, strict=strict)\n  elif imprecise_ip.version == 6:\n    return IPv6(ip, comment, token, strict=strict)\n  raise ValueError('Provided IP string \"%s\" is not a valid v4 or v6 address'\n                   % ip)", "fpath_tuple": ["Security", "capirca", "capirca", "lib", "nacaddr.py"], "context_start_lineno": 25, "line_no": 40, "id": "capirca.lib.nacaddr.IP", "target_function_prompt": "def IP(ip, comment='', token='', strict=True):", "function_signature": "def IP(ip, comment='', token='', strict=True):"}}
{"prompt": "def encode_stream(stream_id: int, stream_name: str) -> str:", "metadata": {"task_id": "Communications/zulip-term/18", "ground_truth": "    stream_name = stream_name.replace(\" \", \"-\")\n    return str(stream_id) + \"-\" + hash_util_encode(stream_name)", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "server_url.py"], "context_start_lineno": 15, "line_no": 21, "id": "zulipterminal.server_url.encode_stream", "target_function_prompt": "def encode_stream(stream_id: int, stream_name: str) -> str:", "function_signature": "def encode_stream(stream_id: int, stream_name: str) -> str:"}}
{"prompt": "def loop(\n    *,\n    seconds: float = MISSING,\n    minutes: float = MISSING,\n    hours: float = MISSING,\n    time: Union[datetime.time, Sequence[datetime.time]] = MISSING,\n    count: Optional[int] = None,\n    reconnect: bool = True,\n) -> Callable[[LF], Loop[LF]]:", "metadata": {"task_id": "Software-Development/discord-py/4", "ground_truth": "    def decorator(func: LF) -> Loop[LF]:\n        return Loop[LF](\n            func,\n            seconds=seconds,\n            minutes=minutes,\n            hours=hours,\n            count=count,\n            time=time,\n            reconnect=reconnect,\n        )\n\n    return decorator", "fpath_tuple": ["Software-Development", "discord-py", "discord", "ext", "tasks", "__init__.py"], "context_start_lineno": 764, "line_no": 814, "id": "discord.ext.tasks.loop", "target_function_prompt": "def loop(\n    *,\n    seconds: float = MISSING,\n    minutes: float = MISSING,\n    hours: float = MISSING,\n    time: Union[datetime.time, Sequence[datetime.time]] = MISSING,\n    count: Optional[int] = None,\n    reconnect: bool = True,\n) -> Callable[[LF], Loop[LF]]:", "function_signature": "def loop(\n    *,\n    seconds: float = MISSING,\n    minutes: float = MISSING,\n    hours: float = MISSING,\n    time: Union[datetime.time, Sequence[datetime.time]] = MISSING,\n    count: Optional[int] = None,\n    reconnect: bool = True,\n) -> Callable[[LF], Loop[LF]]:"}}
{"prompt": "    def download_to_fileobj(self, output_file, chunk_size=DefaultPartSize,\n                            verify_hashes=True,\n                            retry_exceptions=(socket.error,)):", "metadata": {"task_id": "Internet/boto/67", "ground_truth": "        num_chunks = self._calc_num_chunks(chunk_size)\n        self._download_to_fileob(output_file, num_chunks, chunk_size,\n                                 verify_hashes, retry_exceptions)", "fpath_tuple": ["Internet", "boto", "boto", "glacier", "job.py"], "context_start_lineno": 124, "line_no": 142, "id": "boto.glacier.job.Job.download_to_fileobj", "target_function_prompt": "    def download_to_fileobj(self, output_file, chunk_size=DefaultPartSize,\n                            verify_hashes=True,\n                            retry_exceptions=(socket.error,)):", "function_signature": "    def download_to_fileobj(self, output_file, chunk_size=DefaultPartSize,\n                            verify_hashes=True,\n                            retry_exceptions=(socket.error,)):"}}
{"prompt": "def extract(s, offset, size):", "metadata": {"task_id": "Security/barf/8", "ground_truth": "    assert type(s) in (Constant, BitVec)\n\n    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {} {})\".format(offset + size - 1, offset), s)", "fpath_tuple": ["Security", "barf", "barf", "core", "smt", "smtfunction.py"], "context_start_lineno": 49, "line_no": 50, "id": "barf.core.smt.smtfunction.extract", "target_function_prompt": "def extract(s, offset, size):", "function_signature": "def extract(s, offset, size):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/68", "ground_truth": "    from boto.regioninfo import connect\n    from boto.cloudsearch.layer1 import Layer1\n    return connect('cloudsearch', region_name, connection_cls=Layer1,\n                   **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "cloudsearch", "__init__.py"], "context_start_lineno": 38, "line_no": 39, "id": "boto.cloudsearch.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def _get_content_words_in_sentence(self, sentence):", "metadata": {"task_id": "Internet/sumy/18", "ground_truth": "        normalized_words = self._normalize_words(sentence.words)\n        normalized_content_words = self._filter_out_stop_words(normalized_words)\n        stemmed_normalized_content_words = self._stem_words(normalized_content_words)\n        return stemmed_normalized_content_words", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "sum_basic.py"], "context_start_lineno": 32, "line_no": 33, "id": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "target_function_prompt": "    def _get_content_words_in_sentence(self, sentence):", "function_signature": "    def _get_content_words_in_sentence(self, sentence):"}}
{"prompt": "    def len(self):", "metadata": {"task_id": "Utilities/boltons/63", "ground_truth": "        pos = self.tell()\n        if self._rolled:\n            self.seek(0)\n            val = os.fstat(self.fileno()).st_size\n        else:\n            self.seek(0, os.SEEK_END)\n            val = self.tell()\n        self.seek(pos)\n        return val", "fpath_tuple": ["Utilities", "boltons", "boltons", "ioutils.py"], "context_start_lineno": 365, "line_no": 367, "id": "boltons.ioutils.SpooledBytesIO.len", "target_function_prompt": "    def len(self):", "function_signature": "    def len(self):"}}
{"prompt": "def extract_paths(content, existing_only=True):", "metadata": {"task_id": "System/exodus-bundler/10", "ground_truth": "    lines = [line.strip() for line in content.splitlines() if len(line.strip())]\n    if not len(lines):\n        return lines\n\n    # The strace output will start with the exec call of its argument.\n    strace_mode = extract_exec_path(lines[0]) is not None\n    if not strace_mode:\n        return lines\n\n    # Extract files from `open()`, `openat()`, and `exec()` calls.\n    paths = set()\n    for line in lines:\n        path = extract_exec_path(line) or extract_open_path(line) or extract_stat_path(line)\n        if path:\n            blacklisted = any(path.startswith(directory) for directory in blacklisted_directories)\n            if not blacklisted:\n                if not existing_only:\n                    paths.add(path)\n                    continue\n                if os.path.exists(path) and os.access(path, os.R_OK) and not os.path.isdir(path):\n                    paths.add(path)\n\n    return list(paths)", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "input_parsing.py"], "context_start_lineno": 69, "line_no": 79, "id": "exodus_bundler.input_parsing.extract_paths", "target_function_prompt": "def extract_paths(content, existing_only=True):", "function_signature": "def extract_paths(content, existing_only=True):"}}
{"prompt": "    def get_key(self, key_name, headers=None, version_id=None,\n                response_headers=None, validate=True):", "metadata": {"task_id": "Internet/boto/69", "ground_truth": "        from boto.exception import BotoClientError\n        if validate is False:\n            if headers or version_id or response_headers:\n                raise BotoClientError(\n                    \"When providing 'validate=False', no other params \" + \\\n                    \"are allowed.\"\n                )\n\n            # This leans on the default behavior of ``new_key`` (not hitting\n            # the service). If that changes, that behavior should migrate here.\n            return self.new_key(key_name)\n\n        query_args_l = []\n        if version_id:\n            query_args_l.append('versionId=%s' % version_id)\n        if response_headers:\n            for rk, rv in six.iteritems(response_headers):\n                query_args_l.append('%s=%s' % (rk, urllib.parse.quote(rv)))\n\n        key, resp = self._get_key_internal(key_name, headers, query_args_l)\n        return key", "fpath_tuple": ["Internet", "boto", "boto", "s3", "bucket.py"], "context_start_lineno": 144, "line_no": 174, "id": "boto.s3.bucket.Bucket.get_key", "target_function_prompt": "    def get_key(self, key_name, headers=None, version_id=None,\n                response_headers=None, validate=True):", "function_signature": "    def get_key(self, key_name, headers=None, version_id=None,\n                response_headers=None, validate=True):"}}
{"prompt": "    def _validate_narrow_link(self, parsed_link: ParsedNarrowLink) -> str:", "metadata": {"task_id": "Communications/zulip-term/19", "ground_truth": "        if not parsed_link:\n            return \"The narrow link seems to be either broken or unsupported\"\n\n        # Validate stream data.\n        if \"stream\" in parsed_link:\n            error = self._validate_and_patch_stream_data(parsed_link)\n            if error:\n                return error\n\n        # Validate topic name.\n        if \"topic_name\" in parsed_link:\n            topic_name = parsed_link[\"topic_name\"]\n            stream_id = parsed_link[\"stream\"][\"stream_id\"]\n\n            if topic_name not in self.model.topics_in_stream(stream_id):\n                return \"Invalid topic name\"\n\n        # Validate message ID for near.\n        if \"near\" in parsed_link[\"narrow\"]:\n            message_id = parsed_link.get(\"message_id\")\n\n            if message_id is None:\n                return \"Invalid message ID\"\n\n        return \"\"", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "buttons.py"], "context_start_lineno": 550, "line_no": 555, "id": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "target_function_prompt": "    def _validate_narrow_link(self, parsed_link: ParsedNarrowLink) -> str:", "function_signature": "    def _validate_narrow_link(self, parsed_link: ParsedNarrowLink) -> str:"}}
{"prompt": "    def set_up_logging(cls, quiet=False, verbose=False, stream=None):", "metadata": {"task_id": "System/mrjob/65", "ground_truth": "        from mrjob.util import log_to_stream\n        from mrjob.util import log_to_null\n        if quiet:\n            log_to_null(name='mrjob')\n            log_to_null(name='__main__')\n        else:\n            log_to_stream(name='mrjob', debug=verbose, stream=stream)\n            log_to_stream(name='__main__', debug=verbose, stream=stream)", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 651, "line_no": 660, "id": "mrjob.job.MRJob.set_up_logging", "target_function_prompt": "    def set_up_logging(cls, quiet=False, verbose=False, stream=None):", "function_signature": "    def set_up_logging(cls, quiet=False, verbose=False, stream=None):"}}
{"prompt": "def augmented_triad(note):", "metadata": {"task_id": "Multimedia/mingus/32", "ground_truth": "    return [\n        note,\n        intervals.major_third(note),\n        notes.augment(intervals.major_fifth(note)),\n    ]", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "chords.py"], "context_start_lineno": 221, "line_no": 228, "id": "mingus.core.chords.augmented_triad", "target_function_prompt": "def augmented_triad(note):", "function_signature": "def augmented_triad(note):"}}
{"prompt": "    def discover(cls):", "metadata": {"task_id": "Communications/twtxt/4", "ground_truth": "        file = os.path.join(Config.config_dir, Config.config_name)\n        return cls.from_file(file)", "fpath_tuple": ["Communications", "twtxt", "twtxt", "config.py"], "context_start_lineno": 56, "line_no": 58, "id": "twtxt.config.Config.discover", "target_function_prompt": "    def discover(cls):", "function_signature": "    def discover(cls):"}}
{"prompt": "    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:", "metadata": {"task_id": "Database/alembic/29", "ground_truth": "        if self._reverse:\n            cols_and_constraints = self._reverse.columns\n        else:\n            cols_and_constraints = []\n\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        t = schema_obj.table(\n            self.table_name,\n            *cols_and_constraints,\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            schema=self.schema,\n            _constraints_included=self._reverse._constraints_included\n            if self._reverse\n            else False,\n            **self.table_kw,\n        )\n        return t", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 1349, "line_no": 1352, "id": "alembic.operations.ops.DropTableOp.to_table", "target_function_prompt": "    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:", "function_signature": "    def to_table(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> Table:"}}
{"prompt": "    def get_all_instance_status(self, instance_ids=None,\n                                max_results=None, next_token=None,\n                                filters=None, dry_run=False,\n                                include_all_instances=False):", "metadata": {"task_id": "Internet/boto/70", "ground_truth": "        from boto.ec2.instancestatus import InstanceStatusSet\n        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if max_results:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        return self.get_object('DescribeInstanceStatus', params,\n                               InstanceStatusSet, verb='POST')", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "connection.py"], "context_start_lineno": 682, "line_no": 721, "id": "boto.ec2.connection.EC2Connection.get_all_instance_status", "target_function_prompt": "    def get_all_instance_status(self, instance_ids=None,\n                                max_results=None, next_token=None,\n                                filters=None, dry_run=False,\n                                include_all_instances=False):", "function_signature": "    def get_all_instance_status(self, instance_ids=None,\n                                max_results=None, next_token=None,\n                                filters=None, dry_run=False,\n                                include_all_instances=False):"}}
{"prompt": "    def associate(self, instance_id=None, network_interface_id=None, private_ip_address=None, allow_reassociation=False, dry_run=False):", "metadata": {"task_id": "Internet/boto/71", "ground_truth": "        if self.allocation_id:\n            return self.connection.associate_address(\n                instance_id=instance_id,\n                public_ip=self.public_ip,\n                allocation_id=self.allocation_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )\n        return self.connection.associate_address(\n            instance_id=instance_id,\n            public_ip=self.public_ip,\n            network_interface_id=network_interface_id,\n            private_ip_address=private_ip_address,\n            allow_reassociation=allow_reassociation,\n            dry_run=dry_run\n        )", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "address.py"], "context_start_lineno": 91, "line_no": 96, "id": "boto.ec2.address.Address.associate", "target_function_prompt": "    def associate(self, instance_id=None, network_interface_id=None, private_ip_address=None, allow_reassociation=False, dry_run=False):", "function_signature": "    def associate(self, instance_id=None, network_interface_id=None, private_ip_address=None, allow_reassociation=False, dry_run=False):"}}
{"prompt": "    def check_csrf_token(self, request, supplied_token):", "metadata": {"task_id": "Internet/pyramid/66", "ground_truth": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "csrf.py"], "context_start_lineno": 155, "line_no": 157, "id": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "target_function_prompt": "    def check_csrf_token(self, request, supplied_token):", "function_signature": "    def check_csrf_token(self, request, supplied_token):"}}
{"prompt": "    def serialize(self, value, display=False):", "metadata": {"task_id": "Multimedia/Mopidy/26", "ground_truth": "        if not value:\n            return \"\"\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        serialized_values = []\n        for item in value:\n            serialized_value = subtype.serialize(item, display=display)\n            if serialized_value:\n                serialized_values.append(serialized_value)\n\n        return \"\\n  \" + \"\\n  \".join(serialized_values)", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 320, "line_no": 321, "id": "mopidy.config.types.List.serialize", "target_function_prompt": "    def serialize(self, value, display=False):", "function_signature": "    def serialize(self, value, display=False):"}}
{"prompt": "    def critical(self, msg, *args, **kwargs):", "metadata": {"task_id": "Utilities/gunicorn/16", "ground_truth": "        Logger.critical(self, msg, *args, **kwargs)\n        self.increment(\"gunicorn.log.critical\", 1)", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "instrument", "statsd.py"], "context_start_lineno": 43, "line_no": 44, "id": "gunicorn.instrument.statsd.Statsd.critical", "target_function_prompt": "    def critical(self, msg, *args, **kwargs):", "function_signature": "    def critical(self, msg, *args, **kwargs):"}}
{"prompt": "    def _spark_master(self):", "metadata": {"task_id": "System/mrjob/66", "ground_truth": "        num_executors = self._num_cores()\n\n        # for now assigning one core per executor, so we don't have to worry\n        # about a number of cores that's not evenly divisible\n        cores_per_executor = 1\n\n        executor_mem_bytes = _to_num_bytes(\n            self._opts['jobconf'].get('spark.executor.memory') or\n            _DEFAULT_EXECUTOR_MEMORY)\n        executor_mem_mb = math.ceil(executor_mem_bytes / 1024.0 / 1024.0)\n\n        return 'local-cluster[%d,%d,%d]' % (\n            num_executors, cores_per_executor, executor_mem_mb)", "fpath_tuple": ["System", "mrjob", "mrjob", "local.py"], "context_start_lineno": 232, "line_no": 235, "id": "mrjob.local.LocalMRJobRunner._spark_master", "target_function_prompt": "    def _spark_master(self):", "function_signature": "    def _spark_master(self):"}}
{"prompt": "    def iteritems(self):", "metadata": {"task_id": "Database/sqlitedict/4", "ground_truth": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)", "fpath_tuple": ["Database", "sqlitedict", "sqlitedict.py"], "context_start_lineno": 282, "line_no": 283, "id": "sqlitedict.SqliteDict.iteritems", "target_function_prompt": "    def iteritems(self):", "function_signature": "    def iteritems(self):"}}
{"prompt": "    def get_config_dir(cls, config: Config) -> Path:", "metadata": {"task_id": "Multimedia/Mopidy/27", "ground_truth": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "ext.py"], "context_start_lineno": 93, "line_no": 99, "id": "mopidy.ext.Extension.get_config_dir", "target_function_prompt": "    def get_config_dir(cls, config: Config) -> Path:", "function_signature": "    def get_config_dir(cls, config: Config) -> Path:"}}
{"prompt": "    def create_global_secondary_index(self, global_index):", "metadata": {"task_id": "Internet/boto/72", "ground_truth": "        if global_index:\n            gsi_data = []\n            gsi_data_attr_def = []\n\n            gsi_data.append({\n                \"Create\": global_index.schema()\n            })\n\n            for attr_def in global_index.parts:\n                gsi_data_attr_def.append(attr_def.definition())\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n                attribute_definitions=gsi_data_attr_def\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global_index to ' \\\n                  'create_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 459, "line_no": 489, "id": "boto.dynamodb2.table.Table.create_global_secondary_index", "target_function_prompt": "    def create_global_secondary_index(self, global_index):", "function_signature": "    def create_global_secondary_index(self, global_index):"}}
{"prompt": "    @property", "metadata": {"task_id": "Database/datasette/31", "ground_truth": "    def full_path(self):\n        qs = self.query_string", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "asgi.py"], "context_start_lineno": 93, "line_no": 94, "id": "datasette.utils.asgi.Request.full_path", "target_function_prompt": "    @property", "function_signature": "    @property"}}
{"prompt": "def retry_url(url, retry_on_404=True, num_retries=10, timeout=None):", "metadata": {"task_id": "Internet/boto/73", "ground_truth": "    for i in range(0, num_retries):\n        try:\n            proxy_handler = urllib.request.ProxyHandler({})\n            opener = urllib.request.build_opener(proxy_handler)\n            req = urllib.request.Request(url)\n            r = opener.open(req, timeout=timeout)\n            result = r.read()\n\n            if(not isinstance(result, six.string_types) and\n                    hasattr(result, 'decode')):\n                result = result.decode('utf-8')\n\n            return result\n        except urllib.error.HTTPError as e:\n            code = e.getcode()\n            if code == 404 and not retry_on_404:\n                return ''\n        except Exception as e:\n            boto.log.exception('Caught exception reading instance data')\n        # If not on the last iteration of the loop then sleep.\n        if i + 1 != num_retries:\n            boto.log.debug('Sleeping before retrying')\n            time.sleep(min(2 ** i,\n                           boto.config.get('Boto', 'max_retry_delay', 60)))\n    boto.log.error('Unable to read instance data, giving up')\n    return ''", "fpath_tuple": ["Internet", "boto", "boto", "utils.py"], "context_start_lineno": 204, "line_no": 211, "id": "boto.utils.retry_url", "target_function_prompt": "def retry_url(url, retry_on_404=True, num_retries=10, timeout=None):", "function_signature": "def retry_url(url, retry_on_404=True, num_retries=10, timeout=None):"}}
{"prompt": "def split_format_str(fstr):", "metadata": {"task_id": "Utilities/boltons/64", "ground_truth": "    ret = []\n\n    for lit, fname, fspec, conv in Formatter().parse(fstr):\n        if fname is None:\n            ret.append((lit, None))\n            continue\n        field_str = construct_format_field_str(fname, fspec, conv)\n        ret.append((lit, field_str))\n    return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "formatutils.py"], "context_start_lineno": 106, "line_no": 111, "id": "boltons.formatutils.split_format_str", "target_function_prompt": "def split_format_str(fstr):", "function_signature": "def split_format_str(fstr):"}}
{"prompt": "    def convert_to_public(self) -> 'SSHKey':", "metadata": {"task_id": "Security/asyncssh/7", "ground_truth": "        result = decode_ssh_public_key(self.public_data)\n        result.set_comment(self._comment)\n        result.set_filename(self._filename)\n        return result", "fpath_tuple": ["Security", "asyncssh", "asyncssh", "public_key.py"], "context_start_lineno": 623, "line_no": 633, "id": "asyncssh.public_key.SSHKey.convert_to_public", "target_function_prompt": "    def convert_to_public(self) -> 'SSHKey':", "function_signature": "    def convert_to_public(self) -> 'SSHKey':"}}
{"prompt": "def slugify(value: str, allow_dots: bool = False, allow_unicode: bool = False) -> str:", "metadata": {"task_id": "Software-Development/Faker/18", "ground_truth": "    pattern: Pattern = _re_pattern_allow_dots if allow_dots else _re_pattern\n\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize(\"NFKC\", value)\n        value = pattern.sub(\"\", value).strip().lower()\n        return _re_spaces.sub(\"-\", value)\n    value = unicodedata.normalize(\"NFKD\", value).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    value = pattern.sub(\"\", value).strip().lower()\n    return _re_spaces.sub(\"-\", value)", "fpath_tuple": ["Software-Development", "Faker", "faker", "utils", "text.py"], "context_start_lineno": 10, "line_no": 18, "id": "faker.utils.text.slugify", "target_function_prompt": "def slugify(value: str, allow_dots: bool = False, allow_unicode: bool = False) -> str:", "function_signature": "def slugify(value: str, allow_dots: bool = False, allow_unicode: bool = False) -> str:"}}
{"prompt": "    def prepare_partial(self):", "metadata": {"task_id": "Internet/boto/74", "ground_truth": "        final_data = {}\n        fields = set()\n        alterations = self._determine_alterations()\n\n        for key, value in alterations['adds'].items():\n            final_data[key] = {\n                'Action': 'PUT',\n                'Value': self._dynamizer.encode(self._data[key])\n            }\n            fields.add(key)\n\n        for key, value in alterations['changes'].items():\n            final_data[key] = {\n                'Action': 'PUT',\n                'Value': self._dynamizer.encode(self._data[key])\n            }\n            fields.add(key)\n\n        for key in alterations['deletes']:\n            final_data[key] = {\n                'Action': 'DELETE',\n            }\n            fields.add(key)\n\n        return final_data, fields", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "items.py"], "context_start_lineno": 332, "line_no": 342, "id": "boto.dynamodb2.items.Item.prepare_partial", "target_function_prompt": "    def prepare_partial(self):", "function_signature": "    def prepare_partial(self):"}}
{"prompt": "def secure_filename(filename):", "metadata": {"task_id": "Internet/falcon/31", "ground_truth": "    if not filename:\n        raise ValueError('filename may not be an empty string')\n\n    filename = unicodedata.normalize('NFKD', filename)\n    if filename.startswith('.'):\n        filename = filename.replace('.', '_', 1)\n    return _UNSAFE_CHARS.sub('_', filename)", "fpath_tuple": ["Internet", "falcon", "falcon", "util", "misc.py"], "context_start_lineno": 354, "line_no": 385, "id": "falcon.util.misc.secure_filename", "target_function_prompt": "def secure_filename(filename):", "function_signature": "def secure_filename(filename):"}}
{"prompt": "def sequence_match(password, _ranked_dictionaries=RANKED_DICTIONARIES):\n    # Identifies sequences by looking for repeated differences in unicode codepoint.\n    # this allows skipping, such as 9753, and also matches some extended unicode sequences\n    # such as Greek and Cyrillic alphabets.\n    #\n    # for example, consider the input 'abcdb975zy'\n    #\n    # password: a   b   c   d   b    9   7   5   z   y\n    # index:    0   1   2   3   4    5   6   7   8   9\n    # delta:      1   1   1  -2  -41  -2  -2  69   1\n    #\n    # expected result:\n    # [(i, j, delta), ...] = [(0, 3, 1), (5, 7, -2), (8, 9, 1)]", "metadata": {"task_id": "Security/zxcvbn-python/12", "ground_truth": "    if len(password) == 1:\n        return []\n\n    def update(i, j, delta):\n        if j - i > 1 or (delta and abs(delta) == 1):\n            if 0 < abs(delta) <= MAX_DELTA:\n                token = password[i:j + 1]\n                if re.compile(r'^[a-z]+$').match(token):\n                    sequence_name = 'lower'\n                    sequence_space = 26\n                elif re.compile(r'^[A-Z]+$').match(token):\n                    sequence_name = 'upper'\n                    sequence_space = 26\n                elif re.compile(r'^\\d+$').match(token):\n                    sequence_name = 'digits'\n                    sequence_space = 10\n                else:\n                    sequence_name = 'unicode'\n                    sequence_space = 26\n                result.append({\n                    'pattern': 'sequence',\n                    'i': i,\n                    'j': j,\n                    'token': password[i:j + 1],\n                    'sequence_name': sequence_name,\n                    'sequence_space': sequence_space,\n                    'ascending': delta > 0\n                })\n\n    result = []\n    i = 0\n    last_delta = None\n\n    for k in range(1, len(password)):\n        delta = ord(password[k]) - ord(password[k - 1])\n        if last_delta is None:\n            last_delta = delta\n        if delta == last_delta:\n            continue\n        j = k - 1\n        update(i, j, last_delta)\n        i = j\n        last_delta = delta\n    update(i, len(password) - 1, last_delta)\n\n    return result", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "matching.py"], "context_start_lineno": 382, "line_no": 395, "id": "zxcvbn.matching.sequence_match", "target_function_prompt": "def sequence_match(password, _ranked_dictionaries=RANKED_DICTIONARIES):\n    # Identifies sequences by looking for repeated differences in unicode codepoint.\n    # this allows skipping, such as 9753, and also matches some extended unicode sequences\n    # such as Greek and Cyrillic alphabets.\n    #\n    # for example, consider the input 'abcdb975zy'\n    #\n    # password: a   b   c   d   b    9   7   5   z   y\n    # index:    0   1   2   3   4    5   6   7   8   9\n    # delta:      1   1   1  -2  -41  -2  -2  69   1\n    #\n    # expected result:\n    # [(i, j, delta), ...] = [(0, 3, 1), (5, 7, -2), (8, 9, 1)]", "function_signature": "def sequence_match(password, _ranked_dictionaries=RANKED_DICTIONARIES):\n    # Identifies sequences by looking for repeated differences in unicode codepoint.\n    # this allows skipping, such as 9753, and also matches some extended unicode sequences\n    # such as Greek and Cyrillic alphabets.\n    #\n    # for example, consider the input 'abcdb975zy'\n    #\n    # password: a   b   c   d   b    9   7   5   z   y\n    # index:    0   1   2   3   4    5   6   7   8   9\n    # delta:      1   1   1  -2  -41  -2  -2  69   1\n    #\n    # expected result:\n    # [(i, j, delta), ...] = [(0, 3, 1), (5, 7, -2), (8, 9, 1)]"}}
{"prompt": "def pandas_auto_compute(\n    config: Settings, df: pd.DataFrame, summary: dict\n) -> Optional[pd.DataFrame]:", "metadata": {"task_id": "Software-Development/pandas-profiling/7", "ground_truth": "    from ydata_profiling.model.pandas.discretize_pandas import DiscretizationType\n    from ydata_profiling.model.pandas.discretize_pandas import Discretizer\n    threshold = config.categorical_maximum_correlation_distinct\n    numerical_columns = [\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Numeric\", \"TimeSeries\"} and value[\"n_distinct\"] > 1\n    ]\n    categorical_columns = [\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n        and 1 < value[\"n_distinct\"] <= threshold\n    ]\n\n    if len(numerical_columns + categorical_columns) <= 1:\n        return None\n\n    df_discretized = Discretizer(\n        DiscretizationType.UNIFORM, n_bins=config.correlations[\"auto\"].n_bins\n    ).discretize_dataframe(df)\n    columns_tested = numerical_columns + categorical_columns\n    correlation_matrix = pd.DataFrame(\n        np.ones((len(columns_tested), len(columns_tested))),\n        index=columns_tested,\n        columns=columns_tested,\n    )\n    for col_1_name, col_2_name in itertools.combinations(columns_tested, 2):\n\n        method = (\n            _pairwise_spearman\n            if col_1_name and col_2_name not in categorical_columns\n            else _pairwise_cramers\n        )\n\n        def f(col_name: str, method: Callable) -> pd.Series:\n            return (\n                df_discretized\n                if col_name in numerical_columns and method is _pairwise_cramers\n                else df\n            )\n\n        score = method(\n            f(col_1_name, method)[col_1_name], f(col_2_name, method)[col_2_name]\n        )\n        (\n            correlation_matrix.loc[col_1_name, col_2_name],\n            correlation_matrix.loc[col_2_name, col_1_name],\n        ) = (score, score)\n\n    return correlation_matrix", "fpath_tuple": ["Software-Development", "pandas-profiling", "src", "ydata_profiling", "model", "pandas", "correlations_pandas.py"], "context_start_lineno": 160, "line_no": 163, "id": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "target_function_prompt": "def pandas_auto_compute(\n    config: Settings, df: pd.DataFrame, summary: dict\n) -> Optional[pd.DataFrame]:", "function_signature": "def pandas_auto_compute(\n    config: Settings, df: pd.DataFrame, summary: dict\n) -> Optional[pd.DataFrame]:"}}
{"prompt": "def capture_db(dialect=\"postgresql://\"):", "metadata": {"task_id": "Database/alembic/30", "ground_truth": "    from ..util.sqla_compat import create_mock_engine\n    buf = []\n\n    def dump(sql, *multiparams, **params):\n        buf.append(str(sql.compile(dialect=engine.dialect)))\n\n    engine = create_mock_engine(dialect, dump)\n    return engine, buf", "fpath_tuple": ["Database", "alembic", "alembic", "testing", "fixtures.py"], "context_start_lineno": 72, "line_no": 73, "id": "alembic.testing.fixtures.capture_db", "target_function_prompt": "def capture_db(dialect=\"postgresql://\"):", "function_signature": "def capture_db(dialect=\"postgresql://\"):"}}
{"prompt": "    def __repr__(self):", "metadata": {"task_id": "Utilities/boltons/65", "ground_truth": "        cn = self.__class__.__name__\n        if self.headers:\n            return '%s(headers=%r, data=%r)' % (cn, self.headers, self._data)\n        else:\n            return '%s(%r)' % (cn, self._data)", "fpath_tuple": ["Utilities", "boltons", "boltons", "tableutils.py"], "context_start_lineno": 436, "line_no": 437, "id": "boltons.tableutils.Table.__repr__", "target_function_prompt": "    def __repr__(self):", "function_signature": "    def __repr__(self):"}}
{"prompt": "    def connect(self, receiver, name=None, sender=None):", "metadata": {"task_id": "Software-Development/peewee/5", "ground_truth": "        name = name or receiver.__name__\n        key = (name, sender)\n        if key not in self._receivers:\n            self._receivers.add(key)\n            self._receiver_list.append((name, receiver, sender))\n        else:\n            raise ValueError('receiver named %s (for sender=%s) already '\n                             'connected' % (name, sender or 'any'))", "fpath_tuple": ["Software-Development", "peewee", "playhouse", "signals.py"], "context_start_lineno": 14, "line_no": 15, "id": "playhouse.signals.Signal.connect", "target_function_prompt": "    def connect(self, receiver, name=None, sender=None):", "function_signature": "    def connect(self, receiver, name=None, sender=None):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/75", "ground_truth": "    from boto.regioninfo import connect\n    from boto.cloudhsm.layer1 import CloudHSMConnection\n    return connect('cloudhsm', region_name, connection_cls=CloudHSMConnection,\n                   **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "cloudhsm", "__init__.py"], "context_start_lineno": 37, "line_no": 38, "id": "boto.cloudhsm.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def update(self, text):", "metadata": {"task_id": "Database/mssql-cli/14", "ground_truth": "        self.update_keywords(text)\n        self.update_names(text)", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "packages", "prioritization.py"], "context_start_lineno": 28, "line_no": 29, "id": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "target_function_prompt": "    def update(self, text):", "function_signature": "    def update(self, text):"}}
{"prompt": "def notification_from_headers(channel, headers):", "metadata": {"task_id": "Internet/google-api-python-client/5", "ground_truth": "    from googleapiclient import errors\n    headers = _upper_header_keys(headers)\n    channel_id = headers[X_GOOG_CHANNEL_ID]\n    if channel.id != channel_id:\n        raise errors.InvalidNotificationError(\n            \"Channel id mismatch: %s != %s\" % (channel.id, channel_id)\n        )\n    else:\n        message_number = int(headers[X_GOOG_MESSAGE_NUMBER])\n        state = headers[X_GOOG_RESOURCE_STATE]\n        resource_uri = headers[X_GOOG_RESOURCE_URI]\n        resource_id = headers[X_GOOG_RESOURCE_ID]\n        return Notification(message_number, state, resource_uri, resource_id)", "fpath_tuple": ["Internet", "google-api-python-client", "googleapiclient", "channel.py"], "context_start_lineno": 250, "line_no": 266, "id": "googleapiclient.channel.notification_from_headers", "target_function_prompt": "def notification_from_headers(channel, headers):", "function_signature": "def notification_from_headers(channel, headers):"}}
{"prompt": "    def to_json(self, base_dir=None):", "metadata": {"task_id": "Utilities/sacred/28", "ground_truth": "        if base_dir:\n            return (\n                os.path.relpath(self.filename, os.path.realpath(base_dir)),\n                self.digest,\n            )\n        else:\n            return self.filename, self.digest", "fpath_tuple": ["Utilities", "sacred", "sacred", "dependencies.py"], "context_start_lineno": 462, "line_no": 463, "id": "sacred.dependencies.Source.to_json", "target_function_prompt": "    def to_json(self, base_dir=None):", "function_signature": "    def to_json(self, base_dir=None):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/76", "ground_truth": "    from boto.regioninfo import connect\n    from boto.beanstalk.layer1 import Layer1\n    return connect('elasticbeanstalk', region_name, connection_cls=Layer1,\n                   **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "beanstalk", "__init__.py"], "context_start_lineno": 40, "line_no": 41, "id": "boto.beanstalk.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "def _render_check_constraint(\n    constraint: CheckConstraint,\n    autogen_context: AutogenContext,\n    namespace_metadata: Optional[MetaData],\n) -> Optional[str]:", "metadata": {"task_id": "Database/alembic/31", "ground_truth": "    rendered = _user_defined_render(\"check\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    # detect the constraint being part of\n    # a parent type which is probably in the Table already.\n    # ideally SQLAlchemy would give us more of a first class\n    # way to detect this.\n    if (\n        constraint._create_rule  # type:ignore[attr-defined]\n        and hasattr(\n            constraint._create_rule, \"target\"  # type:ignore[attr-defined]\n        )\n        and isinstance(\n            constraint._create_rule.target,  # type:ignore[attr-defined]\n            sqltypes.TypeEngine,\n        )\n    ):\n        return None\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n    return \"%(prefix)sCheckConstraint(%(sqltext)s%(opts)s)\" % {\n        \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n        \"opts\": \", \" + (\", \".join(\"%s=%s\" % (k, v) for k, v in opts))\n        if opts\n        else \"\",\n        \"sqltext\": _render_potential_expr(\n            constraint.sqltext, autogen_context, wrap_in_text=False\n        ),\n    }", "fpath_tuple": ["Database", "alembic", "alembic", "autogenerate", "render.py"], "context_start_lineno": 1031, "line_no": 1036, "id": "alembic.autogenerate.render._render_check_constraint", "target_function_prompt": "def _render_check_constraint(\n    constraint: CheckConstraint,\n    autogen_context: AutogenContext,\n    namespace_metadata: Optional[MetaData],\n) -> Optional[str]:", "function_signature": "def _render_check_constraint(\n    constraint: CheckConstraint,\n    autogen_context: AutogenContext,\n    namespace_metadata: Optional[MetaData],\n) -> Optional[str]:"}}
{"prompt": "    def deserialize(cls, value, *args, **kwargs):", "metadata": {"task_id": "Text-Processing/rows/8", "ground_truth": "        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n\n        dt_object = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n        return datetime.date(dt_object.year, dt_object.month, dt_object.day)", "fpath_tuple": ["Text-Processing", "rows", "rows", "fields.py"], "context_start_lineno": 366, "line_no": 367, "id": "rows.fields.DateField.deserialize", "target_function_prompt": "    def deserialize(cls, value, *args, **kwargs):", "function_signature": "    def deserialize(cls, value, *args, **kwargs):"}}
{"prompt": "def rel_path(base, path):", "metadata": {"task_id": "Utilities/sacred/29", "ground_truth": "    if base == path:\n        return \"\"\n    assert is_prefix(base, path), \"{} not a prefix of {}\".format(base, path)\n    return path[len(base) :].strip(\".\")", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 528, "line_no": 530, "id": "sacred.utils.rel_path", "target_function_prompt": "def rel_path(base, path):", "function_signature": "def rel_path(base, path):"}}
{"prompt": "    def filtered_sources(self, resource_name):", "metadata": {"task_id": "Internet/pyramid/67", "ground_truth": "        for override in self.overrides:\n            o = override(resource_name)\n            if o is not None:\n                yield o", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "assets.py"], "context_start_lineno": 118, "line_no": 119, "id": "pyramid.config.assets.PackageOverrides.filtered_sources", "target_function_prompt": "    def filtered_sources(self, resource_name):", "function_signature": "    def filtered_sources(self, resource_name):"}}
{"prompt": "    def write(self, text):", "metadata": {"task_id": "Communications/chatette/15", "ground_truth": "        if self.redirection_file_path is None and self._file_mode is None:\n            print(text)\n        elif self._file_mode == 'quiet':\n            return\n        else:\n            if self.buffered_text is None:\n                self.buffered_text = str(text)\n            else:\n                self.buffered_text += '\\n' + str(text)", "fpath_tuple": ["Communications", "chatette", "chatette", "cli", "terminal_writer.py"], "context_start_lineno": 60, "line_no": 61, "id": "chatette.cli.terminal_writer.TerminalWriter.write", "target_function_prompt": "    def write(self, text):", "function_signature": "    def write(self, text):"}}
{"prompt": "    def deserialize(self, value, display=False):", "metadata": {"task_id": "Multimedia/Mopidy/28", "ground_truth": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        socket_path = path.get_unix_socket_path(value)\n        if socket_path is not None:\n            path_str = Path(not self._required).deserialize(socket_path)\n            return f\"unix:{path_str}\"\n\n        try:\n            socket.getaddrinfo(value, None)\n        except OSError:\n            raise ValueError(\"must be a resolveable hostname or valid IP\")\n\n        return value", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 384, "line_no": 385, "id": "mopidy.config.types.Hostname.deserialize", "target_function_prompt": "    def deserialize(self, value, display=False):", "function_signature": "    def deserialize(self, value, display=False):"}}
{"prompt": "    def to_payload(\n        cls,\n        batch: ext.NpNDArray,\n        batch_dim: int,\n    ) -> Payload:\n        # skip 0-dimensional array", "metadata": {"task_id": "Scientific-Engineering/bentoml/24", "ground_truth": "        if batch.shape:\n            if not (batch.flags[\"C_CONTIGUOUS\"] or batch.flags[\"F_CONTIGUOUS\"]):\n                # TODO: use fortan contiguous if it's faster\n                batch = np.ascontiguousarray(batch)\n\n            bs: bytes\n            concat_buffer_bs: bytes\n            indices: list[int]\n            bs, concat_buffer_bs, indices = pep574_dumps(batch)\n            bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n            return cls.create_payload(\n                concat_buffer_bs,\n                batch.shape[batch_dim],\n                {\n                    \"format\": \"pickle5\",\n                    \"pickle_bytes_str\": bs_str,\n                    \"indices\": indices,\n                },\n            )\n\n        return cls.create_payload(\n            pickle.dumps(batch),\n            batch.shape[batch_dim],\n            {\"format\": \"default\"},\n        )", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 274, "line_no": 280, "id": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "target_function_prompt": "    def to_payload(\n        cls,\n        batch: ext.NpNDArray,\n        batch_dim: int,\n    ) -> Payload:\n        # skip 0-dimensional array", "function_signature": "    def to_payload(\n        cls,\n        batch: ext.NpNDArray,\n        batch_dim: int,\n    ) -> Payload:\n        # skip 0-dimensional array"}}
{"prompt": "def regions():", "metadata": {"task_id": "Internet/boto/77", "ground_truth": "    from boto.regioninfo import get_regions\n    from boto.opsworks.layer1 import OpsWorksConnection\n    return get_regions('opsworks', connection_cls=OpsWorksConnection)", "fpath_tuple": ["Internet", "boto", "boto", "opsworks", "__init__.py"], "context_start_lineno": 26, "line_no": 33, "id": "boto.opsworks.regions", "target_function_prompt": "def regions():", "function_signature": "def regions():"}}
{"prompt": "    def __repr__(self) -> str:", "metadata": {"task_id": "Utilities/praw/9", "ground_truth": "        return (\n            f\"{self.__class__.__name__}(error_type={self.error_type!r},\"\n            f\" message={self.message!r}, field={self.field!r})\"\n        )", "fpath_tuple": ["Utilities", "praw", "praw", "exceptions.py"], "context_start_lineno": 61, "line_no": 63, "id": "praw.exceptions.RedditErrorItem.__repr__", "target_function_prompt": "    def __repr__(self) -> str:", "function_signature": "    def __repr__(self) -> str:"}}
{"prompt": "    def _get(self):", "metadata": {"task_id": "Utilities/praw/10", "ground_truth": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            raise KeyError\n        return result[0]", "fpath_tuple": ["Utilities", "praw", "praw", "util", "token_manager.py"], "context_start_lineno": 137, "line_no": 138, "id": "praw.util.token_manager.SQLiteTokenManager._get", "target_function_prompt": "    def _get(self):", "function_signature": "    def _get(self):"}}
{"prompt": "    def get_metadata(self) -> tuple:", "metadata": {"task_id": "Database/bplustree/12", "ground_truth": "        try:\n            data = self._read_page(0)\n        except ReachedEndOfFile:\n            raise ValueError('Metadata not set yet')\n        end_root_node_page = PAGE_REFERENCE_BYTES\n        root_node_page = int.from_bytes(\n            data[0:end_root_node_page], ENDIAN\n        )\n        end_page_size = end_root_node_page + OTHERS_BYTES\n        page_size = int.from_bytes(\n            data[end_root_node_page:end_page_size], ENDIAN\n        )\n        end_order = end_page_size + OTHERS_BYTES\n        order = int.from_bytes(\n            data[end_page_size:end_order], ENDIAN\n        )\n        end_key_size = end_order + OTHERS_BYTES\n        key_size = int.from_bytes(\n            data[end_order:end_key_size], ENDIAN\n        )\n        end_value_size = end_key_size + OTHERS_BYTES\n        value_size = int.from_bytes(\n            data[end_key_size:end_value_size], ENDIAN\n        )\n        self._tree_conf = TreeConf(\n            page_size, order, key_size, value_size, self._tree_conf.serializer\n        )\n        return root_node_page, self._tree_conf", "fpath_tuple": ["Database", "bplustree", "bplustree", "memory.py"], "context_start_lineno": 204, "line_no": 205, "id": "bplustree.memory.FileMemory.get_metadata", "target_function_prompt": "    def get_metadata(self) -> tuple:", "function_signature": "    def get_metadata(self) -> tuple:"}}
{"prompt": "    def sections(self) -> List[WikipediaPageSection]:", "metadata": {"task_id": "Communications/Wikipedia-API/7", "ground_truth": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 923, "line_no": 929, "id": "wikipediaapi.WikipediaPage.sections", "target_function_prompt": "    def sections(self) -> List[WikipediaPageSection]:", "function_signature": "    def sections(self) -> List[WikipediaPageSection]:"}}
{"prompt": "def rarest_window_session(\n    session: List[str],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str,\n    end_token: str,\n    use_geo_mean: bool = False,\n) -> Tuple[List[str], float]:", "metadata": {"task_id": "Security/msticpy/9", "ground_truth": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    if len(likelihoods) == 0:\n        return [], np.nan\n    min_lik = min(likelihoods)\n    ind = likelihoods.index(min_lik)\n    return session[ind : ind + window_len], min_lik  # noqa: E203", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "cmds_only.py"], "context_start_lineno": 280, "line_no": 323, "id": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "target_function_prompt": "def rarest_window_session(\n    session: List[str],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str,\n    end_token: str,\n    use_geo_mean: bool = False,\n) -> Tuple[List[str], float]:", "function_signature": "def rarest_window_session(\n    session: List[str],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str,\n    end_token: str,\n    use_geo_mean: bool = False,\n) -> Tuple[List[str], float]:"}}
{"prompt": "    def tzname(self, dt):", "metadata": {"task_id": "Communications/hl7/3", "ground_truth": "        minutes = abs(self.minutes)\n        return \"{0}{1:02}{2:02}\".format(\n            \"-\" if self.minutes < 0 else \"+\", minutes // 60, minutes % 60\n        )", "fpath_tuple": ["Communications", "hl7", "hl7", "datatypes.py"], "context_start_lineno": 18, "line_no": 19, "id": "hl7.datatypes._UTCOffset.tzname", "target_function_prompt": "    def tzname(self, dt):", "function_signature": "    def tzname(self, dt):"}}
{"prompt": "    def select_folder(self, folder, readonly=False):", "metadata": {"task_id": "Communications/IMAPClient/25", "ground_truth": "        self._command_and_check(\"select\", self._normalise_folder(folder), readonly)\n        return self._process_select_response(self._imap.untagged_responses)", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 801, "line_no": 819, "id": "imapclient.imapclient.IMAPClient.select_folder", "target_function_prompt": "    def select_folder(self, folder, readonly=False):", "function_signature": "    def select_folder(self, folder, readonly=False):"}}
{"prompt": "    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> CreateTableOp:", "metadata": {"task_id": "Database/alembic/32", "ground_truth": "        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            list(table.c) + list(table.constraints),  # type:ignore[arg-type]\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            # given a Table() object, this Table will contain full Index()\n            # and UniqueConstraint objects already constructed in response to\n            # each unique=True / index=True flag on a Column.  Carry this\n            # state along so that when we re-convert back into a Table, we\n            # skip unique=True/index=True so that these constraints are\n            # not doubled up. see #844 #848\n            _constraints_included=True,\n            comment=table.comment,\n            info=dict(table.info),\n            prefixes=list(table._prefixes),\n            **table.kwargs,\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 1176, "line_no": 1179, "id": "alembic.operations.ops.CreateTableOp.from_table", "target_function_prompt": "    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> CreateTableOp:", "function_signature": "    def from_table(\n        cls, table: Table, *, _namespace_metadata: Optional[MetaData] = None\n    ) -> CreateTableOp:"}}
{"prompt": "    def determine_chords(self, shorthand=False):", "metadata": {"task_id": "Multimedia/mingus/33", "ground_truth": "        chords = []\n        for x in self.bar:\n            chords.append([x[0], x[2].determine(shorthand)])\n        return chords", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "bar.py"], "context_start_lineno": 187, "line_no": 189, "id": "mingus.containers.bar.Bar.determine_chords", "target_function_prompt": "    def determine_chords(self, shorthand=False):", "function_signature": "    def determine_chords(self, shorthand=False):"}}
{"prompt": "def EntryPoint():", "metadata": {"task_id": "Security/capirca/8", "ground_truth": "  SetupFlags()\n  app.run(main)", "fpath_tuple": ["Security", "capirca", "capirca", "aclgen.py"], "context_start_lineno": 693, "line_no": 695, "id": "capirca.aclgen.EntryPoint", "target_function_prompt": "def EntryPoint():", "function_signature": "def EntryPoint():"}}
{"prompt": "def server_error(request, *args, **kwargs):", "metadata": {"task_id": "Internet/djangorestframework/16", "ground_truth": "    data = {\n        'error': 'Server Error (500)'\n    }\n    return JsonResponse(data, status=status.HTTP_500_INTERNAL_SERVER_ERROR)", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "exceptions.py"], "context_start_lineno": 248, "line_no": 252, "id": "rest_framework.exceptions.server_error", "target_function_prompt": "def server_error(request, *args, **kwargs):", "function_signature": "def server_error(request, *args, **kwargs):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/78", "ground_truth": "    from boto.regioninfo import connect\n    return connect('swf', region_name,\n                   connection_cls=boto.swf.layer1.Layer1, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "swf", "__init__.py"], "context_start_lineno": 42, "line_no": 43, "id": "boto.swf.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def document(self):", "metadata": {"task_id": "Internet/sumy/19", "ground_truth": "        current_paragraph = []\n        paragraphs = []\n        for line in self._text.splitlines():\n            line = line.strip()\n            if line.isupper():\n                heading = Sentence(line, self._tokenizer, is_heading=True)\n                current_paragraph.append(heading)\n            elif not line and current_paragraph:\n                sentences = self._to_sentences(current_paragraph)\n                paragraphs.append(Paragraph(sentences))\n                current_paragraph = []\n            elif line:\n                current_paragraph.append(line)\n\n        sentences = self._to_sentences(current_paragraph)\n        paragraphs.append(Paragraph(sentences))\n\n        return ObjectDocumentModel(paragraphs)", "fpath_tuple": ["Internet", "sumy", "sumy", "parsers", "plaintext.py"], "context_start_lineno": 59, "line_no": 60, "id": "sumy.parsers.plaintext.PlaintextParser.document", "target_function_prompt": "    def document(self):", "function_signature": "    def document(self):"}}
{"prompt": "    def uri(self):", "metadata": {"task_id": "Internet/falcon/32", "ground_truth": "        if self._cached_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.scheme + '://' + self.netloc + self.relative_uri\n\n            self._cached_uri = value\n\n        return self._cached_uri", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 774, "line_no": 775, "id": "falcon.request.Request.uri", "target_function_prompt": "    def uri(self):", "function_signature": "    def uri(self):"}}
{"prompt": "def is_s3_uri(uri):", "metadata": {"task_id": "System/mrjob/67", "ground_truth": "    try:\n        parse_s3_uri(uri)\n        return True\n    except ValueError:\n        return False", "fpath_tuple": ["System", "mrjob", "mrjob", "parse.py"], "context_start_lineno": 42, "line_no": 44, "id": "mrjob.parse.is_s3_uri", "target_function_prompt": "def is_s3_uri(uri):", "function_signature": "def is_s3_uri(uri):"}}
{"prompt": "def ip_bin_to_ip4_addr(ip_bin):", "metadata": {"task_id": "Security/pycoin/25", "ground_truth": "    from pycoin.intbytes import iterbytes\n    return \"%d.%d.%d.%d\" % tuple(iterbytes(ip_bin[-4:]))", "fpath_tuple": ["Security", "pycoin", "pycoin", "message", "PeerAddress.py"], "context_start_lineno": 15, "line_no": 16, "id": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "target_function_prompt": "def ip_bin_to_ip4_addr(ip_bin):", "function_signature": "def ip_bin_to_ip4_addr(ip_bin):"}}
{"prompt": "    def get_range(self):", "metadata": {"task_id": "Multimedia/mingus/34", "ground_truth": "        (min, max) = (100000, -1)\n        for cont in self.bar:\n            for note in cont[2]:\n                if int(note) < int(min):\n                    min = note\n                elif int(note) > int(max):\n                    max = note\n        return (min, max)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "bar.py"], "context_start_lineno": 150, "line_no": 152, "id": "mingus.containers.bar.Bar.get_range", "target_function_prompt": "    def get_range(self):", "function_signature": "    def get_range(self):"}}
{"prompt": "def parse_metadata(content: str) -> dict:", "metadata": {"task_id": "Database/datasette/32", "ground_truth": "    try:\n        return json.loads(content)\n    except json.JSONDecodeError:\n        try:\n            return yaml.safe_load(content)\n        except yaml.YAMLError:\n            raise BadMetadataError(\"Metadata is not valid JSON or YAML\")", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 976, "line_no": 979, "id": "datasette.utils.parse_metadata", "target_function_prompt": "def parse_metadata(content: str) -> dict:", "function_signature": "def parse_metadata(content: str) -> dict:"}}
{"prompt": "    def iter_options(self):", "metadata": {"task_id": "Internet/djangorestframework/17", "ground_truth": "        return iter_options(\n            self.grouped_choices,\n            cutoff=self.html_cutoff,\n            cutoff_text=self.html_cutoff_text\n        )", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "fields.py"], "context_start_lineno": 1389, "line_no": 1393, "id": "rest_framework.fields.ChoiceField.iter_options", "target_function_prompt": "    def iter_options(self):", "function_signature": "    def iter_options(self):"}}
{"prompt": "def parse_policy_document(stream):", "metadata": {"task_id": "Security/trailscraper/6", "ground_truth": "    if isinstance(stream, six.string_types):\n        json_dict = json.loads(stream)\n    else:\n        json_dict = json.load(stream)\n\n    return PolicyDocument(_parse_statements(json_dict['Statement']), Version=json_dict['Version'])", "fpath_tuple": ["Security", "trailscraper", "trailscraper", "iam.py"], "context_start_lineno": 162, "line_no": 164, "id": "trailscraper.iam.parse_policy_document", "target_function_prompt": "def parse_policy_document(stream):", "function_signature": "def parse_policy_document(stream):"}}
{"prompt": "def load_config_file(filename):", "metadata": {"task_id": "Utilities/sacred/30", "ground_truth": "    handler = get_handler(filename)\n    with open(filename, \"r\" + handler.mode) as f:\n        return handler.load(f)", "fpath_tuple": ["Utilities", "sacred", "sacred", "config", "config_files.py"], "context_start_lineno": 58, "line_no": 59, "id": "sacred.config.config_files.load_config_file", "target_function_prompt": "def load_config_file(filename):", "function_signature": "def load_config_file(filename):"}}
{"prompt": "    def deserialize(self, value):", "metadata": {"task_id": "Multimedia/Mopidy/29", "ground_truth": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.lower() in self.true_values:\n            return True\n        elif value.lower() in self.false_values:\n            return False\n        raise ValueError(f\"invalid value for boolean: {value!r}\")", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 201, "line_no": 202, "id": "mopidy.config.types.Boolean.deserialize", "target_function_prompt": "    def deserialize(self, value):", "function_signature": "    def deserialize(self, value):"}}
{"prompt": "def new_webhook_channel(url, token=None, expiration=None, params=None):", "metadata": {"task_id": "Internet/google-api-python-client/6", "ground_truth": "    expiration_ms = 0\n    if expiration:\n        delta = expiration - EPOCH\n        expiration_ms = (\n            delta.microseconds / 1000 + (delta.seconds + delta.days * 24 * 3600) * 1000\n        )\n        if expiration_ms < 0:\n            expiration_ms = 0\n\n    return Channel(\n        \"web_hook\",\n        str(uuid.uuid4()),\n        token,\n        url,\n        expiration=expiration_ms,\n        params=params,\n    )", "fpath_tuple": ["Internet", "google-api-python-client", "googleapiclient", "channel.py"], "context_start_lineno": 282, "line_no": 299, "id": "googleapiclient.channel.new_webhook_channel", "target_function_prompt": "def new_webhook_channel(url, token=None, expiration=None, params=None):", "function_signature": "def new_webhook_channel(url, token=None, expiration=None, params=None):"}}
{"prompt": "def dict_merge(a, b):", "metadata": {"task_id": "Internet/kinto/27", "ground_truth": "    result = dict(**b)\n    for key, value in a.items():\n        if isinstance(value, collections_abc.Mapping):\n            value = dict_merge(value, result.setdefault(key, {}))\n        result[key] = value\n    return result", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "utils.py"], "context_start_lineno": 186, "line_no": 188, "id": "kinto.core.utils.dict_merge", "target_function_prompt": "def dict_merge(a, b):", "function_signature": "def dict_merge(a, b):"}}
{"prompt": "    def timestamp(self):", "metadata": {"task_id": "Internet/kinto/28", "ground_truth": "        try:\n            return self.model.timestamp()\n        except storage_exceptions.ReadonlyError as e:\n            # If the instance is configured to be readonly, and if the\n            # resource is empty, the backend will try to bump the timestamp.\n            # It fails if the configured db user has not write privileges.\n            logger.exception(e)\n            error_msg = (\n                \"Resource timestamp cannot be written. \"\n                \"Plural endpoint must be hit at least once from a \"\n                \"writable instance.\"\n            )\n            raise http_error(HTTPServiceUnavailable(), errno=ERRORS.BACKEND, message=error_msg)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "resource", "__init__.py"], "context_start_lineno": 232, "line_no": 237, "id": "kinto.core.resource.Resource.timestamp", "target_function_prompt": "    def timestamp(self):", "function_signature": "    def timestamp(self):"}}
{"prompt": "def make_read_only(o):", "metadata": {"task_id": "Utilities/sacred/31", "ground_truth": "    if type(o) == dict:\n        return ReadOnlyDict({k: make_read_only(v) for k, v in o.items()})\n    elif type(o) == list:\n        return ReadOnlyList([make_read_only(v) for v in o])\n    elif type(o) == tuple:\n        return tuple(map(make_read_only, o))\n    else:\n        return o", "fpath_tuple": ["Utilities", "sacred", "sacred", "config", "custom_containers.py"], "context_start_lineno": 219, "line_no": 226, "id": "sacred.config.custom_containers.make_read_only", "target_function_prompt": "def make_read_only(o):", "function_signature": "def make_read_only(o):"}}
{"prompt": "    def address(self):", "metadata": {"task_id": "Utilities/gunicorn/17", "ground_truth": "        s = self.settings['bind'].get()\n        return [util.parse_address(util.bytes_to_str(bind)) for bind in s]", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "config.py"], "context_start_lineno": 126, "line_no": 127, "id": "gunicorn.config.Config.address", "target_function_prompt": "    def address(self):", "function_signature": "    def address(self):"}}
{"prompt": "    def _fetch(self, call) -> \"WikipediaPage\":", "metadata": {"task_id": "Communications/Wikipedia-API/8", "ground_truth": "        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 1061, "line_no": 1063, "id": "wikipediaapi.WikipediaPage._fetch", "target_function_prompt": "    def _fetch(self, call) -> \"WikipediaPage\":", "function_signature": "    def _fetch(self, call) -> \"WikipediaPage\":"}}
{"prompt": "    def darwin_installer(self):", "metadata": {"task_id": "Utilities/python-for-android/20", "ground_truth": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 330, "line_no": 331, "id": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "target_function_prompt": "    def darwin_installer(self):", "function_signature": "    def darwin_installer(self):"}}
{"prompt": "    def fit(self, X, y, sample_weight=None):", "metadata": {"task_id": "Security/diffprivlib/16", "ground_truth": "        from diffprivlib.utils import PrivacyLeakWarning\n        self._validate_params()\n        self.accountant.check(self.epsilon, 0)\n\n        if sample_weight is not None:\n            self._warn_unused_args(\"sample_weight\")\n\n        random_state = check_random_state(self.random_state)\n\n        X, y = self._validate_data(X, y, accept_sparse=False, y_numeric=True, multi_output=True)\n\n        if self.bounds_X is None or self.bounds_y is None:\n            warnings.warn(\n                \"Bounds parameters haven't been specified, so falling back to determining bounds from the \"\n                \"data.\\n\"\n                \"This will result in additional privacy leakage. To ensure differential privacy with no \"\n                \"additional privacy loss, specify `bounds_X` and `bounds_y`.\",\n                PrivacyLeakWarning)\n\n            if self.bounds_X is None:\n                self.bounds_X = (np.min(X, axis=0), np.max(X, axis=0))\n            if self.bounds_y is None:\n                self.bounds_y = (np.min(y, axis=0), np.max(y, axis=0))\n\n        # pylint: disable=no-member\n        self.bounds_X = self._check_bounds(self.bounds_X, X.shape[1])\n        self.bounds_y = self._check_bounds(self.bounds_y, y.shape[1] if y.ndim > 1 else 1)\n\n        n_features = X.shape[1]\n        n_targets = y.shape[1] if y.ndim > 1 else 1\n        epsilon_intercept_scale = 1 / (n_features + 1) if self.fit_intercept else 0\n\n        X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n            X, y, fit_intercept=self.fit_intercept, bounds_X=self.bounds_X, bounds_y=self.bounds_y,\n            epsilon=self.epsilon * epsilon_intercept_scale, copy=self.copy_X, random_state=random_state)\n\n        bounds_X = (self.bounds_X[0] - X_offset, self.bounds_X[1] - X_offset)\n        bounds_y = (self.bounds_y[0] - y_offset, self.bounds_y[1] - y_offset)\n\n        objs, obj_coefs = _construct_regression_obj(\n            X, y, bounds_X, bounds_y, epsilon=self.epsilon * (1 - epsilon_intercept_scale), alpha=0,\n            random_state=random_state)\n        coef = np.zeros((n_features, n_targets))\n\n        for i, obj in enumerate(objs):\n            opt_result = minimize(obj, np.zeros(n_features), jac=True)\n            coef[:, i] = opt_result.x\n\n        self.coef_ = coef.T\n        self._obj_coefs = obj_coefs\n\n        if y.ndim == 1:\n            self.coef_ = np.ravel(self.coef_)\n        self._set_intercept(X_offset, y_offset, X_scale)\n\n        self.accountant.spend(self.epsilon, 0)\n\n        return self", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "models", "linear_regression.py"], "context_start_lineno": 239, "line_no": 259, "id": "diffprivlib.models.linear_regression.LinearRegression.fit", "target_function_prompt": "    def fit(self, X, y, sample_weight=None):", "function_signature": "    def fit(self, X, y, sample_weight=None):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/79", "ground_truth": "    from boto.regioninfo import connect\n    from boto.elasticache.layer1 import ElastiCacheConnection\n    return connect('elasticache', region_name,\n                   connection_cls=ElastiCacheConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "elasticache", "__init__.py"], "context_start_lineno": 37, "line_no": 38, "id": "boto.elasticache.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "def get_serve_info() -> ServeInfo:  # pragma: no cover\n    # Returns a safe token for serve as well as timestamp of creating this token", "metadata": {"task_id": "Scientific-Engineering/bentoml/25", "ground_truth": "    return ServeInfo(\n        serve_id=secrets.token_urlsafe(32),\n        serve_started_timestamp=datetime.now(timezone.utc),\n    )", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "utils", "analytics", "usage_stats.py"], "context_start_lineno": 92, "line_no": 94, "id": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "target_function_prompt": "def get_serve_info() -> ServeInfo:  # pragma: no cover\n    # Returns a safe token for serve as well as timestamp of creating this token", "function_signature": "def get_serve_info() -> ServeInfo:  # pragma: no cover\n    # Returns a safe token for serve as well as timestamp of creating this token"}}
{"prompt": "    def add_rule(self, id=None, prefix='', status='Enabled',\n                 expiration=None, transition=None):", "metadata": {"task_id": "Internet/boto/80", "ground_truth": "        rule = Rule(id, prefix, status, expiration, transition)\n        self.append(rule)", "fpath_tuple": ["Internet", "boto", "boto", "s3", "lifecycle.py"], "context_start_lineno": 279, "line_no": 309, "id": "boto.s3.lifecycle.Lifecycle.add_rule", "target_function_prompt": "    def add_rule(self, id=None, prefix='', status='Enabled',\n                 expiration=None, transition=None):", "function_signature": "    def add_rule(self, id=None, prefix='', status='Enabled',\n                 expiration=None, transition=None):"}}
{"prompt": "def is_local_source(filename, modname, experiment_path):", "metadata": {"task_id": "Utilities/sacred/32", "ground_truth": "    filename = Path(os.path.abspath(os.path.realpath(filename)))\n    experiment_path = Path(os.path.abspath(os.path.realpath(experiment_path)))\n    if experiment_path not in filename.parents:\n        return False\n    rel_path = filename.relative_to(experiment_path)\n    path_parts = convert_path_to_module_parts(rel_path)\n\n    mod_parts = modname.split(\".\")\n    if path_parts == mod_parts:\n        return True\n    if len(path_parts) > len(mod_parts):\n        return False\n    abs_path_parts = convert_path_to_module_parts(filename)\n    return all([p == m for p, m in zip(reversed(abs_path_parts), reversed(mod_parts))])", "fpath_tuple": ["Utilities", "sacred", "sacred", "dependencies.py"], "context_start_lineno": 551, "line_no": 575, "id": "sacred.dependencies.is_local_source", "target_function_prompt": "def is_local_source(filename, modname, experiment_path):", "function_signature": "def is_local_source(filename, modname, experiment_path):"}}
{"prompt": "    def verify(self, data: bytes, sig: bytes) -> bool:", "metadata": {"task_id": "Security/asyncssh/8", "ground_truth": "        try:\n            packet = SSHPacket(sig)\n            sig_algorithm = packet.get_string()\n\n            if sig_algorithm not in self.all_sig_algorithms:\n                return False\n\n            return self.verify_ssh(data, sig_algorithm, packet)\n        except PacketDecodeError:\n            return False", "fpath_tuple": ["Security", "asyncssh", "asyncssh", "public_key.py"], "context_start_lineno": 568, "line_no": 571, "id": "asyncssh.public_key.SSHKey.verify", "target_function_prompt": "    def verify(self, data: bytes, sig: bytes) -> bool:", "function_signature": "    def verify(self, data: bytes, sig: bytes) -> bool:"}}
{"prompt": "def url_quote(path_snippet):\n    # type: (Text) -> Text", "metadata": {"task_id": "System/fs/22", "ground_truth": "    if _WINDOWS_PLATFORM and _has_drive_letter(path_snippet):\n        drive_letter, path = path_snippet.split(\":\", 1)\n        if six.PY2:\n            path = path.encode(\"utf-8\")\n        path = six.moves.urllib.request.pathname2url(path)\n        path_snippet = \"{}:{}\".format(drive_letter, path)\n    else:\n        if six.PY2:\n            path_snippet = path_snippet.encode(\"utf-8\")\n        path_snippet = six.moves.urllib.request.pathname2url(path_snippet)\n    return path_snippet", "fpath_tuple": ["System", "fs", "fs", "_url_tools.py"], "context_start_lineno": 12, "line_no": 24, "id": "fs._url_tools.url_quote", "target_function_prompt": "def url_quote(path_snippet):\n    # type: (Text) -> Text", "function_signature": "def url_quote(path_snippet):\n    # type: (Text) -> Text"}}
{"prompt": "def system_resources() -> dict[str, t.Any]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/26", "ground_truth": "    res: dict[str, t.Any] = {}\n    for resource_kind, resource in _RESOURCE_REGISTRY.items():\n        res[resource_kind] = resource.from_system()\n    return res", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "resource.py"], "context_start_lineno": 42, "line_no": 43, "id": "bentoml._internal.resource.system_resources", "target_function_prompt": "def system_resources() -> dict[str, t.Any]:", "function_signature": "def system_resources() -> dict[str, t.Any]:"}}
{"prompt": "    def _stream_box_autocomplete(\n        self, text: str, state: Optional[int]\n    ) -> Optional[str]:", "metadata": {"task_id": "Communications/zulip-term/20", "ground_truth": "        streams_list = self.view.pinned_streams + self.view.unpinned_streams\n        streams = [stream[\"name\"] for stream in streams_list]\n\n        # match_streams takes stream names and typeaheads,\n        # but we don't have typeaheads here.\n        # FIXME: Refactor match_stream\n        stream_data = list(zip(streams, streams))\n        matched_streams = match_stream(stream_data, text, self.view.pinned_streams)\n\n        # matched_streams[0] and matched_streams[1] contains the same data.\n        return self._process_typeaheads(matched_streams[0], state, matched_streams[1])", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "boxes.py"], "context_start_lineno": 461, "line_no": 464, "id": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "target_function_prompt": "    def _stream_box_autocomplete(\n        self, text: str, state: Optional[int]\n    ) -> Optional[str]:", "function_signature": "    def _stream_box_autocomplete(\n        self, text: str, state: Optional[int]\n    ) -> Optional[str]:"}}
{"prompt": "    def install(self, model, skip_fields=None, drop=True, insert=True,\n                update=True, delete=True, create_table=True):", "metadata": {"task_id": "Software-Development/peewee/6", "ground_truth": "        ChangeLog = self.model\n        if create_table:\n            ChangeLog.create_table()\n\n        actions = list(zip((insert, update, delete), self._actions))\n        if drop:\n            for _, action in actions:\n                self.db.execute_sql(self.drop_trigger_sql(model, action))\n\n        for enabled, action in actions:\n            if enabled:\n                sql = self.trigger_sql(model, action, skip_fields)\n                self.db.execute_sql(sql)", "fpath_tuple": ["Software-Development", "peewee", "playhouse", "sqlite_changelog.py"], "context_start_lineno": 113, "line_no": 115, "id": "playhouse.sqlite_changelog.ChangeLog.install", "target_function_prompt": "    def install(self, model, skip_fields=None, drop=True, insert=True,\n                update=True, delete=True, create_table=True):", "function_signature": "    def install(self, model, skip_fields=None, drop=True, insert=True,\n                update=True, delete=True, create_table=True):"}}
{"prompt": "    def article(\n        self, title: str, ns: WikiNamespace = Namespace.MAIN, unquote: bool = False\n    ) -> \"WikipediaPage\":", "metadata": {"task_id": "Communications/Wikipedia-API/9", "ground_truth": "        return self.page(\n            title=title,\n            ns=ns,\n            unquote=unquote,\n        )", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 236, "line_no": 249, "id": "wikipediaapi.Wikipedia.article", "target_function_prompt": "    def article(\n        self, title: str, ns: WikiNamespace = Namespace.MAIN, unquote: bool = False\n    ) -> \"WikipediaPage\":", "function_signature": "    def article(\n        self, title: str, ns: WikiNamespace = Namespace.MAIN, unquote: bool = False\n    ) -> \"WikipediaPage\":"}}
{"prompt": "    def from_spec(cls, spec: t.Any) -> float:", "metadata": {"task_id": "Scientific-Engineering/bentoml/27", "ground_truth": "        if not isinstance(spec, (int, float, str)):\n            raise TypeError(\"cpu must be int, float or str\")\n\n        if isinstance(spec, (int, float)):\n            return float(spec)\n\n        milli_match = re.match(\"([0-9]+)m\", spec)\n        if milli_match:\n            return float(milli_match[1]) / 1000.0\n\n        try:\n            return float(spec)\n        except ValueError:\n            raise BentoMLConfigException(f\"Invalid CPU resource limit '{spec}'. \")", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "resource.py"], "context_start_lineno": 78, "line_no": 88, "id": "bentoml._internal.resource.CpuResource.from_spec", "target_function_prompt": "    def from_spec(cls, spec: t.Any) -> float:", "function_signature": "    def from_spec(cls, spec: t.Any) -> float:"}}
{"prompt": "    def __repr__(self) -> str:", "metadata": {"task_id": "Utilities/mmcv/2", "ground_truth": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys}'\n        repr_str += f', share_random_params = {self.share_random_params})'\n        return repr_str", "fpath_tuple": ["Utilities", "mmcv", "mmcv", "transforms", "wrappers.py"], "context_start_lineno": 530, "line_no": 531, "id": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "target_function_prompt": "    def __repr__(self) -> str:", "function_signature": "    def __repr__(self) -> str:"}}
{"prompt": "    def deserialize(cls, value, *args, **kwargs):", "metadata": {"task_id": "Text-Processing/rows/9", "ground_truth": "        value = super(EmailField, cls).deserialize(value)\n        if value is None or not value.strip():\n            return None\n\n        result = cls.EMAIL_REGEXP.findall(value)\n        if not result:\n            value_error(value, cls)\n        else:\n            return result[0]", "fpath_tuple": ["Text-Processing", "rows", "rows", "fields.py"], "context_start_lineno": 444, "line_no": 445, "id": "rows.fields.EmailField.deserialize", "target_function_prompt": "    def deserialize(cls, value, *args, **kwargs):", "function_signature": "    def deserialize(cls, value, *args, **kwargs):"}}
{"prompt": "    def deserialize(self, values):", "metadata": {"task_id": "Multimedia/Mopidy/30", "ground_truth": "        from mopidy.config import types\n        errors = {}\n        result = {}\n\n        for key, value in values.items():\n            try:\n                result[key] = self[key].deserialize(value)\n            except KeyError:  # not in our schema\n                errors[key] = \"unknown config key.\"\n                suggestion = _did_you_mean(key, self.keys())\n                if suggestion:\n                    errors[key] += f\" Did you mean {suggestion!r}?\"\n            except ValueError as e:  # deserialization failed\n                result[key] = None\n                errors[key] = str(e)\n\n        for key in self.keys():\n            if isinstance(self[key], types.Deprecated):\n                result.pop(key, None)\n            elif key not in result and key not in errors:\n                result[key] = None\n                errors[key] = \"config key not found.\"\n\n        return result, errors", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "schemas.py"], "context_start_lineno": 53, "line_no": 58, "id": "mopidy.config.schemas.ConfigSchema.deserialize", "target_function_prompt": "    def deserialize(self, values):", "function_signature": "    def deserialize(self, values):"}}
{"prompt": "    def AsDict(self):", "metadata": {"task_id": "Internet/python-twitter/1", "ground_truth": "        data = {}\n\n        for (key, value) in self.param_defaults.items():\n\n            # If the value is a list, we need to create a list to hold the\n            # dicts created by an object supporting the AsDict() method,\n            # i.e., if it inherits from TwitterModel. If the item in the list\n            # doesn't support the AsDict() method, then we assign the value\n            # directly. An example being a list of Media objects contained\n            # within a Status object.\n            if isinstance(getattr(self, key, None), (list, tuple, set)):\n                data[key] = list()\n                for subobj in getattr(self, key, None):\n                    if getattr(subobj, 'AsDict', None):\n                        data[key].append(subobj.AsDict())\n                    else:\n                        data[key].append(subobj)\n\n            # Not a list, *but still a subclass of TwitterModel* and\n            # and we can assign the data[key] directly with the AsDict()\n            # method of the object. An example being a Status object contained\n            # within a User object.\n            elif getattr(getattr(self, key, None), 'AsDict', None):\n                data[key] = getattr(self, key).AsDict()\n\n            # If the value doesn't have an AsDict() method, i.e., it's not\n            # something that subclasses TwitterModel, then we can use direct\n            # assigment.\n            elif getattr(self, key, None):\n                data[key] = getattr(self, key, None)\n        return data", "fpath_tuple": ["Internet", "python-twitter", "twitter", "models.py"], "context_start_lineno": 42, "line_no": 45, "id": "twitter.models.TwitterModel.AsDict", "target_function_prompt": "    def AsDict(self):", "function_signature": "    def AsDict(self):"}}
{"prompt": "    def get_or_create(self, request, creator=None):", "metadata": {"task_id": "Internet/pyramid/68", "ground_truth": "        result = self._store.get(request, self.NO_VALUE)\n        if result is self.NO_VALUE:\n            if creator is None:\n                creator = self._creator\n                if creator is None:\n                    raise ValueError(\n                        'no creator function has been registered with the '\n                        'cache or supplied to \"get_or_create\"'\n                    )\n            result = creator(request)\n            self.set(request, result)\n        return result", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "request.py"], "context_start_lineno": 402, "line_no": 413, "id": "pyramid.request.RequestLocalCache.get_or_create", "target_function_prompt": "    def get_or_create(self, request, creator=None):", "function_signature": "    def get_or_create(self, request, creator=None):"}}
{"prompt": "    def get_defaults_dict(self):", "metadata": {"task_id": "Utilities/boltons/66", "ground_truth": "        ret = dict(reversed(list(zip(reversed(self.args),\n                                     reversed(self.defaults or [])))))\n        kwonlydefaults = getattr(self, 'kwonlydefaults', None)\n        if kwonlydefaults:\n            ret.update(kwonlydefaults)\n        return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "funcutils.py"], "context_start_lineno": 950, "line_no": 954, "id": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "target_function_prompt": "    def get_defaults_dict(self):", "function_signature": "    def get_defaults_dict(self):"}}
{"prompt": "    def description(self, step_num=0):", "metadata": {"task_id": "System/mrjob/68", "ground_truth": "        result = dict(\n            (k, getattr(self, k))\n            for k in self._STEP_ATTRS\n            if k not in self._HIDDEN_ATTRS\n        )\n        result['type'] = self._STEP_TYPE\n\n        return result", "fpath_tuple": ["System", "mrjob", "mrjob", "step.py"], "context_start_lineno": 407, "line_no": 410, "id": "mrjob.step._Step.description", "target_function_prompt": "    def description(self, step_num=0):", "function_signature": "    def description(self, step_num=0):"}}
{"prompt": "def remap(root, visit=default_visit, enter=default_enter, exit=default_exit,\n          **kwargs):", "metadata": {"task_id": "Utilities/boltons/67", "ground_truth": "    if not callable(visit):\n        raise TypeError('visit expected callable, not: %r' % visit)\n    if not callable(enter):\n        raise TypeError('enter expected callable, not: %r' % enter)\n    if not callable(exit):\n        raise TypeError('exit expected callable, not: %r' % exit)\n    reraise_visit = kwargs.pop('reraise_visit', True)\n    if kwargs:\n        raise TypeError('unexpected keyword arguments: %r' % kwargs.keys())\n\n    path, registry, stack = (), {}, [(None, root)]\n    new_items_stack = []\n    while stack:\n        key, value = stack.pop()\n        id_value = id(value)\n        if key is _REMAP_EXIT:\n            key, new_parent, old_parent = value\n            id_value = id(old_parent)\n            path, new_items = new_items_stack.pop()\n            value = exit(path, key, old_parent, new_parent, new_items)\n            registry[id_value] = value\n            if not new_items_stack:\n                continue\n        elif id_value in registry:\n            value = registry[id_value]\n        else:\n            res = enter(path, key, value)\n            try:\n                new_parent, new_items = res\n            except TypeError:\n                # TODO: handle False?\n                raise TypeError('enter should return a tuple of (new_parent,'\n                                ' items_iterator), not: %r' % res)\n            if new_items is not False:\n                # traverse unless False is explicitly passed\n                registry[id_value] = new_parent\n                new_items_stack.append((path, []))\n                if value is not root:\n                    path += (key,)\n                stack.append((_REMAP_EXIT, (key, new_parent, value)))\n                if new_items:\n                    stack.extend(reversed(list(new_items)))\n                continue\n        if visit is _orig_default_visit:\n            # avoid function call overhead by inlining identity operation\n            visited_item = (key, value)\n        else:\n            try:\n                visited_item = visit(path, key, value)\n            except Exception:\n                if reraise_visit:\n                    raise\n                visited_item = True\n            if visited_item is False:\n                continue  # drop\n            elif visited_item is True:\n                visited_item = (key, value)\n            # TODO: typecheck?\n            #    raise TypeError('expected (key, value) from visit(),'\n            #                    ' not: %r' % visited_item)\n        try:\n            new_items_stack[-1][1].append(visited_item)\n        except IndexError:\n            raise TypeError('expected remappable root, not: %r' % root)\n    return value", "fpath_tuple": ["Utilities", "boltons", "boltons", "iterutils.py"], "context_start_lineno": 1034, "line_no": 1134, "id": "boltons.iterutils.remap", "target_function_prompt": "def remap(root, visit=default_visit, enter=default_enter, exit=default_exit,\n          **kwargs):", "function_signature": "def remap(root, visit=default_visit, enter=default_enter, exit=default_exit,\n          **kwargs):"}}
{"prompt": "    def flush(self):", "metadata": {"task_id": "Internet/boto/81", "ground_truth": "        batch_data = {\n            self.table.table_name: [\n                # We'll insert data here shortly.\n            ],\n        }\n\n        for put in self._to_put:\n            item = Item(self.table, data=put)\n            batch_data[self.table.table_name].append({\n                'PutRequest': {\n                    'Item': item.prepare_full(),\n                }\n            })\n\n        for delete in self._to_delete:\n            batch_data[self.table.table_name].append({\n                'DeleteRequest': {\n                    'Key': self.table._encode_keys(delete),\n                }\n            })\n\n        resp = self.table.connection.batch_write_item(batch_data)\n        self.handle_unprocessed(resp)\n\n        self._to_put = []\n        self._to_delete = []\n        return True", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 1662, "line_no": 1663, "id": "boto.dynamodb2.table.BatchTable.flush", "target_function_prompt": "    def flush(self):", "function_signature": "    def flush(self):"}}
{"prompt": "def check_target_api(api, arch):", "metadata": {"task_id": "Utilities/python-for-android/21", "ground_truth": "    if api >= ARMEABI_MAX_TARGET_API and arch == 'armeabi':\n        raise BuildInterruptingException(\n            UNSUPPORTED_NDK_API_FOR_ARMEABI_MESSAGE.format(\n                req_ndk_api=api, max_ndk_api=ARMEABI_MAX_TARGET_API\n            ),\n            instructions='You probably want to build with --arch=armeabi-v7a instead')\n\n    if api < MIN_TARGET_API:\n        warning('Target API {} < {}'.format(api, MIN_TARGET_API))\n        warning(OLD_API_MESSAGE)", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "recommendations.py"], "context_start_lineno": 150, "line_no": 156, "id": "pythonforandroid.recommendations.check_target_api", "target_function_prompt": "def check_target_api(api, arch):", "function_signature": "def check_target_api(api, arch):"}}
{"prompt": "def decode(hrp, addr):", "metadata": {"task_id": "Security/pycoin/26", "ground_truth": "    hrpgot, data, spec = bech32_decode(addr)\n    if hrpgot != hrp:\n        return (None, None)\n    decoded = convertbits(data[1:], 5, 8, False)\n    if decoded is None or len(decoded) < 2 or len(decoded) > 40:\n        return (None, None)\n    if data[0] > 16:\n        return (None, None)\n    if data[0] == 0 and len(decoded) != 20 and len(decoded) != 32:\n        return (None, None)\n    if data[0] == 0 and spec != Encoding.BECH32 or data[0] != 0 and spec != Encoding.BECH32M:\n        return (None, None)\n    return (data[0], decoded)", "fpath_tuple": ["Security", "pycoin", "pycoin", "contrib", "bech32m.py"], "context_start_lineno": 114, "line_no": 116, "id": "pycoin.contrib.bech32m.decode", "target_function_prompt": "def decode(hrp, addr):", "function_signature": "def decode(hrp, addr):"}}
{"prompt": "    def darwin_helper(self):", "metadata": {"task_id": "Utilities/python-for-android/22", "ground_truth": "        info(\n            \"Installer for homebrew is not yet supported on macOS,\"\n            \"the nice news is that the installation process is easy!\"\n            \"See: https://brew.sh for further instructions.\"\n        )", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 141, "line_no": 142, "id": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "target_function_prompt": "    def darwin_helper(self):", "function_signature": "    def darwin_helper(self):"}}
{"prompt": "    def get_csrf_token(self, request):", "metadata": {"task_id": "Internet/pyramid/69", "ground_truth": "        token = request.session.get(self.key, None)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "csrf.py"], "context_start_lineno": 77, "line_no": 80, "id": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "target_function_prompt": "    def get_csrf_token(self, request):", "function_signature": "    def get_csrf_token(self, request):"}}
{"prompt": "def load_from_config(config):\n    # If this is called, it means that a ``statsd_url`` was specified in settings.\n    # (see ``kinto.core.initialization``)\n    # Raise a proper error if the ``statsd`` module is not installed.", "metadata": {"task_id": "Internet/kinto/29", "ground_truth": "    if statsd_module is None:\n        error_msg = \"Please install Kinto with monitoring dependencies (e.g. statsd package)\"\n        raise ConfigurationError(error_msg)\n\n    settings = config.get_settings()\n    uri = settings[\"statsd_url\"]\n    uri = urlparse(uri)\n\n    if settings[\"project_name\"] != \"\":\n        prefix = settings[\"project_name\"]\n    else:\n        prefix = settings[\"statsd_prefix\"]\n\n    return Client(uri.hostname, uri.port, prefix)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "statsd.py"], "context_start_lineno": 44, "line_no": 48, "id": "kinto.core.statsd.load_from_config", "target_function_prompt": "def load_from_config(config):\n    # If this is called, it means that a ``statsd_url`` was specified in settings.\n    # (see ``kinto.core.initialization``)\n    # Raise a proper error if the ``statsd`` module is not installed.", "function_signature": "def load_from_config(config):\n    # If this is called, it means that a ``statsd_url`` was specified in settings.\n    # (see ``kinto.core.initialization``)\n    # Raise a proper error if the ``statsd`` module is not installed."}}
{"prompt": "def _statement_matches_resource(statement: dict, resource: str, condition_keys: Optional[CaseInsensitiveDict] = None) -> bool:", "metadata": {"task_id": "Security/principalmapper/1", "ground_truth": "    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n        return False\n    elif 'NotResource' in statement:\n        result = True\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                result = False\n                break\n        return result\n    else:\n        return True", "fpath_tuple": ["Security", "principalmapper", "principalmapper", "querying", "local_policy_simulation.py"], "context_start_lineno": 872, "line_no": 874, "id": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "target_function_prompt": "def _statement_matches_resource(statement: dict, resource: str, condition_keys: Optional[CaseInsensitiveDict] = None) -> bool:", "function_signature": "def _statement_matches_resource(statement: dict, resource: str, condition_keys: Optional[CaseInsensitiveDict] = None) -> bool:"}}
{"prompt": "def compute_likelihood_window(\n    window: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    use_start_token: bool,\n    use_end_token: bool,\n    start_token: str = None,\n    end_token: str = None,\n) -> float:", "metadata": {"task_id": "Security/msticpy/10", "ground_truth": "    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_end_token is True\"\n            )\n\n    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_token is True\"\n            )\n\n    w_len = len(window)\n    if w_len == 0:\n        return np.nan\n    prob: float = 1\n\n    cur_cmd = window[0].name\n    params = window[0].params\n    param_cond_prob = compute_prob_setofparams_given_cmd(\n        cmd=cur_cmd,\n        params=params,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        use_geo_mean=True,\n    )\n\n    if use_start_token:\n        prob *= trans_probs[start_token][cur_cmd] * param_cond_prob\n    else:\n        prob *= prior_probs[cur_cmd] * param_cond_prob\n\n    for i in range(1, w_len):\n        prev, cur = window[i - 1], window[i]\n        prev_cmd, cur_cmd = prev.name, cur.name\n        cur_par = cur.params\n        prob *= trans_probs[prev_cmd][cur_cmd]\n        param_cond_prob = compute_prob_setofparams_given_cmd(\n            cmd=cur_cmd,\n            params=cur_par,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_geo_mean=True,\n        )\n        prob *= param_cond_prob\n\n    if use_end_token:\n        prob *= trans_probs[cur_cmd][end_token]\n\n    return prob", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "cmds_params_only.py"], "context_start_lineno": 223, "line_no": 267, "id": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "target_function_prompt": "def compute_likelihood_window(\n    window: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    use_start_token: bool,\n    use_end_token: bool,\n    start_token: str = None,\n    end_token: str = None,\n) -> float:", "function_signature": "def compute_likelihood_window(\n    window: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    use_start_token: bool,\n    use_end_token: bool,\n    start_token: str = None,\n    end_token: str = None,\n) -> float:"}}
{"prompt": "    def seek(self, pos, mode=0):", "metadata": {"task_id": "Utilities/boltons/68", "ground_truth": "        self._checkClosed()\n        # Seek to position from the start of the file\n        if mode == os.SEEK_SET:\n            self.buffer.seek(0)\n            self._traverse_codepoints(0, pos)\n            self._tell = pos\n        # Seek to new position relative to current position\n        elif mode == os.SEEK_CUR:\n            start_pos = self.tell()\n            self._traverse_codepoints(self.tell(), pos)\n            self._tell = start_pos + pos\n        elif mode == os.SEEK_END:\n            self.buffer.seek(0)\n            dest_position = self.len - pos\n            self._traverse_codepoints(0, dest_position)\n            self._tell = dest_position\n        else:\n            raise ValueError(\n                \"Invalid whence ({0}, should be 0, 1, or 2)\".format(mode)\n            )\n        return self.tell()", "fpath_tuple": ["Utilities", "boltons", "boltons", "ioutils.py"], "context_start_lineno": 449, "line_no": 451, "id": "boltons.ioutils.SpooledStringIO.seek", "target_function_prompt": "    def seek(self, pos, mode=0):", "function_signature": "    def seek(self, pos, mode=0):"}}
{"prompt": "def format_initial(extensions_data):", "metadata": {"task_id": "Multimedia/Mopidy/31", "ground_truth": "    from mopidy.internal import versioning\n    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    defaults.extend(d.extension.get_default_config() for d in extensions_data)\n    raw_config = _load([], defaults, [])\n\n    schemas = _schemas[:]\n    schemas.extend(d.extension.get_config_schema() for d in extensions_data)\n\n    config, errors = _validate(raw_config, schemas)\n\n    versions = [f\"Mopidy {versioning.get_version()}\"]\n    extensions_data = sorted(\n        extensions_data, key=lambda d: d.extension.dist_name\n    )\n    for data in extensions_data:\n        versions.append(f\"{data.extension.dist_name} {data.extension.version}\")\n\n    header = _INITIAL_HELP.strip().format(versions=\"\\n#   \".join(versions))\n    formatted_config = _format(\n        config=config, comments={}, schemas=schemas, display=False, disable=True\n    )\n    return header + \"\\n\\n\" + formatted_config", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "__init__.py"], "context_start_lineno": 124, "line_no": 125, "id": "mopidy.config.format_initial", "target_function_prompt": "def format_initial(extensions_data):", "function_signature": "def format_initial(extensions_data):"}}
{"prompt": "    def get_distrust_timeline(\n        cls, verified_certificate_chain: List[Certificate]\n    ) -> Optional[SymantecDistrustTimelineEnum]:", "metadata": {"task_id": "System/sslyze/7", "ground_truth": "        from sslyze.plugins.certificate_info._certificate_utils import get_public_key_sha256\n        has_whitelisted_cert = False\n        has_blacklisted_cert = False\n\n        # Is there a Symantec root certificate in the chain?\n        for certificate in verified_certificate_chain:\n            key_hash = binascii.hexlify(get_public_key_sha256(certificate)).decode(\"ascii\")\n            if key_hash in cls._CA_KEYS_BLACKLIST:\n                has_blacklisted_cert = True\n            if key_hash in cls._CA_KEYS_WHITELIST:\n                has_whitelisted_cert = True\n\n        distrust_enum = None\n        if has_blacklisted_cert and not has_whitelisted_cert:\n            leaf_cert = verified_certificate_chain[0]\n            if leaf_cert.not_valid_before < datetime(year=2016, month=6, day=1):\n                distrust_enum = SymantecDistrustTimelineEnum.MARCH_2018\n            else:\n                distrust_enum = SymantecDistrustTimelineEnum.SEPTEMBER_2018\n        return distrust_enum", "fpath_tuple": ["System", "sslyze", "sslyze", "plugins", "certificate_info", "_symantec.py"], "context_start_lineno": 101, "line_no": 104, "id": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "target_function_prompt": "    def get_distrust_timeline(\n        cls, verified_certificate_chain: List[Certificate]\n    ) -> Optional[SymantecDistrustTimelineEnum]:", "function_signature": "    def get_distrust_timeline(\n        cls, verified_certificate_chain: List[Certificate]\n    ) -> Optional[SymantecDistrustTimelineEnum]:"}}
{"prompt": "    def get_header(self, name, default=None):", "metadata": {"task_id": "Internet/falcon/33", "ground_truth": "        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('Getting Set-Cookie is not currently supported.')\n\n        return self._headers.get(name, default)", "fpath_tuple": ["Internet", "falcon", "falcon", "response.py"], "context_start_lineno": 582, "line_no": 607, "id": "falcon.response.Response.get_header", "target_function_prompt": "    def get_header(self, name, default=None):", "function_signature": "    def get_header(self, name, default=None):"}}
{"prompt": "def connect(url, unquote_password=False, **connect_params):", "metadata": {"task_id": "Software-Development/peewee/7", "ground_truth": "    parsed = urlparse(url)\n    connect_kwargs = parseresult_to_dict(parsed, unquote_password)\n    connect_kwargs.update(connect_params)\n    database_class = schemes.get(parsed.scheme)\n\n    if database_class is None:\n        if database_class in schemes:\n            raise RuntimeError('Attempted to use \"%s\" but a required library '\n                               'could not be imported.' % parsed.scheme)\n        else:\n            raise RuntimeError('Unrecognized or unsupported scheme: \"%s\".' %\n                               parsed.scheme)\n\n    return database_class(**connect_kwargs)", "fpath_tuple": ["Software-Development", "peewee", "playhouse", "db_url.py"], "context_start_lineno": 90, "line_no": 91, "id": "playhouse.db_url.connect", "target_function_prompt": "def connect(url, unquote_password=False, **connect_params):", "function_signature": "def connect(url, unquote_password=False, **connect_params):"}}
{"prompt": "    def reverse(self) -> AddColumnOp:", "metadata": {"task_id": "Database/alembic/33", "ground_truth": "        if self._reverse is None:\n            raise ValueError(\n                \"operation is not reversible; \"\n                \"original column is not present\"\n            )\n\n        return AddColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self._reverse.column\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 2189, "line_no": 2190, "id": "alembic.operations.ops.DropColumnOp.reverse", "target_function_prompt": "    def reverse(self) -> AddColumnOp:", "function_signature": "    def reverse(self) -> AddColumnOp:"}}
{"prompt": "def date_guesses(match):", "metadata": {"task_id": "Security/zxcvbn-python/13", "ground_truth": "    year_space = max(abs(match['year'] - REFERENCE_YEAR), MIN_YEAR_SPACE)\n    guesses = year_space * 365\n    if match.get('separator', False):\n        guesses *= 4\n\n    return guesses", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "scoring.py"], "context_start_lineno": 317, "line_no": 318, "id": "zxcvbn.scoring.date_guesses", "target_function_prompt": "def date_guesses(match):", "function_signature": "def date_guesses(match):"}}
{"prompt": "def research(root, query=lambda p, k, v: True, reraise=False):", "metadata": {"task_id": "Utilities/boltons/69", "ground_truth": "    ret = []\n\n    if not callable(query):\n        raise TypeError('query expected callable, not: %r' % query)\n\n    def enter(path, key, value):\n        try:\n            if query(path, key, value):\n                ret.append((path + (key,), value))\n        except Exception:\n            if reraise:\n                raise\n        return default_enter(path, key, value)\n\n    remap(root, enter=enter)\n    return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "iterutils.py"], "context_start_lineno": 1280, "line_no": 1322, "id": "boltons.iterutils.research", "target_function_prompt": "def research(root, query=lambda p, k, v: True, reraise=False):", "function_signature": "def research(root, query=lambda p, k, v: True, reraise=False):"}}
{"prompt": "    def freeze(self, query, format='csv', filename=None, file_obj=None,\n               encoding='utf8', **kwargs):", "metadata": {"task_id": "Software-Development/peewee/8", "ground_truth": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'w', encoding)\n\n        exporter = self._export_formats[format](query)\n        exporter.export(file_obj, **kwargs)\n\n        if filename:\n            file_obj.close()", "fpath_tuple": ["Software-Development", "peewee", "playhouse", "dataset.py"], "context_start_lineno": 161, "line_no": 163, "id": "playhouse.dataset.DataSet.freeze", "target_function_prompt": "    def freeze(self, query, format='csv', filename=None, file_obj=None,\n               encoding='utf8', **kwargs):", "function_signature": "    def freeze(self, query, format='csv', filename=None, file_obj=None,\n               encoding='utf8', **kwargs):"}}
{"prompt": "def _convert_value(value):", "metadata": {"task_id": "Utilities/sacred/33", "ground_truth": "    from sacred.settings import SETTINGS\n    from sacred.serializer import restore\n    try:\n        return restore(ast.literal_eval(value))\n    except (ValueError, SyntaxError):\n        if SETTINGS.COMMAND_LINE.STRICT_PARSING:\n            raise\n        # use as string if nothing else worked\n        return value", "fpath_tuple": ["Utilities", "sacred", "sacred", "arg_parser.py"], "context_start_lineno": 205, "line_no": 207, "id": "sacred.arg_parser._convert_value", "target_function_prompt": "def _convert_value(value):", "function_signature": "def _convert_value(value):"}}
{"prompt": "    def create_concrete(self):", "metadata": {"task_id": "Communications/chatette/16", "ground_truth": "        from chatette.units.modifiable.choice import Choice\n        self._check_information()\n        return Choice(\n            self.leading_space, self._build_modifiers_repr(),\n            self.rules\n        )", "fpath_tuple": ["Communications", "chatette", "chatette", "parsing", "__init__.py"], "context_start_lineno": 79, "line_no": 80, "id": "chatette.parsing.ChoiceBuilder.create_concrete", "target_function_prompt": "    def create_concrete(self):", "function_signature": "    def create_concrete(self):"}}
{"prompt": "    def make_wsgi_app(self):", "metadata": {"task_id": "Internet/pyramid/70", "ground_truth": "        from pyramid.events import ApplicationCreated\n        from pyramid.router import Router\n        self.commit()\n        app = Router(self.registry)\n\n        # Allow tools like \"pshell development.ini\" to find the 'last'\n        # registry configured.\n        global_registries.add(self.registry)\n\n        # Push the registry onto the stack in case any code that depends on\n        # the registry threadlocal APIs used in listeners subscribed to the\n        # IApplicationCreated event.\n        self.begin()\n        try:\n            self.registry.notify(ApplicationCreated(app))\n        finally:\n            self.end()\n\n        return app", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "__init__.py"], "context_start_lineno": 880, "line_no": 887, "id": "pyramid.config.Configurator.make_wsgi_app", "target_function_prompt": "    def make_wsgi_app(self):", "function_signature": "    def make_wsgi_app(self):"}}
{"prompt": "def combine_cmds(*cmds):", "metadata": {"task_id": "System/mrjob/69", "ground_truth": "    cmd = combine_values(*cmds)\n\n    if cmd is None:\n        return None\n    elif isinstance(cmd, string_types):\n        return shlex_split(cmd)\n    else:\n        return list(cmd)", "fpath_tuple": ["System", "mrjob", "mrjob", "conf.py"], "context_start_lineno": 415, "line_no": 424, "id": "mrjob.conf.combine_cmds", "target_function_prompt": "def combine_cmds(*cmds):", "function_signature": "def combine_cmds(*cmds):"}}
{"prompt": "def _add_query_parameter(url, name, value):", "metadata": {"task_id": "Internet/google-api-python-client/7", "ground_truth": "    if value is None:\n        return url\n    else:\n        return update_query_params(url, {name: value})", "fpath_tuple": ["Internet", "google-api-python-client", "googleapiclient", "_helpers.py"], "context_start_lineno": 190, "line_no": 203, "id": "googleapiclient._helpers._add_query_parameter", "target_function_prompt": "def _add_query_parameter(url, name, value):", "function_signature": "def _add_query_parameter(url, name, value):"}}
{"prompt": "    def parse_output(self, chunks):", "metadata": {"task_id": "System/mrjob/70", "ground_truth": "        read = self.output_protocol().read\n\n        for line in to_lines(chunks):\n            yield read(line)", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 1298, "line_no": 1302, "id": "mrjob.job.MRJob.parse_output", "target_function_prompt": "    def parse_output(self, chunks):", "function_signature": "    def parse_output(self, chunks):"}}
{"prompt": "def add_pygments_style(theme_meta: Dict[str, Any], urwid_theme: ThemeSpec) -> None:", "metadata": {"task_id": "Communications/zulip-term/21", "ground_truth": "    from zulipterminal.config.color import term16\n    pygments = theme_meta[\"pygments\"]\n    pygments_styles = pygments[\"styles\"]\n    pygments_bg = pygments[\"background\"]\n    pygments_overrides = pygments[\"overrides\"]\n\n    term16_styles = term16.styles\n    term16_bg = term16.background_color\n\n    for token, css_class in STANDARD_TYPES.items():\n        if css_class in pygments_overrides:\n            pygments_styles[token] = pygments_overrides[css_class]\n\n        # Inherit parent pygments style if not defined.\n        # Eg: Use `String` if `String.Double` is not present.\n        if pygments_styles[token] == \"\":\n            try:\n                t = [k for k, v in STANDARD_TYPES.items() if v == css_class[0]]\n                pygments_styles[token] = pygments_styles[t[0]]\n            except IndexError:\n                pass\n\n        if term16_styles[token] == \"\":\n            try:\n                t = [k for k, v in STANDARD_TYPES.items() if v == css_class[0]]\n                term16_styles[token] = term16_styles[t[0]]\n            except IndexError:\n                pass\n\n        new_style = (\n            f\"pygments:{css_class}\",\n            term16_styles[token],\n            term16_bg,\n            \"bold\",  # Mono style\n            pygments_styles[token],\n            pygments_bg,\n        )\n        urwid_theme.append(new_style)", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "config", "themes.py"], "context_start_lineno": 221, "line_no": 236, "id": "zulipterminal.config.themes.add_pygments_style", "target_function_prompt": "def add_pygments_style(theme_meta: Dict[str, Any], urwid_theme: ThemeSpec) -> None:", "function_signature": "def add_pygments_style(theme_meta: Dict[str, Any], urwid_theme: ThemeSpec) -> None:"}}
{"prompt": "    def render(self) -> str:", "metadata": {"task_id": "Software-Development/pandas-profiling/8", "ground_truth": "        from ydata_profiling.report.presentation.flavours.html import templates\n        if isinstance(self.content[\"rows\"][0], list):\n            html = \"\"\n\n            kwargs = self.content.copy()\n            del kwargs[\"rows\"]\n            for idx, rows in enumerate(self.content[\"rows\"]):\n                html += templates.template(\"frequency_table.html\").render(\n                    rows=rows, idx=idx, **kwargs\n                )\n            return html\n        else:\n            return templates.template(\"frequency_table.html\").render(\n                **self.content, idx=0\n            )", "fpath_tuple": ["Software-Development", "pandas-profiling", "src", "ydata_profiling", "report", "presentation", "flavours", "html", "frequency_table.py"], "context_start_lineno": 5, "line_no": 6, "id": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "target_function_prompt": "    def render(self) -> str:", "function_signature": "    def render(self) -> str:"}}
{"prompt": "def known_iam_actions(prefix):", "metadata": {"task_id": "Security/trailscraper/7", "ground_truth": "    knowledge = pipe(all_known_iam_permissions(),\n                     mapz(_parse_action),\n                     groupbyz(lambda x: x.prefix))\n\n    return knowledge.get(prefix, [])", "fpath_tuple": ["Security", "trailscraper", "trailscraper", "iam.py"], "context_start_lineno": 178, "line_no": 181, "id": "trailscraper.iam.known_iam_actions", "target_function_prompt": "def known_iam_actions(prefix):", "function_signature": "def known_iam_actions(prefix):"}}
{"prompt": "def der_decode(data: bytes) -> object:", "metadata": {"task_id": "Security/asyncssh/9", "ground_truth": "    value, end = der_decode_partial(data)\n\n    if end < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n\n    return value", "fpath_tuple": ["Security", "asyncssh", "asyncssh", "asn1.py"], "context_start_lineno": 751, "line_no": 780, "id": "asyncssh.asn1.der_decode", "target_function_prompt": "def der_decode(data: bytes) -> object:", "function_signature": "def der_decode(data: bytes) -> object:"}}
{"prompt": "def build_fingerprint(path, version, hash_value):", "metadata": {"task_id": "Software-Development/dash/13", "ground_truth": "    path_parts = path.split(\"/\")\n    filename, extension = path_parts[-1].split(\".\", 1)\n    file_path = \"/\".join(path_parts[:-1] + [filename])\n    v_str = re.sub(version_clean, \"_\", str(version))\n\n    return f\"{file_path}.v{v_str}m{hash_value}.{extension}\"", "fpath_tuple": ["Software-Development", "dash", "dash", "fingerprint.py"], "context_start_lineno": 6, "line_no": 7, "id": "dash.fingerprint.build_fingerprint", "target_function_prompt": "def build_fingerprint(path, version, hash_value):", "function_signature": "def build_fingerprint(path, version, hash_value):"}}
{"prompt": "def bech32_encode(hrp, data, spec):", "metadata": {"task_id": "Security/pycoin/27", "ground_truth": "    combined = data + bech32_create_checksum(hrp, data, spec)\n    return hrp + '1' + ''.join([CHARSET[d] for d in combined])", "fpath_tuple": ["Security", "pycoin", "pycoin", "contrib", "bech32m.py"], "context_start_lineno": 68, "line_no": 70, "id": "pycoin.contrib.bech32m.bech32_encode", "target_function_prompt": "def bech32_encode(hrp, data, spec):", "function_signature": "def bech32_encode(hrp, data, spec):"}}
{"prompt": "    def __str__(self):", "metadata": {"task_id": "Internet/pyramid/71", "ground_truth": "        srclines = self.src.split('\\n')\n        src = '\\n'.join('    %s' % x for x in srclines)\n        return 'Line %s of file %s:\\n%s' % (self.line, self.file, src)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "actions.py"], "context_start_lineno": 536, "line_no": 537, "id": "pyramid.config.actions.ActionInfo.__str__", "target_function_prompt": "    def __str__(self):", "function_signature": "    def __str__(self):"}}
{"prompt": "def expand_dependencies(recipes, ctx):", "metadata": {"task_id": "Utilities/python-for-android/23", "ground_truth": "    recipes_with_deps = list(recipes)\n    for entry in recipes:\n        if not isinstance(entry, (tuple, list)) or len(entry) == 1:\n            if isinstance(entry, (tuple, list)):\n                entry = entry[0]\n            try:\n                recipe = Recipe.get_recipe(entry, ctx)\n                recipes_with_deps += recipe.depends\n            except ValueError:\n                # it's a pure python package without a recipe, so we\n                # don't know the dependencies...skipping for now\n                pass\n\n    # Split up lists by available alternatives:\n    recipe_lists = [[]]\n    for recipe in recipes_with_deps:\n        if isinstance(recipe, (tuple, list)):\n            new_recipe_lists = []\n            for alternative in recipe:\n                for old_list in recipe_lists:\n                    new_list = [i for i in old_list]\n                    new_list.append(alternative)\n                    new_recipe_lists.append(new_list)\n            recipe_lists = new_recipe_lists\n        else:\n            for existing_list in recipe_lists:\n                existing_list.append(recipe)\n    return recipe_lists", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "bootstrap.py"], "context_start_lineno": 402, "line_no": 411, "id": "pythonforandroid.bootstrap.expand_dependencies", "target_function_prompt": "def expand_dependencies(recipes, ctx):", "function_signature": "def expand_dependencies(recipes, ctx):"}}
{"prompt": "    def read(self, size=-1):", "metadata": {"task_id": "Internet/falcon/34", "ground_truth": "        if size < 0:\n            size = self.remaining\n        else:\n            size = min(size, self.remaining)\n        data = self.fh.read(size)\n        self.remaining -= len(data)\n        return data", "fpath_tuple": ["Internet", "falcon", "falcon", "routing", "static.py"], "context_start_lineno": 76, "line_no": 78, "id": "falcon.routing.static._BoundedFile.read", "target_function_prompt": "    def read(self, size=-1):", "function_signature": "    def read(self, size=-1):"}}
{"prompt": "def iso8601_datetime(d):", "metadata": {"task_id": "Communications/twilio-fatisar/19", "ground_truth": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime) or isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, str):\n        return d", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "base", "serialize.py"], "context_start_lineno": 21, "line_no": 26, "id": "twilio.base.serialize.iso8601_datetime", "target_function_prompt": "def iso8601_datetime(d):", "function_signature": "def iso8601_datetime(d):"}}
{"prompt": "    def parse_server_string(cls, server_str: str) -> Tuple[str, Optional[str], Optional[int]]:\n        # Extract ip from target", "metadata": {"task_id": "System/sslyze/8", "ground_truth": "        ip = None\n        if \"{\" in server_str and \"}\" in server_str:\n            raw_target = server_str.split(\"{\")\n            raw_ip = raw_target[1]\n\n            ip = raw_ip.replace(\"}\", \"\")\n\n            # Clean the target\n            server_str = raw_target[0]\n\n        # Look for ipv6 hint in target\n        if \"[\" in server_str:\n            (host, port) = cls._parse_ipv6_server_string(server_str)\n        else:\n            # Look for ipv6 hint in the ip\n            if ip is not None and \"[\" in ip:\n                (ip, port) = cls._parse_ipv6_server_string(ip)\n\n            # Fallback to ipv4\n            (host, port) = cls._parse_ipv4_server_string(server_str)\n\n        return host, ip, port", "fpath_tuple": ["System", "sslyze", "sslyze", "cli", "server_string_parser.py"], "context_start_lineno": 21, "line_no": 23, "id": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "target_function_prompt": "    def parse_server_string(cls, server_str: str) -> Tuple[str, Optional[str], Optional[int]]:\n        # Extract ip from target", "function_signature": "    def parse_server_string(cls, server_str: str) -> Tuple[str, Optional[str], Optional[int]]:\n        # Extract ip from target"}}
{"prompt": "def get_cached_validation_key(username, registry):", "metadata": {"task_id": "Internet/kinto/30", "ground_truth": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n    cache = registry.cache\n    activation_key = cache.get(cache_key)\n    return activation_key", "fpath_tuple": ["Internet", "kinto", "kinto", "plugins", "accounts", "utils.py"], "context_start_lineno": 92, "line_no": 94, "id": "kinto.plugins.accounts.utils.get_cached_validation_key", "target_function_prompt": "def get_cached_validation_key(username, registry):", "function_signature": "def get_cached_validation_key(username, registry):"}}
{"prompt": "    def construct_arguments(self, args, kwargs, options, bound=False):", "metadata": {"task_id": "Utilities/sacred/34", "ground_truth": "        expected_args = self._get_expected_args(bound)\n        self._assert_no_unexpected_args(expected_args, args)\n        self._assert_no_unexpected_kwargs(expected_args, kwargs)\n        self._assert_no_duplicate_args(expected_args, args, kwargs)\n\n        args, kwargs = self._fill_in_options(args, kwargs, options, bound)\n\n        self._assert_no_missing_args(args, kwargs, bound)\n        return args, kwargs", "fpath_tuple": ["Utilities", "sacred", "sacred", "config", "signature.py"], "context_start_lineno": 69, "line_no": 82, "id": "sacred.config.signature.Signature.construct_arguments", "target_function_prompt": "    def construct_arguments(self, args, kwargs, options, bound=False):", "function_signature": "    def construct_arguments(self, args, kwargs, options, bound=False):"}}
{"prompt": "    def schema(self):\n        # Pick up the includes.", "metadata": {"task_id": "Internet/boto/82", "ground_truth": "        schema_data = IncludeIndex.schema(self)\n        # Also the throughput.\n        schema_data.update(GlobalBaseIndexField.schema(self))\n        return schema_data", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "fields.py"], "context_start_lineno": 331, "line_no": 333, "id": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "target_function_prompt": "    def schema(self):\n        # Pick up the includes.", "function_signature": "    def schema(self):\n        # Pick up the includes."}}
{"prompt": "    def set(self, name, value):", "metadata": {"task_id": "Utilities/gunicorn/18", "ground_truth": "        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "config.py"], "context_start_lineno": 73, "line_no": 74, "id": "gunicorn.config.Config.set", "target_function_prompt": "    def set(self, name, value):", "function_signature": "    def set(self, name, value):"}}
{"prompt": "    def revelation(self):", "metadata": {"task_id": "Utilities/sacred/35", "ground_truth": "        missing = set()\n        for key in self.fixed:\n            if not dict.__contains__(self, key):\n                self[key] = self.fixed[key]\n                missing.add(key)\n\n            if isinstance(self[key], (DogmaticDict, DogmaticList)):\n                missing |= {key + \".\" + k for k in self[key].revelation()}\n        return missing", "fpath_tuple": ["Utilities", "sacred", "sacred", "config", "custom_containers.py"], "context_start_lineno": 107, "line_no": 108, "id": "sacred.config.custom_containers.DogmaticDict.revelation", "target_function_prompt": "    def revelation(self):", "function_signature": "    def revelation(self):"}}
{"prompt": "def check_connection(conn):", "metadata": {"task_id": "Database/datasette/33", "ground_truth": "    tables = [\n        r[0]\n        for r in conn.execute(\n            \"select name from sqlite_master where type='table'\"\n        ).fetchall()\n    ]\n    for table in tables:\n        try:\n            conn.execute(\n                f\"PRAGMA table_info({escape_sqlite(table)});\",\n            )\n        except sqlite3.OperationalError as e:\n            if e.args[0] == \"no such module: VirtualSpatialIndex\":\n                raise SpatialiteConnectionProblem(e)\n            else:\n                raise ConnectionProblem(e)", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 952, "line_no": 953, "id": "datasette.utils.check_connection", "target_function_prompt": "def check_connection(conn):", "function_signature": "def check_connection(conn):"}}
{"prompt": "    def langlinks(self) -> PagesDict:", "metadata": {"task_id": "Communications/Wikipedia-API/10", "ground_truth": "        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 982, "line_no": 993, "id": "wikipediaapi.WikipediaPage.langlinks", "target_function_prompt": "    def langlinks(self) -> PagesDict:", "function_signature": "    def langlinks(self) -> PagesDict:"}}
{"prompt": "    def remove(self, key, val):", "metadata": {"task_id": "Utilities/boltons/70", "ground_truth": "        self.data[key].remove(val)\n        if not self.data[key]:\n            del self.data[key]\n        self.inv.data[val].remove(key)\n        if not self.inv.data[val]:\n            del self.inv.data[val]", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 974, "line_no": 975, "id": "boltons.dictutils.ManyToMany.remove", "target_function_prompt": "    def remove(self, key, val):", "function_signature": "    def remove(self, key, val):"}}
{"prompt": "def extract_tables(sql):", "metadata": {"task_id": "Database/litecli/2", "ground_truth": "    parsed = sqlparse.parse(sql)\n    if not parsed:\n        return []\n\n    # INSERT statements must stop looking for tables at the sign of first\n    # Punctuation. eg: INSERT INTO abc (col1, col2) VALUES (1, 2)\n    # abc is the table name, but if we don't stop at the first lparen, then\n    # we'll identify abc, col1 and col2 as table names.\n    insert_stmt = parsed[0].token_first().value.lower() == \"insert\"\n    stream = extract_from_part(parsed[0], stop_at_punctuation=insert_stmt)\n    return list(extract_table_identifiers(stream))", "fpath_tuple": ["Database", "litecli", "litecli", "packages", "parseutils.py"], "context_start_lineno": 148, "line_no": 154, "id": "litecli.packages.parseutils.extract_tables", "target_function_prompt": "def extract_tables(sql):", "function_signature": "def extract_tables(sql):"}}
{"prompt": "    async def get(self, path, **kwargs):", "metadata": {"task_id": "Database/datasette/34", "ground_truth": "        async with httpx.AsyncClient(app=self.app) as client:\n            return await client.get(self._fix(path), **kwargs)", "fpath_tuple": ["Database", "datasette", "datasette", "app.py"], "context_start_lineno": 1555, "line_no": 1556, "id": "datasette.app.DatasetteClient.get", "target_function_prompt": "    async def get(self, path, **kwargs):", "function_signature": "    async def get(self, path, **kwargs):"}}
{"prompt": "def _to_paradigm(lexeme, paradigm_prefixes):", "metadata": {"task_id": "Text-Processing/pymorphy2/1", "ground_truth": "    from pymorphy2.utils import longest_common_substring\n    forms, tags = list(zip(*lexeme))\n\n    if len(forms) == 1:\n        stem = forms[0]\n        prefixes = ['']\n    else:\n        stem = longest_common_substring(forms)\n        prefixes = [form[:form.index(stem)] for form in forms]\n\n        # only allow prefixes from PARADIGM_PREFIXES\n        if any(pref not in paradigm_prefixes for pref in prefixes):\n            # With right PARADIGM_PREFIXES empty stem is fine;\n            # os.path.commonprefix doesn't return anything useful\n            # for prediction.\n            # stem = os.path.commonprefix(forms)\n            stem = \"\"\n            prefixes = [''] * len(tags)\n\n    suffixes = (\n        form[len(pref)+len(stem):]\n        for form, pref in zip(forms, prefixes)\n    )\n    return stem, tuple(zip(suffixes, tags, prefixes))", "fpath_tuple": ["Text-Processing", "pymorphy2", "pymorphy2", "opencorpora_dict", "compile.py"], "context_start_lineno": 240, "line_no": 246, "id": "pymorphy2.opencorpora_dict.compile._to_paradigm", "target_function_prompt": "def _to_paradigm(lexeme, paradigm_prefixes):", "function_signature": "def _to_paradigm(lexeme, paradigm_prefixes):"}}
{"prompt": "    def pop(self):", "metadata": {"task_id": "Internet/pyramid/72", "ground_truth": "        if self.stack:\n            return self.stack.pop()", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "threadlocal.py"], "context_start_lineno": 20, "line_no": 21, "id": "pyramid.threadlocal.ThreadLocalManager.pop", "target_function_prompt": "    def pop(self):", "function_signature": "    def pop(self):"}}
{"prompt": "def suggest_column_types(records):", "metadata": {"task_id": "Database/sqlite-utils/5", "ground_truth": "    all_column_types = {}\n    for record in records:\n        for key, value in record.items():\n            all_column_types.setdefault(key, set()).add(type(value))\n    return types_for_column_types(all_column_types)", "fpath_tuple": ["Database", "sqlite-utils", "sqlite_utils", "utils.py"], "context_start_lineno": 87, "line_no": 88, "id": "sqlite_utils.utils.suggest_column_types", "target_function_prompt": "def suggest_column_types(records):", "function_signature": "def suggest_column_types(records):"}}
{"prompt": "def get_package_version(name):", "metadata": {"task_id": "Utilities/sacred/36", "ground_truth": "    version_string = importlib.import_module(name).__version__\n    return parse_version(version_string)", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 697, "line_no": 699, "id": "sacred.utils.get_package_version", "target_function_prompt": "def get_package_version(name):", "function_signature": "def get_package_version(name):"}}
{"prompt": "    def merge(self, translations):", "metadata": {"task_id": "Internet/pyramid/73", "ground_truth": "        if isinstance(translations, gettext.GNUTranslations):\n            self._catalog.update(translations._catalog)\n            if isinstance(translations, Translations):\n                self.files.extend(translations.files)\n\n        return self", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "i18n.py"], "context_start_lineno": 308, "line_no": 320, "id": "pyramid.i18n.Translations.merge", "target_function_prompt": "    def merge(self, translations):", "function_signature": "    def merge(self, translations):"}}
{"prompt": "def get_type(filename):", "metadata": {"task_id": "Security/oletools/2", "ground_truth": "    parser = XmlParser(filename)\n    if parser.is_single_xml():\n        match = None\n        with uopen(filename, 'r') as handle:\n            match = re.search(OFFICE_XML_PROGID_REGEX, handle.read(1024))\n        if not match:\n            return DOCTYPE_NONE\n        prog_id = match.groups()[0]\n        if prog_id == WORD_XML_PROG_ID:\n            return DOCTYPE_WORD_XML\n        if prog_id == EXCEL_XML_PROG_ID:\n            return DOCTYPE_EXCEL_XML\n        return DOCTYPE_NONE\n\n    is_doc = False\n    is_xls = False\n    is_ppt = False\n    try:\n        for _, elem, _ in parser.iter_xml(FILE_CONTENT_TYPES):\n            logger.debug(u'  ' + debug_str(elem))\n            try:\n                content_type = elem.attrib['ContentType']\n            except KeyError:         # ContentType not an attr\n                continue\n            is_xls |= content_type.startswith(CONTENT_TYPES_EXCEL)\n            is_doc |= content_type.startswith(CONTENT_TYPES_WORD)\n            is_ppt |= content_type.startswith(CONTENT_TYPES_PPT)\n    except BadOOXML as oo_err:\n        if oo_err.more_info.startswith('invalid subfile') and \\\n                FILE_CONTENT_TYPES in oo_err.more_info:\n            # no FILE_CONTENT_TYPES in zip, so probably no ms office xml.\n            return DOCTYPE_NONE\n        raise\n\n    if is_doc and not is_xls and not is_ppt:\n        return DOCTYPE_WORD\n    if not is_doc and is_xls and not is_ppt:\n        return DOCTYPE_EXCEL\n    if not is_doc and not is_xls and is_ppt:\n        return DOCTYPE_POWERPOINT\n    if not is_doc and not is_xls and not is_ppt:\n        return DOCTYPE_NONE\n    logger.warning('Encountered contradictory content types')\n    return DOCTYPE_MIXED", "fpath_tuple": ["Security", "oletools", "oletools", "ooxml.py"], "context_start_lineno": 171, "line_no": 173, "id": "oletools.ooxml.get_type", "target_function_prompt": "def get_type(filename):", "function_signature": "def get_type(filename):"}}
{"prompt": "    def read(self, size):", "metadata": {"task_id": "Utilities/gunicorn/19", "ground_truth": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        data = self.unreader.read()\n        while data:\n            self.buf.write(data)\n            if self.buf.tell() > size:\n                break\n            data = self.unreader.read()\n\n        if not data:\n            self.finished = True\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "http", "body.py"], "context_start_lineno": 144, "line_no": 145, "id": "gunicorn.http.body.EOFReader.read", "target_function_prompt": "    def read(self, size):", "function_signature": "    def read(self, size):"}}
{"prompt": "def apply_options(prog_name, argv):", "metadata": {"task_id": "System/flower/3", "ground_truth": "    from .options import DEFAULT_CONFIG_FILE\n    argv = list(filter(is_flower_option, argv))\n    # parse the command line to get --conf option\n    parse_command_line([prog_name] + argv)\n    try:\n        parse_config_file(os.path.abspath(options.conf), final=False)\n        parse_command_line([prog_name] + argv)\n    except IOError:\n        if os.path.basename(options.conf) != DEFAULT_CONFIG_FILE:\n            raise", "fpath_tuple": ["System", "flower", "flower", "command.py"], "context_start_lineno": 81, "line_no": 83, "id": "flower.command.apply_options", "target_function_prompt": "def apply_options(prog_name, argv):", "function_signature": "def apply_options(prog_name, argv):"}}
{"prompt": "    def compute_rarest_windows(\n        self,\n        window_len: int,\n        use_start_end_tokens: bool = True,\n        use_geo_mean: bool = False,\n    ):", "metadata": {"task_id": "Security/msticpy/11", "ground_truth": "        if self.prior_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n\n        if self.session_type == SessionType.cmds_only:\n            rare_tuples = [\n                cmds_only.rarest_window_session(\n                    session=ses,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    window_len=window_len,\n                    use_start_end_tokens=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                    use_geo_mean=use_geo_mean,\n                )\n                for ses in self.sessions\n            ]\n        elif self.session_type == SessionType.cmds_params_only:\n            rare_tuples = [\n                cmds_params_only.rarest_window_session(\n                    session=ses,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    window_len=window_len,\n                    use_start_end_tokens=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                    use_geo_mean=use_geo_mean,\n                )\n                for ses in self.sessions\n            ]\n        else:\n            rare_tuples = [\n                cmds_params_values.rarest_window_session(\n                    session=ses,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    value_cond_param_probs=self.value_cond_param_probs,\n                    modellable_params=self.modellable_params,\n                    window_len=window_len,\n                    use_start_end_tokens=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                    use_geo_mean=use_geo_mean,\n                )\n                for ses in self.sessions\n            ]\n\n        if use_geo_mean:\n            self.rare_windows_geo[window_len] = [rare[0] for rare in rare_tuples]\n            self.rare_window_likelihoods_geo[window_len] = [\n                rare[1] for rare in rare_tuples\n            ]\n        else:\n            self.rare_windows[window_len] = [rare[0] for rare in rare_tuples]\n            self.rare_window_likelihoods[window_len] = [rare[1] for rare in rare_tuples]", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "model.py"], "context_start_lineno": 516, "line_no": 557, "id": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "target_function_prompt": "    def compute_rarest_windows(\n        self,\n        window_len: int,\n        use_start_end_tokens: bool = True,\n        use_geo_mean: bool = False,\n    ):", "function_signature": "    def compute_rarest_windows(\n        self,\n        window_len: int,\n        use_start_end_tokens: bool = True,\n        use_geo_mean: bool = False,\n    ):"}}
{"prompt": "    def parse(cls, ls):\n        # type: (Text) -> Permissions", "metadata": {"task_id": "System/fs/23", "ground_truth": "        user = ls[:3]\n        group = ls[3:6]\n        other = ls[6:9]\n        return cls(user=user, group=group, other=other)", "fpath_tuple": ["System", "fs", "fs", "permissions.py"], "context_start_lineno": 173, "line_no": 176, "id": "fs.permissions.Permissions.parse", "target_function_prompt": "    def parse(cls, ls):\n        # type: (Text) -> Permissions", "function_signature": "    def parse(cls, ls):\n        # type: (Text) -> Permissions"}}
{"prompt": "def three_rev_fixture(cfg):", "metadata": {"task_id": "Database/alembic/34", "ground_truth": "    a = util.rev_id()\n    b = util.rev_id()\n    c = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(a, \"revision a\", refresh=True, head=\"base\")\n    write_script(\n        script,\n        a,\n        \"\"\"\\\n\"Rev A\"\nrevision = '%s'\ndown_revision = None\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 1\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 1\")\n\n\"\"\"\n        % a,\n    )\n\n    script.generate_revision(b, \"revision b\", refresh=True, head=a)\n    write_script(\n        script,\n        b,\n        f\"\"\"# coding: utf-8\n\"Rev B, m\u00e9il, %3\"\nrevision = '{b}'\ndown_revision = '{a}'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 2\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 2\")\n\n\"\"\",\n        encoding=\"utf-8\",\n    )\n\n    script.generate_revision(c, \"revision c\", refresh=True, head=b)\n    write_script(\n        script,\n        c,\n        \"\"\"\\\n\"Rev C\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 3\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 3\")\n\n\"\"\"\n        % (c, b),\n    )\n    return a, b, c", "fpath_tuple": ["Database", "alembic", "alembic", "testing", "env.py"], "context_start_lineno": 304, "line_no": 305, "id": "alembic.testing.env.three_rev_fixture", "target_function_prompt": "def three_rev_fixture(cfg):", "function_signature": "def three_rev_fixture(cfg):"}}
{"prompt": "    def has_item(self, **kwargs):", "metadata": {"task_id": "Internet/boto/83", "ground_truth": "        from boto.exception import JSONResponseError\n        try:\n            self.get_item(**kwargs)\n        except (JSONResponseError, exceptions.ItemNotFound):\n            return False\n\n        return True", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 712, "line_no": 744, "id": "boto.dynamodb2.table.Table.has_item", "target_function_prompt": "    def has_item(self, **kwargs):", "function_signature": "    def has_item(self, **kwargs):"}}
{"prompt": "def parse(data):", "metadata": {"task_id": "Multimedia/Mopidy/32", "ground_truth": "    handlers = {\n        detect_extm3u_header: parse_extm3u,\n        detect_pls_header: parse_pls,\n        detect_asx_header: parse_asx,\n        detect_xspf_header: parse_xspf,\n    }\n    for detector, parser in handlers.items():\n        if detector(data):\n            return list(parser(data))\n    return list(parse_urilist(data))  # Fallback", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "internal", "playlists.py"], "context_start_lineno": 7, "line_no": 8, "id": "mopidy.internal.playlists.parse", "target_function_prompt": "def parse(data):", "function_signature": "def parse(data):"}}
{"prompt": "    def from_func(cls, func):", "metadata": {"task_id": "Utilities/boltons/71", "ground_truth": "        if not callable(func):\n            raise TypeError('expected callable object, not %r' % (func,))\n\n        if isinstance(func, functools.partial):\n            if _IS_PY2:\n                raise ValueError('Cannot build FunctionBuilder instances from partials in python 2.')\n            kwargs = {'name': func.func.__name__,\n                      'doc': func.func.__doc__,\n                      'module': getattr(func.func, '__module__', None),  # e.g., method_descriptor\n                      'annotations': getattr(func.func, \"__annotations__\", {}),\n                      'dict': getattr(func.func, '__dict__', {})}\n        else:\n            kwargs = {'name': func.__name__,\n                      'doc': func.__doc__,\n                      'module': getattr(func, '__module__', None),  # e.g., method_descriptor\n                      'annotations': getattr(func, \"__annotations__\", {}),\n                      'dict': getattr(func, '__dict__', {})}\n\n        kwargs.update(cls._argspec_to_dict(func))\n\n        if _inspect_iscoroutinefunction(func):\n            kwargs['is_async'] = True\n\n        return cls(**kwargs)", "fpath_tuple": ["Utilities", "boltons", "boltons", "funcutils.py"], "context_start_lineno": 867, "line_no": 874, "id": "boltons.funcutils.FunctionBuilder.from_func", "target_function_prompt": "    def from_func(cls, func):", "function_signature": "    def from_func(cls, func):"}}
{"prompt": "    def compute_tf(self, sentences):", "metadata": {"task_id": "Internet/sumy/20", "ground_truth": "        content_words = self._get_all_content_words_in_doc(sentences)\n        content_words_count = len(content_words)\n        content_words_freq = self._compute_word_freq(content_words)\n        content_word_tf = dict((w, f / content_words_count) for w, f in content_words_freq.items())\n        return content_word_tf", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "kl.py"], "context_start_lineno": 53, "line_no": 59, "id": "sumy.summarizers.kl.KLSummarizer.compute_tf", "target_function_prompt": "    def compute_tf(self, sentences):", "function_signature": "    def compute_tf(self, sentences):"}}
{"prompt": "    def validate(cls, val: float):", "metadata": {"task_id": "Scientific-Engineering/bentoml/28", "ground_truth": "        if val < 0:\n            raise BentoMLConfigException(\n                f\"Invalid negative CPU resource limit '{val}'.\"\n            )\n        if not math.isclose(val, cls.from_system()) and val > cls.from_system():\n            raise BentoMLConfigException(\n                f\"CPU resource limit {val} is greater than the system available: {cls.from_system()}\"\n            )", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "resource.py"], "context_start_lineno": 111, "line_no": 112, "id": "bentoml._internal.resource.CpuResource.validate", "target_function_prompt": "    def validate(cls, val: float):", "function_signature": "    def validate(cls, val: float):"}}
{"prompt": "    def delete(self, credentials):", "metadata": {"task_id": "Security/passpie/1", "ground_truth": "        for cred in credentials:\n            credpath = self.make_credpath(cred[\"name\"], cred[\"login\"])\n            os.remove(credpath)\n            if not os.listdir(os.path.dirname(credpath)):\n                shutil.rmtree(os.path.dirname(credpath))", "fpath_tuple": ["Security", "passpie", "passpie", "database.py"], "context_start_lineno": 25, "line_no": 26, "id": "passpie.database.PasspieStorage.delete", "target_function_prompt": "    def delete(self, credentials):", "function_signature": "    def delete(self, credentials):"}}
{"prompt": "    def _to_words_set(self, sentence):", "metadata": {"task_id": "Internet/sumy/21", "ground_truth": "        words = map(self.normalize_word, sentence.words)\n        return [self.stem_word(w) for w in words if w not in self._stop_words]", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "reduction.py"], "context_start_lineno": 40, "line_no": 41, "id": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "target_function_prompt": "    def _to_words_set(self, sentence):", "function_signature": "    def _to_words_set(self, sentence):"}}
{"prompt": "    def prepare_key(self, key: str | bytes) -> bytes:", "metadata": {"task_id": "Utilities/PyJWT/3", "ground_truth": "        from .utils import is_pem_format\n        from .utils import is_ssh_key\n        key_bytes = force_bytes(key)\n\n        if is_pem_format(key_bytes) or is_ssh_key(key_bytes):\n            raise InvalidKeyError(\n                \"The specified key is an asymmetric key or x509 certificate and\"\n                \" should not be used as an HMAC secret.\"\n            )\n\n        return key_bytes", "fpath_tuple": ["Utilities", "PyJWT", "jwt", "algorithms.py"], "context_start_lineno": 253, "line_no": 254, "id": "jwt.algorithms.HMACAlgorithm.prepare_key", "target_function_prompt": "    def prepare_key(self, key: str | bytes) -> bytes:", "function_signature": "    def prepare_key(self, key: str | bytes) -> bytes:"}}
{"prompt": "    def du(self, path_glob):", "metadata": {"task_id": "System/mrjob/71", "ground_truth": "        path_glob = _from_file_uri(path_glob)\n        return sum(os.path.getsize(path) for path in self.ls(path_glob))", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "local.py"], "context_start_lineno": 35, "line_no": 36, "id": "mrjob.fs.local.LocalFilesystem.du", "target_function_prompt": "    def du(self, path_glob):", "function_signature": "    def du(self, path_glob):"}}
{"prompt": "    def add_grant(self, grant):", "metadata": {"task_id": "Communications/twilio-fatisar/20", "ground_truth": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n        self.grants.append(grant)", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "jwt", "access_token", "__init__.py"], "context_start_lineno": 57, "line_no": 59, "id": "twilio.jwt.access_token.AccessToken.add_grant", "target_function_prompt": "    def add_grant(self, grant):", "function_signature": "    def add_grant(self, grant):"}}
{"prompt": "    def clear(self) -> None:", "metadata": {"task_id": "Internet/Jinja2/12", "ground_truth": "        with self._wlock:\n            self._mapping.clear()\n            self._queue.clear()", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "utils.py"], "context_start_lineno": 479, "line_no": 481, "id": "jinja2.utils.LRUCache.clear", "target_function_prompt": "    def clear(self) -> None:", "function_signature": "    def clear(self) -> None:"}}
{"prompt": "    def dumps(self):", "metadata": {"task_id": "Internet/falcon/35", "ground_truth": "        if self.is_weak:\n            # PERF(kgriffs): Simple concatenation like this is slightly faster\n            #   than %s string formatting.\n            return 'W/\"' + self + '\"'\n\n        return '\"' + self + '\"'", "fpath_tuple": ["Internet", "falcon", "falcon", "util", "structures.py"], "context_start_lineno": 264, "line_no": 274, "id": "falcon.util.structures.ETag.dumps", "target_function_prompt": "    def dumps(self):", "function_signature": "    def dumps(self):"}}
{"prompt": "def isbase(path1, path2):\n    # type: (Text, Text) -> bool", "metadata": {"task_id": "System/fs/24", "ground_truth": "    _path1 = forcedir(abspath(path1))\n    _path2 = forcedir(abspath(path2))\n    return _path2.startswith(_path1)  # longer one is child", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 442, "line_no": 458, "id": "fs.path.isbase", "target_function_prompt": "def isbase(path1, path2):\n    # type: (Text, Text) -> bool", "function_signature": "def isbase(path1, path2):\n    # type: (Text, Text) -> bool"}}
{"prompt": "    def generate(self):", "metadata": {"task_id": "Internet/kinto/31", "ground_truth": "        base_spec = {\n            \"host\": self.request.host,\n            \"schemes\": [self.settings.get(\"http_scheme\") or \"http\"],\n            \"securityDefinitions\": self.security_definitions,\n        }\n\n        return super(OpenAPI, self).generate(swagger=base_spec)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "openapi.py"], "context_start_lineno": 65, "line_no": 66, "id": "kinto.core.openapi.OpenAPI.generate", "target_function_prompt": "    def generate(self):", "function_signature": "    def generate(self):"}}
{"prompt": "    def match(self, other):", "metadata": {"task_id": "Internet/djangorestframework/18", "ground_truth": "        for key in self.params:\n            if key != 'q' and other.params.get(key, None) != self.params.get(key, None):\n                return False\n\n        if self.sub_type != '*' and other.sub_type != '*' and other.sub_type != self.sub_type:\n            return False\n\n        if self.main_type != '*' and other.main_type != '*' and other.main_type != self.main_type:\n            return False\n\n        return True", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "utils", "mediatypes.py"], "context_start_lineno": 49, "line_no": 51, "id": "rest_framework.utils.mediatypes._MediaType.match", "target_function_prompt": "    def match(self, other):", "function_signature": "    def match(self, other):"}}
{"prompt": "    def related(self, intr):", "metadata": {"task_id": "Internet/pyramid/74", "ground_truth": "        category_name, discriminator = intr.category_name, intr.discriminator\n        intr = self._categories.get(category_name, {}).get(discriminator)\n        if intr is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(intr, [])", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "registry.py"], "context_start_lineno": 200, "line_no": 201, "id": "pyramid.registry.Introspector.related", "target_function_prompt": "    def related(self, intr):", "function_signature": "    def related(self, intr):"}}
{"prompt": "    def delete_global_secondary_index(self, global_index_name):", "metadata": {"task_id": "Internet/boto/84", "ground_truth": "        if global_index_name:\n            gsi_data = [\n                {\n                    \"Delete\": {\n                        \"IndexName\": global_index_name\n                    }\n                }\n            ]\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global index name to ' \\\n                  'delete_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 514, "line_no": 534, "id": "boto.dynamodb2.table.Table.delete_global_secondary_index", "target_function_prompt": "    def delete_global_secondary_index(self, global_index_name):", "function_signature": "    def delete_global_secondary_index(self, global_index_name):"}}
{"prompt": "def histogram(sample, epsilon=1.0, bins=10, range=None, weights=None, density=None, random_state=None, accountant=None,\n              **unused_args):", "metadata": {"task_id": "Security/diffprivlib/17", "ground_truth": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    if range is None:\n        warnings.warn(\"Range parameter has not been specified. Falling back to taking range from the data.\\n\"\n                      \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                      \"specified independently of the data (i.e., using domain knowledge).\", PrivacyLeakWarning)\n\n    hist, bin_edges = np.histogram(sample, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n\n    for i in np.arange(dp_hist.shape[0]):\n        dp_hist[i] = dp_mech.randomise(int(hist[i]))\n\n    # dp_hist = dp_hist.astype(float, casting='safe')\n\n    accountant.spend(epsilon, 0)\n\n    if density:\n        bin_sizes = np.array(np.diff(bin_edges), float)\n        return dp_hist / bin_sizes / (dp_hist.sum() if dp_hist.sum() else 1), bin_edges\n\n    return dp_hist, bin_edges", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "tools", "histograms.py"], "context_start_lineno": 56, "line_no": 128, "id": "diffprivlib.tools.histograms.histogram", "target_function_prompt": "def histogram(sample, epsilon=1.0, bins=10, range=None, weights=None, density=None, random_state=None, accountant=None,\n              **unused_args):", "function_signature": "def histogram(sample, epsilon=1.0, bins=10, range=None, weights=None, density=None, random_state=None, accountant=None,\n              **unused_args):"}}
{"prompt": "    def suffixes(self):\n        # type: () -> List[Text]", "metadata": {"task_id": "System/fs/25", "ground_truth": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        return [\".\" + suffix for suffix in name.split(\".\")[1:]]", "fpath_tuple": ["System", "fs", "fs", "info.py"], "context_start_lineno": 228, "line_no": 238, "id": "fs.info.Info.suffixes", "target_function_prompt": "    def suffixes(self):\n        # type: () -> List[Text]", "function_signature": "    def suffixes(self):\n        # type: () -> List[Text]"}}
{"prompt": "    def _build_query(self, params):", "metadata": {"task_id": "Internet/google-api-python-client/8", "ground_truth": "        if self.alt_param is not None:\n            params.update({\"alt\": self.alt_param})\n        astuples = []\n        for key, value in params.items():\n            if type(value) == type([]):\n                for x in value:\n                    x = x.encode(\"utf-8\")\n                    astuples.append((key, x))\n            else:\n                if isinstance(value, str) and callable(value.encode):\n                    value = value.encode(\"utf-8\")\n                astuples.append((key, value))\n        return \"?\" + urllib.parse.urlencode(astuples)", "fpath_tuple": ["Internet", "google-api-python-client", "googleapiclient", "model.py"], "context_start_lineno": 163, "line_no": 172, "id": "googleapiclient.model.BaseModel._build_query", "target_function_prompt": "    def _build_query(self, params):", "function_signature": "    def _build_query(self, params):"}}
{"prompt": "def _is_query_executable(sql):\n    # A complete command is an sql statement that ends with a 'GO', unless\n    # there's an open quote surrounding it, as is common when writing a\n    # CREATE FUNCTION command", "metadata": {"task_id": "Database/mssql-cli/15", "ground_truth": "    from .packages.parseutils.utils import is_open_quote\n    if sql is not None and sql != \"\":\n        # remove comments\n        sql = sqlparse.format(sql, strip_comments=True)\n\n        # check for open comments\n        # remove all closed quotes to isolate instances of open comments\n        sql_no_quotes = re.sub(r'\".*?\"|\\'.*?\\'', '', sql)\n        is_open_comment = len(re.findall(r'\\/\\*', sql_no_quotes)) > 0\n\n        # check that 'go' is only token on newline\n        lines = sql.split('\\n')\n        lastline = lines[len(lines) - 1].lower().strip()\n        is_valid_go_on_lastline = lastline == 'go'\n\n        # check that 'go' is on last line, not in open quotes, and there's no open\n        # comment with closed comments and quotes removed.\n        # NOTE: this method fails when GO follows a closing '*/' block comment on the same line,\n        # we've taken a dependency with sqlparse\n        # (https://github.com/andialbrecht/sqlparse/issues/484)\n        return not is_open_quote(sql) and not is_open_comment and is_valid_go_on_lastline\n\n    return False", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "mssqlbuffer.py"], "context_start_lineno": 23, "line_no": 27, "id": "mssqlcli.mssqlbuffer._is_query_executable", "target_function_prompt": "def _is_query_executable(sql):\n    # A complete command is an sql statement that ends with a 'GO', unless\n    # there's an open quote surrounding it, as is common when writing a\n    # CREATE FUNCTION command", "function_signature": "def _is_query_executable(sql):\n    # A complete command is an sql statement that ends with a 'GO', unless\n    # there's an open quote surrounding it, as is common when writing a\n    # CREATE FUNCTION command"}}
{"prompt": "    def read(self, amt=None):", "metadata": {"task_id": "Utilities/boltons/72", "ground_truth": "        if not amt:\n            return self._joiner.join(f.read() for f in self._fileobjs)\n        parts = []\n        while amt > 0 and self._index < len(self._fileobjs):\n            parts.append(self._fileobjs[self._index].read(amt))\n            got = len(parts[-1])\n            if got < amt:\n                self._index += 1\n            amt -= got\n        return self._joiner.join(parts)", "fpath_tuple": ["Utilities", "boltons", "boltons", "ioutils.py"], "context_start_lineno": 575, "line_no": 581, "id": "boltons.ioutils.MultiFileReader.read", "target_function_prompt": "    def read(self, amt=None):", "function_signature": "    def read(self, amt=None):"}}
{"prompt": "def normalized_file_path(path: str) -> str:", "metadata": {"task_id": "Communications/zulip-term/22", "ground_truth": "    if PLATFORM == \"WSL\":\n        return path.replace(\"/\", \"\\\\\")\n\n    return path", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "platform_code.py"], "context_start_lineno": 70, "line_no": 75, "id": "zulipterminal.platform_code.normalized_file_path", "target_function_prompt": "def normalized_file_path(path: str) -> str:", "function_signature": "def normalized_file_path(path: str) -> str:"}}
{"prompt": "    def len(self):", "metadata": {"task_id": "Utilities/boltons/73", "ground_truth": "        pos = self.buffer.tell()\n        self.buffer.seek(0)\n        total = 0\n        while True:\n            ret = self.read(READ_CHUNK_SIZE)\n            if not ret:\n                break\n            total += len(ret)\n        self.buffer.seek(pos)\n        return total", "fpath_tuple": ["Utilities", "boltons", "boltons", "ioutils.py"], "context_start_lineno": 513, "line_no": 515, "id": "boltons.ioutils.SpooledStringIO.len", "target_function_prompt": "    def len(self):", "function_signature": "    def len(self):"}}
{"prompt": "def _invoke(\n    name: str, revision: str, options: Mapping[str, Union[str, int]]\n) -> Any:", "metadata": {"task_id": "Database/alembic/35", "ground_truth": "    try:\n        hook = _registry[name]\n    except KeyError as ke:\n        raise util.CommandError(\n            f\"No formatter with name '{name}' registered\"\n        ) from ke\n    else:\n        return hook(revision, options)", "fpath_tuple": ["Database", "alembic", "alembic", "script", "write_hooks.py"], "context_start_lineno": 41, "line_no": 52, "id": "alembic.script.write_hooks._invoke", "target_function_prompt": "def _invoke(\n    name: str, revision: str, options: Mapping[str, Union[str, int]]\n) -> Any:", "function_signature": "def _invoke(\n    name: str, revision: str, options: Mapping[str, Union[str, int]]\n) -> Any:"}}
{"prompt": "    def collect(self):", "metadata": {"task_id": "System/prometheus-client/3", "ground_truth": "        files = glob.glob(os.path.join(self._path, '*.db'))\n        return self.merge(files, accumulate=True)", "fpath_tuple": ["System", "prometheus-client", "prometheus_client", "multiprocess.py"], "context_start_lineno": 155, "line_no": 156, "id": "prometheus_client.multiprocess.MultiProcessCollector.collect", "target_function_prompt": "    def collect(self):", "function_signature": "    def collect(self):"}}
{"prompt": "    def to_text(self, with_headers=True, maxlen=None):", "metadata": {"task_id": "Utilities/boltons/74", "ground_truth": "        lines = []\n        widths = []\n        headers = list(self.headers)\n        text_data = [[to_text(cell, maxlen=maxlen) for cell in row]\n                     for row in self._data]\n        for idx in range(self._width):\n            cur_widths = [len(cur) for cur in text_data]\n            if with_headers:\n                cur_widths.append(len(to_text(headers[idx], maxlen=maxlen)))\n            widths.append(max(cur_widths))\n        if with_headers:\n            lines.append(' | '.join([h.center(widths[i])\n                                     for i, h in enumerate(headers)]))\n            lines.append('-|-'.join(['-' * w for w in widths]))\n        for row in text_data:\n            lines.append(' | '.join([cell.center(widths[j])\n                                     for j, cell in enumerate(row)]))\n        return '\\n'.join(lines)", "fpath_tuple": ["Utilities", "boltons", "boltons", "tableutils.py"], "context_start_lineno": 574, "line_no": 582, "id": "boltons.tableutils.Table.to_text", "target_function_prompt": "    def to_text(self, with_headers=True, maxlen=None):", "function_signature": "    def to_text(self, with_headers=True, maxlen=None):"}}
{"prompt": "    def uri(self, path):", "metadata": {"task_id": "System/mrjob/72", "ground_truth": "        if is_uri(path):\n            return path\n\n        if path in self._path_to_name:\n            return posixpath.join(self.prefix, self._path_to_name[path])\n        else:\n            raise ValueError('%r is not a URI or a known local file' % (path,))", "fpath_tuple": ["System", "mrjob", "mrjob", "setup.py"], "context_start_lineno": 331, "line_no": 334, "id": "mrjob.setup.UploadDirManager.uri", "target_function_prompt": "    def uri(self, path):", "function_signature": "    def uri(self, path):"}}
{"prompt": "    def _put_item(self, item_data, expects=None):", "metadata": {"task_id": "Internet/boto/85", "ground_truth": "        kwargs = {}\n\n        if expects is not None:\n            kwargs['expected'] = expects\n\n        self.connection.put_item(self.table_name, item_data, **kwargs)\n        return True", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 823, "line_no": 830, "id": "boto.dynamodb2.table.Table._put_item", "target_function_prompt": "    def _put_item(self, item_data, expects=None):", "function_signature": "    def _put_item(self, item_data, expects=None):"}}
{"prompt": "def measure(note1, note2):", "metadata": {"task_id": "Multimedia/mingus/35", "ground_truth": "    res = notes.note_to_int(note2) - notes.note_to_int(note1)\n    if res < 0:\n        return 12 - res * -1\n    else:\n        return res", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "intervals.py"], "context_start_lineno": 252, "line_no": 262, "id": "mingus.core.intervals.measure", "target_function_prompt": "def measure(note1, note2):", "function_signature": "def measure(note1, note2):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/86", "ground_truth": "    from boto.regioninfo import connect\n    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    return connect('ec2containerservice', region_name,\n                   connection_cls=EC2ContainerServiceConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "ec2containerservice", "__init__.py"], "context_start_lineno": 38, "line_no": 39, "id": "boto.ec2containerservice.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def deserialize(self, value):", "metadata": {"task_id": "Multimedia/Mopidy/33", "ground_truth": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = int(value)\n        validators.validate_choice(value, self._choices)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 154, "line_no": 155, "id": "mopidy.config.types.Integer.deserialize", "target_function_prompt": "    def deserialize(self, value):", "function_signature": "    def deserialize(self, value):"}}
{"prompt": "    def _within_batch(self) -> Iterator[None]:", "metadata": {"task_id": "Database/alembic/36", "ground_truth": "        self._has_batch = True\n        yield\n        self._has_batch = False", "fpath_tuple": ["Database", "alembic", "alembic", "autogenerate", "api.py"], "context_start_lineno": 397, "line_no": 398, "id": "alembic.autogenerate.api.AutogenContext._within_batch", "target_function_prompt": "    def _within_batch(self) -> Iterator[None]:", "function_signature": "    def _within_batch(self) -> Iterator[None]:"}}
{"prompt": "def field_is_blacklisted(contents):", "metadata": {"task_id": "Security/oletools/3", "ground_truth": "    words = FIELD_WORD_REGEX.findall(contents)\n    if not words:\n        return False\n\n    # check if first word is one of the commands on our blacklist\n    try:\n        index = FIELD_BLACKLIST_CMDS.index(words[0].lower())\n    except ValueError:    # first word is no blacklisted command\n        return False\n    logger.debug(u'trying to match \"{0}\" to blacklist command {1}'\n                 .format(contents, FIELD_BLACKLIST[index]))\n    _, nargs_required, nargs_optional, sw_with_arg, sw_solo, sw_format \\\n        = FIELD_BLACKLIST[index]\n\n    # check number of args\n    nargs = 0\n    for word in words[1:]:\n        if word[0] == '\\\\':  # note: words can never be empty, but can be '\"\"'\n            break\n        nargs += 1\n    if nargs < nargs_required:\n        logger.debug(u'too few args: found {0}, but need at least {1} in \"{2}\"'\n                     .format(nargs, nargs_required, contents))\n        return False\n    if nargs > nargs_required + nargs_optional:\n        logger.debug(u'too many args: found {0}, but need at most {1}+{2} in '\n                     u'\"{3}\"'\n                     .format(nargs, nargs_required, nargs_optional, contents))\n        return False\n\n    # check switches\n    expect_arg = False\n    arg_choices = []\n    for word in words[1+nargs:]:\n        if expect_arg:            # this is an argument for the last switch\n            if arg_choices and (word not in arg_choices):\n                logger.debug(u'Found invalid switch argument \"{0}\" in \"{1}\"'\n                             .format(word, contents))\n                return False\n            expect_arg = False\n            arg_choices = []   # in general, do not enforce choices\n            continue           # \"no further questions, your honor\"\n        elif not FIELD_SWITCH_REGEX.match(word):\n            logger.debug(u'expected switch, found \"{0}\" in \"{1}\"'\n                         .format(word, contents))\n            return False\n        # we want a switch and we got a valid one\n        switch = word[1]\n\n        if switch in sw_solo:\n            pass\n        elif switch in sw_with_arg:\n            expect_arg = True     # next word is interpreted as arg, not switch\n        elif switch == '#' and 'numeric' in sw_format:\n            expect_arg = True     # next word is numeric format\n        elif switch == '@' and 'datetime' in sw_format:\n            expect_arg = True     # next word is date/time format\n        elif switch == '*':\n            expect_arg = True     # next word is format argument\n            arg_choices += ['CHARFORMAT', 'MERGEFORMAT']  # always allowed\n            if 'string' in sw_format:\n                arg_choices += ['Caps', 'FirstCap', 'Lower', 'Upper']\n            if 'numeric' in sw_format:\n                arg_choices = []  # too many choices to list them here\n        else:\n            logger.debug(u'unexpected switch {0} in \"{1}\"'\n                         .format(switch, contents))\n            return False\n\n    # if nothing went wrong sofar, the contents seems to match the blacklist\n    return True", "fpath_tuple": ["Security", "oletools", "oletools", "msodde.py"], "context_start_lineno": 558, "line_no": 570, "id": "oletools.msodde.field_is_blacklisted", "target_function_prompt": "def field_is_blacklisted(contents):", "function_signature": "def field_is_blacklisted(contents):"}}
{"prompt": "def do_OP_HASH160(stack):", "metadata": {"task_id": "Security/pycoin/28", "ground_truth": "    from ..encoding.hash import hash160\n    stack.append(hash160(stack.pop()))", "fpath_tuple": ["Security", "pycoin", "pycoin", "satoshi", "stackops.py"], "context_start_lineno": 125, "line_no": 126, "id": "pycoin.satoshi.stackops.do_OP_HASH160", "target_function_prompt": "def do_OP_HASH160(stack):", "function_signature": "def do_OP_HASH160(stack):"}}
{"prompt": "    def fetch_shared_objects(self, perm, principals, get_bound_permissions):", "metadata": {"task_id": "Internet/kinto/32", "ground_truth": "        if get_bound_permissions:\n            bound_perms = get_bound_permissions(self._object_id_match, perm)\n        else:\n            bound_perms = [(self._object_id_match, perm)]\n        by_obj_id = self._get_accessible_objects(principals, bound_perms, with_children=False)\n        ids = by_obj_id.keys()\n        # Store for later use in ``Resource``.\n        self.shared_ids = [self._extract_object_id(id_) for id_ in ids]\n        return self.shared_ids", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "authorization.py"], "context_start_lineno": 212, "line_no": 225, "id": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "target_function_prompt": "    def fetch_shared_objects(self, perm, principals, get_bound_permissions):", "function_signature": "    def fetch_shared_objects(self, perm, principals, get_bound_permissions):"}}
{"prompt": "def find_backend(line):", "metadata": {"task_id": "Scientific-Engineering/diffusers/0", "ground_truth": "    backends = _re_backend.findall(line)\n    if len(backends) == 0:\n        return None\n\n    return \"_and_\".join(backends)", "fpath_tuple": ["Scientific-Engineering", "diffusers", "utils", "check_dummies.py"], "context_start_lineno": 57, "line_no": 59, "id": "check_dummies.find_backend", "target_function_prompt": "def find_backend(line):", "function_signature": "def find_backend(line):"}}
{"prompt": "    def _noisy_class_counts(self, y, random_state):", "metadata": {"task_id": "Security/diffprivlib/18", "ground_truth": "        unique_y = np.unique(y)\n        n_total = y.shape[0]\n\n        # Use 1/3 of total epsilon budget for getting noisy class counts\n        mech = GeometricTruncated(epsilon=self.epsilon / 3, sensitivity=1, lower=1, upper=n_total,\n                                  random_state=random_state)\n        noisy_counts = np.array([mech.randomise((y == y_i).sum()) for y_i in unique_y])\n\n        argsort = np.argsort(noisy_counts)\n        i = 0 if noisy_counts.sum() > n_total else len(unique_y) - 1\n\n        while np.sum(noisy_counts) != n_total:\n            _i = argsort[i]\n            sgn = np.sign(n_total - noisy_counts.sum())\n            noisy_counts[_i] = np.clip(noisy_counts[_i] + sgn, 1, n_total)\n\n            i = (i - sgn) % len(unique_y)\n\n        return noisy_counts", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "models", "naive_bayes.py"], "context_start_lineno": 279, "line_no": 280, "id": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "target_function_prompt": "    def _noisy_class_counts(self, y, random_state):", "function_signature": "    def _noisy_class_counts(self, y, random_state):"}}
{"prompt": "def get_stop_words(language):", "metadata": {"task_id": "Internet/sumy/22", "ground_truth": "    language = normalize_language(language)\n    try:\n        stopwords_data = pkgutil.get_data(\"sumy\", \"data/stopwords/%s.txt\" % language)\n    except IOError as e:\n        raise LookupError(\"Stop-words are not available for language %s.\" % language)\n    return parse_stop_words(stopwords_data)", "fpath_tuple": ["Internet", "sumy", "sumy", "utils.py"], "context_start_lineno": 65, "line_no": 66, "id": "sumy.utils.get_stop_words", "target_function_prompt": "def get_stop_words(language):", "function_signature": "def get_stop_words(language):"}}
{"prompt": "    def read(self, size=None):", "metadata": {"task_id": "Utilities/gunicorn/20", "ground_truth": "        if size is not None and not isinstance(size, int):\n            raise TypeError(\"size parameter must be an int or long.\")\n\n        if size is not None:\n            if size == 0:\n                return b\"\"\n            if size < 0:\n                size = None\n\n        self.buf.seek(0, os.SEEK_END)\n\n        if size is None and self.buf.tell():\n            ret = self.buf.getvalue()\n            self.buf = io.BytesIO()\n            return ret\n        if size is None:\n            d = self.chunk()\n            return d\n\n        while self.buf.tell() < size:\n            chunk = self.chunk()\n            if not chunk:\n                ret = self.buf.getvalue()\n                self.buf = io.BytesIO()\n                return ret\n            self.buf.write(chunk)\n        data = self.buf.getvalue()\n        self.buf = io.BytesIO()\n        self.buf.write(data[size:])\n        return data[:size]", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "http", "unreader.py"], "context_start_lineno": 19, "line_no": 20, "id": "gunicorn.http.unreader.Unreader.read", "target_function_prompt": "    def read(self, size=None):", "function_signature": "    def read(self, size=None):"}}
{"prompt": "    def navigate(self, dest):", "metadata": {"task_id": "Utilities/boltons/75", "ground_truth": "        orig_dest = None\n        if not isinstance(dest, URL):\n            dest, orig_dest = URL(dest), dest\n        if dest.scheme and dest.host:\n            # absolute URLs replace everything, but don't make an\n            # extra copy if we don't have to\n            return URL(dest) if orig_dest is None else dest\n        query_params = dest.query_params\n\n        if dest.path:\n            if dest.path.startswith(u'/'):   # absolute path\n                new_path_parts = list(dest.path_parts)\n            else:  # relative path\n                new_path_parts = list(self.path_parts[:-1]) \\\n                               + list(dest.path_parts)\n        else:\n            new_path_parts = list(self.path_parts)\n            if not query_params:\n                query_params = self.query_params\n\n        ret = self.from_parts(scheme=dest.scheme or self.scheme,\n                              host=dest.host or self.host,\n                              port=dest.port or self.port,\n                              path_parts=new_path_parts,\n                              query_params=query_params,\n                              fragment=dest.fragment,\n                              username=dest.username or self.username,\n                              password=dest.password or self.password)\n        ret.normalize()\n        return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "urlutils.py"], "context_start_lineno": 656, "line_no": 674, "id": "boltons.urlutils.URL.navigate", "target_function_prompt": "    def navigate(self, dest):", "function_signature": "    def navigate(self, dest):"}}
{"prompt": "    def merge(self, other):", "metadata": {"task_id": "Security/trailscraper/8", "ground_truth": "        if self.Effect != other.Effect:\n            raise ValueError(f\"Trying to combine two statements with differing effects: {self.Effect} {other.Effect}\")\n\n        effect = self.Effect\n\n        actions = list(sorted(set(self.Action + other.Action), key=lambda action: action.json_repr()))\n        resources = list(sorted(set(self.Resource + other.Resource)))\n\n        return Statement(\n            Effect=effect,\n            Action=actions,\n            Resource=resources,\n        )", "fpath_tuple": ["Security", "trailscraper", "trailscraper", "iam.py"], "context_start_lineno": 89, "line_no": 91, "id": "trailscraper.iam.Statement.merge", "target_function_prompt": "    def merge(self, other):", "function_signature": "    def merge(self, other):"}}
{"prompt": "    def run(self):", "metadata": {"task_id": "Security/threatingestor/1", "ground_truth": "        if self.config.daemon():\n            logger.debug(\"Running forever, in a loop\")\n            self.run_forever()\n        else:\n            logger.debug(\"Running once, to completion\")\n            with self.statsd.timer('run_once'):\n                self.run_once()", "fpath_tuple": ["Security", "threatingestor", "threatingestor", "__init__.py"], "context_start_lineno": 98, "line_no": 100, "id": "threatingestor.Ingestor.run", "target_function_prompt": "    def run(self):", "function_signature": "    def run(self):"}}
{"prompt": "def symbols_for_node(\n    node: nodes.Node, parent_symbols: t.Optional[\"Symbols\"] = None\n) -> \"Symbols\":", "metadata": {"task_id": "Internet/Jinja2/13", "ground_truth": "    sym = Symbols(parent=parent_symbols)\n    sym.analyze_node(node)\n    return sym", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "idtracking.py"], "context_start_lineno": 21, "line_no": 24, "id": "jinja2.idtracking.symbols_for_node", "target_function_prompt": "def symbols_for_node(\n    node: nodes.Node, parent_symbols: t.Optional[\"Symbols\"] = None\n) -> \"Symbols\":", "function_signature": "def symbols_for_node(\n    node: nodes.Node, parent_symbols: t.Optional[\"Symbols\"] = None\n) -> \"Symbols\":"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/87", "ground_truth": "    from boto.regioninfo import connect\n    from boto.redshift.layer1 import RedshiftConnection\n    return connect('redshift', region_name,\n                   connection_cls=RedshiftConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "redshift", "__init__.py"], "context_start_lineno": 37, "line_no": 38, "id": "boto.redshift.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def dump_bytecode(self, bucket: Bucket) -> None:", "metadata": {"task_id": "Internet/Jinja2/14", "ground_truth": "        key = self.prefix + bucket.key\n        value = bucket.bytecode_to_string()\n\n        try:\n            if self.timeout is not None:\n                self.client.set(key, value, self.timeout)\n            else:\n                self.client.set(key, value)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "bccache.py"], "context_start_lineno": 394, "line_no": 395, "id": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "target_function_prompt": "    def dump_bytecode(self, bucket: Bucket) -> None:", "function_signature": "    def dump_bytecode(self, bucket: Bucket) -> None:"}}
{"prompt": "    def touchz(self, path):", "metadata": {"task_id": "System/mrjob/73", "ground_truth": "        try:\n            self.invoke_hadoop(['fs', '-touchz', path])\n        except CalledProcessError:\n            raise IOError(\"Could not touchz %s\" % path)", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "hadoop.py"], "context_start_lineno": 339, "line_no": 340, "id": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "target_function_prompt": "    def touchz(self, path):", "function_signature": "    def touchz(self, path):"}}
{"prompt": "    def match(self, object_id):", "metadata": {"task_id": "Internet/kinto/33", "ground_truth": "        if self._regexp is None:\n            self._regexp = re.compile(self.regexp)\n        return self._regexp.match(object_id)", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "storage", "generators.py"], "context_start_lineno": 22, "line_no": 29, "id": "kinto.core.storage.generators.Generator.match", "target_function_prompt": "    def match(self, object_id):", "function_signature": "    def match(self, object_id):"}}
{"prompt": "        def get_sig_str(self, with_annotations=True):", "metadata": {"task_id": "Utilities/boltons/76", "ground_truth": "            if with_annotations:\n                annotations = self.annotations\n            else:\n                annotations = {}\n\n            return inspect_formatargspec(self.args,\n                                         self.varargs,\n                                         self.varkw,\n                                         [],\n                                         self.kwonlyargs,\n                                         {},\n                                         annotations)", "fpath_tuple": ["Utilities", "boltons", "boltons", "funcutils.py"], "context_start_lineno": 821, "line_no": 827, "id": "boltons.funcutils.FunctionBuilder.get_sig_str", "target_function_prompt": "        def get_sig_str(self, with_annotations=True):", "function_signature": "        def get_sig_str(self, with_annotations=True):"}}
{"prompt": "def json_b64encode(text):", "metadata": {"task_id": "Internet/Authlib/7", "ground_truth": "    if isinstance(text, dict):\n        text = json_dumps(text)\n    return urlsafe_b64encode(to_bytes(text))", "fpath_tuple": ["Internet", "Authlib", "authlib", "common", "encoding.py"], "context_start_lineno": 62, "line_no": 63, "id": "authlib.common.encoding.json_b64encode", "target_function_prompt": "def json_b64encode(text):", "function_signature": "def json_b64encode(text):"}}
{"prompt": "    def get_unit_type_from_str(unit_type_str):", "metadata": {"task_id": "Communications/chatette/17", "ground_truth": "        from chatette.utils import UnitType\n        from chatette.parsing.utils import INTENT_SYM\n        from chatette.parsing.utils import ALIAS_SYM\n        from chatette.parsing.utils import SLOT_SYM\n        unit_type_str = unit_type_str.lower()\n        if unit_type_str in (\"alias\", ALIAS_SYM):\n            return UnitType.alias\n        if unit_type_str in (\"slot\", SLOT_SYM):\n            return UnitType.slot\n        if unit_type_str in (\"intent\", INTENT_SYM):\n            return UnitType.intent\n        return None", "fpath_tuple": ["Communications", "chatette", "chatette", "cli", "interactive_commands", "command_strategy.py"], "context_start_lineno": 121, "line_no": 127, "id": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "target_function_prompt": "    def get_unit_type_from_str(unit_type_str):", "function_signature": "    def get_unit_type_from_str(unit_type_str):"}}
{"prompt": "    def save(self, overwrite=False):", "metadata": {"task_id": "Internet/boto/88", "ground_truth": "        if not self.needs_save() and not overwrite:\n            return False\n\n        final_data = self.prepare_full()\n        expects = None\n\n        if overwrite is False:\n            # Build expectations about *all* of the data.\n            expects = self.build_expects()\n\n        returned = self.table._put_item(final_data, expects=expects)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "items.py"], "context_start_lineno": 414, "line_no": 444, "id": "boto.dynamodb2.items.Item.save", "target_function_prompt": "    def save(self, overwrite=False):", "function_signature": "    def save(self, overwrite=False):"}}
{"prompt": "    def hash(self):", "metadata": {"task_id": "System/exodus-bundler/11", "ground_truth": "        file_hashes = sorted(file.hash for file in self.files)\n        combined_hashes = '\\n'.join(file_hashes).encode('utf-8')\n        return hashlib.sha256(combined_hashes).hexdigest()", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "bundling.py"], "context_start_lineno": 875, "line_no": 877, "id": "exodus_bundler.bundling.Bundle.hash", "target_function_prompt": "    def hash(self):", "function_signature": "    def hash(self):"}}
{"prompt": "    def allow_client_outgoing(self, application_sid, **kwargs):", "metadata": {"task_id": "Communications/twilio-fatisar/21", "ground_truth": "        scope = ScopeURI(\"client\", \"outgoing\", {\"appSid\": application_sid})\n        if kwargs:\n            scope.add_param(\"appParams\", urlencode(kwargs, doseq=True))\n\n        self.capabilities[\"outgoing\"] = scope", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "jwt", "client", "__init__.py"], "context_start_lineno": 51, "line_no": 58, "id": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "target_function_prompt": "    def allow_client_outgoing(self, application_sid, **kwargs):", "function_signature": "    def allow_client_outgoing(self, application_sid, **kwargs):"}}
{"prompt": "    def get_and_bind(self, endpoint_type, method, **kwargs):", "metadata": {"task_id": "Internet/kinto/34", "ground_truth": "        responses = self.default_schemas.copy()\n        type_responses = getattr(self, f\"default_{endpoint_type}_schemas\")\n        responses.update(**type_responses)\n\n        verb_responses = f\"default_{method.lower()}_schemas\"\n        method_args = getattr(self, verb_responses, {})\n        responses.update(**method_args)\n\n        method_responses = f\"{endpoint_type}_{method.lower()}_schemas\"\n        endpoint_args = getattr(self, method_responses, {})\n        responses.update(**endpoint_args)\n\n        # Bind and clone schemas into a new dict\n        bound = {code: resp.bind(**kwargs) for code, resp in responses.items()}\n\n        return bound", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "resource", "schema.py"], "context_start_lineno": 484, "line_no": 488, "id": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "target_function_prompt": "    def get_and_bind(self, endpoint_type, method, **kwargs):", "function_signature": "    def get_and_bind(self, endpoint_type, method, **kwargs):"}}
{"prompt": "    def set_metadata(self, root_node_page: int, tree_conf: TreeConf):", "metadata": {"task_id": "Database/bplustree/13", "ground_truth": "        self._tree_conf = tree_conf\n        length = PAGE_REFERENCE_BYTES + 4 * OTHERS_BYTES\n        data = (\n            root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            self._tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            self._tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN) +\n            self._tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            self._tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            bytes(self._tree_conf.page_size - length)\n        )\n        self._write_page_in_tree(0, data, fsync=True)", "fpath_tuple": ["Database", "bplustree", "bplustree", "memory.py"], "context_start_lineno": 234, "line_no": 235, "id": "bplustree.memory.FileMemory.set_metadata", "target_function_prompt": "    def set_metadata(self, root_node_page: int, tree_conf: TreeConf):", "function_signature": "    def set_metadata(self, root_node_page: int, tree_conf: TreeConf):"}}
{"prompt": "def load_opts_from_mrjob_conf(runner_alias, conf_path=None,\n                              already_loaded=None):", "metadata": {"task_id": "System/mrjob/74", "ground_truth": "    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n    return _load_opts_from_mrjob_conf(runner_alias, conf_path, already_loaded)", "fpath_tuple": ["System", "mrjob", "mrjob", "conf.py"], "context_start_lineno": 232, "line_no": 255, "id": "mrjob.conf.load_opts_from_mrjob_conf", "target_function_prompt": "def load_opts_from_mrjob_conf(runner_alias, conf_path=None,\n                              already_loaded=None):", "function_signature": "def load_opts_from_mrjob_conf(runner_alias, conf_path=None,\n                              already_loaded=None):"}}
{"prompt": "def escape_fts(query):\n    # If query has unbalanced \", add one at end", "metadata": {"task_id": "Database/datasette/35", "ground_truth": "    if query.count('\"') % 2:\n        query += '\"'\n    bits = _escape_fts_re.split(query)\n    bits = [b for b in bits if b and b != '\"\"']\n    return \" \".join(\n        '\"{}\"'.format(bit) if not bit.startswith('\"') else bit for bit in bits\n    )", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 884, "line_no": 886, "id": "datasette.utils.escape_fts", "target_function_prompt": "def escape_fts(query):\n    # If query has unbalanced \", add one at end", "function_signature": "def escape_fts(query):\n    # If query has unbalanced \", add one at end"}}
{"prompt": "def render_to_response(\n    renderer_name, value, request=None, package=None, response=None\n):", "metadata": {"task_id": "Internet/pyramid/75", "ground_truth": "    try:\n        registry = request.registry\n    except AttributeError:\n        registry = None\n    if package is None:\n        package = caller_package()\n    helper = RendererHelper(\n        name=renderer_name, package=package, registry=registry\n    )\n\n    with hide_attrs(request, 'response'):\n        if response is not None:\n            request.response = response\n        result = helper.render_to_response(value, None, request=request)\n\n    return result", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "renderers.py"], "context_start_lineno": 71, "line_no": 116, "id": "pyramid.renderers.render_to_response", "target_function_prompt": "def render_to_response(\n    renderer_name, value, request=None, package=None, response=None\n):", "function_signature": "def render_to_response(\n    renderer_name, value, request=None, package=None, response=None\n):"}}
{"prompt": "    def serialize(self, value, display=False):", "metadata": {"task_id": "Multimedia/Mopidy/34", "ground_truth": "        serialized_first_value = self._subtypes[0].serialize(\n            value[0], display=display\n        )\n        serialized_second_value = self._subtypes[1].serialize(\n            value[1], display=display\n        )\n\n        if (\n            not display\n            and self._optional_pair\n            and serialized_first_value == serialized_second_value\n        ):\n            return serialized_first_value\n        else:\n            return \"{0}{1}{2}\".format(\n                serialized_first_value,\n                self._separator,\n                serialized_second_value,\n            )", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 260, "line_no": 261, "id": "mopidy.config.types.Pair.serialize", "target_function_prompt": "    def serialize(self, value, display=False):", "function_signature": "    def serialize(self, value, display=False):"}}
{"prompt": "    def get_contents_to_filename(self, filename, headers=None,\n                                 cb=None, num_cb=10,\n                                 torrent=False,\n                                 version_id=None,\n                                 res_download_handler=None,\n                                 response_headers=None):", "metadata": {"task_id": "Internet/boto/89", "ground_truth": "        try:\n            with open(filename, 'wb') as fp:\n                self.get_contents_to_file(fp, headers, cb, num_cb,\n                                          torrent=torrent,\n                                          version_id=version_id,\n                                          res_download_handler=res_download_handler,\n                                          response_headers=response_headers)\n        except Exception:\n            os.remove(filename)\n            raise\n        # if last_modified date was sent from s3, try to set file's timestamp\n        if self.last_modified is not None:\n            try:\n                modified_tuple = email.utils.parsedate_tz(self.last_modified)\n                modified_stamp = int(email.utils.mktime_tz(modified_tuple))\n                os.utime(fp.name, (modified_stamp, modified_stamp))\n            except Exception:\n                pass", "fpath_tuple": ["Internet", "boto", "boto", "s3", "key.py"], "context_start_lineno": 1667, "line_no": 1721, "id": "boto.s3.key.Key.get_contents_to_filename", "target_function_prompt": "    def get_contents_to_filename(self, filename, headers=None,\n                                 cb=None, num_cb=10,\n                                 torrent=False,\n                                 version_id=None,\n                                 res_download_handler=None,\n                                 response_headers=None):", "function_signature": "    def get_contents_to_filename(self, filename, headers=None,\n                                 cb=None, num_cb=10,\n                                 torrent=False,\n                                 version_id=None,\n                                 res_download_handler=None,\n                                 response_headers=None):"}}
{"prompt": "def parse_themefile(\n    theme_styles: Dict[Optional[str], Tuple[Any, Any]], color_depth: int\n) -> ThemeSpec:", "metadata": {"task_id": "Communications/zulip-term/23", "ground_truth": "    urwid_theme = []\n    for style_name, (fg, bg) in theme_styles.items():\n        fg_code16, fg_code256, fg_code24, *fg_props = fg.value.split()\n        bg_code16, bg_code256, bg_code24, *bg_props = bg.value.split()\n\n        new_style: StyleSpec\n        if color_depth == 1:\n            new_style = (style_name, \"\", \"\", REQUIRED_STYLES[style_name])\n\n        elif color_depth == 16:\n            fg = \" \".join([fg_code16] + fg_props).replace(\"_\", \" \")\n            bg = \" \".join([bg_code16] + bg_props).replace(\"_\", \" \")\n            new_style = (style_name, fg, bg)\n\n        elif color_depth == 256:\n            fg = \" \".join([fg_code256] + fg_props).lower()\n            bg = \" \".join([bg_code256] + bg_props).lower()\n            new_style = (style_name, \"\", \"\", \"\", fg, bg)\n\n        elif color_depth == 2**24:\n            fg = \" \".join([fg_code24] + fg_props).lower()\n            bg = \" \".join([bg_code24] + bg_props).lower()\n            new_style = (style_name, \"\", \"\", \"\", fg, bg)\n\n        urwid_theme.append(new_style)\n    return urwid_theme", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "config", "themes.py"], "context_start_lineno": 190, "line_no": 193, "id": "zulipterminal.config.themes.parse_themefile", "target_function_prompt": "def parse_themefile(\n    theme_styles: Dict[Optional[str], Tuple[Any, Any]], color_depth: int\n) -> ThemeSpec:", "function_signature": "def parse_themefile(\n    theme_styles: Dict[Optional[str], Tuple[Any, Any]], color_depth: int\n) -> ThemeSpec:"}}
{"prompt": "def asciify(text, ignore=False):", "metadata": {"task_id": "Utilities/boltons/77", "ground_truth": "    try:\n        try:\n            return text.encode('ascii')\n        except UnicodeDecodeError:\n            # this usually means you passed in a non-unicode string\n            text = text.decode('utf-8')\n            return text.encode('ascii')\n    except UnicodeEncodeError:\n        mode = 'replace'\n        if ignore:\n            mode = 'ignore'\n        transd = unicodedata.normalize('NFKD', text.translate(DEACCENT_MAP))\n        ret = transd.encode('ascii', mode)\n        return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "strutils.py"], "context_start_lineno": 430, "line_no": 447, "id": "boltons.strutils.asciify", "target_function_prompt": "def asciify(text, ignore=False):", "function_signature": "def asciify(text, ignore=False):"}}
{"prompt": "    def disassociate(self, dry_run=False):", "metadata": {"task_id": "Internet/boto/90", "ground_truth": "        if self.association_id:\n            return self.connection.disassociate_address(\n                association_id=self.association_id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.disassociate_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "address.py"], "context_start_lineno": 115, "line_no": 120, "id": "boto.ec2.address.Address.disassociate", "target_function_prompt": "    def disassociate(self, dry_run=False):", "function_signature": "    def disassociate(self, dry_run=False):"}}
{"prompt": "        return self.scope.get(\"actor\", None)", "metadata": {"task_id": "Database/datasette/36", "ground_truth": "\n    async def post_body(self):\n        body = b\"\"\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            assert message[\"type\"] == \"http.request\", message\n            body += message.get(\"body\", b\"\")", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "asgi.py"], "context_start_lineno": 112, "line_no": 113, "id": "datasette.utils.asgi.Request.post_body", "target_function_prompt": "        return self.scope.get(\"actor\", None)", "function_signature": "        return self.scope.get(\"actor\", None)"}}
{"prompt": "    def vegalite_major_version(self) -> int:", "metadata": {"task_id": "Scientific-Engineering/folium/2", "ground_truth": "        if \"$schema\" not in self.data:\n            return None\n\n        schema = self.data[\"$schema\"]\n\n        return int(schema.split(\"/\")[-1].split(\".\")[0].lstrip(\"v\"))", "fpath_tuple": ["Scientific-Engineering", "folium", "folium", "features.py"], "context_start_lineno": 322, "line_no": 323, "id": "folium.features.VegaLite.vegalite_major_version", "target_function_prompt": "    def vegalite_major_version(self) -> int:", "function_signature": "    def vegalite_major_version(self) -> int:"}}
{"prompt": "    def _generate_number(self, prefix: str, length: int) -> str:", "metadata": {"task_id": "Software-Development/Faker/19", "ground_truth": "        number = prefix\n        # Generate random char digits\n        number += \"#\" * (length - len(prefix) - 1)\n        number = self.numerify(number)\n        reverse = number[::-1]\n        # Calculate sum\n        tot = 0\n        pos = 0\n        while pos < length - 1:\n            tot += Provider.luhn_lookup[reverse[pos]]\n            if pos != (length - 2):\n                tot += int(reverse[pos + 1])\n            pos += 2\n        # Calculate check digit\n        check_digit = (10 - (tot % 10)) % 10\n        number += str(check_digit)\n        return number", "fpath_tuple": ["Software-Development", "Faker", "faker", "providers", "credit_card", "__init__.py"], "context_start_lineno": 189, "line_no": 196, "id": "faker.providers.credit_card.Provider._generate_number", "target_function_prompt": "    def _generate_number(self, prefix: str, length: int) -> str:", "function_signature": "    def _generate_number(self, prefix: str, length: int) -> str:"}}
{"prompt": "    def _cat_file(self, path):", "metadata": {"task_id": "System/mrjob/75", "ground_truth": "        from mrjob.cat import decompress\n        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            for chunk in decompress(f, path):\n                yield chunk", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "local.py"], "context_start_lineno": 51, "line_no": 52, "id": "mrjob.fs.local.LocalFilesystem._cat_file", "target_function_prompt": "    def _cat_file(self, path):", "function_signature": "    def _cat_file(self, path):"}}
{"prompt": "    async def peek(self, size=-1):", "metadata": {"task_id": "Internet/falcon/36", "ground_truth": "        if size < 0 or size > self._chunk_size:\n            size = self._chunk_size\n\n        if self._buffer_pos > 0:\n            self._trim_buffer()\n\n        if self._buffer_len < size:\n            async for chunk in self._source:\n                self._buffer += chunk\n                self._buffer_len = len(self._buffer)\n                if self._buffer_len >= size:  # pragma: no py39,py310 cover\n                    break\n\n        return self._buffer[:size]", "fpath_tuple": ["Internet", "falcon", "falcon", "asgi", "reader.py"], "context_start_lineno": 241, "line_no": 242, "id": "falcon.asgi.reader.BufferedReader.peek", "target_function_prompt": "    async def peek(self, size=-1):", "function_signature": "    async def peek(self, size=-1):"}}
{"prompt": "    def get_state(self, name):", "metadata": {"task_id": "Security/threatingestor/2", "ground_truth": "        logger.debug(f\"Getting state for '{name}'\")\n        self.cursor.execute('SELECT state FROM states WHERE name=?', (name,))\n        res = self.cursor.fetchone()\n        return res[0] if res else res", "fpath_tuple": ["Security", "threatingestor", "threatingestor", "state.py"], "context_start_lineno": 30, "line_no": 32, "id": "threatingestor.state.State.get_state", "target_function_prompt": "    def get_state(self, name):", "function_signature": "    def get_state(self, name):"}}
{"prompt": "def determine(value):", "metadata": {"task_id": "Multimedia/mingus/36", "ground_truth": "    i = -2\n    for v in base_values:\n        if value == v:\n            return (value, 0, 1, 1)\n        if value < v:\n            break\n        i += 1\n    scaled = float(value) / 2 ** i\n    if scaled >= 0.9375:  # base value\n        return (base_values[i], 0, 1, 1)\n    elif scaled >= 0.8125:\n        # septuplet: scaled = 0.875\n        return (base_values[i + 1], 0, 7, 4)\n    elif scaled >= 17 / 24.0:\n        # triplet: scaled = 0.75\n        return (base_values[i + 1], 0, 3, 2)\n    elif scaled >= 31 / 48.0:\n        # dotted note (one dot): scaled = 2/3.0\n        return (v, 1, 1, 1)\n    elif scaled >= 67 / 112.0:\n        # quintuplet: scaled = 0.625\n        return (base_values[i + 1], 0, 5, 4)\n    d = 3\n    for x in range(2, 5):\n        d += 2 ** x\n        if scaled == 2.0 ** x / d:\n            return (v, x, 1, 1)\n    return (base_values[i + 1], 0, 1, 1)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "value.py"], "context_start_lineno": 236, "line_no": 253, "id": "mingus.core.value.determine", "target_function_prompt": "def determine(value):", "function_signature": "def determine(value):"}}
{"prompt": "    def to_payload(self):", "metadata": {"task_id": "Communications/twilio-fatisar/22", "ground_truth": "        if self.params:\n            sorted_params = sorted([(k, v) for k, v in self.params.items()])\n            encoded_params = urlencode(sorted_params)\n            param_string = \"?{}\".format(encoded_params)\n        else:\n            param_string = \"\"\n        return \"scope:{}:{}{}\".format(self.service, self.privilege, param_string)", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "jwt", "client", "__init__.py"], "context_start_lineno": 106, "line_no": 107, "id": "twilio.jwt.client.ScopeURI.to_payload", "target_function_prompt": "    def to_payload(self):", "function_signature": "    def to_payload(self):"}}
{"prompt": "    def darwin_checker(self):", "metadata": {"task_id": "Utilities/python-for-android/24", "ground_truth": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 356, "line_no": 357, "id": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "target_function_prompt": "    def darwin_checker(self):", "function_signature": "    def darwin_checker(self):"}}
{"prompt": "def encode_string(string: str) -> bytes:", "metadata": {"task_id": "Communications/hbmqtt/2", "ground_truth": "    data = string.encode(encoding='utf-8')\n    data_length = len(data)\n    return int_to_bytes(data_length, 2) + data", "fpath_tuple": ["Communications", "hbmqtt", "hbmqtt", "codecs.py"], "context_start_lineno": 89, "line_no": 90, "id": "hbmqtt.codecs.encode_string", "target_function_prompt": "def encode_string(string: str) -> bytes:", "function_signature": "def encode_string(string: str) -> bytes:"}}
{"prompt": "    def candidates(self, word):", "metadata": {"task_id": "Text-Processing/pycorrector/4", "ground_truth": "        self.check_init()\n        return self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word}", "fpath_tuple": ["Text-Processing", "pycorrector", "pycorrector", "en_spell.py"], "context_start_lineno": 89, "line_no": 95, "id": "pycorrector.en_spell.EnSpell.candidates", "target_function_prompt": "    def candidates(self, word):", "function_signature": "    def candidates(self, word):"}}
{"prompt": "    def verify(self):", "metadata": {"task_id": "Communications/ehforwarderbot/3", "ground_truth": "        super().verify()\n        assert all(isinstance(member, ChatMember) for member in self.members), \\\n            f\"Some members of this chat is not a valid one: {self.members!r}\"", "fpath_tuple": ["Communications", "ehforwarderbot", "ehforwarderbot", "chat.py"], "context_start_lineno": 674, "line_no": 675, "id": "ehforwarderbot.chat.PrivateChat.verify", "target_function_prompt": "    def verify(self):", "function_signature": "    def verify(self):"}}
{"prompt": "    def get_bootstrap(cls, name, ctx):", "metadata": {"task_id": "Utilities/python-for-android/25", "ground_truth": "        if name is None:\n            return None\n        if not hasattr(cls, 'bootstraps'):\n            cls.bootstraps = {}\n        if name in cls.bootstraps:\n            return cls.bootstraps[name]\n        mod = importlib.import_module('pythonforandroid.bootstraps.{}'\n                                      .format(name))\n        if len(logger.handlers) > 1:\n            logger.removeHandler(logger.handlers[1])\n        bootstrap = mod.bootstrap\n        bootstrap.bootstrap_dir = join(ctx.root_dir, 'bootstraps', name)\n        bootstrap.ctx = ctx\n        return bootstrap", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "bootstrap.py"], "context_start_lineno": 297, "line_no": 303, "id": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "target_function_prompt": "    def get_bootstrap(cls, name, ctx):", "function_signature": "    def get_bootstrap(cls, name, ctx):"}}
{"prompt": "    def allow_event_stream(self, **kwargs):", "metadata": {"task_id": "Communications/twilio-fatisar/23", "ground_truth": "        scope = ScopeURI(\"stream\", \"subscribe\", {\"path\": \"/2010-04-01/Events\"})\n        if kwargs:\n            scope.add_param(\"params\", urlencode(kwargs, doseq=True))\n\n        self.capabilities[\"events\"] = scope", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "jwt", "client", "__init__.py"], "context_start_lineno": 75, "line_no": 79, "id": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "target_function_prompt": "    def allow_event_stream(self, **kwargs):", "function_signature": "    def allow_event_stream(self, **kwargs):"}}
{"prompt": "    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> PrimaryKeyConstraint:", "metadata": {"task_id": "Database/alembic/37", "ground_truth": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.primary_key_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 280, "line_no": 283, "id": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "target_function_prompt": "    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> PrimaryKeyConstraint:", "function_signature": "    def to_constraint(\n        self, migration_context: Optional[MigrationContext] = None\n    ) -> PrimaryKeyConstraint:"}}
{"prompt": "    def get_permission_object_id(self, request, object_id=None):", "metadata": {"task_id": "Internet/kinto/35", "ground_truth": "        object_uri = utils.strip_uri_prefix(request.path)\n\n        if self.on_plural_endpoint and object_id is not None:\n            # With the current request on a plural endpoint, the object URI must\n            # be found out by inspecting the \"plural\" service and its sibling\n            # \"object\" service. (see `register_resource()`)\n            matchdict = {**request.matchdict, \"id\": object_id}\n            try:\n                object_uri = utils.instance_uri(request, self.resource_name, **matchdict)\n                object_uri = object_uri.replace(\"%2A\", \"*\")\n            except KeyError:\n                # Maybe the resource has no single object endpoint.\n                # We consider that object URIs in permissions backend will\n                # be stored naively:\n                object_uri = f\"{object_uri}/{object_id}\"\n\n        return object_uri", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "authorization.py"], "context_start_lineno": 235, "line_no": 244, "id": "kinto.core.authorization.RouteFactory.get_permission_object_id", "target_function_prompt": "    def get_permission_object_id(self, request, object_id=None):", "function_signature": "    def get_permission_object_id(self, request, object_id=None):"}}
{"prompt": "    def from_ready_archive_status(xlog_dir):", "metadata": {"task_id": "System/wal-e/8", "ground_truth": "        status_dir = path.join(xlog_dir, 'archive_status')\n        statuses = os.listdir(status_dir)\n\n        # Try to send earliest segments first.\n        statuses.sort()\n\n        for status in statuses:\n            # Only bother with segments, not history files and such;\n            # it seems like special treatment of such quantities is\n            # more likely to change than that of the WAL segments,\n            # which are bulky and situated in a particular place for\n            # crash recovery.\n            match = re.match(storage.SEGMENT_READY_REGEXP, status)\n\n            if match:\n                seg_name = match.groupdict()['filename']\n                seg_path = path.join(xlog_dir, seg_name)\n\n                yield WalSegment(seg_path, explicit=False)", "fpath_tuple": ["System", "wal-e", "wal_e", "worker", "pg", "wal_transfer.py"], "context_start_lineno": 67, "line_no": 68, "id": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "target_function_prompt": "    def from_ready_archive_status(xlog_dir):", "function_signature": "    def from_ready_archive_status(xlog_dir):"}}
{"prompt": "def _load(files, defaults, overrides):", "metadata": {"task_id": "Multimedia/Mopidy/35", "ground_truth": "    from mopidy.internal import path\n    parser = configparser.RawConfigParser(inline_comment_prefixes=(\";\",))\n\n    # TODO: simply return path to config file for defaults so we can load it\n    # all in the same way?\n    logger.info(\"Loading config from builtin defaults\")\n    for default in defaults:\n        if isinstance(default, bytes):\n            default = default.decode()\n        parser.read_string(default)\n\n    # Load config from a series of config files\n    for f in files:\n        f = path.expand_path(f)\n        if f.is_dir():\n            for g in f.iterdir():\n                if g.is_file() and g.suffix == \".conf\":\n                    _load_file(parser, g.resolve())\n        else:\n            _load_file(parser, f.resolve())\n\n    raw_config = {}\n    for section in parser.sections():\n        raw_config[section] = dict(parser.items(section))\n\n    logger.info(\"Loading config from command line options\")\n    for section, key, value in overrides:\n        raw_config.setdefault(section, {})[key] = value\n\n    return raw_config", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "__init__.py"], "context_start_lineno": 150, "line_no": 151, "id": "mopidy.config._load", "target_function_prompt": "def _load(files, defaults, overrides):", "function_signature": "def _load(files, defaults, overrides):"}}
{"prompt": "def bernoulli_neg_exp(gamma, random_state=None):", "metadata": {"task_id": "Security/diffprivlib/19", "ground_truth": "    if gamma < 0:\n        raise ValueError(f\"Gamma must be non-negative, got {gamma}.\")\n\n    rng = check_random_state(random_state, True)\n\n    while gamma > 1:\n        gamma -= 1\n        if not bernoulli_neg_exp(1, rng):\n            return 0\n\n    counter = 1\n\n    while rng.random() <= gamma / counter:\n        counter += 1\n\n    return counter % 2", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "mechanisms", "base.py"], "context_start_lineno": 233, "line_no": 253, "id": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "target_function_prompt": "def bernoulli_neg_exp(gamma, random_state=None):", "function_signature": "def bernoulli_neg_exp(gamma, random_state=None):"}}
{"prompt": "    def get_histogram_counts(self, bins=None, **kw):", "metadata": {"task_id": "Utilities/boltons/78", "ground_truth": "        bin_digits = int(kw.pop('bin_digits', 1))\n        if kw:\n            raise TypeError('unexpected keyword arguments: %r' % kw.keys())\n\n        if not bins:\n            bins = self._get_bin_bounds()\n        else:\n            try:\n                bin_count = int(bins)\n            except TypeError:\n                try:\n                    bins = [float(x) for x in bins]\n                except Exception:\n                    raise ValueError('bins expected integer bin count or list'\n                                     ' of float bin boundaries, not %r' % bins)\n                if self.min < bins[0]:\n                    bins = [self.min] + bins\n            else:\n                bins = self._get_bin_bounds(bin_count)\n\n        # floor and ceil really should have taken ndigits, like round()\n        round_factor = 10.0 ** bin_digits\n        bins = [floor(b * round_factor) / round_factor for b in bins]\n        bins = sorted(set(bins))\n\n        idxs = [bisect.bisect(bins, d) - 1 for d in self.data]\n        count_map = {}  # would have used Counter, but py26 support\n        for idx in idxs:\n            try:\n                count_map[idx] += 1\n            except KeyError:\n                count_map[idx] = 1\n\n        bin_counts = [(b, count_map.get(i, 0)) for i, b in enumerate(bins)]\n\n        return bin_counts", "fpath_tuple": ["Utilities", "boltons", "boltons", "statsutils.py"], "context_start_lineno": 556, "line_no": 574, "id": "boltons.statsutils.Stats.get_histogram_counts", "target_function_prompt": "    def get_histogram_counts(self, bins=None, **kw):", "function_signature": "    def get_histogram_counts(self, bins=None, **kw):"}}
{"prompt": "    def encode(self, attr):", "metadata": {"task_id": "Internet/boto/91", "ground_truth": "        dynamodb_type = self._get_dynamodb_type(attr)\n        try:\n            encoder = getattr(self, '_encode_%s' % dynamodb_type.lower())\n        except AttributeError:\n            raise ValueError(\"Unable to encode dynamodb type: %s\" %\n                             dynamodb_type)\n        return {dynamodb_type: encoder(attr)}", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb", "types.py"], "context_start_lineno": 266, "line_no": 272, "id": "boto.dynamodb.types.Dynamizer.encode", "target_function_prompt": "    def encode(self, attr):", "function_signature": "    def encode(self, attr):"}}
{"prompt": "def unescape(data):", "metadata": {"task_id": "Text-Processing/dominate/4", "ground_truth": "  cc = re.compile(r'&(?:(?:#(\\d+))|([^;]+));')\n\n  result = []\n  m = cc.search(data)\n  while m:\n    result.append(data[0:m.start()])\n    d = m.group(1)\n    if d:\n      d = int(d)\n      result.append(unichr(d))\n    else:\n      d = _unescape.get(m.group(2), ord('?'))\n      result.append(unichr(d))\n\n    data = data[m.end():]\n    m = cc.search(data)\n\n  result.append(data)\n  return ''.join(result)", "fpath_tuple": ["Text-Processing", "dominate", "dominate", "util.py"], "context_start_lineno": 84, "line_no": 88, "id": "dominate.util.unescape", "target_function_prompt": "def unescape(data):", "function_signature": "def unescape(data):"}}
{"prompt": "    def can_play_notes(self, notes):", "metadata": {"task_id": "Multimedia/mingus/37", "ground_truth": "        if len(notes) > 6:\n            return False\n        return Instrument.can_play_notes(self, notes)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "instrument.py"], "context_start_lineno": 118, "line_no": 119, "id": "mingus.containers.instrument.Guitar.can_play_notes", "target_function_prompt": "    def can_play_notes(self, notes):", "function_signature": "    def can_play_notes(self, notes):"}}
{"prompt": "    def set_property(self, callable, name=None, reify=False):", "metadata": {"task_id": "Internet/pyramid/76", "ground_truth": "        InstancePropertyHelper.set_property(\n            self, callable, name=name, reify=reify\n        )", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "util.py"], "context_start_lineno": 195, "line_no": 247, "id": "pyramid.util.InstancePropertyMixin.set_property", "target_function_prompt": "    def set_property(self, callable, name=None, reify=False):", "function_signature": "    def set_property(self, callable, name=None, reify=False):"}}
{"prompt": "def nansum(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n           **unused_args):", "metadata": {"task_id": "Security/diffprivlib/20", "ground_truth": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "tools", "utils.py"], "context_start_lineno": 656, "line_no": 708, "id": "diffprivlib.tools.utils.nansum", "target_function_prompt": "def nansum(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n           **unused_args):", "function_signature": "def nansum(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n           **unused_args):"}}
{"prompt": "def import_from_string(import_str: str) -> typing.Any:", "metadata": {"task_id": "Internet/databases/0", "ground_truth": "    module_str, _, attrs_str = import_str.partition(\":\")\n    if not module_str or not attrs_str:\n        message = (\n            'Import string \"{import_str}\" must be in format \"<module>:<attribute>\".'\n        )\n        raise ImportFromStringError(message.format(import_str=import_str))\n\n    try:\n        module = importlib.import_module(module_str)\n    except ImportError as exc:\n        if exc.name != module_str:\n            raise exc from None\n        message = 'Could not import module \"{module_str}\".'\n        raise ImportFromStringError(message.format(module_str=module_str))\n\n    instance = module\n    try:\n        for attr_str in attrs_str.split(\".\"):\n            instance = getattr(instance, attr_str)\n    except AttributeError as exc:\n        message = 'Attribute \"{attrs_str}\" not found in module \"{module_str}\".'\n        raise ImportFromStringError(\n            message.format(attrs_str=attrs_str, module_str=module_str)\n        )\n\n    return instance", "fpath_tuple": ["Internet", "databases", "databases", "importer.py"], "context_start_lineno": 8, "line_no": 9, "id": "databases.importer.import_from_string", "target_function_prompt": "def import_from_string(import_str: str) -> typing.Any:", "function_signature": "def import_from_string(import_str: str) -> typing.Any:"}}
{"prompt": "def _render_constraint(\n    constraint: Constraint,\n    autogen_context: AutogenContext,\n    namespace_metadata: Optional[MetaData],\n) -> Optional[str]:", "metadata": {"task_id": "Database/alembic/38", "ground_truth": "    try:\n        renderer = _constraint_renderers.dispatch(constraint)\n    except ValueError:\n        util.warn(\"No renderer is established for object %r\" % constraint)\n        return \"[Unknown Python object %r]\" % constraint\n    else:\n        return renderer(constraint, autogen_context, namespace_metadata)", "fpath_tuple": ["Database", "alembic", "alembic", "autogenerate", "render.py"], "context_start_lineno": 883, "line_no": 888, "id": "alembic.autogenerate.render._render_constraint", "target_function_prompt": "def _render_constraint(\n    constraint: Constraint,\n    autogen_context: AutogenContext,\n    namespace_metadata: Optional[MetaData],\n) -> Optional[str]:", "function_signature": "def _render_constraint(\n    constraint: Constraint,\n    autogen_context: AutogenContext,\n    namespace_metadata: Optional[MetaData],\n) -> Optional[str]:"}}
{"prompt": "    def insert(self, index, item):", "metadata": {"task_id": "Utilities/boltons/79", "ground_truth": "        if len(self.lists) == 1:\n            self.lists[0].insert(index, item)\n            self._balance_list(0)\n        else:\n            list_idx, rel_idx = self._translate_index(index)\n            if list_idx is None:\n                raise IndexError()\n            self.lists[list_idx].insert(rel_idx, item)\n            self._balance_list(list_idx)\n        return", "fpath_tuple": ["Utilities", "boltons", "boltons", "listutils.py"], "context_start_lineno": 143, "line_no": 144, "id": "boltons.listutils.BarrelList.insert", "target_function_prompt": "    def insert(self, index, item):", "function_signature": "    def insert(self, index, item):"}}
{"prompt": "    def add_revision(self, revision: Revision, _replace: bool = False) -> None:", "metadata": {"task_id": "Database/alembic/39", "ground_truth": "        map_ = self._revision_map\n        if not _replace and revision.revision in map_:\n            util.warn(\n                \"Revision %s is present more than once\" % revision.revision\n            )\n        elif _replace and revision.revision not in map_:\n            raise Exception(\"revision %s not in map\" % revision.revision)\n\n        map_[revision.revision] = revision\n\n        revisions = [revision]\n        self._add_branches(revisions, map_)\n        self._map_branch_labels(revisions, map_)\n        self._add_depends_on(revisions, map_)\n\n        if revision.is_base:\n            self.bases += (revision.revision,)\n        if revision._is_real_base:\n            self._real_bases += (revision.revision,)\n\n        for downrev in revision._all_down_revisions:\n            if downrev not in map_:\n                util.warn(\n                    \"Revision %s referenced from %s is not present\"\n                    % (downrev, revision)\n                )\n            not_none(map_[downrev]).add_nextrev(revision)\n\n        self._normalize_depends_on(revisions, map_)\n\n        if revision._is_real_head:\n            self._real_heads = tuple(\n                head\n                for head in self._real_heads\n                if head\n                not in set(revision._all_down_revisions).union(\n                    [revision.revision]\n                )\n            ) + (revision.revision,)\n        if revision.is_head:\n            self.heads = tuple(\n                head\n                for head in self.heads\n                if head\n                not in set(revision._versioned_down_revisions).union(\n                    [revision.revision]\n                )\n            ) + (revision.revision,)", "fpath_tuple": ["Database", "alembic", "alembic", "script", "revision.py"], "context_start_lineno": 413, "line_no": 420, "id": "alembic.script.revision.RevisionMap.add_revision", "target_function_prompt": "    def add_revision(self, revision: Revision, _replace: bool = False) -> None:", "function_signature": "    def add_revision(self, revision: Revision, _replace: bool = False) -> None:"}}
{"prompt": "def parse_to_datetime(timestamp: bytes, normalise: bool = True) -> datetime:", "metadata": {"task_id": "Communications/IMAPClient/26", "ground_truth": "    time_tuple = parsedate_tz(_munge(timestamp))\n    if time_tuple is None:\n        raise ValueError(\"couldn't parse datetime %r\" % timestamp)\n\n    tz_offset_seconds = time_tuple[-1]\n    tz = None\n    if tz_offset_seconds is not None:\n        tz = FixedOffset(tz_offset_seconds / 60)\n\n    dt = datetime(*time_tuple[:6], tzinfo=tz)\n    if normalise and tz:\n        dt = datetime_to_native(dt)\n\n    return dt", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "datetime_util.py"], "context_start_lineno": 13, "line_no": 22, "id": "imapclient.datetime_util.parse_to_datetime", "target_function_prompt": "def parse_to_datetime(timestamp: bytes, normalise: bool = True) -> datetime:", "function_signature": "def parse_to_datetime(timestamp: bytes, normalise: bool = True) -> datetime:"}}
{"prompt": "def parse_authorization_code_response(uri, state=None):", "metadata": {"task_id": "Internet/Authlib/8", "ground_truth": "    query = urlparse.urlparse(uri).query\n    params = dict(urlparse.parse_qsl(query))\n\n    if 'code' not in params:\n        raise MissingCodeException()\n\n    params_state = params.get('state')\n    if state and params_state != state:\n        raise MismatchingStateException()\n\n    return params", "fpath_tuple": ["Internet", "Authlib", "authlib", "oauth2", "rfc6749", "parameters.py"], "context_start_lineno": 101, "line_no": 139, "id": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "target_function_prompt": "def parse_authorization_code_response(uri, state=None):", "function_signature": "def parse_authorization_code_response(uri, state=None):"}}
{"prompt": "    def parse(self, stream, media_type=None, parser_context=None):", "metadata": {"task_id": "Internet/djangorestframework/19", "ground_truth": "        from rest_framework.utils import json\n        parser_context = parser_context or {}\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n\n        try:\n            decoded_stream = codecs.getreader(encoding)(stream)\n            parse_constant = json.strict_constant if self.strict else None\n            return json.load(decoded_stream, parse_constant=parse_constant)\n        except ValueError as exc:\n            raise ParseError('JSON parse error - %s' % str(exc))", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "parsers.py"], "context_start_lineno": 53, "line_no": 57, "id": "rest_framework.parsers.JSONParser.parse", "target_function_prompt": "    def parse(self, stream, media_type=None, parser_context=None):", "function_signature": "    def parse(self, stream, media_type=None, parser_context=None):"}}
{"prompt": "def base64url_decode(input: Union[bytes, str]) -> bytes:", "metadata": {"task_id": "Utilities/PyJWT/4", "ground_truth": "    input_bytes = force_bytes(input)\n\n    rem = len(input_bytes) % 4\n\n    if rem > 0:\n        input_bytes += b\"=\" * (4 - rem)\n\n    return base64.urlsafe_b64decode(input_bytes)", "fpath_tuple": ["Utilities", "PyJWT", "jwt", "utils.py"], "context_start_lineno": 24, "line_no": 25, "id": "jwt.utils.base64url_decode", "target_function_prompt": "def base64url_decode(input: Union[bytes, str]) -> bytes:", "function_signature": "def base64url_decode(input: Union[bytes, str]) -> bytes:"}}
{"prompt": "    def iterate_revisions(\n        self,\n        upper: _RevisionIdentifierType,\n        lower: _RevisionIdentifierType,\n        implicit_base: bool = False,\n        inclusive: bool = False,\n        assert_relative_length: bool = True,\n        select_for_downgrade: bool = False,\n    ) -> Iterator[Revision]:", "metadata": {"task_id": "Database/alembic/40", "ground_truth": "        fn: Callable\n        if select_for_downgrade:\n            fn = self._collect_downgrade_revisions\n        else:\n            fn = self._collect_upgrade_revisions\n\n        revisions, heads = fn(\n            upper,\n            lower,\n            inclusive=inclusive,\n            implicit_base=implicit_base,\n            assert_relative_length=assert_relative_length,\n        )\n\n        for node in self._topological_sort(revisions, heads):\n            yield not_none(self.get_revision(node))", "fpath_tuple": ["Database", "alembic", "alembic", "script", "revision.py"], "context_start_lineno": 773, "line_no": 793, "id": "alembic.script.revision.RevisionMap.iterate_revisions", "target_function_prompt": "    def iterate_revisions(\n        self,\n        upper: _RevisionIdentifierType,\n        lower: _RevisionIdentifierType,\n        implicit_base: bool = False,\n        inclusive: bool = False,\n        assert_relative_length: bool = True,\n        select_for_downgrade: bool = False,\n    ) -> Iterator[Revision]:", "function_signature": "    def iterate_revisions(\n        self,\n        upper: _RevisionIdentifierType,\n        lower: _RevisionIdentifierType,\n        implicit_base: bool = False,\n        inclusive: bool = False,\n        assert_relative_length: bool = True,\n        select_for_downgrade: bool = False,\n    ) -> Iterator[Revision]:"}}
{"prompt": "    def update_recipients(self, write_box: ReadlineEdit) -> None:", "metadata": {"task_id": "Communications/zulip-term/24", "ground_truth": "        from zulipterminal.config.regexes import REGEX_RECIPIENT_EMAIL\n        self.recipient_emails = re.findall(REGEX_RECIPIENT_EMAIL, write_box.edit_text)\n        self._set_regular_and_typing_recipient_user_ids(\n            [self.model.user_dict[email][\"user_id\"] for email in self.recipient_emails]\n        )", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "boxes.py"], "context_start_lineno": 272, "line_no": 273, "id": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "target_function_prompt": "    def update_recipients(self, write_box: ReadlineEdit) -> None:", "function_signature": "    def update_recipients(self, write_box: ReadlineEdit) -> None:"}}
{"prompt": "    def touchz(self, path):", "metadata": {"task_id": "System/mrjob/76", "ground_truth": "        path = _from_file_uri(path)\n        if os.path.isfile(path) and os.path.getsize(path) != 0:\n            raise OSError('Non-empty file %r already exists!' % (path,))\n\n        # zero out the file\n        with open(path, 'w'):\n            pass", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "local.py"], "context_start_lineno": 82, "line_no": 83, "id": "mrjob.fs.local.LocalFilesystem.touchz", "target_function_prompt": "    def touchz(self, path):", "function_signature": "    def touchz(self, path):"}}
{"prompt": "def infer_positional_format_args(fstr):", "metadata": {"task_id": "Utilities/boltons/80", "ground_truth": "    ret, max_anon = '', 0\n    # look for {: or {! or {. or {[ or {}\n    start, end, prev_end = 0, 0, 0\n    for match in _pos_farg_re.finditer(fstr):\n        start, end, group = match.start(), match.end(), match.group()\n        if prev_end < start:\n            ret += fstr[prev_end:start]\n        prev_end = end\n        if group == '{{' or group == '}}':\n            ret += group\n            continue\n        ret += '{%s%s' % (max_anon, group[1:])\n        max_anon += 1\n    ret += fstr[prev_end:]\n    return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "formatutils.py"], "context_start_lineno": 122, "line_no": 130, "id": "boltons.formatutils.infer_positional_format_args", "target_function_prompt": "def infer_positional_format_args(fstr):", "function_signature": "def infer_positional_format_args(fstr):"}}
{"prompt": "    def create(filename, save_git_info=True):", "metadata": {"task_id": "Utilities/sacred/37", "ground_truth": "        if not filename or not os.path.exists(filename):\n            raise ValueError('invalid filename or file not found \"{}\"'.format(filename))\n\n        main_file = get_py_file_if_possible(os.path.abspath(filename))\n        repo, commit, is_dirty = get_commit_if_possible(main_file, save_git_info)\n        return Source(main_file, get_digest(main_file), repo, commit, is_dirty)", "fpath_tuple": ["Utilities", "sacred", "sacred", "dependencies.py"], "context_start_lineno": 454, "line_no": 455, "id": "sacred.dependencies.Source.create", "target_function_prompt": "    def create(filename, save_git_info=True):", "function_signature": "    def create(filename, save_git_info=True):"}}
{"prompt": "    def start(self):", "metadata": {"task_id": "Database/mssql-cli/16", "ground_truth": "        logger.debug('Json Rpc client started.')\n        self.request_thread = threading.Thread(\n            target=self._listen_for_request,\n            name=self.REQUEST_THREAD_NAME)\n        self.request_thread.daemon = True\n        self.request_thread.start()\n\n        self.response_thread = threading.Thread(\n            target=self._listen_for_response,\n            name=self.RESPONSE_THREAD_NAME)\n        self.response_thread.daemon = True\n        self.response_thread.start()", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "jsonrpc", "jsonrpcclient.py"], "context_start_lineno": 30, "line_no": 36, "id": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "target_function_prompt": "    def start(self):", "function_signature": "    def start(self):"}}
{"prompt": "def downgrade(\n    config: Config,\n    revision: str,\n    sql: bool = False,\n    tag: Optional[str] = None,\n) -> None:", "metadata": {"task_id": "Database/alembic/41", "ground_truth": "    script = ScriptDirectory.from_config(config)\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n    elif sql:\n        raise util.CommandError(\n            \"downgrade with --sql requires <fromrev>:<torev>\"\n        )\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "fpath_tuple": ["Database", "alembic", "alembic", "command.py"], "context_start_lineno": 400, "line_no": 420, "id": "alembic.command.downgrade", "target_function_prompt": "def downgrade(\n    config: Config,\n    revision: str,\n    sql: bool = False,\n    tag: Optional[str] = None,\n) -> None:", "function_signature": "def downgrade(\n    config: Config,\n    revision: str,\n    sql: bool = False,\n    tag: Optional[str] = None,\n) -> None:"}}
{"prompt": "    def buffer(self, data):", "metadata": {"task_id": "Utilities/boltons/81", "ground_truth": "        with self._send_lock:\n            self.sbuf.append(data)\n        return", "fpath_tuple": ["Utilities", "boltons", "boltons", "socketutils.py"], "context_start_lineno": 478, "line_no": 480, "id": "boltons.socketutils.BufferedSocket.buffer", "target_function_prompt": "    def buffer(self, data):", "function_signature": "    def buffer(self, data):"}}
{"prompt": "def prepare_grant_uri(uri, client_id, response_type, redirect_uri=None,\n                      scope=None, state=None, **kwargs):", "metadata": {"task_id": "Internet/Authlib/9", "ground_truth": "    from authlib.common.urls import add_params_to_uri\n    params = [\n        ('response_type', response_type),\n        ('client_id', client_id)\n    ]\n\n    if redirect_uri:\n        params.append(('redirect_uri', redirect_uri))\n    if scope:\n        params.append(('scope', list_to_scope(scope)))\n    if state:\n        params.append(('state', state))\n\n    for k in kwargs:\n        if kwargs[k] is not None:\n            params.append((to_unicode(k), kwargs[k]))\n\n    return add_params_to_uri(uri, params)", "fpath_tuple": ["Internet", "Authlib", "authlib", "oauth2", "rfc6749", "parameters.py"], "context_start_lineno": 6, "line_no": 40, "id": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "target_function_prompt": "def prepare_grant_uri(uri, client_id, response_type, redirect_uri=None,\n                      scope=None, state=None, **kwargs):", "function_signature": "def prepare_grant_uri(uri, client_id, response_type, redirect_uri=None,\n                      scope=None, state=None, **kwargs):"}}
{"prompt": "def histogram2d(array_x, array_y, epsilon=1.0, bins=10, range=None, weights=None, density=None, random_state=None,\n                accountant=None, **unused_args):", "metadata": {"task_id": "Security/diffprivlib/21", "ground_truth": "    warn_unused_args(unused_args)\n\n    try:\n        num_bins = len(bins)\n    except TypeError:\n        num_bins = 1\n\n    if num_bins not in (1, 2):\n        xedges = yedges = np.asarray(bins)\n        bins = [xedges, yedges]\n\n    hist, edges = histogramdd([array_x, array_y], epsilon=epsilon, bins=bins, range=range, weights=weights,\n                              density=density, random_state=random_state, accountant=accountant)\n    return hist, edges[0], edges[1]", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "tools", "histograms.py"], "context_start_lineno": 278, "line_no": 351, "id": "diffprivlib.tools.histograms.histogram2d", "target_function_prompt": "def histogram2d(array_x, array_y, epsilon=1.0, bins=10, range=None, weights=None, density=None, random_state=None,\n                accountant=None, **unused_args):", "function_signature": "def histogram2d(array_x, array_y, epsilon=1.0, bins=10, range=None, weights=None, density=None, random_state=None,\n                accountant=None, **unused_args):"}}
{"prompt": "    def config(self, function):", "metadata": {"task_id": "Utilities/sacred/38", "ground_truth": "        self.configurations.append(ConfigScope(function))\n        return self.configurations[-1]", "fpath_tuple": ["Utilities", "sacred", "sacred", "ingredient.py"], "context_start_lineno": 149, "line_no": 161, "id": "sacred.ingredient.Ingredient.config", "target_function_prompt": "    def config(self, function):", "function_signature": "    def config(self, function):"}}
{"prompt": "    def to_sentences(self, paragraph):", "metadata": {"task_id": "Internet/sumy/23", "ground_truth": "        from .._compat import unicode\n        if hasattr(self._sentence_tokenizer, '_params'):\n            extra_abbreviations = self.LANGUAGE_EXTRA_ABREVS.get(self._language, [])\n            self._sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)\n        sentences = self._sentence_tokenizer.tokenize(to_unicode(paragraph))\n        return tuple(map(unicode.strip, sentences))", "fpath_tuple": ["Internet", "sumy", "sumy", "nlp", "tokenizers.py"], "context_start_lineno": 185, "line_no": 186, "id": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "target_function_prompt": "    def to_sentences(self, paragraph):", "function_signature": "    def to_sentences(self, paragraph):"}}
{"prompt": "    def get_database(self, name=None, route=None):", "metadata": {"task_id": "Database/datasette/37", "ground_truth": "        if route is not None:\n            matches = [db for db in self.databases.values() if db.route == route]\n            if not matches:\n                raise KeyError\n            return matches[0]\n        if name is None:\n            # Return first database that isn't \"_internal\"\n            name = [key for key in self.databases.keys() if key != \"_internal\"][0]\n        return self.databases[name]", "fpath_tuple": ["Database", "datasette", "datasette", "app.py"], "context_start_lineno": 403, "line_no": 404, "id": "datasette.app.Datasette.get_database", "target_function_prompt": "    def get_database(self, name=None, route=None):", "function_signature": "    def get_database(self, name=None, route=None):"}}
{"prompt": "    def generate_x509_user_certificate(\n            self, user_key: 'SSHKey', subject: str,\n            issuer: Optional[str] = None, serial: Optional[int] = None,\n            principals: _CertPrincipals = (), valid_after: _Time = 0,\n            valid_before: _Time = 0xffffffffffffffff,\n            purposes: X509CertPurposes = 'secureShellClient',\n            hash_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHX509Certificate':", "metadata": {"task_id": "Security/asyncssh/10", "ground_truth": "        return self._generate_x509_certificate(user_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, principals, (),\n                                               hash_alg, comment)", "fpath_tuple": ["Security", "asyncssh", "asyncssh", "public_key.py"], "context_start_lineno": 822, "line_no": 887, "id": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "target_function_prompt": "    def generate_x509_user_certificate(\n            self, user_key: 'SSHKey', subject: str,\n            issuer: Optional[str] = None, serial: Optional[int] = None,\n            principals: _CertPrincipals = (), valid_after: _Time = 0,\n            valid_before: _Time = 0xffffffffffffffff,\n            purposes: X509CertPurposes = 'secureShellClient',\n            hash_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHX509Certificate':", "function_signature": "    def generate_x509_user_certificate(\n            self, user_key: 'SSHKey', subject: str,\n            issuer: Optional[str] = None, serial: Optional[int] = None,\n            principals: _CertPrincipals = (), valid_after: _Time = 0,\n            valid_before: _Time = 0xffffffffffffffff,\n            purposes: X509CertPurposes = 'secureShellClient',\n            hash_alg: DefTuple[str] = (),\n            comment: DefTuple[_Comment] = ()) -> 'SSHX509Certificate':"}}
{"prompt": "def _relative_object_uri(resource_name, object_uri):", "metadata": {"task_id": "Internet/kinto/36", "ground_truth": "    obj_parts = object_uri.split(\"/\")\n    for length in range(len(obj_parts) + 1):\n        parent_uri = \"/\".join(obj_parts[:length])\n        parent_resource_name, _ = _resource_endpoint(parent_uri)\n        if resource_name == parent_resource_name:\n            return parent_uri\n\n    error_msg = f\"Cannot get URL of resource '{resource_name}' from parent '{object_uri}'.\"\n    raise ValueError(error_msg)", "fpath_tuple": ["Internet", "kinto", "kinto", "authorization.py"], "context_start_lineno": 80, "line_no": 82, "id": "kinto.authorization._relative_object_uri", "target_function_prompt": "def _relative_object_uri(resource_name, object_uri):", "function_signature": "def _relative_object_uri(resource_name, object_uri):"}}
{"prompt": "def translate_jobconf_for_all_versions(variable):", "metadata": {"task_id": "System/mrjob/77", "ground_truth": "    return sorted(\n        set([variable] + list(_JOBCONF_MAP.get(variable, {}).values())))", "fpath_tuple": ["System", "mrjob", "mrjob", "compat.py"], "context_start_lineno": 669, "line_no": 672, "id": "mrjob.compat.translate_jobconf_for_all_versions", "target_function_prompt": "def translate_jobconf_for_all_versions(variable):", "function_signature": "def translate_jobconf_for_all_versions(variable):"}}
{"prompt": "def to_lines(chunks):", "metadata": {"task_id": "System/mrjob/78", "ground_truth": "    if hasattr(chunks, 'readline'):\n        return chunks\n    else:\n        return _to_lines(chunks)", "fpath_tuple": ["System", "mrjob", "mrjob", "util.py"], "context_start_lineno": 256, "line_no": 265, "id": "mrjob.util.to_lines", "target_function_prompt": "def to_lines(chunks):", "function_signature": "def to_lines(chunks):"}}
{"prompt": "def rarest_window_session(\n    session: List[Cmd],\n    prior_probs: StateMatrix,\n    trans_probs: StateMatrix,\n    param_cond_cmd_probs: StateMatrix,\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str,\n    end_token: str,\n    use_geo_mean=False,\n) -> Tuple[List[Cmd], float]:", "metadata": {"task_id": "Security/msticpy/12", "ground_truth": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n    if len(likelihoods) == 0:\n        return [], np.nan\n\n    min_lik = min(likelihoods)\n    ind = likelihoods.index(min_lik)\n    return session[ind : ind + window_len], min_lik  # noqa E203", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "cmds_params_only.py"], "context_start_lineno": 407, "line_no": 456, "id": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "target_function_prompt": "def rarest_window_session(\n    session: List[Cmd],\n    prior_probs: StateMatrix,\n    trans_probs: StateMatrix,\n    param_cond_cmd_probs: StateMatrix,\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str,\n    end_token: str,\n    use_geo_mean=False,\n) -> Tuple[List[Cmd], float]:", "function_signature": "def rarest_window_session(\n    session: List[Cmd],\n    prior_probs: StateMatrix,\n    trans_probs: StateMatrix,\n    param_cond_cmd_probs: StateMatrix,\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str,\n    end_token: str,\n    use_geo_mean=False,\n) -> Tuple[List[Cmd], float]:"}}
{"prompt": "    def deserialize(cls, value, *args, **kwargs):", "metadata": {"task_id": "Text-Processing/rows/10", "ground_truth": "        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")", "fpath_tuple": ["Text-Processing", "rows", "rows", "fields.py"], "context_start_lineno": 158, "line_no": 159, "id": "rows.fields.BoolField.deserialize", "target_function_prompt": "    def deserialize(cls, value, *args, **kwargs):", "function_signature": "    def deserialize(cls, value, *args, **kwargs):"}}
{"prompt": "    def deserialize(self, value):", "metadata": {"task_id": "Multimedia/Mopidy/36", "ground_truth": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        transformer = getattr(self, \"_transformer\", None)\n        if transformer:\n            transformed_value = transformer(value)\n            value = _TransformedValue(value, transformed_value)\n\n        validators.validate_choice(value, self._choices)\n        return value", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 97, "line_no": 98, "id": "mopidy.config.types.String.deserialize", "target_function_prompt": "    def deserialize(self, value):", "function_signature": "    def deserialize(self, value):"}}
{"prompt": "    def update(self, order, size, price, value, commission, pnl,\n               comminfo):", "metadata": {"task_id": "Software-Development/backtrader/0", "ground_truth": "        if not size:\n            return  # empty update, skip all other calculations\n\n        # Commission can only increase\n        self.commission += commission\n\n        # Update size and keep a reference for logic an calculations\n        oldsize = self.size\n        self.size += size  # size will carry the opposite sign if reducing\n\n        # Check if it has been currently opened\n        self.justopened = bool(not oldsize and size)\n\n        if self.justopened:\n            self.baropen = len(self.data)\n            self.dtopen = 0.0 if order.p.simulated else self.data.datetime[0]\n            self.long = self.size > 0\n\n        # Any size means the trade was opened\n        self.isopen = bool(self.size)\n\n        # Update current trade length\n        self.barlen = len(self.data) - self.baropen\n\n        # record if the position was closed (set to null)\n        self.isclosed = bool(oldsize and not self.size)\n\n        # record last bar for the trade\n        if self.isclosed:\n            self.isopen = False\n            self.barclose = len(self.data)\n            self.dtclose = self.data.datetime[0]\n\n            self.status = self.Closed\n        elif self.isopen:\n            self.status = self.Open\n\n        if abs(self.size) > abs(oldsize):\n            # position increased (be it positive or negative)\n            # update the average price\n            self.price = (oldsize * self.price + size * price) / self.size\n            pnl = 0.0\n\n        else:  # abs(self.size) < abs(oldsize)\n            # position reduced/closed\n            pnl = comminfo.profitandloss(-size, self.price, price)\n\n        self.pnl += pnl\n        self.pnlcomm = self.pnl - self.commission\n\n        self.value = comminfo.getvaluesize(self.size, self.price)\n\n        # Update the history if needed\n        if self.historyon:\n            dt0 = self.data.datetime[0] if not order.p.simulated else 0.0\n            histentry = TradeHistory(\n                self.status, dt0, self.barlen,\n                self.size, self.price, self.value,\n                self.pnl, self.pnlcomm, self.data._tz)\n            histentry.doupdate(order, size, price, commission)\n            self.history.append(histentry)", "fpath_tuple": ["Software-Development", "backtrader", "backtrader", "trade.py"], "context_start_lineno": 212, "line_no": 243, "id": "backtrader.trade.Trade.update", "target_function_prompt": "    def update(self, order, size, price, value, commission, pnl,\n               comminfo):", "function_signature": "    def update(self, order, size, price, value, commission, pnl,\n               comminfo):"}}
{"prompt": "    def add_system_member(self, name: str = \"\", alias: Optional[str] = None, id: ChatID = ChatID(\"\"),\n                          uid: ChatID = ChatID(\"\"),\n                          vendor_specific: Dict[str, Any] = None, description: str = \"\",\n                          middleware: Optional[Middleware] = None) -> SystemChatMember:", "metadata": {"task_id": "Communications/ehforwarderbot/4", "ground_truth": "        member = self.make_system_member(name=name, alias=alias, id=id, uid=uid,\n                                         vendor_specific=vendor_specific, description=description,\n                                         middleware=middleware)\n        self.members.append(member)\n        return member", "fpath_tuple": ["Communications", "ehforwarderbot", "ehforwarderbot", "chat.py"], "context_start_lineno": 566, "line_no": 591, "id": "ehforwarderbot.chat.Chat.add_system_member", "target_function_prompt": "    def add_system_member(self, name: str = \"\", alias: Optional[str] = None, id: ChatID = ChatID(\"\"),\n                          uid: ChatID = ChatID(\"\"),\n                          vendor_specific: Dict[str, Any] = None, description: str = \"\",\n                          middleware: Optional[Middleware] = None) -> SystemChatMember:", "function_signature": "    def add_system_member(self, name: str = \"\", alias: Optional[str] = None, id: ChatID = ChatID(\"\"),\n                          uid: ChatID = ChatID(\"\"),\n                          vendor_specific: Dict[str, Any] = None, description: str = \"\",\n                          middleware: Optional[Middleware] = None) -> SystemChatMember:"}}
{"prompt": "    def command(self, function=None, prefix=None, unobserved=False):", "metadata": {"task_id": "Utilities/sacred/39", "ground_truth": "        captured_f = self.capture(function, prefix=prefix)\n        captured_f.unobserved = unobserved\n        self.commands[function.__name__] = captured_f\n        return captured_f", "fpath_tuple": ["Utilities", "sacred", "sacred", "ingredient.py"], "context_start_lineno": 129, "line_no": 144, "id": "sacred.ingredient.Ingredient.command", "target_function_prompt": "    def command(self, function=None, prefix=None, unobserved=False):", "function_signature": "    def command(self, function=None, prefix=None, unobserved=False):"}}
{"prompt": "    def __getstate__(self):", "metadata": {"task_id": "Security/diffprivlib/22", "ground_truth": "        d = {\"max_depth\": self.max_depth,\n             \"node_count\": self.node_count,\n             \"nodes\": np.array([tuple(node) for node in self.nodes], dtype=NODE_DTYPE),\n             \"values\": self.values_}\n        return d", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "models", "forest.py"], "context_start_lineno": 502, "line_no": 504, "id": "diffprivlib.models.forest._FittingTree.__getstate__", "target_function_prompt": "    def __getstate__(self):", "function_signature": "    def __getstate__(self):"}}
{"prompt": "    def next(self):", "metadata": {"task_id": "Communications/hl7/4", "ground_truth": "        if len(self.containers) > 1:\n            # Return a new instance of this class using the tails of\n            # the separators and containers lists. Use self.__class__()\n            # in case :class:`hl7.ParsePlan` is subclassed\n            return self.__class__(\n                self.separators[self.separators.find(self.separator) + 1],\n                self.separators,\n                self.containers[1:],\n                self.esc,\n                self.factory,\n            )\n        # When we have no separators and containers left, return None,\n        # which indicates that we have nothing further.\n        return None", "fpath_tuple": ["Communications", "hl7", "hl7", "parser.py"], "context_start_lineno": 411, "line_no": 416, "id": "hl7.parser._ParsePlan.next", "target_function_prompt": "    def next(self):", "function_signature": "    def next(self):"}}
{"prompt": "    def idle(self):", "metadata": {"task_id": "Communications/IMAPClient/27", "ground_truth": "        self._idle_tag = self._imap._command(\"IDLE\")\n        resp = self._imap._get_response()\n        if resp is not None:\n            raise exceptions.IMAPClientError(\"Unexpected IDLE response: %s\" % resp)", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 888, "line_no": 903, "id": "imapclient.imapclient.IMAPClient.idle", "target_function_prompt": "    def idle(self):", "function_signature": "    def idle(self):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/92", "ground_truth": "    from boto.regioninfo import connect\n    return connect('cloudformation', region_name,\n                   connection_cls=CloudFormationConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "cloudformation", "__init__.py"], "context_start_lineno": 42, "line_no": 53, "id": "boto.cloudformation.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def __repr__(self):", "metadata": {"task_id": "Communications/Wikipedia-API/11", "ground_truth": "        return \"Section: {} ({}):\\n{}\\nSubsections ({}):\\n{}\".format(\n            self._title,\n            self._level,\n            self._text,\n            len(self._section),\n            \"\\n\".join(map(repr, self._section)),\n        )", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 776, "line_no": 777, "id": "wikipediaapi.WikipediaPageSection.__repr__", "target_function_prompt": "    def __repr__(self):", "function_signature": "    def __repr__(self):"}}
{"prompt": "def _render_tokens(\n    *,\n    tokens: List[_PrettyToken],\n    font_bold: Optional[Callable[[str], str]] = None,\n    font_dim: Optional[Callable[[str], str]] = None,\n    font_red: Optional[Callable[[str], str]] = None,\n    font_blue: Optional[Callable[[str], str]] = None,\n    font_normal: Optional[Callable[[str], str]] = None,\n) -> str:", "metadata": {"task_id": "Text-Processing/online-judge-tools/2", "ground_truth": "    if font_bold is None:\n        font_bold = lambda s: colorama.Style.BRIGHT + s + colorama.Style.RESET_ALL\n    if font_dim is None:\n        font_dim = lambda s: colorama.Style.DIM + s + colorama.Style.RESET_ALL\n    if font_red is None:\n        font_red = lambda s: colorama.Fore.RED + s + colorama.Style.RESET_ALL\n    if font_blue is None:\n        font_blue = lambda s: colorama.Fore.CYAN + s + colorama.Style.RESET_ALL\n    if font_normal is None:\n        font_normal = lambda s: s\n\n    result = []\n    for key, value in tokens:\n        if key == _PrettyTokenType.BODY:\n            value = font_bold(value)\n        elif key == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            value = font_red(value)\n        elif key == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            value = font_red(value)\n        elif key == _PrettyTokenType.WHITESPACE:\n            value = font_dim(_replace_whitespace(value))\n        elif key == _PrettyTokenType.NEWLINE:\n            value = font_dim(_replace_whitespace(value))\n        elif key == _PrettyTokenType.HINT:\n            value = font_dim(value)\n        elif key == _PrettyTokenType.LINENO:\n            value = font_blue(value)\n        elif key == _PrettyTokenType.OTHERS:\n            value = font_normal(value)\n        else:\n            assert False\n        result.append(value)\n    return ''.join(result)", "fpath_tuple": ["Text-Processing", "online-judge-tools", "onlinejudge_command", "pretty_printers.py"], "context_start_lineno": 162, "line_no": 174, "id": "onlinejudge_command.pretty_printers._render_tokens", "target_function_prompt": "def _render_tokens(\n    *,\n    tokens: List[_PrettyToken],\n    font_bold: Optional[Callable[[str], str]] = None,\n    font_dim: Optional[Callable[[str], str]] = None,\n    font_red: Optional[Callable[[str], str]] = None,\n    font_blue: Optional[Callable[[str], str]] = None,\n    font_normal: Optional[Callable[[str], str]] = None,\n) -> str:", "function_signature": "def _render_tokens(\n    *,\n    tokens: List[_PrettyToken],\n    font_bold: Optional[Callable[[str], str]] = None,\n    font_dim: Optional[Callable[[str], str]] = None,\n    font_red: Optional[Callable[[str], str]] = None,\n    font_blue: Optional[Callable[[str], str]] = None,\n    font_normal: Optional[Callable[[str], str]] = None,\n) -> str:"}}
{"prompt": "def _connectable_has_table(\n    connectable: Connection, tablename: str, schemaname: Union[str, None]\n) -> bool:", "metadata": {"task_id": "Database/alembic/42", "ground_truth": "    if sqla_14:\n        return inspect(connectable).has_table(tablename, schemaname)\n    else:\n        return connectable.dialect.has_table(\n            connectable, tablename, schemaname\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "util", "sqla_compat.py"], "context_start_lineno": 274, "line_no": 277, "id": "alembic.util.sqla_compat._connectable_has_table", "target_function_prompt": "def _connectable_has_table(\n    connectable: Connection, tablename: str, schemaname: Union[str, None]\n) -> bool:", "function_signature": "def _connectable_has_table(\n    connectable: Connection, tablename: str, schemaname: Union[str, None]\n) -> bool:"}}
{"prompt": "    def put_item(self, data, overwrite=False):", "metadata": {"task_id": "Internet/boto/93", "ground_truth": "        self._to_put.append(data)\n\n        if self.should_flush():\n            self.flush()", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 1644, "line_no": 1645, "id": "boto.dynamodb2.table.BatchTable.put_item", "target_function_prompt": "    def put_item(self, data, overwrite=False):", "function_signature": "    def put_item(self, data, overwrite=False):"}}
{"prompt": "    def host(self):", "metadata": {"task_id": "Security/pycoin/29", "ground_truth": "        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin[-4:])\n        return ip_bin_to_ip6_addr(self.ip_bin)", "fpath_tuple": ["Security", "pycoin", "pycoin", "message", "PeerAddress.py"], "context_start_lineno": 34, "line_no": 35, "id": "pycoin.message.PeerAddress.PeerAddress.host", "target_function_prompt": "    def host(self):", "function_signature": "    def host(self):"}}
{"prompt": "def sum(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n        **unused_args):", "metadata": {"task_id": "Security/diffprivlib/23", "ground_truth": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "tools", "utils.py"], "context_start_lineno": 598, "line_no": 650, "id": "diffprivlib.tools.utils.sum", "target_function_prompt": "def sum(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n        **unused_args):", "function_signature": "def sum(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n        **unused_args):"}}
{"prompt": "    def detach(self, force=False, dry_run=False):", "metadata": {"task_id": "Internet/boto/94", "ground_truth": "        attachment_id = getattr(self.attachment, 'id', None)\n\n        return self.connection.detach_network_interface(\n            attachment_id,\n            force,\n            dry_run=dry_run\n        )", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "networkinterface.py"], "context_start_lineno": 213, "line_no": 224, "id": "boto.ec2.networkinterface.NetworkInterface.detach", "target_function_prompt": "    def detach(self, force=False, dry_run=False):", "function_signature": "    def detach(self, force=False, dry_run=False):"}}
{"prompt": "def skip(roman_numeral, skip_count=1):", "metadata": {"task_id": "Multimedia/mingus/38", "ground_truth": "    i = numerals.index(roman_numeral) + skip_count\n    return numerals[i % 7]", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "progressions.py"], "context_start_lineno": 524, "line_no": 535, "id": "mingus.core.progressions.skip", "target_function_prompt": "def skip(roman_numeral, skip_count=1):", "function_signature": "def skip(roman_numeral, skip_count=1):"}}
{"prompt": "    def clear(self):", "metadata": {"task_id": "Utilities/boltons/82", "ground_truth": "        dict.clear(self)\n        dict.clear(self.inv)", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 848, "line_no": 849, "id": "boltons.dictutils.OneToOne.clear", "target_function_prompt": "    def clear(self):", "function_signature": "    def clear(self):"}}
{"prompt": "    def unauthenticated_userid(self, request):", "metadata": {"task_id": "Internet/pyramid/77", "ground_truth": "        identity = self._get_identity(request)\n        if identity is None:\n            return None\n        return identity['repoze.who.userid']", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "authentication.py"], "context_start_lineno": 266, "line_no": 268, "id": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "target_function_prompt": "    def unauthenticated_userid(self, request):", "function_signature": "    def unauthenticated_userid(self, request):"}}
{"prompt": "    def real_loader(self):", "metadata": {"task_id": "Internet/pyramid/78", "ground_truth": "        if self._real_loader is None:\n            raise NotImplementedError()\n        return self._real_loader", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "assets.py"], "context_start_lineno": 160, "line_no": 161, "id": "pyramid.config.assets.PackageOverrides.real_loader", "target_function_prompt": "    def real_loader(self):", "function_signature": "    def real_loader(self):"}}
{"prompt": "    def process(self, instance: _Traversable):", "metadata": {"task_id": "Internet/falcon/37", "ground_truth": "        try:\n            return getattr(self, 'visit_{}'.format(instance.__visit_name__))(instance)\n        except AttributeError as e:\n            raise RuntimeError(\n                'This visitor does not support {}'.format(type(instance))\n            ) from e", "fpath_tuple": ["Internet", "falcon", "falcon", "inspect.py"], "context_start_lineno": 551, "line_no": 560, "id": "falcon.inspect.InspectVisitor.process", "target_function_prompt": "    def process(self, instance: _Traversable):", "function_signature": "    def process(self, instance: _Traversable):"}}
{"prompt": "def copy(src, dst):", "metadata": {"task_id": "Utilities/mackup/4", "ground_truth": "    assert isinstance(src, str)\n    assert os.path.exists(src)\n    assert isinstance(dst, str)\n\n    # Create the path to the dst file if it does not exist\n    abs_path = os.path.dirname(os.path.abspath(dst))\n    if not os.path.isdir(abs_path):\n        os.makedirs(abs_path)\n\n    # We need to copy a single file\n    if os.path.isfile(src):\n        # Copy the src file to dst\n        shutil.copy(src, dst)\n\n    # We need to copy a whole folder\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n\n    # What the heck is this?\n    else:\n        raise ValueError(\"Unsupported file: {}\".format(src))\n\n    # Set the good mode to the file or folder recursively\n    chmod(dst)", "fpath_tuple": ["Utilities", "mackup", "mackup", "utils.py"], "context_start_lineno": 70, "line_no": 88, "id": "mackup.utils.copy", "target_function_prompt": "def copy(src, dst):", "function_signature": "def copy(src, dst):"}}
{"prompt": "    def batch_to_payloads(\n        cls,\n        batch: list[t.Any],\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/29", "ground_truth": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 537, "line_no": 543, "id": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "target_function_prompt": "    def batch_to_payloads(\n        cls,\n        batch: list[t.Any],\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:", "function_signature": "    def batch_to_payloads(\n        cls,\n        batch: list[t.Any],\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/95", "ground_truth": "    from boto.regioninfo import connect\n    from boto.codedeploy.layer1 import CodeDeployConnection\n    return connect('codedeploy', region_name,\n                   connection_cls=CodeDeployConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "codedeploy", "__init__.py"], "context_start_lineno": 36, "line_no": 37, "id": "boto.codedeploy.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def check_csrf_token(self, request, supplied_token):", "metadata": {"task_id": "Internet/pyramid/79", "ground_truth": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "csrf.py"], "context_start_lineno": 42, "line_no": 44, "id": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "target_function_prompt": "    def check_csrf_token(self, request, supplied_token):", "function_signature": "    def check_csrf_token(self, request, supplied_token):"}}
{"prompt": "def extract_header(header_segment, error_cls):", "metadata": {"task_id": "Internet/Authlib/10", "ground_truth": "    header_data = extract_segment(header_segment, error_cls, 'header')\n\n    try:\n        header = json_loads(header_data.decode('utf-8'))\n    except ValueError as e:\n        raise error_cls('Invalid header string: {}'.format(e))\n\n    if not isinstance(header, dict):\n        raise error_cls('Header must be a json object')\n    return header", "fpath_tuple": ["Internet", "Authlib", "authlib", "jose", "util.py"], "context_start_lineno": 5, "line_no": 6, "id": "authlib.jose.util.extract_header", "target_function_prompt": "def extract_header(header_segment, error_cls):", "function_signature": "def extract_header(header_segment, error_cls):"}}
{"prompt": "def datetime_to_INTERNALDATE(dt: datetime) -> str:", "metadata": {"task_id": "Communications/IMAPClient/28", "ground_truth": "    if not dt.tzinfo:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n    fmt = \"%d-\" + _SHORT_MONTHS[dt.month] + \"-%Y %H:%M:%S %z\"\n    return dt.strftime(fmt)", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "datetime_util.py"], "context_start_lineno": 42, "line_no": 48, "id": "imapclient.datetime_util.datetime_to_INTERNALDATE", "target_function_prompt": "def datetime_to_INTERNALDATE(dt: datetime) -> str:", "function_signature": "def datetime_to_INTERNALDATE(dt: datetime) -> str:"}}
{"prompt": "    def sms(\n        self,\n        message,\n        to=None,\n        from_=None,\n        action=None,\n        method=None,\n        status_callback=None,\n        **kwargs\n    ):", "metadata": {"task_id": "Communications/twilio-fatisar/24", "ground_truth": "        return self.nest(\n            Sms(\n                message,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "twiml", "voice_response.py"], "context_start_lineno": 390, "line_no": 413, "id": "twilio.twiml.voice_response.VoiceResponse.sms", "target_function_prompt": "    def sms(\n        self,\n        message,\n        to=None,\n        from_=None,\n        action=None,\n        method=None,\n        status_callback=None,\n        **kwargs\n    ):", "function_signature": "    def sms(\n        self,\n        message,\n        to=None,\n        from_=None,\n        action=None,\n        method=None,\n        status_callback=None,\n        **kwargs\n    ):"}}
{"prompt": "def try_ipv6_socket() -> bool:", "metadata": {"task_id": "Multimedia/Mopidy/37", "ground_truth": "    if not socket.has_ipv6:\n        return False\n    try:\n        socket.socket(socket.AF_INET6).close()\n        return True\n    except OSError as exc:\n        logger.debug(\n            f\"Platform supports IPv6, but socket creation failed, \"\n            f\"disabling: {exc}\"\n        )\n    return False", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "internal", "network.py"], "context_start_lineno": 7, "line_no": 9, "id": "mopidy.internal.network.try_ipv6_socket", "target_function_prompt": "def try_ipv6_socket() -> bool:", "function_signature": "def try_ipv6_socket() -> bool:"}}
{"prompt": "    def headers(self):\n        # NOTE(kgriffs: First time here will cache the dict so all we\n        # have to do is clone it in the future.", "metadata": {"task_id": "Internet/falcon/38", "ground_truth": "        if self._cached_headers is None:\n            headers = self._cached_headers = {}\n\n            env = self.env\n            for name, value in env.items():\n                if name.startswith('HTTP_'):\n                    # NOTE(kgriffs): Don't take the time to fix the case\n                    # since headers are supposed to be case-insensitive\n                    # anyway.\n                    headers[name[5:].replace('_', '-')] = value\n\n                elif name in WSGI_CONTENT_HEADERS:\n                    headers[name.replace('_', '-')] = value\n\n        return self._cached_headers", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 877, "line_no": 880, "id": "falcon.request.Request.headers", "target_function_prompt": "    def headers(self):\n        # NOTE(kgriffs: First time here will cache the dict so all we\n        # have to do is clone it in the future.", "function_signature": "    def headers(self):\n        # NOTE(kgriffs: First time here will cache the dict so all we\n        # have to do is clone it in the future."}}
{"prompt": "def nanvar(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n           **unused_args):", "metadata": {"task_id": "Security/diffprivlib/24", "ground_truth": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "tools", "utils.py"], "context_start_lineno": 363, "line_no": 420, "id": "diffprivlib.tools.utils.nanvar", "target_function_prompt": "def nanvar(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n           **unused_args):", "function_signature": "def nanvar(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n           **unused_args):"}}
{"prompt": "def get_sane_embedded_filenames(filename, src_path, tmp_path, max_len,\n                                noname_index):", "metadata": {"task_id": "Security/oletools/4", "ground_truth": "    suffixes = []\n    candidates_without_suffix = []  # remember these as fallback\n    for candidate in (filename, src_path, tmp_path):\n        # remove path component. Could be from linux, mac or windows\n        idx = max(candidate.rfind('/'), candidate.rfind('\\\\'))\n        candidate = candidate[idx+1:].strip()\n\n        # sanitize\n        candidate = sanitize_filename(candidate, max_len=max_len)\n\n        if not candidate:\n            continue    # skip whitespace-only\n\n        # identify suffix. Dangerous suffixes are all short\n        idx = candidate.rfind('.')\n        if idx is -1:\n            candidates_without_suffix.append(candidate)\n            continue\n        elif idx < len(candidate)-5:\n            candidates_without_suffix.append(candidate)\n            continue\n\n        # remember suffix\n        suffixes.append(candidate[idx:])\n\n        yield candidate\n\n    # parts with suffix not good enough? try those without one\n    for candidate in candidates_without_suffix:\n        yield candidate\n\n    # then try random\n    suffixes.append('')  # ensure there is something in there\n    for _ in range(MAX_FILENAME_ATTEMPTS):\n        for suffix in suffixes:\n            leftover_len = max_len - len(suffix)\n            if leftover_len < 1:\n                continue\n            name = ''.join(random.sample('abcdefghijklmnopqrstuvwxyz',\n                                         min(26, leftover_len)))\n            yield name + suffix\n\n    # still not returned? Then we have to make up a name ourselves\n    # do not care any more about max_len (maybe it was 0 or negative)\n    yield 'oleobj_%03d' % noname_index", "fpath_tuple": ["Security", "oletools", "oletools", "oleobj.py"], "context_start_lineno": 549, "line_no": 565, "id": "oletools.oleobj.get_sane_embedded_filenames", "target_function_prompt": "def get_sane_embedded_filenames(filename, src_path, tmp_path, max_len,\n                                noname_index):", "function_signature": "def get_sane_embedded_filenames(filename, src_path, tmp_path, max_len,\n                                noname_index):"}}
{"prompt": "    def create_concrete(self):", "metadata": {"task_id": "Communications/chatette/18", "ground_truth": "        from chatette.units.modifiable.unit_reference import UnitReference\n        self._check_information()\n        return UnitReference(\n            self.identifier, self.type,\n            self.leading_space, self._build_modifiers_repr()\n        )", "fpath_tuple": ["Communications", "chatette", "chatette", "parsing", "__init__.py"], "context_start_lineno": 109, "line_no": 110, "id": "chatette.parsing.UnitRefBuilder.create_concrete", "target_function_prompt": "    def create_concrete(self):", "function_signature": "    def create_concrete(self):"}}
{"prompt": "    def _process_response_callbacks(self, response):", "metadata": {"task_id": "Internet/pyramid/80", "ground_truth": "        callbacks = self.response_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self, response)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "request.py"], "context_start_lineno": 75, "line_no": 76, "id": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "target_function_prompt": "    def _process_response_callbacks(self, response):", "function_signature": "    def _process_response_callbacks(self, response):"}}
{"prompt": "def join(*paths):\n    # type: (*Text) -> Text", "metadata": {"task_id": "System/fs/26", "ground_truth": "    absolute = False\n    relpaths = []  # type: List[Text]\n    for p in paths:\n        if p:\n            if p[0] == \"/\":\n                del relpaths[:]\n                absolute = True\n            relpaths.append(p)\n\n    path = normpath(\"/\".join(relpaths))\n    if absolute:\n        path = abspath(path)\n    return path", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 209, "line_no": 228, "id": "fs.path.join", "target_function_prompt": "def join(*paths):\n    # type: (*Text) -> Text", "function_signature": "def join(*paths):\n    # type: (*Text) -> Text"}}
{"prompt": "    def get_value(self, dictionary):", "metadata": {"task_id": "Internet/djangorestframework/20", "ground_truth": "        if self.field_name not in dictionary:\n            if getattr(self.root, 'partial', False):\n                return empty\n        # We override the default field access in order to support\n        # lists in HTML forms.\n        if html.is_html_input(dictionary):\n            return dictionary.getlist(self.field_name)\n        return dictionary.get(self.field_name, empty)", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "fields.py"], "context_start_lineno": 1428, "line_no": 1429, "id": "rest_framework.fields.MultipleChoiceField.get_value", "target_function_prompt": "    def get_value(self, dictionary):", "function_signature": "    def get_value(self, dictionary):"}}
{"prompt": "def _validate(raw_config, schemas):\n    # Get validated config", "metadata": {"task_id": "Multimedia/Mopidy/38", "ground_truth": "    config = {}\n    errors = {}\n    sections = set(raw_config)\n    for schema in schemas:\n        sections.discard(schema.name)\n        values = raw_config.get(schema.name, {})\n        result, error = schema.deserialize(values)\n        if error:\n            errors[schema.name] = error\n        if result:\n            config[schema.name] = result\n\n    for section in sections:\n        logger.warning(\n            f\"Ignoring config section {section!r} \"\n            f\"because no matching extension was found\"\n        )\n\n    return config, errors", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "__init__.py"], "context_start_lineno": 218, "line_no": 220, "id": "mopidy.config._validate", "target_function_prompt": "def _validate(raw_config, schemas):\n    # Get validated config", "function_signature": "def _validate(raw_config, schemas):\n    # Get validated config"}}
{"prompt": "    def registerSubscriptionAdapter(self, *arg, **kw):", "metadata": {"task_id": "Internet/pyramid/81", "ground_truth": "        result = Components.registerSubscriptionAdapter(self, *arg, **kw)\n        self.has_listeners = True\n        return result", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "registry.py"], "context_start_lineno": 67, "line_no": 68, "id": "pyramid.registry.Registry.registerSubscriptionAdapter", "target_function_prompt": "    def registerSubscriptionAdapter(self, *arg, **kw):", "function_signature": "    def registerSubscriptionAdapter(self, *arg, **kw):"}}
{"prompt": "    def to_statement(self):", "metadata": {"task_id": "Security/trailscraper/9", "ground_truth": "        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n\n        if self.event_source == \"apigateway.amazonaws.com\":\n            return self._to_api_gateway_statement()\n\n        return Statement(\n            Effect=\"Allow\",\n            Action=[Action(self._source_to_iam_prefix(), self._event_name_to_iam_action())],\n            Resource=sorted(self.resource_arns)\n        )", "fpath_tuple": ["Security", "trailscraper", "trailscraper", "cloudtrail.py"], "context_start_lineno": 154, "line_no": 156, "id": "trailscraper.cloudtrail.Record.to_statement", "target_function_prompt": "    def to_statement(self):", "function_signature": "    def to_statement(self):"}}
{"prompt": "    def get_key_fields(self):", "metadata": {"task_id": "Internet/boto/96", "ground_truth": "        if not self.schema:\n            # We don't know the structure of the table. Get a description to\n            # populate the schema.\n            self.describe()\n\n        return [field.name for field in self.schema]", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 918, "line_no": 938, "id": "boto.dynamodb2.table.Table.get_key_fields", "target_function_prompt": "    def get_key_fields(self):", "function_signature": "    def get_key_fields(self):"}}
{"prompt": "    def submit_request(self, method, params, request_id=None):", "metadata": {"task_id": "Database/mssql-cli/17", "ground_truth": "        if not method or not params:\n            raise ValueError(u'Method or Parameter was not found in request')\n\n        request = {u'method': method, u'params': params, u'id': request_id}\n        self.request_queue.put(request)", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "jsonrpc", "jsonrpcclient.py"], "context_start_lineno": 49, "line_no": 53, "id": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "target_function_prompt": "    def submit_request(self, method, params, request_id=None):", "function_signature": "    def submit_request(self, method, params, request_id=None):"}}
{"prompt": "def check_fingerprint(path):", "metadata": {"task_id": "Software-Development/dash/14", "ground_truth": "    path_parts = path.split(\"/\")\n    name_parts = path_parts[-1].split(\".\")\n\n    # Check if the resource has a fingerprint\n    if len(name_parts) > 2 and cache_regex.match(name_parts[1]):\n        original_name = \".\".join([name_parts[0]] + name_parts[2:])\n        return \"/\".join(path_parts[:-1] + [original_name]), True\n\n    return path, False", "fpath_tuple": ["Software-Development", "dash", "dash", "fingerprint.py"], "context_start_lineno": 15, "line_no": 16, "id": "dash.fingerprint.check_fingerprint", "target_function_prompt": "def check_fingerprint(path):", "function_signature": "def check_fingerprint(path):"}}
{"prompt": "def gather_sources_and_dependencies(globs, save_git_info, base_dir=None):", "metadata": {"task_id": "Utilities/sacred/40", "ground_truth": "    import sacred.optional as opt\n    experiment_path, main = get_main_file(globs, save_git_info)\n\n    base_dir = base_dir or experiment_path\n\n    gather_sources = source_discovery_strategies[SETTINGS[\"DISCOVER_SOURCES\"]]\n    sources = gather_sources(globs, base_dir, save_git_info)\n    if main is not None:\n        sources.add(main)\n\n    gather_dependencies = dependency_discovery_strategies[\n        SETTINGS[\"DISCOVER_DEPENDENCIES\"]\n    ]\n    dependencies = gather_dependencies(globs, base_dir)\n\n    if opt.has_numpy:\n        # Add numpy as a dependency because it might be used for randomness\n        dependencies.add(PackageDependency.create(opt.np))\n\n    return main, sources, dependencies", "fpath_tuple": ["Utilities", "sacred", "sacred", "dependencies.py"], "context_start_lineno": 725, "line_no": 727, "id": "sacred.dependencies.gather_sources_and_dependencies", "target_function_prompt": "def gather_sources_and_dependencies(globs, save_git_info, base_dir=None):", "function_signature": "def gather_sources_and_dependencies(globs, save_git_info, base_dir=None):"}}
{"prompt": "    def build_expects(self, fields=None):", "metadata": {"task_id": "Internet/boto/97", "ground_truth": "        expects = {}\n\n        if fields is None:\n            fields = list(self._data.keys()) + list(self._orig_data.keys())\n\n        # Only uniques.\n        fields = set(fields)\n\n        for key in fields:\n            expects[key] = {\n                'Exists': True,\n            }\n            value = None\n\n            # Check for invalid keys.\n            if not key in self._orig_data and not key in self._data:\n                raise ValueError(\"Unknown key %s provided.\" % key)\n\n            # States:\n            # * New field (only in _data)\n            # * Unchanged field (in both _data & _orig_data, same data)\n            # * Modified field (in both _data & _orig_data, different data)\n            # * Deleted field (only in _orig_data)\n            orig_value = self._orig_data.get(key, NEWVALUE)\n            current_value = self._data.get(key, NEWVALUE)\n\n            if orig_value == current_value:\n                # Existing field unchanged.\n                value = current_value\n            else:\n                if key in self._data:\n                    if not key in self._orig_data:\n                        # New field.\n                        expects[key]['Exists'] = False\n                    else:\n                        # Existing field modified.\n                        value = orig_value\n                else:\n                   # Existing field deleted.\n                    value = orig_value\n\n            if value is not None:\n                expects[key]['Value'] = self._dynamizer.encode(value)\n\n        return expects", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "items.py"], "context_start_lineno": 252, "line_no": 258, "id": "boto.dynamodb2.items.Item.build_expects", "target_function_prompt": "    def build_expects(self, fields=None):", "function_signature": "    def build_expects(self, fields=None):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/98", "ground_truth": "    from boto.regioninfo import connect\n    return connect('elasticloadbalancing', region_name,\n                   connection_cls=ELBConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "elb", "__init__.py"], "context_start_lineno": 50, "line_no": 61, "id": "boto.ec2.elb.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "def epoch_to_datetime(t):\n    # type: (Optional[int]) -> Optional[datetime]", "metadata": {"task_id": "System/fs/27", "ground_truth": "    if t is None:\n        return None\n    return datetime.fromtimestamp(t, tz=timezone.utc)", "fpath_tuple": ["System", "fs", "fs", "time.py"], "context_start_lineno": 37, "line_no": 40, "id": "fs.time.epoch_to_datetime", "target_function_prompt": "def epoch_to_datetime(t):\n    # type: (Optional[int]) -> Optional[datetime]", "function_signature": "def epoch_to_datetime(t):\n    # type: (Optional[int]) -> Optional[datetime]"}}
{"prompt": "    def dump(self) -> bytes:", "metadata": {"task_id": "Database/bplustree/14", "ground_truth": "        assert self.value is None or self.overflow_page is None\n        key_as_bytes = self._tree_conf.serializer.serialize(\n            self.key, self._tree_conf.key_size\n        )\n        used_key_length = len(key_as_bytes)\n        overflow_page = self.overflow_page or 0\n        if overflow_page:\n            value = b''\n        else:\n            value = self.value\n        used_value_length = len(value)\n\n        data = (\n            used_key_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            key_as_bytes +\n            bytes(self._tree_conf.key_size - used_key_length) +\n            used_value_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            value +\n            bytes(self._tree_conf.value_size - used_value_length) +\n            overflow_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n        return data", "fpath_tuple": ["Database", "bplustree", "bplustree", "entry.py"], "context_start_lineno": 95, "line_no": 96, "id": "bplustree.entry.Record.dump", "target_function_prompt": "    def dump(self) -> bytes:", "function_signature": "    def dump(self) -> bytes:"}}
{"prompt": "    def render(self, value, system_values, request=None):", "metadata": {"task_id": "Internet/pyramid/82", "ground_truth": "        from pyramid.events import BeforeRender\n        renderer = self.renderer\n        if system_values is None:\n            system_values = {\n                'view': None,\n                'renderer_name': self.name,  # b/c\n                'renderer_info': self,\n                'context': getattr(request, 'context', None),\n                'request': request,\n                'req': request,\n                'get_csrf_token': partial(get_csrf_token, request),\n            }\n\n        system_values = BeforeRender(system_values, value)\n\n        registry = self.registry\n        registry.notify(system_values)\n        result = renderer(value, system_values)\n        return result", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "renderers.py"], "context_start_lineno": 444, "line_no": 445, "id": "pyramid.renderers.RendererHelper.render", "target_function_prompt": "    def render(self, value, system_values, request=None):", "function_signature": "    def render(self, value, system_values, request=None):"}}
{"prompt": "    def detach(self, force=False, dry_run=False):", "metadata": {"task_id": "Internet/boto/99", "ground_truth": "        instance_id = None\n        if self.attach_data:\n            instance_id = self.attach_data.instance_id\n        device = None\n        if self.attach_data:\n            device = self.attach_data.device\n        return self.connection.detach_volume(\n            self.id,\n            instance_id,\n            device,\n            force,\n            dry_run=dry_run\n        )", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "volume.py"], "context_start_lineno": 158, "line_no": 175, "id": "boto.ec2.volume.Volume.detach", "target_function_prompt": "    def detach(self, force=False, dry_run=False):", "function_signature": "    def detach(self, force=False, dry_run=False):"}}
{"prompt": "    def get_cookie_values(self, name):", "metadata": {"task_id": "Internet/falcon/39", "ground_truth": "        if self._cookies is None:\n            # PERF(kgriffs): While this code isn't exactly DRY (the same code\n            # is duplicated by the cookies property) it does make things a bit\n            # more performant by removing the extra function call that would\n            # be required to factor this out. If we ever have to do this in a\n            # *third* place, we would probably want to factor it out at that\n            # point.\n            header_value = self.get_header('Cookie')\n            if header_value:\n                self._cookies = helpers.parse_cookie_header(header_value)\n            else:\n                self._cookies = {}\n\n        return self._cookies.get(name)", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 1206, "line_no": 1222, "id": "falcon.request.Request.get_cookie_values", "target_function_prompt": "    def get_cookie_values(self, name):", "function_signature": "    def get_cookie_values(self, name):"}}
{"prompt": "    def decode(self, attr):", "metadata": {"task_id": "Internet/boto/100", "ground_truth": "        if len(attr) > 1 or not attr or is_str(attr):\n            return attr\n        dynamodb_type = list(attr.keys())[0]\n        if dynamodb_type.lower() == dynamodb_type:\n            # It's not an actual type, just a single character attr that\n            # overlaps with the DDB types. Return it.\n            return attr\n        try:\n            decoder = getattr(self, '_decode_%s' % dynamodb_type.lower())\n        except AttributeError:\n            return attr\n        return decoder(attr[dynamodb_type])", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb", "types.py"], "context_start_lineno": 329, "line_no": 335, "id": "boto.dynamodb.types.Dynamizer.decode", "target_function_prompt": "    def decode(self, attr):", "function_signature": "    def decode(self, attr):"}}
{"prompt": "def _parse_spark_log(lines, record_callback=None):", "metadata": {"task_id": "System/mrjob/79", "ground_truth": "    def yield_records():\n        for record in _parse_hadoop_log4j_records(lines):\n            if record_callback:\n                record_callback(record)\n            yield record\n\n    return _parse_spark_log_from_log4j_records(yield_records())", "fpath_tuple": ["System", "mrjob", "mrjob", "logs", "spark.py"], "context_start_lineno": 29, "line_no": 31, "id": "mrjob.logs.spark._parse_spark_log", "target_function_prompt": "def _parse_spark_log(lines, record_callback=None):", "function_signature": "def _parse_spark_log(lines, record_callback=None):"}}
{"prompt": "    def add_child(self, name, text=None, ns=True):", "metadata": {"task_id": "Communications/PySimpleSOAP/1", "ground_truth": "        if not ns or self.__ns is False:\n            ##log.debug('adding %s without namespace', name)\n            element = self.__document.createElement(name)\n        else:\n            ##log.debug('adding %s ns \"%s\" %s', name, self.__ns, ns)\n            if isinstance(ns, basestring):\n                element = self.__document.createElement(name)\n                if ns:\n                    element.setAttribute(\"xmlns\", ns)\n            elif self.__prefix:\n                element = self.__document.createElementNS(self.__ns, \"%s:%s\" % (self.__prefix, name))\n            else:\n                element = self.__document.createElementNS(self.__ns, name)\n        # don't append null tags!\n        if text is not None:\n            if isinstance(text, xml.dom.minidom.CDATASection):\n                element.appendChild(self.__document.createCDATASection(text.data))\n            else:\n                element.appendChild(self.__document.createTextNode(text))\n        self._element.appendChild(element)\n        return SimpleXMLElement(\n            elements=[element],\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )", "fpath_tuple": ["Communications", "PySimpleSOAP", "pysimplesoap", "simplexml.py"], "context_start_lineno": 64, "line_no": 66, "id": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "target_function_prompt": "    def add_child(self, name, text=None, ns=True):", "function_signature": "    def add_child(self, name, text=None, ns=True):"}}
{"prompt": "    def get(self, a):", "metadata": {"task_id": "Utilities/boltons/83", "ground_truth": "        try:\n            return self.mapping[a][0]  # if object is mapped, return ID\n        except KeyError:\n            pass\n\n        if self.free:  # if there are any free IDs, use the smallest\n            nxt = heapq.heappop(self.free)\n        else:  # if there are no free numbers, use the next highest ID\n            nxt = len(self.mapping)\n        ref = weakref.ref(a, self._clean)\n        self.mapping[a] = (nxt, ref)\n        self.ref_map[ref] = nxt\n        return nxt", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 838, "line_no": 839, "id": "boltons.cacheutils.MinIDMap.get", "target_function_prompt": "    def get(self, a):", "function_signature": "    def get(self, a):"}}
{"prompt": "def murmur3(data, seed=0):", "metadata": {"task_id": "Security/pycoin/30", "ground_truth": "    from pycoin.intbytes import indexbytes\n    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n\n    length = len(data)\n    h1 = seed\n    roundedEnd = (length & 0xfffffffc)  # round down to 4 byte block\n    for i in range(0, roundedEnd, 4):\n        # little endian load order\n        k1 = (indexbytes(data, i) & 0xff) | ((indexbytes(data, i + 1) & 0xff) << 8) | \\\n            ((indexbytes(data, i + 2) & 0xff) << 16) | (indexbytes(data, i + 3) << 24)\n        k1 *= c1\n        k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)  # ROTL32(k1,15)\n        k1 *= c2\n\n        h1 ^= k1\n        h1 = (h1 << 13) | ((h1 & 0xffffffff) >> 19)  # ROTL32(h1,13)\n        h1 = h1 * 5 + 0xe6546b64\n\n    # tail\n    k1 = 0\n\n    val = length & 0x03\n    if val == 3:\n        k1 = (indexbytes(data, roundedEnd + 2) & 0xff) << 16\n    # fallthrough\n    if val in [2, 3]:\n        k1 |= (indexbytes(data, roundedEnd + 1) & 0xff) << 8\n    # fallthrough\n    if val in [1, 2, 3]:\n        k1 |= indexbytes(data, roundedEnd) & 0xff\n        k1 *= c1\n        k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)  # ROTL32(k1,15)\n        k1 *= c2\n        h1 ^= k1\n\n    # finalization\n    h1 ^= length\n\n    # fmix(h1)\n    h1 ^= ((h1 & 0xffffffff) >> 16)\n    h1 *= 0x85ebca6b\n    h1 ^= ((h1 & 0xffffffff) >> 13)\n    h1 *= 0xc2b2ae35\n    h1 ^= ((h1 & 0xffffffff) >> 16)\n\n    return h1 & 0xffffffff", "fpath_tuple": ["Security", "pycoin", "pycoin", "bloomfilter.py"], "context_start_lineno": 71, "line_no": 72, "id": "pycoin.bloomfilter.murmur3", "target_function_prompt": "def murmur3(data, seed=0):", "function_signature": "def murmur3(data, seed=0):"}}
{"prompt": "    def update(self, validate=False, dry_run=False):", "metadata": {"task_id": "Internet/boto/101", "ground_truth": "        unfiltered_rs = self.connection.get_all_volumes(\n            [self.id],\n            dry_run=dry_run\n        )\n        rs = [x for x in unfiltered_rs if x.id == self.id]\n        if len(rs) > 0:\n            self._update(rs[0])\n        elif validate:\n            raise ValueError('%s is not a valid Volume ID' % self.id)\n        return self.status", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "volume.py"], "context_start_lineno": 104, "line_no": 116, "id": "boto.ec2.volume.Volume.update", "target_function_prompt": "    def update(self, validate=False, dry_run=False):", "function_signature": "    def update(self, validate=False, dry_run=False):"}}
{"prompt": "def pop_global_arguments(\n    kwargs: dict[str, Any],\n    state: Optional[\"State\"] = None,\n    host: Optional[\"Host\"] = None,\n    keys_to_check=None,\n) -> Tuple[AllArguments, list[str]]:", "metadata": {"task_id": "System/pyinfra/13", "ground_truth": "    from pyinfra.api.state import State\n    from pyinfra import context\n    state = state or context.state\n    host = host or context.host\n\n    config = state.config\n    if context.ctx_config.isset():\n        config = context.config\n\n    meta_kwargs = host.current_deploy_kwargs or {}\n\n    arguments: AllArguments = {}\n    found_keys: list[str] = []\n\n    for key, type_ in AllArguments.__annotations__.items():\n        if keys_to_check and key not in keys_to_check:\n            continue\n\n        argument_meta = all_argument_meta[key]\n        handler = argument_meta.handler\n        default: Any = argument_meta.default(config)\n\n        host_default = getattr(host.data, key, default_sentinel)\n        if host_default is not default_sentinel:\n            default = host_default\n\n        if key in kwargs:\n            found_keys.append(key)\n            value = kwargs.pop(key)\n        else:\n            value = meta_kwargs.get(key, default)\n\n        if handler is not default_sentinel:\n            value = handler(config, value)\n\n        # TODO: why is type failing here?\n        arguments[key] = value  # type: ignore\n    return arguments, found_keys", "fpath_tuple": ["System", "pyinfra", "pyinfra", "api", "arguments.py"], "context_start_lineno": 266, "line_no": 291, "id": "pyinfra.api.arguments.pop_global_arguments", "target_function_prompt": "def pop_global_arguments(\n    kwargs: dict[str, Any],\n    state: Optional[\"State\"] = None,\n    host: Optional[\"Host\"] = None,\n    keys_to_check=None,\n) -> Tuple[AllArguments, list[str]]:", "function_signature": "def pop_global_arguments(\n    kwargs: dict[str, Any],\n    state: Optional[\"State\"] = None,\n    host: Optional[\"Host\"] = None,\n    keys_to_check=None,\n) -> Tuple[AllArguments, list[str]]:"}}
{"prompt": "def network_for_netcode(symbol):", "metadata": {"task_id": "Security/pycoin/31", "ground_truth": "    symbol = symbol.upper()\n    netcode = symbol.lower()\n    for prefix in search_prefixes():\n        try:\n            module = importlib.import_module(\"%s.%s\" % (prefix, netcode))\n            if module.network.symbol.upper() == symbol:\n                module.symbol = symbol\n                return module.network\n        except (AttributeError, ImportError):\n            pass\n    raise ValueError(\"no network with symbol %s found\" % netcode)", "fpath_tuple": ["Security", "pycoin", "pycoin", "networks", "registry.py"], "context_start_lineno": 14, "line_no": 15, "id": "pycoin.networks.registry.network_for_netcode", "target_function_prompt": "def network_for_netcode(symbol):", "function_signature": "def network_for_netcode(symbol):"}}
{"prompt": "    def reset_instance(cls, *args, **kwargs):", "metadata": {"task_id": "Communications/chatette/19", "ground_truth": "        cls._instance = None\n        cls._instance = cls(*args, **kwargs)\n        return cls._instance", "fpath_tuple": ["Communications", "chatette", "chatette", "utils.py"], "context_start_lineno": 44, "line_no": 50, "id": "chatette.utils.Singleton.reset_instance", "target_function_prompt": "    def reset_instance(cls, *args, **kwargs):", "function_signature": "    def reset_instance(cls, *args, **kwargs):"}}
{"prompt": "def zero_extend(s, size):", "metadata": {"task_id": "Security/barf/9", "ground_truth": "    assert type(s) in (Constant, BitVec) and size - s.size >= 0\n\n    if size == s.size:\n        return s\n\n    return BitVec(size, \"(_ zero_extend {})\".format(size - s.size), s)", "fpath_tuple": ["Security", "barf", "barf", "core", "smt", "smtfunction.py"], "context_start_lineno": 31, "line_no": 32, "id": "barf.core.smt.smtfunction.zero_extend", "target_function_prompt": "def zero_extend(s, size):", "function_signature": "def zero_extend(s, size):"}}
{"prompt": "    def path_to_uri(self):", "metadata": {"task_id": "System/mrjob/80", "ground_truth": "        return dict((path, self.uri(path))\n                    for path in self._path_to_name)", "fpath_tuple": ["System", "mrjob", "mrjob", "setup.py"], "context_start_lineno": 342, "line_no": 345, "id": "mrjob.setup.UploadDirManager.path_to_uri", "target_function_prompt": "    def path_to_uri(self):", "function_signature": "    def path_to_uri(self):"}}
{"prompt": "    def get_assign_targets_with_attr(self, node: ast.AST) -> List[ast.Attribute]:", "metadata": {"task_id": "System/viztracer/1", "ground_truth": "        if isinstance(node, ast.Attribute):\n            return [node]\n        elif isinstance(node, (ast.Name, ast.Subscript, ast.Starred)):\n            return []\n        elif isinstance(node, (ast.Tuple, ast.List)):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(elt) for elt in node.elts])\n        color_print(\"WARNING\", \"Unexpected node type {} for ast.Assign. \\\n            Please report to the author github.com/gaogaotiantian/viztracer\".format(type(node)))\n        return []", "fpath_tuple": ["System", "viztracer", "src", "viztracer", "code_monkey.py"], "context_start_lineno": 108, "line_no": 112, "id": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "target_function_prompt": "    def get_assign_targets_with_attr(self, node: ast.AST) -> List[ast.Attribute]:", "function_signature": "    def get_assign_targets_with_attr(self, node: ast.AST) -> List[ast.Attribute]:"}}
{"prompt": "def complete_and_incomplete_themes() -> Tuple[List[str], List[str]]:", "metadata": {"task_id": "Communications/zulip-term/25", "ground_truth": "    complete = {\n        name\n        for name, theme in THEMES.items()\n        if set(theme.STYLES) == set(REQUIRED_STYLES)\n        if set(theme.META) == set(REQUIRED_META)\n        for meta, conf in theme.META.items()\n        if set(conf) == set(REQUIRED_META.get(meta, {}))\n    }\n    incomplete = list(set(THEMES) - complete)\n    return sorted(list(complete)), sorted(incomplete)", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "config", "themes.py"], "context_start_lineno": 139, "line_no": 140, "id": "zulipterminal.config.themes.complete_and_incomplete_themes", "target_function_prompt": "def complete_and_incomplete_themes() -> Tuple[List[str], List[str]]:", "function_signature": "def complete_and_incomplete_themes() -> Tuple[List[str], List[str]]:"}}
{"prompt": "def get_params_to_model_values(\n    param_counts: Union[StateMatrix, dict], param_value_counts: Union[StateMatrix, dict]\n) -> set:", "metadata": {"task_id": "Security/msticpy/13", "ground_truth": "    param_stats = [\n        (param, len(vals), param_counts[param], 100 * len(vals) / param_counts[param])\n        for param, vals in param_value_counts.items()\n    ]\n\n    modellable_params = [\n        param[0]\n        for param in param_stats\n        if param[1] <= 20 <= param[2] and param[3] <= 10\n    ]\n\n    return set(modellable_params)", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "cmds_params_values.py"], "context_start_lineno": 231, "line_no": 252, "id": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "target_function_prompt": "def get_params_to_model_values(\n    param_counts: Union[StateMatrix, dict], param_value_counts: Union[StateMatrix, dict]\n) -> set:", "function_signature": "def get_params_to_model_values(\n    param_counts: Union[StateMatrix, dict], param_value_counts: Union[StateMatrix, dict]\n) -> set:"}}
{"prompt": "def compute_likelihood_window(\n    window: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    value_cond_param_probs: Union[StateMatrix, dict],\n    modellable_params: set,\n    use_start_token: bool,\n    use_end_token: bool,\n    start_token: str = None,\n    end_token: str = None,\n) -> float:", "metadata": {"task_id": "Security/msticpy/14", "ground_truth": "    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_token is True\"\n            )\n    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_end_token is True\"\n            )\n\n    w_len = len(window)\n    if w_len == 0:\n        return np.nan\n    prob: float = 1\n\n    cur_cmd = window[0].name\n    params = window[0].params\n    param_vals_prob = compute_prob_setofparams_given_cmd(\n        cmd=cur_cmd,\n        params_with_vals=params,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        use_geo_mean=True,\n    )\n\n    if use_start_token:\n        prob *= trans_probs[start_token][cur_cmd] * param_vals_prob\n    else:\n        prob *= prior_probs[cur_cmd] * param_vals_prob\n\n    for i in range(1, w_len):\n        prev, cur = window[i - 1], window[i]\n        prev_cmd, cur_cmd = prev.name, cur.name\n        cur_par = cur.params\n        prob *= trans_probs[prev_cmd][cur_cmd]\n        param_vals_prob = compute_prob_setofparams_given_cmd(\n            cmd=cur_cmd,\n            params_with_vals=cur_par,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_geo_mean=True,\n        )\n        prob *= param_vals_prob\n\n    if use_end_token:\n        prob *= trans_probs[cur_cmd][end_token]\n\n    return prob", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "cmds_params_values.py"], "context_start_lineno": 338, "line_no": 392, "id": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "target_function_prompt": "def compute_likelihood_window(\n    window: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    value_cond_param_probs: Union[StateMatrix, dict],\n    modellable_params: set,\n    use_start_token: bool,\n    use_end_token: bool,\n    start_token: str = None,\n    end_token: str = None,\n) -> float:", "function_signature": "def compute_likelihood_window(\n    window: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    value_cond_param_probs: Union[StateMatrix, dict],\n    modellable_params: set,\n    use_start_token: bool,\n    use_end_token: bool,\n    start_token: str = None,\n    end_token: str = None,\n) -> float:"}}
{"prompt": "def septuplet(value, in_fourths=True):", "metadata": {"task_id": "Multimedia/mingus/39", "ground_truth": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "value.py"], "context_start_lineno": 196, "line_no": 215, "id": "mingus.core.value.septuplet", "target_function_prompt": "def septuplet(value, in_fourths=True):", "function_signature": "def septuplet(value, in_fourths=True):"}}
{"prompt": "    def getrecvbuffer(self):", "metadata": {"task_id": "Utilities/boltons/84", "ground_truth": "        with self._recv_lock:\n            return self.rbuf", "fpath_tuple": ["Utilities", "boltons", "boltons", "socketutils.py"], "context_start_lineno": 188, "line_no": 190, "id": "boltons.socketutils.BufferedSocket.getrecvbuffer", "target_function_prompt": "    def getrecvbuffer(self):", "function_signature": "    def getrecvbuffer(self):"}}
{"prompt": "    def client_accepts(self, media_type):", "metadata": {"task_id": "Internet/falcon/40", "ground_truth": "        accept = self.accept\n\n        # PERF(kgriffs): Usually the following will be true, so\n        # try it first.\n        if (accept == media_type) or (accept == '*/*'):\n            return True\n\n        # Fall back to full-blown parsing\n        try:\n            return mimeparse.quality(media_type, accept) != 0.0\n        except ValueError:\n            return False", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 1075, "line_no": 1087, "id": "falcon.request.Request.client_accepts", "target_function_prompt": "    def client_accepts(self, media_type):", "function_signature": "    def client_accepts(self, media_type):"}}
{"prompt": "def get_supported_server_auth_methods(conn: 'SSHServerConnection') -> \\\n        Sequence[bytes]:", "metadata": {"task_id": "Security/asyncssh/11", "ground_truth": "    auth_methods = []\n\n    for method in _auth_methods:\n        if _server_auth_handlers[method].supported(conn):\n            auth_methods.append(method)\n\n    return auth_methods", "fpath_tuple": ["Security", "asyncssh", "asyncssh", "auth.py"], "context_start_lineno": 953, "line_no": 957, "id": "asyncssh.auth.get_supported_server_auth_methods", "target_function_prompt": "def get_supported_server_auth_methods(conn: 'SSHServerConnection') -> \\\n        Sequence[bytes]:", "function_signature": "def get_supported_server_auth_methods(conn: 'SSHServerConnection') -> \\\n        Sequence[bytes]:"}}
{"prompt": "    def _update_mean_variance(self, n_past, mu, var, X, random_state, sample_weight=None, n_noisy=None):", "metadata": {"task_id": "Security/diffprivlib/25", "ground_truth": "        if n_noisy is None:\n            warnings.warn(\"Noisy class count has not been specified and will be read from the data. To use this \"\n                          \"method correctly, make sure it is run by the parent GaussianNB class.\", PrivacyLeakWarning)\n            n_noisy = X.shape[0]\n\n        if not n_noisy:\n            return mu, var\n\n        if sample_weight is not None:\n            warn_unused_args(\"sample_weight\")\n\n        # Split epsilon between each feature, using 1/3 of total budget for each of mean and variance\n        n_features = X.shape[1]\n        local_epsilon = self.epsilon / 3 / n_features\n\n        new_mu = np.zeros((n_features,))\n        new_var = np.zeros((n_features,))\n\n        for feature in range(n_features):\n            temp_x = X[:, feature]\n            lower, upper = self.bounds[0][feature], self.bounds[1][feature]\n            local_diameter = upper - lower\n\n            mech_mu = LaplaceTruncated(epsilon=local_epsilon, delta=0, sensitivity=local_diameter,\n                                       lower=lower * n_noisy, upper=upper * n_noisy, random_state=random_state)\n            _mu = mech_mu.randomise(temp_x.sum()) / n_noisy\n\n            local_sq_sens = max(_mu - lower, upper - _mu) ** 2\n            mech_var = LaplaceBoundedDomain(epsilon=local_epsilon, delta=0, sensitivity=local_sq_sens, lower=0,\n                                            upper=local_sq_sens * n_noisy, random_state=random_state)\n            _var = mech_var.randomise(((temp_x - _mu) ** 2).sum()) / n_noisy\n\n            new_mu[feature] = _mu\n            new_var[feature] = _var\n\n        if n_past == 0:\n            return new_mu, new_var\n\n        n_total = float(n_past + n_noisy)\n\n        # Combine mean of old and new data, taking into consideration\n        # (weighted) number of observations\n        total_mu = (n_noisy * new_mu + n_past * mu) / n_total\n\n        # Combine variance of old and new data, taking into consideration\n        # (weighted) number of observations. This is achieved by combining\n        # the sum-of-squared-differences (ssd)\n        old_ssd = n_past * var\n        new_ssd = n_noisy * new_var\n        total_ssd = old_ssd + new_ssd + (n_past / float(n_noisy * n_total)) * (n_noisy * mu - n_noisy * new_mu) ** 2\n        total_var = total_ssd / n_total\n\n        return total_mu, total_var", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "models", "naive_bayes.py"], "context_start_lineno": 182, "line_no": 225, "id": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "target_function_prompt": "    def _update_mean_variance(self, n_past, mu, var, X, random_state, sample_weight=None, n_noisy=None):", "function_signature": "    def _update_mean_variance(self, n_past, mu, var, X, random_state, sample_weight=None, n_noisy=None):"}}
{"prompt": "    def conference(\n        self,\n        name,\n        muted=None,\n        beep=None,\n        start_conference_on_enter=None,\n        end_conference_on_exit=None,\n        wait_url=None,\n        wait_method=None,\n        max_participants=None,\n        record=None,\n        region=None,\n        coach=None,\n        trim=None,\n        status_callback_event=None,\n        status_callback=None,\n        status_callback_method=None,\n        recording_status_callback=None,\n        recording_status_callback_method=None,\n        recording_status_callback_event=None,\n        event_callback_url=None,\n        jitter_buffer_size=None,\n        participant_label=None,\n        **kwargs\n    ):", "metadata": {"task_id": "Communications/twilio-fatisar/25", "ground_truth": "        return self.nest(\n            Conference(\n                name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "twiml", "voice_response.py"], "context_start_lineno": 2008, "line_no": 2061, "id": "twilio.twiml.voice_response.Dial.conference", "target_function_prompt": "    def conference(\n        self,\n        name,\n        muted=None,\n        beep=None,\n        start_conference_on_enter=None,\n        end_conference_on_exit=None,\n        wait_url=None,\n        wait_method=None,\n        max_participants=None,\n        record=None,\n        region=None,\n        coach=None,\n        trim=None,\n        status_callback_event=None,\n        status_callback=None,\n        status_callback_method=None,\n        recording_status_callback=None,\n        recording_status_callback_method=None,\n        recording_status_callback_event=None,\n        event_callback_url=None,\n        jitter_buffer_size=None,\n        participant_label=None,\n        **kwargs\n    ):", "function_signature": "    def conference(\n        self,\n        name,\n        muted=None,\n        beep=None,\n        start_conference_on_enter=None,\n        end_conference_on_exit=None,\n        wait_url=None,\n        wait_method=None,\n        max_participants=None,\n        record=None,\n        region=None,\n        coach=None,\n        trim=None,\n        status_callback_event=None,\n        status_callback=None,\n        status_callback_method=None,\n        recording_status_callback=None,\n        recording_status_callback_method=None,\n        recording_status_callback_event=None,\n        event_callback_url=None,\n        jitter_buffer_size=None,\n        participant_label=None,\n        **kwargs\n    ):"}}
{"prompt": "def load_config_file(path: str) -> dict[str, t.Any]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/30", "ground_truth": "    if not os.path.exists(path):\n        raise BentoMLConfigException(\n            \"Configuration file %s not found.\" % path\n        ) from None\n    with open(path, \"rb\") as f:\n        config = yaml.safe_load(f)\n    return config", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "configuration", "helpers.py"], "context_start_lineno": 90, "line_no": 92, "id": "bentoml._internal.configuration.helpers.load_config_file", "target_function_prompt": "def load_config_file(path: str) -> dict[str, t.Any]:", "function_signature": "def load_config_file(path: str) -> dict[str, t.Any]:"}}
{"prompt": "    def to_internal_value(self, data):", "metadata": {"task_id": "Internet/djangorestframework/21", "ground_truth": "        if self.pk_field is not None:\n            data = self.pk_field.to_internal_value(data)\n        queryset = self.get_queryset()\n        try:\n            if isinstance(data, bool):\n                raise TypeError\n            return queryset.get(pk=data)\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', pk_value=data)\n        except (TypeError, ValueError):\n            self.fail('incorrect_type', data_type=type(data).__name__)", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "relations.py"], "context_start_lineno": 294, "line_no": 295, "id": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "target_function_prompt": "    def to_internal_value(self, data):", "function_signature": "    def to_internal_value(self, data):"}}
{"prompt": "def create_dummy_object(name, backend_name):", "metadata": {"task_id": "Scientific-Engineering/diffusers/1", "ground_truth": "    if name.isupper():\n        return DUMMY_CONSTANT.format(name)\n    elif name.islower():\n        return DUMMY_FUNCTION.format(name, backend_name)\n    else:\n        return DUMMY_CLASS.format(name, backend_name)", "fpath_tuple": ["Scientific-Engineering", "diffusers", "utils", "check_dummies.py"], "context_start_lineno": 104, "line_no": 106, "id": "check_dummies.create_dummy_object", "target_function_prompt": "def create_dummy_object(name, backend_name):", "function_signature": "def create_dummy_object(name, backend_name):"}}
{"prompt": "    def run(\n        self,\n        command_name: Optional[str] = None,\n        config_updates: Optional[dict] = None,\n        named_configs: Sequence[str] = (),\n        info: Optional[dict] = None,\n        meta_info: Optional[dict] = None,\n        options: Optional[dict] = None,\n    ) -> Run:", "metadata": {"task_id": "Utilities/sacred/41", "ground_truth": "        run = self._create_run(\n            command_name, config_updates, named_configs, info, meta_info, options\n        )\n        run()\n        return run", "fpath_tuple": ["Utilities", "sacred", "sacred", "experiment.py"], "context_start_lineno": 236, "line_no": 272, "id": "sacred.experiment.Experiment.run", "target_function_prompt": "    def run(\n        self,\n        command_name: Optional[str] = None,\n        config_updates: Optional[dict] = None,\n        named_configs: Sequence[str] = (),\n        info: Optional[dict] = None,\n        meta_info: Optional[dict] = None,\n        options: Optional[dict] = None,\n    ) -> Run:", "function_signature": "    def run(\n        self,\n        command_name: Optional[str] = None,\n        config_updates: Optional[dict] = None,\n        named_configs: Sequence[str] = (),\n        info: Optional[dict] = None,\n        meta_info: Optional[dict] = None,\n        options: Optional[dict] = None,\n    ) -> Run:"}}
{"prompt": "    def _compute_tf(self, sentences):", "metadata": {"task_id": "Internet/sumy/24", "ground_truth": "        content_words = self._get_all_content_words_in_doc(sentences)\n        content_words_count = len(content_words)\n        content_words_freq = self._compute_word_freq(content_words)\n        content_word_tf = dict((k, v / content_words_count) for (k, v) in content_words_freq.items())\n        return content_word_tf", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "sum_basic.py"], "context_start_lineno": 60, "line_no": 64, "id": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "target_function_prompt": "    def _compute_tf(self, sentences):", "function_signature": "    def _compute_tf(self, sentences):"}}
{"prompt": "def iter_prefixes(path):", "metadata": {"task_id": "Utilities/sacred/42", "ground_truth": "    split_path = path.split(\".\")\n    for i in range(1, len(split_path) + 1):\n        yield join_paths(*split_path[:i])", "fpath_tuple": ["Utilities", "sacred", "sacred", "utils.py"], "context_start_lineno": 502, "line_no": 511, "id": "sacred.utils.iter_prefixes", "target_function_prompt": "def iter_prefixes(path):", "function_signature": "def iter_prefixes(path):"}}
{"prompt": "    def get_all_load_balancers(self, load_balancer_names=None, marker=None):", "metadata": {"task_id": "Internet/boto/102", "ground_truth": "        params = {}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names,\n                                   'LoadBalancerNames.member.%d')\n\n        if marker:\n            params['Marker'] = marker\n\n        return self.get_list('DescribeLoadBalancers', params,\n                             [('member', LoadBalancer)])", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "elb", "__init__.py"], "context_start_lineno": 108, "line_no": 125, "id": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "target_function_prompt": "    def get_all_load_balancers(self, load_balancer_names=None, marker=None):", "function_signature": "    def get_all_load_balancers(self, load_balancer_names=None, marker=None):"}}
{"prompt": "    def dump_stores(self) -> t.Dict[str, str]:", "metadata": {"task_id": "Internet/Jinja2/15", "ground_truth": "        rv: t.Dict[str, str] = {}\n        node: t.Optional[\"Symbols\"] = self\n\n        while node is not None:\n            for name in sorted(node.stores):\n                if name not in rv:\n                    rv[name] = self.find_ref(name)  # type: ignore\n\n            node = node.parent\n\n        return rv", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "idtracking.py"], "context_start_lineno": 146, "line_no": 147, "id": "jinja2.idtracking.Symbols.dump_stores", "target_function_prompt": "    def dump_stores(self) -> t.Dict[str, str]:", "function_signature": "    def dump_stores(self) -> t.Dict[str, str]:"}}
{"prompt": "    def deserialize(cls, value, *args, **kwargs):", "metadata": {"task_id": "Text-Processing/rows/11", "ground_truth": "        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return as_string(value)", "fpath_tuple": ["Text-Processing", "rows", "rows", "fields.py"], "context_start_lineno": 419, "line_no": 420, "id": "rows.fields.TextField.deserialize", "target_function_prompt": "    def deserialize(cls, value, *args, **kwargs):", "function_signature": "    def deserialize(cls, value, *args, **kwargs):"}}
{"prompt": "def queries_start_with(queries, prefixes):", "metadata": {"task_id": "Database/litecli/3", "ground_truth": "    for query in sqlparse.split(queries):\n        if query and query_starts_with(query, prefixes) is True:\n            return True\n    return False", "fpath_tuple": ["Database", "litecli", "litecli", "packages", "parseutils.py"], "context_start_lineno": 210, "line_no": 212, "id": "litecli.packages.parseutils.queries_start_with", "target_function_prompt": "def queries_start_with(queries, prefixes):", "function_signature": "def queries_start_with(queries, prefixes):"}}
{"prompt": "    def bundle_root(self):", "metadata": {"task_id": "System/exodus-bundler/12", "ground_truth": "        path = os.path.join(self.working_directory, 'bundles', self.hash)\n        return os.path.normpath(os.path.abspath(path))", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "bundling.py"], "context_start_lineno": 869, "line_no": 871, "id": "exodus_bundler.bundling.Bundle.bundle_root", "target_function_prompt": "    def bundle_root(self):", "function_signature": "    def bundle_root(self):"}}
{"prompt": "    def update(self, throughput=None, global_indexes=None):", "metadata": {"task_id": "Internet/boto/103", "ground_truth": "        data = None\n\n        if throughput:\n            self.throughput = throughput\n            data = {\n                'ReadCapacityUnits': int(self.throughput['read']),\n                'WriteCapacityUnits': int(self.throughput['write']),\n            }\n\n        gsi_data = None\n\n        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n        if throughput or global_indexes:\n            self.connection.update_table(\n                self.table_name,\n                provisioned_throughput=data,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide either the throughput or the ' \\\n                  'global_indexes to update method'\n            boto.log.error(msg)\n\n            return False", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 380, "line_no": 419, "id": "boto.dynamodb2.table.Table.update", "target_function_prompt": "    def update(self, throughput=None, global_indexes=None):", "function_signature": "    def update(self, throughput=None, global_indexes=None):"}}
{"prompt": "    def pop(self, key, default=_MISSING):", "metadata": {"task_id": "Utilities/boltons/85", "ground_truth": "        if key in self:\n            dict.__delitem__(self.inv, self[key])\n            return dict.pop(self, key)\n        if default is not _MISSING:\n            return default\n        raise KeyError()", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 855, "line_no": 856, "id": "boltons.dictutils.OneToOne.pop", "target_function_prompt": "    def pop(self, key, default=_MISSING):", "function_signature": "    def pop(self, key, default=_MISSING):"}}
{"prompt": "def get_verbosity_level(\n    logging_config: LoggingConfig,\n    base_verbosity_level: int,\n    args_verbosity_level: int,\n) -> int:", "metadata": {"task_id": "Multimedia/Mopidy/39", "ground_truth": "    if args_verbosity_level:\n        result = base_verbosity_level + args_verbosity_level\n    else:\n        result = base_verbosity_level + (logging_config[\"verbosity\"] or 0)\n\n    if result < min(LOG_LEVELS.keys()):\n        result = min(LOG_LEVELS.keys())\n    if result > max(LOG_LEVELS.keys()):\n        result = max(LOG_LEVELS.keys())\n\n    return result", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "internal", "log.py"], "context_start_lineno": 119, "line_no": 124, "id": "mopidy.internal.log.get_verbosity_level", "target_function_prompt": "def get_verbosity_level(\n    logging_config: LoggingConfig,\n    base_verbosity_level: int,\n    args_verbosity_level: int,\n) -> int:", "function_signature": "def get_verbosity_level(\n    logging_config: LoggingConfig,\n    base_verbosity_level: int,\n    args_verbosity_level: int,\n) -> int:"}}
{"prompt": "    def _normalized_down_revisions(self) -> Tuple[str, ...]:", "metadata": {"task_id": "Database/alembic/43", "ground_truth": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._normalized_resolved_dependencies\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "script", "revision.py"], "context_start_lineno": 1610, "line_no": 1615, "id": "alembic.script.revision.Revision._normalized_down_revisions", "target_function_prompt": "    def _normalized_down_revisions(self) -> Tuple[str, ...]:", "function_signature": "    def _normalized_down_revisions(self) -> Tuple[str, ...]:"}}
{"prompt": "def parse_mr_job_stderr(stderr, counters=None):", "metadata": {"task_id": "System/mrjob/81", "ground_truth": "    from mrjob.py2 import to_unicode\n    if isinstance(stderr, bytes):\n        stderr = BytesIO(stderr)\n\n    if counters is None:\n        counters = {}\n    statuses = []\n    other = []\n\n    for line in stderr:\n        m = _COUNTER_RE.match(line.rstrip(b'\\r\\n'))\n        if m:\n            group, counter, amount_str = m.groups()\n\n            # don't leave these as bytes on Python 3\n            group = to_unicode(group)\n            counter = to_unicode(counter)\n\n            counters.setdefault(group, {})\n            counters[group].setdefault(counter, 0)\n            counters[group][counter] += int(amount_str)\n            continue\n\n        m = _STATUS_RE.match(line.rstrip(b'\\r\\n'))\n        if m:\n            # don't leave as bytes on Python 3\n            statuses.append(to_unicode(m.group(1)))\n            continue\n\n        other.append(to_unicode(line))\n\n    return {'counters': counters, 'statuses': statuses, 'other': other}", "fpath_tuple": ["System", "mrjob", "mrjob", "parse.py"], "context_start_lineno": 114, "line_no": 129, "id": "mrjob.parse.parse_mr_job_stderr", "target_function_prompt": "def parse_mr_job_stderr(stderr, counters=None):", "function_signature": "def parse_mr_job_stderr(stderr, counters=None):"}}
{"prompt": "    def queue(\n        self,\n        name,\n        url=None,\n        method=None,\n        reservation_sid=None,\n        post_work_activity_sid=None,\n        **kwargs\n    ):", "metadata": {"task_id": "Communications/twilio-fatisar/26", "ground_truth": "        return self.nest(\n            Queue(\n                name,\n                url=url,\n                method=method,\n                reservation_sid=reservation_sid,\n                post_work_activity_sid=post_work_activity_sid,\n                **kwargs\n            )\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "twiml", "voice_response.py"], "context_start_lineno": 2150, "line_no": 2171, "id": "twilio.twiml.voice_response.Dial.queue", "target_function_prompt": "    def queue(\n        self,\n        name,\n        url=None,\n        method=None,\n        reservation_sid=None,\n        post_work_activity_sid=None,\n        **kwargs\n    ):", "function_signature": "    def queue(\n        self,\n        name,\n        url=None,\n        method=None,\n        reservation_sid=None,\n        post_work_activity_sid=None,\n        **kwargs\n    ):"}}
{"prompt": "def chunk_ranges(input_size, chunk_size, input_offset=0, overlap_size=0, align=False):", "metadata": {"task_id": "Utilities/boltons/86", "ground_truth": "    input_size = _validate_positive_int(input_size, 'input_size', strictly_positive=False)\n    chunk_size = _validate_positive_int(chunk_size, 'chunk_size')\n    input_offset = _validate_positive_int(input_offset, 'input_offset', strictly_positive=False)\n    overlap_size = _validate_positive_int(overlap_size, 'overlap_size', strictly_positive=False)\n\n    input_stop = input_offset + input_size\n\n    if align:\n        initial_chunk_len = chunk_size - input_offset % (chunk_size - overlap_size)\n        if initial_chunk_len != overlap_size:\n            yield (input_offset, min(input_offset + initial_chunk_len, input_stop))\n            if input_offset + initial_chunk_len >= input_stop:\n                return\n            input_offset = input_offset + initial_chunk_len - overlap_size\n\n    for i in range(input_offset, input_stop, chunk_size - overlap_size):\n        yield (i, min(i + chunk_size, input_stop))\n\n        if i + chunk_size >= input_stop:\n            return", "fpath_tuple": ["Utilities", "boltons", "boltons", "iterutils.py"], "context_start_lineno": 376, "line_no": 404, "id": "boltons.iterutils.chunk_ranges", "target_function_prompt": "def chunk_ranges(input_size, chunk_size, input_offset=0, overlap_size=0, align=False):", "function_signature": "def chunk_ranges(input_size, chunk_size, input_offset=0, overlap_size=0, align=False):"}}
{"prompt": "def get_dirs():", "metadata": {"task_id": "Multimedia/Mopidy/40", "ground_truth": "    dirs = {\n        \"XDG_CACHE_DIR\": pathlib.Path(\n            os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\")\n        ).expanduser(),\n        \"XDG_CONFIG_DIR\": pathlib.Path(\n            os.getenv(\"XDG_CONFIG_HOME\", \"~/.config\")\n        ).expanduser(),\n        \"XDG_DATA_DIR\": pathlib.Path(\n            os.getenv(\"XDG_DATA_HOME\", \"~/.local/share\")\n        ).expanduser(),\n    }\n\n    dirs.update(_get_user_dirs(dirs[\"XDG_CONFIG_DIR\"]))\n\n    return dirs", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "internal", "xdg.py"], "context_start_lineno": 5, "line_no": 18, "id": "mopidy.internal.xdg.get_dirs", "target_function_prompt": "def get_dirs():", "function_signature": "def get_dirs():"}}
{"prompt": "    def relative_datetime(self):", "metadata": {"task_id": "Communications/twtxt/5", "ground_truth": "        now = datetime.now(timezone.utc)\n        created_at = self.created_at.astimezone(timezone.utc)\n\n        delta = humanize.naturaldelta(abs(created_at - now))\n        tense = \"from now\" if now < created_at else \"ago\"\n\n        return f\"{delta} {tense}\"", "fpath_tuple": ["Communications", "twtxt", "twtxt", "models.py"], "context_start_lineno": 74, "line_no": 76, "id": "twtxt.models.Tweet.relative_datetime", "target_function_prompt": "    def relative_datetime(self):", "function_signature": "    def relative_datetime(self):"}}
{"prompt": "    def notify(self, *events):", "metadata": {"task_id": "Internet/pyramid/83", "ground_truth": "        if self.has_listeners:\n            # iterating over subscribers assures they get executed\n            [_ for _ in self.subscribers(events, None)]", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "registry.py"], "context_start_lineno": 99, "line_no": 100, "id": "pyramid.registry.Registry.notify", "target_function_prompt": "    def notify(self, *events):", "function_signature": "    def notify(self, *events):"}}
{"prompt": "    def join(self, path, *paths):", "metadata": {"task_id": "System/mrjob/82", "ground_truth": "        from mrjob.parse import urlparse\n        from mrjob.parse import is_uri\n        all_paths = (path,) + paths\n\n        # if there's a URI, we only care about it and what follows\n        for i in range(len(all_paths), 0, -1):\n            if is_uri(all_paths[i - 1]):\n                scheme, netloc, uri_path = urlparse(all_paths[i - 1])[:3]\n                return '%s://%s%s' % (\n                    scheme, netloc, posixpath.join(\n                        uri_path or '/', *all_paths[i:]))\n        else:\n            return os.path.join(*all_paths)", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "base.py"], "context_start_lineno": 96, "line_no": 98, "id": "mrjob.fs.base.Filesystem.join", "target_function_prompt": "    def join(self, path, *paths):", "function_signature": "    def join(self, path, *paths):"}}
{"prompt": "    def deserialize(self, value):", "metadata": {"task_id": "Multimedia/Mopidy/41", "ground_truth": "        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels.keys())\n        return self.levels.get(value.lower())", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 366, "line_no": 367, "id": "mopidy.config.types.LogLevel.deserialize", "target_function_prompt": "    def deserialize(self, value):", "function_signature": "    def deserialize(self, value):"}}
{"prompt": "def estimate_guesses(match, password):", "metadata": {"task_id": "Security/zxcvbn-python/14", "ground_truth": "    if match.get('guesses', False):\n        return Decimal(match['guesses'])\n\n    min_guesses = 1\n    if len(match['token']) < len(password):\n        if len(match['token']) == 1:\n            min_guesses = MIN_SUBMATCH_GUESSES_SINGLE_CHAR\n        else:\n            min_guesses = MIN_SUBMATCH_GUESSES_MULTI_CHAR\n\n    estimation_functions = {\n        'bruteforce': bruteforce_guesses,\n        'dictionary': dictionary_guesses,\n        'spatial': spatial_guesses,\n        'repeat': repeat_guesses,\n        'sequence': sequence_guesses,\n        'regex': regex_guesses,\n        'date': date_guesses,\n    }\n\n    guesses = estimation_functions[match['pattern']](match)\n    match['guesses'] = max(guesses, min_guesses)\n    match['guesses_log10'] = log(match['guesses'], 10)\n\n    return Decimal(match['guesses'])", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "scoring.py"], "context_start_lineno": 221, "line_no": 222, "id": "zxcvbn.scoring.estimate_guesses", "target_function_prompt": "def estimate_guesses(match, password):", "function_signature": "def estimate_guesses(match, password):"}}
{"prompt": "    def run_validation(self, data=empty):\n        # Test for the empty string here so that it does not get validated,\n        # and so that subclasses do not need to handle it explicitly\n        # inside the `to_internal_value()` method.", "metadata": {"task_id": "Internet/djangorestframework/22", "ground_truth": "        if data == '' or (self.trim_whitespace and str(data).strip() == ''):\n            if not self.allow_blank:\n                self.fail('blank')\n            return ''\n        return super().run_validation(data)", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "fields.py"], "context_start_lineno": 742, "line_no": 746, "id": "rest_framework.fields.CharField.run_validation", "target_function_prompt": "    def run_validation(self, data=empty):\n        # Test for the empty string here so that it does not get validated,\n        # and so that subclasses do not need to handle it explicitly\n        # inside the `to_internal_value()` method.", "function_signature": "    def run_validation(self, data=empty):\n        # Test for the empty string here so that it does not get validated,\n        # and so that subclasses do not need to handle it explicitly\n        # inside the `to_internal_value()` method."}}
{"prompt": "    def absolute_asset_spec(self, relative_spec):", "metadata": {"task_id": "Internet/pyramid/84", "ground_truth": "        if not isinstance(relative_spec, str):\n            return relative_spec\n        return self._make_spec(relative_spec)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "__init__.py"], "context_start_lineno": 738, "line_no": 747, "id": "pyramid.config.Configurator.absolute_asset_spec", "target_function_prompt": "    def absolute_asset_spec(self, relative_spec):", "function_signature": "    def absolute_asset_spec(self, relative_spec):"}}
{"prompt": "    def delete(self):", "metadata": {"task_id": "Internet/boto/104", "ground_truth": "        key_data = self.get_keys()\n        return self.table.delete_item(**key_data)", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "items.py"], "context_start_lineno": 459, "line_no": 471, "id": "boto.dynamodb2.items.Item.delete", "target_function_prompt": "    def delete(self):", "function_signature": "    def delete(self):"}}
{"prompt": "    def __permitted__(self, context, request):", "metadata": {"task_id": "Internet/pyramid/85", "ground_truth": "        view = self.match(context, request)\n        if hasattr(view, '__permitted__'):\n            return view.__permitted__(context, request)\n        return True", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "views.py"], "context_start_lineno": 131, "line_no": 132, "id": "pyramid.config.views.MultiView.__permitted__", "target_function_prompt": "    def __permitted__(self, context, request):", "function_signature": "    def __permitted__(self, context, request):"}}
{"prompt": "    def multiappend(self, folder, msgs):", "metadata": {"task_id": "Communications/IMAPClient/29", "ground_truth": "        def chunks():\n            for m in msgs:\n                if isinstance(m, dict):\n                    if \"flags\" in m:\n                        yield to_bytes(seq_to_parenstr(m[\"flags\"]))\n                    if \"date\" in m:\n                        yield to_bytes('\"%s\"' % datetime_to_INTERNALDATE(m[\"date\"]))\n                    yield _literal(to_bytes(m[\"msg\"]))\n                else:\n                    yield _literal(to_bytes(m))\n\n        msgs = list(chunks())\n\n        return self._raw_command(\n            b\"APPEND\",\n            [self._normalise_folder(folder)] + msgs,\n            uid=False,\n        )", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1426, "line_no": 1438, "id": "imapclient.imapclient.IMAPClient.multiappend", "target_function_prompt": "    def multiappend(self, folder, msgs):", "function_signature": "    def multiappend(self, folder, msgs):"}}
{"prompt": "def _render_server_default(\n    default: Optional[\n        Union[FetchedValue, str, TextClause, ColumnElement[Any]]\n    ],\n    autogen_context: AutogenContext,\n    repr_: bool = True,\n) -> Optional[str]:", "metadata": {"task_id": "Database/alembic/44", "ground_truth": "    rendered = _user_defined_render(\"server_default\", default, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if sqla_compat._server_default_is_computed(default):\n        return _render_computed(cast(\"Computed\", default), autogen_context)\n    elif sqla_compat._server_default_is_identity(default):\n        return _render_identity(cast(\"Identity\", default), autogen_context)\n    elif isinstance(default, sa_schema.DefaultClause):\n        if isinstance(default.arg, str):\n            default = default.arg\n        else:\n            return _render_potential_expr(\n                default.arg, autogen_context, is_server_default=True\n            )\n\n    if isinstance(default, str) and repr_:\n        default = repr(re.sub(r\"^'|'$\", \"\", default))\n\n    return cast(str, default)", "fpath_tuple": ["Database", "alembic", "alembic", "autogenerate", "render.py"], "context_start_lineno": 719, "line_no": 726, "id": "alembic.autogenerate.render._render_server_default", "target_function_prompt": "def _render_server_default(\n    default: Optional[\n        Union[FetchedValue, str, TextClause, ColumnElement[Any]]\n    ],\n    autogen_context: AutogenContext,\n    repr_: bool = True,\n) -> Optional[str]:", "function_signature": "def _render_server_default(\n    default: Optional[\n        Union[FetchedValue, str, TextClause, ColumnElement[Any]]\n    ],\n    autogen_context: AutogenContext,\n    repr_: bool = True,\n) -> Optional[str]:"}}
{"prompt": "    def stop(self) -> None:", "metadata": {"task_id": "System/viztracer/2", "ground_truth": "        self.enable = False\n        if self.log_print:\n            self.restore_print()\n        self._tracer.stop()", "fpath_tuple": ["System", "viztracer", "src", "viztracer", "tracer.py"], "context_start_lineno": 253, "line_no": 254, "id": "viztracer.tracer._VizTracer.stop", "target_function_prompt": "    def stop(self) -> None:", "function_signature": "    def stop(self) -> None:"}}
{"prompt": "    def note_in_range(self, note):", "metadata": {"task_id": "Multimedia/mingus/40", "ground_truth": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        if note >= self.range[0] and note <= self.range[1]:\n            return True\n        return False", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "instrument.py"], "context_start_lineno": 61, "line_no": 66, "id": "mingus.containers.instrument.Instrument.note_in_range", "target_function_prompt": "    def note_in_range(self, note):", "function_signature": "    def note_in_range(self, note):"}}
{"prompt": "def _len_lcs(x, y):", "metadata": {"task_id": "Internet/sumy/25", "ground_truth": "    table = _lcs(x, y)\n    n, m = _get_index_of_lcs(x, y)\n    return table[n, m]", "fpath_tuple": ["Internet", "sumy", "sumy", "evaluation", "rouge.py"], "context_start_lineno": 41, "line_no": 51, "id": "sumy.evaluation.rouge._len_lcs", "target_function_prompt": "def _len_lcs(x, y):", "function_signature": "def _len_lcs(x, y):"}}
{"prompt": "    def spend(self, epsilon, delta):", "metadata": {"task_id": "Security/diffprivlib/26", "ground_truth": "        self.check(epsilon, delta)\n        self.__spent_budget.append((epsilon, delta))\n        return self", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "accountant.py"], "context_start_lineno": 361, "line_no": 380, "id": "diffprivlib.accountant.BudgetAccountant.spend", "target_function_prompt": "    def spend(self, epsilon, delta):", "function_signature": "    def spend(self, epsilon, delta):"}}
{"prompt": "def filter_records(records,\n                   arns_to_filter_for=None,\n                   from_date=datetime.datetime(1970, 1, 1, tzinfo=pytz.utc),\n                   to_date=datetime.datetime.now(tz=pytz.utc)):", "metadata": {"task_id": "Security/trailscraper/10", "ground_truth": "    result = list(pipe(records, filterz(_by_timeframe(from_date, to_date)), filterz(_by_role_arns(arns_to_filter_for))))\n    if not result and records:\n        logging.warning(ALL_RECORDS_FILTERED)\n\n    return result", "fpath_tuple": ["Security", "trailscraper", "trailscraper", "cloudtrail.py"], "context_start_lineno": 256, "line_no": 261, "id": "trailscraper.cloudtrail.filter_records", "target_function_prompt": "def filter_records(records,\n                   arns_to_filter_for=None,\n                   from_date=datetime.datetime(1970, 1, 1, tzinfo=pytz.utc),\n                   to_date=datetime.datetime.now(tz=pytz.utc)):", "function_signature": "def filter_records(records,\n                   arns_to_filter_for=None,\n                   from_date=datetime.datetime(1970, 1, 1, tzinfo=pytz.utc),\n                   to_date=datetime.datetime.now(tz=pytz.utc)):"}}
{"prompt": "def get_plugins():", "metadata": {"task_id": "Database/sqlite-utils/6", "ground_truth": "    plugins = []\n    plugin_to_distinfo = dict(pm.list_plugin_distinfo())\n    for plugin in pm.get_plugins():\n        plugin_info = {\n            \"name\": plugin.__name__,\n            \"hooks\": [h.name for h in pm.get_hookcallers(plugin)],\n        }\n        distinfo = plugin_to_distinfo.get(plugin)\n        if distinfo:\n            plugin_info[\"version\"] = distinfo.version\n            plugin_info[\"name\"] = distinfo.project_name\n        plugins.append(plugin_info)\n    return plugins", "fpath_tuple": ["Database", "sqlite-utils", "sqlite_utils", "plugins.py"], "context_start_lineno": 12, "line_no": 13, "id": "sqlite_utils.plugins.get_plugins", "target_function_prompt": "def get_plugins():", "function_signature": "def get_plugins():"}}
{"prompt": "def to_css_class(s):", "metadata": {"task_id": "Database/datasette/38", "ground_truth": "    if css_class_re.match(s):\n        return s\n    md5_suffix = hashlib.md5(s.encode(\"utf8\")).hexdigest()[:6]\n    # Strip leading _, -\n    s = s.lstrip(\"_\").lstrip(\"-\")\n    # Replace any whitespace with hyphens\n    s = \"-\".join(s.split())\n    # Remove any remaining invalid characters\n    s = css_invalid_chars_re.sub(\"\", s)\n    # Attach the md5 suffix\n    bits = [b for b in (s, md5_suffix) if b]\n    return \"-\".join(bits)", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 705, "line_no": 714, "id": "datasette.utils.to_css_class", "target_function_prompt": "def to_css_class(s):", "function_signature": "def to_css_class(s):"}}
{"prompt": "def get_item(d, keys):", "metadata": {"task_id": "Text-Processing/python-benedict/2", "ground_truth": "    items = get_items(d, keys)\n    return items[-1] if items else (None, None, None)", "fpath_tuple": ["Text-Processing", "python-benedict", "benedict", "dicts", "keylist", "keylist_util.py"], "context_start_lineno": 49, "line_no": 50, "id": "benedict.dicts.keylist.keylist_util.get_item", "target_function_prompt": "def get_item(d, keys):", "function_signature": "def get_item(d, keys):"}}
{"prompt": "def move(source, destination):", "metadata": {"task_id": "Utilities/python-for-android/26", "ground_truth": "    LOGGER.debug(\"Moving {} to {}\".format(source, destination))\n    shutil.move(source, destination)", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "util.py"], "context_start_lineno": 125, "line_no": 126, "id": "pythonforandroid.util.move", "target_function_prompt": "def move(source, destination):", "function_signature": "def move(source, destination):"}}
{"prompt": "    def tables(self):", "metadata": {"task_id": "Software-Development/peewee/9", "ground_truth": "        tables = self._database.get_tables()\n        if self._include_views:\n            tables += self.views\n        return tables", "fpath_tuple": ["Software-Development", "peewee", "playhouse", "dataset.py"], "context_start_lineno": 83, "line_no": 84, "id": "playhouse.dataset.DataSet.tables", "target_function_prompt": "    def tables(self):", "function_signature": "    def tables(self):"}}
{"prompt": "def camel_case_to_pep8(name: str) -> str:", "metadata": {"task_id": "Database/happybase/2", "ground_truth": "    converted = CAPITALS.sub(lambda m: '_' + m.groups()[0].lower(), name)\n    return converted[1:] if converted[0] == '_' else converted", "fpath_tuple": ["Database", "happybase", "aiohappybase", "_util.py"], "context_start_lineno": 20, "line_no": 22, "id": "aiohappybase._util.camel_case_to_pep8", "target_function_prompt": "def camel_case_to_pep8(name: str) -> str:", "function_signature": "def camel_case_to_pep8(name: str) -> str:"}}
{"prompt": "def dict_subset(d, keys):", "metadata": {"task_id": "Internet/kinto/37", "ground_truth": "    result = {}\n\n    for key in keys:\n        if \".\" in key:\n            field, subfield = key.split(\".\", 1)\n            if isinstance(d.get(field), collections_abc.Mapping):\n                subvalue = dict_subset(d[field], [subfield])\n                result[field] = dict_merge(subvalue, result.get(field, {}))\n            elif field in d:\n                result[field] = d[field]\n        else:\n            if key in d:\n                result[key] = d[key]\n\n    return result", "fpath_tuple": ["Internet", "kinto", "kinto", "core", "utils.py"], "context_start_lineno": 167, "line_no": 169, "id": "kinto.core.utils.dict_subset", "target_function_prompt": "def dict_subset(d, keys):", "function_signature": "def dict_subset(d, keys):"}}
{"prompt": "    def to_text(self, full_quote=False):", "metadata": {"task_id": "Utilities/boltons/87", "ground_truth": "        ret_list = []\n        for k, v in self.iteritems(multi=True):\n            key = quote_query_part(to_unicode(k), full_quote=full_quote)\n            if v is None:\n                ret_list.append(key)\n            else:\n                val = quote_query_part(to_unicode(v), full_quote=full_quote)\n                ret_list.append(u'='.join((key, val)))\n        return u'&'.join(ret_list)", "fpath_tuple": ["Utilities", "boltons", "boltons", "urlutils.py"], "context_start_lineno": 1575, "line_no": 1583, "id": "boltons.urlutils.QueryParamDict.to_text", "target_function_prompt": "    def to_text(self, full_quote=False):", "function_signature": "    def to_text(self, full_quote=False):"}}
{"prompt": "def substitute_diminished_for_dominant(\n    progression, substitute_index, ignore_suffix=False\n):", "metadata": {"task_id": "Multimedia/mingus/41", "ground_truth": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            dom = skip(last, 5)\n            a = interval_diff(last, dom, 8) + acc\n            res.append(tuple_to_string((dom, a, \"dom7\")))\n            last = next\n    return res", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "progressions.py"], "context_start_lineno": 397, "line_no": 400, "id": "mingus.core.progressions.substitute_diminished_for_dominant", "target_function_prompt": "def substitute_diminished_for_dominant(\n    progression, substitute_index, ignore_suffix=False\n):", "function_signature": "def substitute_diminished_for_dominant(\n    progression, substitute_index, ignore_suffix=False\n):"}}
{"prompt": "    def links(self) -> PagesDict:", "metadata": {"task_id": "Communications/Wikipedia-API/12", "ground_truth": "        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links", "fpath_tuple": ["Communications", "Wikipedia-API", "wikipediaapi", "__init__.py"], "context_start_lineno": 998, "line_no": 1009, "id": "wikipediaapi.WikipediaPage.links", "target_function_prompt": "    def links(self) -> PagesDict:", "function_signature": "    def links(self) -> PagesDict:"}}
{"prompt": "    def to_shorthand(self):", "metadata": {"task_id": "Multimedia/mingus/42", "ground_truth": "        if self.octave < 3:\n            res = self.name\n        else:\n            res = str.lower(self.name)\n        o = self.octave - 3\n        while o < -1:\n            res += \",\"\n            o += 1\n        while o > 0:\n            res += \"'\"\n            o -= 1\n        return res", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "note.py"], "context_start_lineno": 248, "line_no": 261, "id": "mingus.containers.note.Note.to_shorthand", "target_function_prompt": "    def to_shorthand(self):", "function_signature": "    def to_shorthand(self):"}}
{"prompt": "    def process(self, source: Any):", "metadata": {"task_id": "System/viztracer/3", "ground_truth": "        if isinstance(source, bytes):\n            source = source.decode(\"utf-8\")\n        elif not isinstance(source, str):\n            return source\n\n        new_lines = []\n\n        for line in source.splitlines():\n            for pattern, transform in self.re_patterns:\n                m = pattern.match(line)\n                if m:\n                    new_lines.append(transform(self, m))\n                    break\n            else:\n                new_lines.append(line)\n\n        return \"\\n\".join(new_lines)", "fpath_tuple": ["System", "viztracer", "src", "viztracer", "code_monkey.py"], "context_start_lineno": 285, "line_no": 286, "id": "viztracer.code_monkey.SourceProcessor.process", "target_function_prompt": "    def process(self, source: Any):", "function_signature": "    def process(self, source: Any):"}}
{"prompt": "    def __repr__(self) -> str:", "metadata": {"task_id": "Utilities/mmcv/3", "ground_truth": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys})'\n        return repr_str", "fpath_tuple": ["Utilities", "mmcv", "mmcv", "transforms", "wrappers.py"], "context_start_lineno": 342, "line_no": 343, "id": "mmcv.transforms.wrappers.KeyMapper.__repr__", "target_function_prompt": "    def __repr__(self) -> str:", "function_signature": "    def __repr__(self) -> str:"}}
{"prompt": "    def paths(self, type=None):", "metadata": {"task_id": "System/mrjob/83", "ground_truth": "        paths = set()\n\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                paths.add(path)\n\n        for path_type, path in self._name_to_typed_path.values():\n            if type is None or path_type == type:\n                paths.add(path)\n\n        return paths", "fpath_tuple": ["System", "mrjob", "mrjob", "setup.py"], "context_start_lineno": 489, "line_no": 491, "id": "mrjob.setup.WorkingDirManager.paths", "target_function_prompt": "    def paths(self, type=None):", "function_signature": "    def paths(self, type=None):"}}
{"prompt": "    def send_request(self, method, params, request_id=None):", "metadata": {"task_id": "Database/mssql-cli/18", "ground_truth": "        content_body = {\n            u'jsonrpc': u'2.0',\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n\n        json_content = json.dumps(content_body, sort_keys=True)\n        header = self.HEADER.format(str(len(json_content)))\n        try:\n            self.stream.write(header.encode(u'ascii'))\n            self.stream.write(json_content.encode(self.encoding))\n            self.stream.flush()\n\n        except ValueError as ex:\n            logger.debug(u'Send Request encountered exception %s', ex)\n            raise", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "jsonrpc", "jsonrpcclient.py"], "context_start_lineno": 200, "line_no": 208, "id": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "target_function_prompt": "    def send_request(self, method, params, request_id=None):", "function_signature": "    def send_request(self, method, params, request_id=None):"}}
{"prompt": "def compute_likelihood_window(\n    window: List[str],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    use_start_token: bool,\n    use_end_token: bool,\n    start_token: str = None,\n    end_token: str = None,\n) -> float:", "metadata": {"task_id": "Security/msticpy/15", "ground_truth": "    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_token is True\"\n            )\n\n    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_end_token is True\"\n            )\n\n    w_len = len(window)\n    if w_len == 0:\n        return np.nan\n    prob = 1\n\n    cur = window[0]\n    if use_start_token:\n        prob *= trans_probs[start_token][cur]\n    else:\n        prob *= prior_probs[cur]\n\n    for i in range(1, w_len):\n        prev, cur = window[i - 1], window[i]\n        prob *= trans_probs[prev][cur]\n\n    if use_end_token:\n        prob *= trans_probs[cur][end_token]\n\n    return prob", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "cmds_only.py"], "context_start_lineno": 123, "line_no": 163, "id": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "target_function_prompt": "def compute_likelihood_window(\n    window: List[str],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    use_start_token: bool,\n    use_end_token: bool,\n    start_token: str = None,\n    end_token: str = None,\n) -> float:", "function_signature": "def compute_likelihood_window(\n    window: List[str],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    use_start_token: bool,\n    use_end_token: bool,\n    start_token: str = None,\n    end_token: str = None,\n) -> float:"}}
{"prompt": "    def client_accepts_msgpack(self):", "metadata": {"task_id": "Internet/falcon/41", "ground_truth": "        return self.client_accepts('application/x-msgpack') or self.client_accepts(\n            'application/msgpack'\n        )", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 580, "line_no": 581, "id": "falcon.request.Request.client_accepts_msgpack", "target_function_prompt": "    def client_accepts_msgpack(self):", "function_signature": "    def client_accepts_msgpack(self):"}}
{"prompt": "    def to_text(self, full_quote=False):", "metadata": {"task_id": "Utilities/boltons/88", "ground_truth": "        scheme = self.scheme\n        path = u'/'.join([quote_path_part(p, full_quote=full_quote)\n                          for p in self.path_parts])\n        authority = self.get_authority(full_quote=full_quote,\n                                       with_userinfo=True)\n        query_string = self.query_params.to_text(full_quote=full_quote)\n        fragment = quote_fragment_part(self.fragment, full_quote=full_quote)\n\n        parts = []\n        _add = parts.append\n        if scheme:\n            _add(scheme)\n            _add(':')\n        if authority:\n            _add('//')\n            _add(authority)\n        elif (scheme and path[:2] != '//' and self.uses_netloc):\n            _add('//')\n        if path:\n            if scheme and authority and path[:1] != '/':\n                _add('/')\n                # TODO: i think this is here because relative paths\n                # with absolute authorities = undefined\n            _add(path)\n        if query_string:\n            _add('?')\n            _add(query_string)\n        if fragment:\n            _add('#')\n            _add(fragment)\n        return u''.join(parts)", "fpath_tuple": ["Utilities", "boltons", "boltons", "urlutils.py"], "context_start_lineno": 752, "line_no": 768, "id": "boltons.urlutils.URL.to_text", "target_function_prompt": "    def to_text(self, full_quote=False):", "function_signature": "    def to_text(self, full_quote=False):"}}
{"prompt": "    def from_string(cls, tb_str):", "metadata": {"task_id": "Utilities/boltons/89", "ground_truth": "        if not isinstance(tb_str, text):\n            tb_str = tb_str.decode('utf-8')\n        tb_lines = tb_str.lstrip().splitlines()\n\n        # First off, handle some ignored exceptions. These can be the\n        # result of exceptions raised by __del__ during garbage\n        # collection\n        while tb_lines:\n            cl = tb_lines[-1]\n            if cl.startswith('Exception ') and cl.endswith('ignored'):\n                tb_lines.pop()\n            else:\n                break\n        if tb_lines and tb_lines[0].strip() == 'Traceback (most recent call last):':\n            start_line = 1\n            frame_re = _frame_re\n        elif len(tb_lines) > 1 and tb_lines[-2].lstrip().startswith('^'):\n            # This is to handle the slight formatting difference\n            # associated with SyntaxErrors, which also don't really\n            # have tracebacks\n            start_line = 0\n            frame_re = _se_frame_re\n        else:\n            raise ValueError('unrecognized traceback string format')\n\n        frames = []\n        line_no = start_line\n        while True:\n            frame_line = tb_lines[line_no].strip()\n            frame_match = frame_re.match(frame_line)\n            if frame_match:\n                frame_dict = frame_match.groupdict()\n                try:\n                    next_line = tb_lines[line_no + 1]\n                except IndexError:\n                    # We read what we could\n                    next_line = ''\n                next_line_stripped = next_line.strip()\n                if (\n                        frame_re.match(next_line_stripped) or\n                        # The exception message will not be indented\n                        # This check is to avoid overrunning on eval-like\n                        # tracebacks where the last frame doesn't have source\n                        # code in the traceback\n                        not next_line.startswith(' ')\n                ):\n                    frame_dict['source_line'] = ''\n                else:\n                    frame_dict['source_line'] = next_line_stripped\n                    line_no += 1\n            else:\n                break\n            line_no += 1\n            frames.append(frame_dict)\n\n        try:\n            exc_line = '\\n'.join(tb_lines[line_no:])\n            exc_type, _, exc_msg = exc_line.partition(': ')\n        except Exception:\n            exc_type, exc_msg = '', ''\n\n        return cls(exc_type, exc_msg, frames)", "fpath_tuple": ["Utilities", "boltons", "boltons", "tbutils.py"], "context_start_lineno": 748, "line_no": 762, "id": "boltons.tbutils.ParsedException.from_string", "target_function_prompt": "    def from_string(cls, tb_str):", "function_signature": "    def from_string(cls, tb_str):"}}
{"prompt": "    def maybe_resolve(self, dotted):", "metadata": {"task_id": "Internet/pyramid/86", "ground_truth": "        if isinstance(dotted, str):\n            package = self.package\n            if package is CALLER_PACKAGE:\n                package = caller_package()\n            return self._resolve(dotted, package)\n        return dotted", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "path.py"], "context_start_lineno": 277, "line_no": 291, "id": "pyramid.path.DottedNameResolver.maybe_resolve", "target_function_prompt": "    def maybe_resolve(self, dotted):", "function_signature": "    def maybe_resolve(self, dotted):"}}
{"prompt": "def reverse_dictionary_match(password,\n                             _ranked_dictionaries=RANKED_DICTIONARIES):", "metadata": {"task_id": "Security/zxcvbn-python/15", "ground_truth": "    reversed_password = ''.join(reversed(password))\n    matches = dictionary_match(reversed_password, _ranked_dictionaries)\n    for match in matches:\n        match['token'] = ''.join(reversed(match['token']))\n        match['reversed'] = True\n        match['i'], match['j'] = len(password) - 1 - match['j'], \\\n                                 len(password) - 1 - match['i']\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "matching.py"], "context_start_lineno": 120, "line_no": 122, "id": "zxcvbn.matching.reverse_dictionary_match", "target_function_prompt": "def reverse_dictionary_match(password,\n                             _ranked_dictionaries=RANKED_DICTIONARIES):", "function_signature": "def reverse_dictionary_match(password,\n                             _ranked_dictionaries=RANKED_DICTIONARIES):"}}
{"prompt": "    def fit(self, X, y=None, sample_weight=None):", "metadata": {"task_id": "Security/diffprivlib/27", "ground_truth": "        from diffprivlib.utils import PrivacyLeakWarning\n        from diffprivlib.utils import check_random_state\n        self._validate_params()\n        self.accountant.check(self.epsilon, 0)\n\n        if sample_weight is not None:\n            self._warn_unused_args(\"sample_weight\")\n\n        del y\n\n        random_state = check_random_state(self.random_state)\n\n        X = self._validate_data(X, accept_sparse=False, dtype=[np.float64, np.float32])\n        n_samples, n_dims = X.shape\n\n        if n_samples < self.n_clusters:\n            raise ValueError(f\"n_samples={n_samples} should be >= n_clusters={self.n_clusters}\")\n\n        iters = self._calc_iters(n_dims, n_samples)\n\n        if self.bounds is None:\n            warnings.warn(\"Bounds have not been specified and will be calculated on the data provided.  This will \"\n                          \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                          \"privacy leakage, specify `bounds` for each dimension.\", PrivacyLeakWarning)\n            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n        self.bounds = self._check_bounds(self.bounds, n_dims, min_separation=1e-5)\n        X = self._clip_to_bounds(X, self.bounds)\n\n        centers = self._init_centers(n_dims, random_state=random_state)\n        labels = None\n        distances = None\n\n        # Run _update_centers first to ensure consistency of `labels` and `centers`, since convergence unlikely\n        for _ in range(-1, iters):\n            if labels is not None:\n                centers = self._update_centers(X, centers=centers, labels=labels, dims=n_dims, total_iters=iters,\n                                               random_state=random_state)\n\n            distances, labels = self._distances_labels(X, centers)\n\n        self.cluster_centers_ = centers\n        self.labels_ = labels\n        self.inertia_ = distances[np.arange(len(labels)), labels].sum()\n        self.n_iter_ = iters\n\n        self.accountant.spend(self.epsilon, 0)\n\n        return self", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "models", "k_means.py"], "context_start_lineno": 99, "line_no": 118, "id": "diffprivlib.models.k_means.KMeans.fit", "target_function_prompt": "    def fit(self, X, y=None, sample_weight=None):", "function_signature": "    def fit(self, X, y=None, sample_weight=None):"}}
{"prompt": "    def exists(self, path_glob):", "metadata": {"task_id": "System/mrjob/84", "ground_truth": "        try:\n            return_code = self.invoke_hadoop(\n                ['fs', '-ls', path_glob],\n                ok_returncodes=[0, -1, 255],\n                ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n\n            return (return_code == 0)\n        except CalledProcessError:\n            raise IOError(\"Could not check path %s\" % path_glob)", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "hadoop.py"], "context_start_lineno": 299, "line_no": 305, "id": "mrjob.fs.hadoop.HadoopFilesystem.exists", "target_function_prompt": "    def exists(self, path_glob):", "function_signature": "    def exists(self, path_glob):"}}
{"prompt": "    def transform(self, data=None):", "metadata": {"task_id": "Multimedia/hypertools/10", "ground_truth": "        from .tools.align import align as aligner\n        from .tools.normalize import normalize as normalizer\n        from .tools.reduce import reduce as reducer\n        if data is None:\n            return self.xform_data\n        else:\n            formatted = format_data(\n                data,\n                semantic=self.semantic,\n                vectorizer=self.vectorizer,\n                corpus=self.corpus,\n                ppca=True)\n            norm = normalizer(formatted, normalize=self.normalize)\n            reduction = reducer(\n                norm,\n                reduce=self.reduce,\n                ndims=self.reduce['params']['n_components'])\n            return aligner(reduction, align=self.align)", "fpath_tuple": ["Multimedia", "hypertools", "hypertools", "datageometry.py"], "context_start_lineno": 110, "line_no": 128, "id": "hypertools.datageometry.DataGeometry.transform", "target_function_prompt": "    def transform(self, data=None):", "function_signature": "    def transform(self, data=None):"}}
{"prompt": "def format_hostname(hostname: str) -> str:", "metadata": {"task_id": "Multimedia/Mopidy/42", "ground_truth": "    if has_ipv6 and re.match(r\"\\d+.\\d+.\\d+.\\d+\", hostname) is not None:\n        hostname = f\"::ffff:{hostname}\"\n    return hostname", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "internal", "network.py"], "context_start_lineno": 26, "line_no": 28, "id": "mopidy.internal.network.format_hostname", "target_function_prompt": "def format_hostname(hostname: str) -> str:", "function_signature": "def format_hostname(hostname: str) -> str:"}}
{"prompt": "    def put(self, tpart):", "metadata": {"task_id": "System/wal-e/9", "ground_truth": "        from wal_e.exception import UserCritical\n        if self.closed:\n            raise UserCritical(msg='attempt to upload tar after closing',\n                               hint='report a bug')\n\n        while True:\n            too_many = (\n                self.concurrency_burden + 1 > self.max_concurrency\n                or self.member_burden + len(tpart) > self.max_members\n            )\n\n            if too_many:\n                # If there are not enough resources to start an upload\n                # even with zero uploads in progress, then something\n                # has gone wrong: the user should not be given enough\n                # rope to hang themselves in this way.\n                if self.concurrency_burden == 0:\n                    raise UserCritical(\n                        msg=('not enough resources in pool to '\n                             'support an upload'),\n                        hint='report a bug')\n\n                # _wait blocks until an upload finishes and clears its\n                # used resources, after which another attempt to\n                # evaluate scheduling resources for another upload\n                # might be worth evaluating.\n                #\n                # Alternatively, an error was encountered in a\n                # previous upload in which case it'll be raised here\n                # and cause the process to regard the upload as a\n                # failure.\n                self._wait()\n                gc.collect()\n            else:\n                # Enough resources available: commence upload\n                self._start(tpart)\n                return", "fpath_tuple": ["System", "wal-e", "wal_e", "worker", "upload_pool.py"], "context_start_lineno": 70, "line_no": 77, "id": "wal_e.worker.upload_pool.TarUploadPool.put", "target_function_prompt": "    def put(self, tpart):", "function_signature": "    def put(self, tpart):"}}
{"prompt": "def get_notes(key=\"C\"):", "metadata": {"task_id": "Multimedia/mingus/43", "ground_truth": "    if key in _key_cache:\n        return _key_cache[key]\n    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n    result = []\n\n    # Calculate notes\n    altered_notes = [x[0] for x in get_key_signature_accidentals(key)]\n\n    if get_key_signature(key) < 0:\n        symbol = \"b\"\n    elif get_key_signature(key) > 0:\n        symbol = \"#\"\n\n    raw_tonic_index = base_scale.index(key.upper()[0])\n\n    for note in islice(cycle(base_scale), raw_tonic_index, raw_tonic_index + 7):\n        if note in altered_notes:\n            result.append(\"%s%s\" % (note, symbol))\n        else:\n            result.append(note)\n\n    # Save result to cache\n    _key_cache[key] = result\n    return result", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "keys.py"], "context_start_lineno": 108, "line_no": 117, "id": "mingus.core.keys.get_notes", "target_function_prompt": "def get_notes(key=\"C\"):", "function_signature": "def get_notes(key=\"C\"):"}}
{"prompt": "    def get_token(self):", "metadata": {"task_id": "Security/msticpy/16", "ground_truth": "        chosen_account = self.app.get_accounts(username=self.username)\n        if chosen_account:\n            self.result = self.app.acquire_token_silent_with_error(\n                scopes=self.scopes, account=chosen_account[0]\n            )\n            if not self.result:\n                self.result = self._app_auth(self.auth_type)\n        else:\n            self.result = self._app_auth(self.auth_type)\n        self.refresh_token()", "fpath_tuple": ["Security", "msticpy", "msticpy", "auth", "msal_auth.py"], "context_start_lineno": 76, "line_no": 78, "id": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "target_function_prompt": "    def get_token(self):", "function_signature": "    def get_token(self):"}}
{"prompt": "def delete(filepath):", "metadata": {"task_id": "Utilities/mackup/5", "ground_truth": "    remove_acl(filepath)\n\n    # Some files have immutable attributes, let's remove them recursively\n    remove_immutable_attribute(filepath)\n\n    # Finally remove the files and folders\n    if os.path.isfile(filepath) or os.path.islink(filepath):\n        os.remove(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)", "fpath_tuple": ["Utilities", "mackup", "mackup", "utils.py"], "context_start_lineno": 48, "line_no": 58, "id": "mackup.utils.delete", "target_function_prompt": "def delete(filepath):", "function_signature": "def delete(filepath):"}}
{"prompt": "    def NewFromJsonDict(cls, data, **kwargs):", "metadata": {"task_id": "Internet/python-twitter/2", "ground_truth": "        json_data = data.copy()\n        if kwargs:\n            for key, val in kwargs.items():\n                json_data[key] = val\n\n        c = cls(**json_data)\n        c._json = data\n        return c", "fpath_tuple": ["Internet", "python-twitter", "twitter", "models.py"], "context_start_lineno": 78, "line_no": 87, "id": "twitter.models.TwitterModel.NewFromJsonDict", "target_function_prompt": "    def NewFromJsonDict(cls, data, **kwargs):", "function_signature": "    def NewFromJsonDict(cls, data, **kwargs):"}}
{"prompt": "    def relate(self, *pairs):", "metadata": {"task_id": "Internet/pyramid/87", "ground_truth": "        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.setdefault(x, [])\n            if x is not y and y not in L:\n                L.append(y)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "registry.py"], "context_start_lineno": 184, "line_no": 185, "id": "pyramid.registry.Introspector.relate", "target_function_prompt": "    def relate(self, *pairs):", "function_signature": "    def relate(self, *pairs):"}}
{"prompt": "    def _consume_until_tagged_response(self, tag, command):", "metadata": {"task_id": "Communications/IMAPClient/30", "ground_truth": "        tagged_commands = self._imap.tagged_commands\n        resps = []\n        while True:\n            line = self._imap._get_response()\n            if tagged_commands[tag]:\n                break\n            resps.append(_parse_untagged_response(line))\n        typ, data = tagged_commands.pop(tag)\n        self._checkok(command, typ, data)\n        return data[0], resps", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 1641, "line_no": 1642, "id": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "target_function_prompt": "    def _consume_until_tagged_response(self, tag, command):", "function_signature": "    def _consume_until_tagged_response(self, tag, command):"}}
{"prompt": "def app_get_relative_path(requests_pathname, path):", "metadata": {"task_id": "Software-Development/dash/15", "ground_truth": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    if requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    if not path.startswith(\"/\"):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            Paths that aren't prefixed with a leading / are not supported.\n            You supplied: {path}\n            \"\"\"\n        )\n    return \"/\".join([requests_pathname.rstrip(\"/\"), path.lstrip(\"/\")])", "fpath_tuple": ["Software-Development", "dash", "dash", "_get_paths.py"], "context_start_lineno": 65, "line_no": 66, "id": "dash._get_paths.app_get_relative_path", "target_function_prompt": "def app_get_relative_path(requests_pathname, path):", "function_signature": "def app_get_relative_path(requests_pathname, path):"}}
{"prompt": "    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.NpNDArray],\n        batch_dim: int = 0,\n    ) -> tuple[ext.NpNDArray, list[int]]:\n        # numpy.concatenate may consume lots of memory, need optimization later", "metadata": {"task_id": "Scientific-Engineering/bentoml/31", "ground_truth": "        batch: ext.NpNDArray = np.concatenate(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 227, "line_no": 233, "id": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "target_function_prompt": "    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.NpNDArray],\n        batch_dim: int = 0,\n    ) -> tuple[ext.NpNDArray, list[int]]:\n        # numpy.concatenate may consume lots of memory, need optimization later", "function_signature": "    def batches_to_batch(\n        cls,\n        batches: t.Sequence[ext.NpNDArray],\n        batch_dim: int = 0,\n    ) -> tuple[ext.NpNDArray, list[int]]:\n        # numpy.concatenate may consume lots of memory, need optimization later"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/105", "ground_truth": "    from boto.regioninfo import connect\n    return connect('rds', region_name, region_cls=RDSRegionInfo,\n                   connection_cls=RDSConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "rds", "__init__.py"], "context_start_lineno": 52, "line_no": 66, "id": "boto.rds.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def update_widget(\n        self, count_text: Tuple[Optional[str], str], text_color: Optional[str]\n    ) -> Any:", "metadata": {"task_id": "Communications/zulip-term/26", "ground_truth": "        if self.prefix_character:\n            prefix = [\" \", self.prefix_character, \" \"]\n        else:\n            prefix = [\" \"]\n        if count_text[1]:\n            suffix = [\" \", count_text, \" \"]\n        else:\n            suffix = [\"  \"]\n        self.button_prefix.set_text(prefix)\n        self.set_label(self._caption)\n        self.button_suffix.set_text(suffix)\n        self._w.set_attr_map({None: text_color})", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "buttons.py"], "context_start_lineno": 68, "line_no": 71, "id": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "target_function_prompt": "    def update_widget(\n        self, count_text: Tuple[Optional[str], str], text_color: Optional[str]\n    ) -> Any:", "function_signature": "    def update_widget(\n        self, count_text: Tuple[Optional[str], str], text_color: Optional[str]\n    ) -> Any:"}}
{"prompt": "    def commit(self):\n        # Commit is a no-op when there is no uncommitted pages", "metadata": {"task_id": "Database/bplustree/15", "ground_truth": "        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)", "fpath_tuple": ["Database", "bplustree", "bplustree", "memory.py"], "context_start_lineno": 417, "line_no": 419, "id": "bplustree.memory.WAL.commit", "target_function_prompt": "    def commit(self):\n        # Commit is a no-op when there is no uncommitted pages", "function_signature": "    def commit(self):\n        # Commit is a no-op when there is no uncommitted pages"}}
{"prompt": "    def update_cache(self, table=None):", "metadata": {"task_id": "Software-Development/peewee/10", "ground_truth": "        if table:\n            dependencies = [table]\n            if table in self._models:\n                model_class = self._models[table]\n                dependencies.extend([\n                    related._meta.table_name for _, related, _ in\n                    model_class._meta.model_graph()])\n            else:\n                dependencies.extend(self.get_table_dependencies(table))\n        else:\n            dependencies = None  # Update all tables.\n            self._models = {}\n        updated = self._introspector.generate_models(\n            skip_invalid=True,\n            table_names=dependencies,\n            literal_column_names=True,\n            include_views=self._include_views)\n        self._models.update(updated)", "fpath_tuple": ["Software-Development", "peewee", "playhouse", "dataset.py"], "context_start_lineno": 102, "line_no": 103, "id": "playhouse.dataset.DataSet.update_cache", "target_function_prompt": "    def update_cache(self, table=None):", "function_signature": "    def update_cache(self, table=None):"}}
{"prompt": "def parse_tweets(raw_tweets, source, now=None):", "metadata": {"task_id": "Communications/twtxt/6", "ground_truth": "    if now is None:\n        now = datetime.now(timezone.utc)\n\n    tweets = []\n    for line in raw_tweets:\n        try:\n            tweet = parse_tweet(line, source, now)\n        except (ValueError, OverflowError) as e:\n            logger.debug(\"{0} - {1}\".format(source.url, e))\n        else:\n            tweets.append(tweet)\n\n    return tweets", "fpath_tuple": ["Communications", "twtxt", "twtxt", "parser.py"], "context_start_lineno": 31, "line_no": 43, "id": "twtxt.parser.parse_tweets", "target_function_prompt": "def parse_tweets(raw_tweets, source, now=None):", "function_signature": "def parse_tweets(raw_tweets, source, now=None):"}}
{"prompt": "    def _find_hadoop_streaming_jar(self):", "metadata": {"task_id": "System/mrjob/85", "ground_truth": "        for path in unique(self._hadoop_streaming_jar_dirs()):\n            log.info('Looking for Hadoop streaming jar in %s...' % path)\n\n            streaming_jars = []\n            for path in self.fs.ls(path):\n                if _HADOOP_STREAMING_JAR_RE.match(posixpath.basename(path)):\n                    streaming_jars.append(path)\n\n            if streaming_jars:\n                # prefer shorter names and shallower paths\n                def sort_key(p):\n                    return (len(p.split('/')),\n                            len(posixpath.basename(p)),\n                            p)\n\n                streaming_jars.sort(key=sort_key)\n\n                return streaming_jars[0]\n\n        return None", "fpath_tuple": ["System", "mrjob", "mrjob", "hadoop.py"], "context_start_lineno": 236, "line_no": 239, "id": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "target_function_prompt": "    def _find_hadoop_streaming_jar(self):", "function_signature": "    def _find_hadoop_streaming_jar(self):"}}
{"prompt": "    def bounded_stream(self):", "metadata": {"task_id": "Internet/falcon/42", "ground_truth": "        if self._bounded_stream is None:\n            self._bounded_stream = self._get_wrapped_wsgi_input()\n\n        return self._bounded_stream", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 627, "line_no": 628, "id": "falcon.request.Request.bounded_stream", "target_function_prompt": "    def bounded_stream(self):", "function_signature": "    def bounded_stream(self):"}}
{"prompt": "    def __getitem__(self, name):", "metadata": {"task_id": "Internet/pyramid/88", "ground_truth": "        ob = self.subs[name]\n        return ob", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "testing.py"], "context_start_lineno": 180, "line_no": 182, "id": "pyramid.testing.DummyResource.__getitem__", "target_function_prompt": "    def __getitem__(self, name):", "function_signature": "    def __getitem__(self, name):"}}
{"prompt": "    def rm(self, path_glob):", "metadata": {"task_id": "System/mrjob/86", "ground_truth": "        if not is_uri(path_glob):\n            super(HadoopFilesystem, self).rm(path_glob)\n\n        version = self.get_hadoop_version()\n        if uses_yarn(version):\n            args = ['fs', '-rm', '-R', '-f', '-skipTrash', path_glob]\n        else:\n            args = ['fs', '-rmr', '-skipTrash', path_glob]\n\n        try:\n            self.invoke_hadoop(\n                args,\n                return_stdout=True, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not rm %s\" % path_glob)", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "hadoop.py"], "context_start_lineno": 322, "line_no": 323, "id": "mrjob.fs.hadoop.HadoopFilesystem.rm", "target_function_prompt": "    def rm(self, path_glob):", "function_signature": "    def rm(self, path_glob):"}}
{"prompt": "    def fit(self, X, y):", "metadata": {"task_id": "Security/diffprivlib/28", "ground_truth": "        if not self.nodes:\n            raise ValueError(\"Fitting Tree must be built before calling fit().\")\n\n        leaves = self.apply(X)\n        unique_leaves = np.unique(leaves)\n        values = np.zeros(shape=(self.node_count, 1, len(self.classes)))\n\n        # Populate value of real leaves\n        for leaf in unique_leaves:\n            idxs = (leaves == leaf)\n            leaf_y = y[idxs]\n\n            counts = [np.sum(leaf_y == cls) for cls in self.classes]\n            mech = PermuteAndFlip(epsilon=self.epsilon, sensitivity=1, monotonic=True, utility=counts,\n                                  random_state=self.random_state)\n            values[leaf, 0, mech.randomise()] = 1\n\n        # Populate value of empty leaves\n        for node in self.nodes:\n            if values[node.node_id].sum() or node.left_child != self._TREE_LEAF:\n                continue\n\n            values[node.node_id, 0, self.random_state.randint(len(self.classes))] = 1\n\n        self.values_ = values\n\n        return self", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "models", "forest.py"], "context_start_lineno": 555, "line_no": 567, "id": "diffprivlib.models.forest._FittingTree.fit", "target_function_prompt": "    def fit(self, X, y):", "function_signature": "    def fit(self, X, y):"}}
{"prompt": "def create_sockets(conf, log, fds=None):", "metadata": {"task_id": "Utilities/gunicorn/21", "ground_truth": "    listeners = []\n\n    # get it only once\n    addr = conf.address\n    fdaddr = [bind for bind in addr if isinstance(bind, int)]\n    if fds:\n        fdaddr += list(fds)\n    laddr = [bind for bind in addr if not isinstance(bind, int)]\n\n    # check ssl config early to raise the error on startup\n    # only the certfile is needed since it can contains the keyfile\n    if conf.certfile and not os.path.exists(conf.certfile):\n        raise ValueError('certfile \"%s\" does not exist' % conf.certfile)\n\n    if conf.keyfile and not os.path.exists(conf.keyfile):\n        raise ValueError('keyfile \"%s\" does not exist' % conf.keyfile)\n\n    # sockets are already bound\n    if fdaddr:\n        for fd in fdaddr:\n            sock = socket.fromfd(fd, socket.AF_UNIX, socket.SOCK_STREAM)\n            sock_name = sock.getsockname()\n            sock_type = _sock_type(sock_name)\n            listener = sock_type(sock_name, conf, log, fd=fd)\n            listeners.append(listener)\n\n        return listeners\n\n    # no sockets is bound, first initialization of gunicorn in this env.\n    for addr in laddr:\n        sock_type = _sock_type(addr)\n        sock = None\n        for i in range(5):\n            try:\n                sock = sock_type(addr, conf, log)\n            except socket.error as e:\n                if e.args[0] == errno.EADDRINUSE:\n                    log.error(\"Connection in use: %s\", str(addr))\n                if e.args[0] == errno.EADDRNOTAVAIL:\n                    log.error(\"Invalid address: %s\", str(addr))\n                if i < 5:\n                    msg = \"connection to {addr} failed: {error}\"\n                    log.debug(msg.format(addr=str(addr), error=str(e)))\n                    log.error(\"Retrying in 1 second.\")\n                    time.sleep(1)\n            else:\n                break\n\n        if sock is None:\n            log.error(\"Can't connect to %s\", str(addr))\n            sys.exit(1)\n\n        listeners.append(sock)\n\n    return listeners", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "sock.py"], "context_start_lineno": 142, "line_no": 150, "id": "gunicorn.sock.create_sockets", "target_function_prompt": "def create_sockets(conf, log, fds=None):", "function_signature": "def create_sockets(conf, log, fds=None):"}}
{"prompt": "def minor_second(note):", "metadata": {"task_id": "Multimedia/mingus/44", "ground_truth": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 1)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "intervals.py"], "context_start_lineno": 167, "line_no": 168, "id": "mingus.core.intervals.minor_second", "target_function_prompt": "def minor_second(note):", "function_signature": "def minor_second(note):"}}
{"prompt": "    def declaration(self):", "metadata": {"task_id": "Security/barf/10", "ground_truth": "        return \"(declare-fun {} () (Array (_ BitVec {}) (_ BitVec {})))\".format(self.name, self.key_size,\n                                                                                self.value_size)", "fpath_tuple": ["Security", "barf", "barf", "core", "smt", "smtsymbol.py"], "context_start_lineno": 277, "line_no": 278, "id": "barf.core.smt.smtsymbol.BitVecArray.declaration", "target_function_prompt": "    def declaration(self):", "function_signature": "    def declaration(self):"}}
{"prompt": "def determine(note1, note2, shorthand=False):", "metadata": {"task_id": "Multimedia/mingus/45", "ground_truth": "    if note1[0] == note2[0]:\n\n        def get_val(note):\n            \"\"\"Private function: count the value of accidentals.\"\"\"\n            r = 0\n            for x in note[1:]:\n                if x == \"b\":\n                    r -= 1\n                elif x == \"#\":\n                    r += 1\n            return r\n\n        x = get_val(note1)\n        y = get_val(note2)\n        if x == y:\n            if not shorthand:\n                return \"major unison\"\n            return \"1\"\n        elif x < y:\n            if not shorthand:\n                return \"augmented unison\"\n            return \"#1\"\n        elif x - y == 1:\n            if not shorthand:\n                return \"minor unison\"\n            return \"b1\"\n        else:\n            if not shorthand:\n                return \"diminished unison\"\n            return \"bb1\"\n\n    # Other intervals\n    n1 = notes.fifths.index(note1[0])\n    n2 = notes.fifths.index(note2[0])\n    number_of_fifth_steps = n2 - n1\n    if n2 < n1:\n        number_of_fifth_steps = len(notes.fifths) - n1 + n2\n\n    # [name, shorthand_name, half notes for major version of this interval]\n    fifth_steps = [\n        [\"unison\", \"1\", 0],\n        [\"fifth\", \"5\", 7],\n        [\"second\", \"2\", 2],\n        [\"sixth\", \"6\", 9],\n        [\"third\", \"3\", 4],\n        [\"seventh\", \"7\", 11],\n        [\"fourth\", \"4\", 5],\n    ]\n\n    # Count half steps between note1 and note2\n    half_notes = measure(note1, note2)\n\n    # Get the proper list from the number of fifth steps\n    current = fifth_steps[number_of_fifth_steps]\n\n    # maj = number of major steps for this interval\n    maj = current[2]\n\n    # if maj is equal to the half steps between note1 and note2 the interval is\n    # major or perfect\n    if maj == half_notes:\n        # Corner cases for perfect fifths and fourths\n        if current[0] == \"fifth\":\n            if not shorthand:\n                return \"perfect fifth\"\n        elif current[0] == \"fourth\":\n            if not shorthand:\n                return \"perfect fourth\"\n        if not shorthand:\n            return \"major \" + current[0]\n        return current[1]\n    elif maj + 1 <= half_notes:\n        # if maj + 1 is equal to half_notes, the interval is augmented.\n        if not shorthand:\n            return \"augmented \" + current[0]\n        return \"#\" * (half_notes - maj) + current[1]\n    elif maj - 1 == half_notes:\n        # etc.\n        if not shorthand:\n            return \"minor \" + current[0]\n        return \"b\" + current[1]\n    elif maj - 2 >= half_notes:\n        if not shorthand:\n            return \"diminished \" + current[0]\n        return \"b\" * (maj - half_notes) + current[1]", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "intervals.py"], "context_start_lineno": 324, "line_no": 345, "id": "mingus.core.intervals.determine", "target_function_prompt": "def determine(note1, note2, shorthand=False):", "function_signature": "def determine(note1, note2, shorthand=False):"}}
{"prompt": "def load_components(metadata_path, namespace=\"default_namespace\"):", "metadata": {"task_id": "Software-Development/dash/16", "ground_truth": "    from .base_component import ComponentRegistry\n    ComponentRegistry.registry.add(namespace)\n    components = []\n\n    data = _get_metadata(metadata_path)\n\n    # Iterate over each property name (which is a path to the component)\n    for componentPath in data:\n        componentData = data[componentPath]\n\n        # Extract component name from path\n        # e.g. src/components/MyControl.react.js\n        # TODO Make more robust - some folks will write .jsx and others\n        # will be on windows. Unfortunately react-docgen doesn't include\n        # the name of the component atm.\n        name = componentPath.split(\"/\").pop().split(\".\")[0]\n        component = generate_class(\n            name, componentData[\"props\"], componentData[\"description\"], namespace, None\n        )\n\n        components.append(component)\n\n    return components", "fpath_tuple": ["Software-Development", "dash", "dash", "development", "component_loader.py"], "context_start_lineno": 18, "line_no": 33, "id": "dash.development.component_loader.load_components", "target_function_prompt": "def load_components(metadata_path, namespace=\"default_namespace\"):", "function_signature": "def load_components(metadata_path, namespace=\"default_namespace\"):"}}
{"prompt": "def map_http_methods(resource, suffix=None):", "metadata": {"task_id": "Internet/falcon/43", "ground_truth": "    method_map = {}\n\n    for method in constants.COMBINED_METHODS:\n        try:\n            responder_name = 'on_' + method.lower()\n            if suffix:\n                responder_name += '_' + suffix\n\n            responder = getattr(resource, responder_name)\n        except AttributeError:\n            # resource does not implement this method\n            pass\n        else:\n            # Usually expect a method, but any callable will do\n            if callable(responder):\n                method_map[method] = responder\n\n    # If suffix is specified and doesn't map to any methods, raise an error\n    if suffix and not method_map:\n        raise SuffixedMethodNotFoundError(\n            'No responders found for the specified suffix'\n        )\n\n    return method_map", "fpath_tuple": ["Internet", "falcon", "falcon", "routing", "util.py"], "context_start_lineno": 101, "line_no": 122, "id": "falcon.routing.util.map_http_methods", "target_function_prompt": "def map_http_methods(resource, suffix=None):", "function_signature": "def map_http_methods(resource, suffix=None):"}}
{"prompt": "def commands_for_random_tips() -> List[KeyBinding]:", "metadata": {"task_id": "Communications/zulip-term/27", "ground_truth": "    return [\n        key_binding\n        for key_binding in KEY_BINDINGS.values()\n        if not key_binding.get(\"excluded_from_random_tips\", False)\n    ]", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "config", "keys.py"], "context_start_lineno": 451, "line_no": 455, "id": "zulipterminal.config.keys.commands_for_random_tips", "target_function_prompt": "def commands_for_random_tips() -> List[KeyBinding]:", "function_signature": "def commands_for_random_tips() -> List[KeyBinding]:"}}
{"prompt": "    def delete_key(self, key_name, headers=None, version_id=None,\n                   mfa_token=None):", "metadata": {"task_id": "Internet/boto/106", "ground_truth": "        if not key_name:\n            raise ValueError('Empty key names are not allowed')\n        return self._delete_key_internal(key_name, headers=headers,\n                                         version_id=version_id,\n                                         mfa_token=mfa_token,\n                                         query_args_l=None)", "fpath_tuple": ["Internet", "boto", "boto", "s3", "bucket.py"], "context_start_lineno": 732, "line_no": 757, "id": "boto.s3.bucket.Bucket.delete_key", "target_function_prompt": "    def delete_key(self, key_name, headers=None, version_id=None,\n                   mfa_token=None):", "function_signature": "    def delete_key(self, key_name, headers=None, version_id=None,\n                   mfa_token=None):"}}
{"prompt": "def _fix_clear_tags(x):", "metadata": {"task_id": "System/mrjob/87", "ground_truth": "    _fix = _fix_clear_tags\n\n    if isinstance(x, list):\n        return [_fix(_strip_clear_tag(item)) for item in x]\n\n    elif isinstance(x, dict):\n        d = dict((_fix(k), _fix(v)) for k, v in x.items())\n\n        # handle cleared keys\n        for k, v in list(d.items()):\n            if isinstance(k, ClearedValue):\n                del d[k]\n                d[_strip_clear_tag(k)] = ClearedValue(_strip_clear_tag(v))\n\n        return d\n\n    elif isinstance(x, ClearedValue):\n        return ClearedValue(_fix(x.value))\n\n    else:\n        return x", "fpath_tuple": ["System", "mrjob", "mrjob", "conf.py"], "context_start_lineno": 155, "line_no": 166, "id": "mrjob.conf._fix_clear_tags", "target_function_prompt": "def _fix_clear_tags(x):", "function_signature": "def _fix_clear_tags(x):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/107", "ground_truth": "    from boto.regioninfo import connect\n    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    return connect('cognito-identity', region_name,\n                   connection_cls=CognitoIdentityConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "cognito", "identity", "__init__.py"], "context_start_lineno": 38, "line_no": 39, "id": "boto.cognito.identity.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def to_internal_value(self, data):", "metadata": {"task_id": "Internet/djangorestframework/23", "ground_truth": "        data = smart_str(data).strip()\n\n        if self.localize:\n            data = sanitize_separators(data)\n\n        if len(data) > self.MAX_STRING_LENGTH:\n            self.fail('max_string_length')\n\n        try:\n            value = decimal.Decimal(data)\n        except decimal.DecimalException:\n            self.fail('invalid')\n\n        if value.is_nan():\n            self.fail('invalid')\n\n        # Check for infinity and negative infinity.\n        if value in (decimal.Decimal('Inf'), decimal.Decimal('-Inf')):\n            self.fail('invalid')\n\n        return self.quantize(self.validate_precision(value))", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "fields.py"], "context_start_lineno": 1005, "line_no": 1011, "id": "rest_framework.fields.DecimalField.to_internal_value", "target_function_prompt": "    def to_internal_value(self, data):", "function_signature": "    def to_internal_value(self, data):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/108", "ground_truth": "    from boto.regioninfo import connect\n    from boto.kinesis.layer1 import KinesisConnection\n    return connect('kinesis', region_name,\n                   connection_cls=KinesisConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "kinesis", "__init__.py"], "context_start_lineno": 37, "line_no": 38, "id": "boto.kinesis.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def post_refresh_callback(self, authorizer):", "metadata": {"task_id": "Utilities/praw/11", "ground_truth": "        self._set(authorizer.refresh_token)\n\n        # While the following line is not strictly necessary, it ensures that the\n        # refresh token is not used elsewhere. And also forces the pre_refresh_callback\n        # to always load the latest refresh_token from the database.\n        authorizer.refresh_token = None", "fpath_tuple": ["Utilities", "praw", "praw", "util", "token_manager.py"], "context_start_lineno": 166, "line_no": 168, "id": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "target_function_prompt": "    def post_refresh_callback(self, authorizer):", "function_signature": "    def post_refresh_callback(self, authorizer):"}}
{"prompt": "def flatten_dict(\n    d: t.MutableMapping[str, t.Any],\n    parent: str = \"\",\n    sep: str = \".\",\n) -> t.Generator[tuple[str, t.Any], None, None]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/32", "ground_truth": "    for k, v in d.items():\n        k = f'\"{k}\"' if any(i in punctuation for i in k) else k\n        nkey = parent + sep + k if parent else k\n        if isinstance(v, t.MutableMapping):\n            yield from flatten_dict(\n                t.cast(t.MutableMapping[str, t.Any], v), parent=nkey, sep=sep\n            )\n        else:\n            yield nkey, v", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "configuration", "helpers.py"], "context_start_lineno": 73, "line_no": 79, "id": "bentoml._internal.configuration.helpers.flatten_dict", "target_function_prompt": "def flatten_dict(\n    d: t.MutableMapping[str, t.Any],\n    parent: str = \"\",\n    sep: str = \".\",\n) -> t.Generator[tuple[str, t.Any], None, None]:", "function_signature": "def flatten_dict(\n    d: t.MutableMapping[str, t.Any],\n    parent: str = \"\",\n    sep: str = \".\",\n) -> t.Generator[tuple[str, t.Any], None, None]:"}}
{"prompt": "    def batch_to_payloads(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/33", "ground_truth": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 430, "line_no": 436, "id": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "target_function_prompt": "    def batch_to_payloads(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:", "function_signature": "    def batch_to_payloads(\n        cls,\n        batch: ext.PdDataFrame,\n        indices: t.Sequence[int],\n        batch_dim: int = 0,\n    ) -> list[Payload]:"}}
{"prompt": "def iso8601_datetime(\n    s: str,\n) -> Union[datetime.datetime, str]:", "metadata": {"task_id": "Communications/twilio-fatisar/27", "ground_truth": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT).replace(\n            tzinfo=datetime.timezone.utc\n        )\n    except (TypeError, ValueError):\n        return s", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "base", "deserialize.py"], "context_start_lineno": 26, "line_no": 34, "id": "twilio.base.deserialize.iso8601_datetime", "target_function_prompt": "def iso8601_datetime(\n    s: str,\n) -> Union[datetime.datetime, str]:", "function_signature": "def iso8601_datetime(\n    s: str,\n) -> Union[datetime.datetime, str]:"}}
{"prompt": "    def reset_search_text(self) -> None:", "metadata": {"task_id": "Communications/zulip-term/28", "ground_truth": "        self.set_caption(self.search_text)\n        self.set_edit_text(\"\")", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "boxes.py"], "context_start_lineno": 2037, "line_no": 2038, "id": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "target_function_prompt": "    def reset_search_text(self) -> None:", "function_signature": "    def reset_search_text(self) -> None:"}}
{"prompt": "    def pop(self, index=None):", "metadata": {"task_id": "Utilities/boltons/90", "ground_truth": "        item_index_map = self.item_index_map\n        len_self = len(item_index_map)\n        if index is None or index == -1 or index == len_self - 1:\n            ret = self.item_list.pop()\n            del item_index_map[ret]\n        else:\n            real_index = self._get_real_index(index)\n            ret = self.item_list[real_index]\n            self.item_list[real_index] = _MISSING\n            del item_index_map[ret]\n            self._add_dead(real_index)\n        self._cull()\n        return ret", "fpath_tuple": ["Utilities", "boltons", "boltons", "setutils.py"], "context_start_lineno": 425, "line_no": 427, "id": "boltons.setutils.IndexedSet.pop", "target_function_prompt": "    def pop(self, index=None):", "function_signature": "    def pop(self, index=None):"}}
{"prompt": "    def get_spark_submit_bin(self):", "metadata": {"task_id": "System/mrjob/88", "ground_truth": "        if not self._spark_submit_bin:\n            self._spark_submit_bin = self._find_spark_submit_bin()\n        return self._spark_submit_bin", "fpath_tuple": ["System", "mrjob", "mrjob", "bin.py"], "context_start_lineno": 870, "line_no": 873, "id": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "target_function_prompt": "    def get_spark_submit_bin(self):", "function_signature": "    def get_spark_submit_bin(self):"}}
{"prompt": "def parse(\n    data: str,\n    raw: bool = False,\n    quiet: bool = False\n) -> JSONDictType:", "metadata": {"task_id": "Utilities/jc/6", "ground_truth": "    jc.utils.compatibility(__name__, info.compatible, quiet)\n    raw_output = jc.parsers.kv.parse(data, raw, quiet)\n\n    return raw_output if raw else _process(raw_output)", "fpath_tuple": ["Utilities", "jc", "jc", "parsers", "os_release.py"], "context_start_lineno": 91, "line_no": 109, "id": "jc.parsers.os_release.parse", "target_function_prompt": "def parse(\n    data: str,\n    raw: bool = False,\n    quiet: bool = False\n) -> JSONDictType:", "function_signature": "def parse(\n    data: str,\n    raw: bool = False,\n    quiet: bool = False\n) -> JSONDictType:"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/109", "ground_truth": "    from boto.regioninfo import connect\n    return connect('sts', region_name, connection_cls=STSConnection,\n                   **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "sts", "__init__.py"], "context_start_lineno": 37, "line_no": 49, "id": "boto.sts.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def reverse(self) -> DropColumnOp:", "metadata": {"task_id": "Database/alembic/45", "ground_truth": "        return DropColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self.column\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 2011, "line_no": 2012, "id": "alembic.operations.ops.AddColumnOp.reverse", "target_function_prompt": "    def reverse(self) -> DropColumnOp:", "function_signature": "    def reverse(self) -> DropColumnOp:"}}
{"prompt": "def combine_jobconfs(*jobconfs):", "metadata": {"task_id": "System/mrjob/89", "ground_truth": "    j = combine_dicts(*jobconfs)\n\n    return {k: _to_java_str(v) for k, v in j.items() if v is not None}", "fpath_tuple": ["System", "mrjob", "mrjob", "conf.py"], "context_start_lineno": 509, "line_no": 513, "id": "mrjob.conf.combine_jobconfs", "target_function_prompt": "def combine_jobconfs(*jobconfs):", "function_signature": "def combine_jobconfs(*jobconfs):"}}
{"prompt": "def _parse_progress_from_resource_manager(html_bytes):", "metadata": {"task_id": "System/mrjob/90", "ground_truth": "    for line in html_bytes.splitlines():\n        m = _RESOURCE_MANAGER_JS_RE.match(line)\n        if m:\n            return float(m.group('percent'))\n\n    return None", "fpath_tuple": ["System", "mrjob", "mrjob", "parse.py"], "context_start_lineno": 196, "line_no": 203, "id": "mrjob.parse._parse_progress_from_resource_manager", "target_function_prompt": "def _parse_progress_from_resource_manager(html_bytes):", "function_signature": "def _parse_progress_from_resource_manager(html_bytes):"}}
{"prompt": "    def make_names_data(limit=None):", "metadata": {"task_id": "System/pyinfra/14", "ground_truth": "        from pyinfra.api.exceptions import InventoryError\n        mech_ssh_info = get_mech_config(limit)\n\n        logger.debug(\"Got Mech SSH info: \\n%s\", mech_ssh_info)\n\n        hosts = []\n        current_host = None\n\n        for line in mech_ssh_info:\n            if not line:\n                if current_host:\n                    hosts.append(_make_name_data(current_host))\n\n                current_host = None\n                continue\n\n            key, value = line.strip().split(\" \", 1)\n\n            if key == \"Host\":\n                if current_host:\n                    hosts.append(_make_name_data(current_host))\n\n                # Set the new host\n                current_host = {\n                    key: value,\n                }\n\n            elif current_host:\n                current_host[key] = value\n\n            else:\n                logger.debug(\"Extra Mech SSH key/value (%s=%s)\", key, value)\n\n        if current_host:\n            hosts.append(_make_name_data(current_host))\n\n        if not hosts:\n            raise InventoryError(\"No running Mech instances found!\")\n\n        return hosts", "fpath_tuple": ["System", "pyinfra", "pyinfra", "connectors", "mech.py"], "context_start_lineno": 149, "line_no": 150, "id": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "target_function_prompt": "    def make_names_data(limit=None):", "function_signature": "    def make_names_data(limit=None):"}}
{"prompt": "    def pre_refresh_callback(self, authorizer):", "metadata": {"task_id": "Utilities/praw/12", "ground_truth": "        assert authorizer.refresh_token is None\n        authorizer.refresh_token = self._get()", "fpath_tuple": ["Utilities", "praw", "praw", "util", "token_manager.py"], "context_start_lineno": 175, "line_no": 177, "id": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "target_function_prompt": "    def pre_refresh_callback(self, authorizer):", "function_signature": "    def pre_refresh_callback(self, authorizer):"}}
{"prompt": "def run_ldd(ldd, binary):", "metadata": {"task_id": "System/exodus-bundler/13", "ground_truth": "    if not detect_elf_binary(resolve_binary(binary)):\n        raise InvalidElfBinaryError('The \"%s\" file is not a binary ELF file.' % binary)\n\n    process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n    stdout, stderr = process.communicate()\n    return stdout.decode('utf-8').split('\\n') + stderr.decode('utf-8').split('\\n')", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "bundling.py"], "context_start_lineno": 218, "line_no": 220, "id": "exodus_bundler.bundling.run_ldd", "target_function_prompt": "def run_ldd(ldd, binary):", "function_signature": "def run_ldd(ldd, binary):"}}
{"prompt": "    def access(self, resp, req, environ, request_time):", "metadata": {"task_id": "Utilities/gunicorn/22", "ground_truth": "        Logger.access(self, resp, req, environ, request_time)\n        duration_in_ms = request_time.seconds * 1000 + float(request_time.microseconds) / 10 ** 3\n        status = resp.status\n        if isinstance(status, str):\n            status = int(status.split(None, 1)[0])\n        self.histogram(\"gunicorn.request.duration\", duration_in_ms)\n        self.increment(\"gunicorn.requests\", 1)\n        self.increment(\"gunicorn.request.status.%d\" % status, 1)", "fpath_tuple": ["Utilities", "gunicorn", "gunicorn", "instrument", "statsd.py"], "context_start_lineno": 93, "line_no": 97, "id": "gunicorn.instrument.statsd.Statsd.access", "target_function_prompt": "    def access(self, resp, req, environ, request_time):", "function_signature": "    def access(self, resp, req, environ, request_time):"}}
{"prompt": "    def settings(self):", "metadata": {"task_id": "Internet/pyramid/89", "ground_truth": "        settings = self.registry.settings\n        if settings is None:\n            settings = {}\n        return settings", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "renderers.py"], "context_start_lineno": 416, "line_no": 417, "id": "pyramid.renderers.RendererHelper.settings", "target_function_prompt": "    def settings(self):", "function_signature": "    def settings(self):"}}
{"prompt": "    def set(self, request, value):", "metadata": {"task_id": "Internet/pyramid/90", "ground_truth": "        already_set = request in self._store\n        self._store[request] = value\n\n        # avoid registering the callback more than once\n        if not already_set:\n            request.add_finished_callback(self._store.pop)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "request.py"], "context_start_lineno": 435, "line_no": 440, "id": "pyramid.request.RequestLocalCache.set", "target_function_prompt": "    def set(self, request, value):", "function_signature": "    def set(self, request, value):"}}
{"prompt": "def format_mentions(text, format_callback=format_mention):", "metadata": {"task_id": "Communications/twtxt/7", "ground_truth": "    def handle_mention(match):\n        name, url = match.groups()\n        return format_callback(name, url)\n\n    return mention_re.sub(handle_mention, text)", "fpath_tuple": ["Communications", "twtxt", "twtxt", "mentions.py"], "context_start_lineno": 68, "line_no": 80, "id": "twtxt.mentions.format_mentions", "target_function_prompt": "def format_mentions(text, format_callback=format_mention):", "function_signature": "def format_mentions(text, format_callback=format_mention):"}}
{"prompt": "def refactor_dataframes(conn, dataframes, foreign_keys, index_fts):", "metadata": {"task_id": "Database/csvs-to-sqlite/1", "ground_truth": "    lookup_tables = {}\n    for column, (table_name, value_column) in foreign_keys.items():\n        # Now apply this to the dataframes\n        for dataframe in dataframes:\n            if column in dataframe.columns:\n                lookup_table = lookup_tables.get(table_name)\n                if lookup_table is None:\n                    lookup_table = LookupTable(\n                        conn=conn,\n                        table_name=table_name,\n                        value_column=value_column,\n                        index_fts=index_fts,\n                    )\n                    lookup_tables[table_name] = lookup_table\n                dataframe[column] = dataframe[column].apply(lookup_table.id_for_value)\n    return dataframes", "fpath_tuple": ["Database", "csvs-to-sqlite", "csvs_to_sqlite", "utils.py"], "context_start_lineno": 237, "line_no": 238, "id": "csvs_to_sqlite.utils.refactor_dataframes", "target_function_prompt": "def refactor_dataframes(conn, dataframes, foreign_keys, index_fts):", "function_signature": "def refactor_dataframes(conn, dataframes, foreign_keys, index_fts):"}}
{"prompt": "def stamp(\n    config: Config,\n    revision: _RevIdType,\n    sql: bool = False,\n    tag: Optional[str] = None,\n    purge: bool = False,\n) -> None:", "metadata": {"task_id": "Database/alembic/46", "ground_truth": "    script = ScriptDirectory.from_config(config)\n\n    if sql:\n        destination_revs = []\n        starting_rev = None\n        for _revision in util.to_list(revision):\n            if \":\" in _revision:\n                srev, _revision = _revision.split(\":\", 2)\n\n                if starting_rev != srev:\n                    if starting_rev is None:\n                        starting_rev = srev\n                    else:\n                        raise util.CommandError(\n                            \"Stamp operation with --sql only supports a \"\n                            \"single starting revision at a time\"\n                        )\n            destination_revs.append(_revision)\n    else:\n        destination_revs = util.to_list(revision)\n\n    def do_stamp(rev, context):\n        return script._stamp_revs(util.to_tuple(destination_revs), rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=do_stamp,\n        as_sql=sql,\n        starting_rev=starting_rev if sql else None,\n        destination_rev=util.to_tuple(destination_revs),\n        tag=tag,\n        purge=purge,\n    ):\n        script.run_env()", "fpath_tuple": ["Database", "alembic", "alembic", "command.py"], "context_start_lineno": 622, "line_no": 650, "id": "alembic.command.stamp", "target_function_prompt": "def stamp(\n    config: Config,\n    revision: _RevIdType,\n    sql: bool = False,\n    tag: Optional[str] = None,\n    purge: bool = False,\n) -> None:", "function_signature": "def stamp(\n    config: Config,\n    revision: _RevIdType,\n    sql: bool = False,\n    tag: Optional[str] = None,\n    purge: bool = False,\n) -> None:"}}
{"prompt": "    def prepare_full(self):", "metadata": {"task_id": "Internet/boto/110", "ground_truth": "        final_data = {}\n\n        for key, value in self._data.items():\n            if not self._is_storable(value):\n                continue\n\n            final_data[key] = self._dynamizer.encode(value)\n\n        return final_data", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "items.py"], "context_start_lineno": 313, "line_no": 322, "id": "boto.dynamodb2.items.Item.prepare_full", "target_function_prompt": "    def prepare_full(self):", "function_signature": "    def prepare_full(self):"}}
{"prompt": "    def update(self, resp):", "metadata": {"task_id": "Internet/google-api-python-client/9", "ground_truth": "        for json_name, param_name in CHANNEL_PARAMS.items():\n            value = resp.get(json_name)\n            if value is not None:\n                setattr(self, param_name, value)", "fpath_tuple": ["Internet", "google-api-python-client", "googleapiclient", "channel.py"], "context_start_lineno": 234, "line_no": 244, "id": "googleapiclient.channel.Channel.update", "target_function_prompt": "    def update(self, resp):", "function_signature": "    def update(self, resp):"}}
{"prompt": "    def attachment_state(self):", "metadata": {"task_id": "Internet/boto/111", "ground_truth": "        state = None\n        if self.attach_data:\n            state = self.attach_data.status\n        return state", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "volume.py"], "context_start_lineno": 212, "line_no": 216, "id": "boto.ec2.volume.Volume.attachment_state", "target_function_prompt": "    def attachment_state(self):", "function_signature": "    def attachment_state(self):"}}
{"prompt": "def display_txt_frames(txt_frames, stdout, num_loops, seconds_per_frame):", "metadata": {"task_id": "Multimedia/gif-for-cli/4", "ground_truth": "    from .constants import ANSI_RESET\n    from .constants import ANSI_CURSOR_UP\n    previous_line_count = 0\n    remaining_loops = num_loops or None\n\n    try:\n        while remaining_loops is None or remaining_loops > 0:\n            for txt_frame in txt_frames:\n                stdout.write(ANSI_CURSOR_UP * previous_line_count)\n                stdout.write(txt_frame)\n                stdout.write('\\n')\n                stdout.flush()\n                previous_line_count = len(txt_frames[0].split('\\n'))\n                time.sleep(seconds_per_frame)\n\n            if remaining_loops is not None:\n                remaining_loops -= 1\n        stdout.write(ANSI_RESET)\n    except KeyboardInterrupt:\n        # ensure styling is reset\n        stdout.write(ANSI_RESET)\n        # we'll want an extra new line if CTRL+C was pressed\n        stdout.write('\\n')\n\n    stdout.flush()", "fpath_tuple": ["Multimedia", "gif-for-cli", "gif_for_cli", "display.py"], "context_start_lineno": 21, "line_no": 22, "id": "gif_for_cli.display.display_txt_frames", "target_function_prompt": "def display_txt_frames(txt_frames, stdout, num_loops, seconds_per_frame):", "function_signature": "def display_txt_frames(txt_frames, stdout, num_loops, seconds_per_frame):"}}
{"prompt": "    def target(self):", "metadata": {"task_id": "Utilities/python-for-android/27", "ground_truth": "        target_data = self.command_prefix.split('-')\n        return '{triplet}{ndk_api}'.format(\n            triplet='-'.join(['armv7a', target_data[1], target_data[2]]),\n            ndk_api=self.ctx.ndk_api,\n        )", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "archs.py"], "context_start_lineno": 243, "line_no": 244, "id": "pythonforandroid.archs.ArchARM.target", "target_function_prompt": "    def target(self):", "function_signature": "    def target(self):"}}
{"prompt": "    def delete_keys(self, keys):", "metadata": {"task_id": "System/wal-e/10", "ground_truth": "        for k in keys:\n            key_path = os.path.join(\"/\", k.strip(\"/\"))\n            os.remove(key_path)\n        # deleting files can leave empty dirs => trim them\n        common_path = os.path.join(\"/\", common_dir_path(keys).strip(\"/\"))\n        remove_empty_dirs(common_path)", "fpath_tuple": ["System", "wal-e", "wal_e", "blobstore", "file", "calling_format.py"], "context_start_lineno": 63, "line_no": 64, "id": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "target_function_prompt": "    def delete_keys(self, keys):", "function_signature": "    def delete_keys(self, keys):"}}
{"prompt": "def inspect_sinks(app: App) -> 'List[SinkInfo]':", "metadata": {"task_id": "Internet/falcon/44", "ground_truth": "    sinks = []\n    for prefix, sink, _ in app._sinks:\n        source_info, name = _get_source_info_and_name(sink)\n        info = SinkInfo(prefix.pattern, name, source_info)\n        sinks.append(info)\n    return sinks", "fpath_tuple": ["Internet", "falcon", "falcon", "inspect.py"], "context_start_lineno": 122, "line_no": 132, "id": "falcon.inspect.inspect_sinks", "target_function_prompt": "def inspect_sinks(app: App) -> 'List[SinkInfo]':", "function_signature": "def inspect_sinks(app: App) -> 'List[SinkInfo]':"}}
{"prompt": "    def from_hex(cls, hex):", "metadata": {"task_id": "Utilities/boltons/91", "ground_truth": "        if isinstance(hex, bytes):\n            hex = hex.decode('ascii')\n        if not hex.startswith('0x'):\n            hex = '0x' + hex\n        return cls(hex)", "fpath_tuple": ["Utilities", "boltons", "boltons", "mathutils.py"], "context_start_lineno": 243, "line_no": 244, "id": "boltons.mathutils.Bits.from_hex", "target_function_prompt": "    def from_hex(cls, hex):", "function_signature": "    def from_hex(cls, hex):"}}
{"prompt": "    def prefix(self):", "metadata": {"task_id": "Internet/falcon/45", "ground_truth": "        if self._cached_prefix is None:\n            self._cached_prefix = self.scheme + '://' + self.netloc + self.app\n\n        return self._cached_prefix", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 814, "line_no": 815, "id": "falcon.request.Request.prefix", "target_function_prompt": "    def prefix(self):", "function_signature": "    def prefix(self):"}}
{"prompt": "    def rm(self, path_glob):", "metadata": {"task_id": "System/mrjob/91", "ground_truth": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                log.debug('Recursively deleting %s' % path)\n                shutil.rmtree(path)\n            else:\n                log.debug('Deleting %s' % path)\n                os.remove(path)", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "local.py"], "context_start_lineno": 72, "line_no": 73, "id": "mrjob.fs.local.LocalFilesystem.rm", "target_function_prompt": "    def rm(self, path_glob):", "function_signature": "    def rm(self, path_glob):"}}
{"prompt": "    def target(self):\n        # As of NDK r19, the toolchains installed by default with the\n        # NDK may be used in-place. The make_standalone_toolchain.py script\n        # is no longer needed for interfacing with arbitrary build systems.\n        # See: https://developer.android.com/ndk/guides/other_build_systems", "metadata": {"task_id": "Utilities/python-for-android/28", "ground_truth": "        return '{triplet}{ndk_api}'.format(\n            triplet=self.command_prefix, ndk_api=self.ctx.ndk_api\n        )", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "archs.py"], "context_start_lineno": 71, "line_no": 76, "id": "pythonforandroid.archs.Arch.target", "target_function_prompt": "    def target(self):\n        # As of NDK r19, the toolchains installed by default with the\n        # NDK may be used in-place. The make_standalone_toolchain.py script\n        # is no longer needed for interfacing with arbitrary build systems.\n        # See: https://developer.android.com/ndk/guides/other_build_systems", "function_signature": "    def target(self):\n        # As of NDK r19, the toolchains installed by default with the\n        # NDK may be used in-place. The make_standalone_toolchain.py script\n        # is no longer needed for interfacing with arbitrary build systems.\n        # See: https://developer.android.com/ndk/guides/other_build_systems"}}
{"prompt": "def set_default_providers_for_netcode(netcode, provider_list):", "metadata": {"task_id": "Security/pycoin/32", "ground_truth": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list", "fpath_tuple": ["Security", "pycoin", "pycoin", "services", "providers.py"], "context_start_lineno": 144, "line_no": 145, "id": "pycoin.services.providers.set_default_providers_for_netcode", "target_function_prompt": "def set_default_providers_for_netcode(netcode, provider_list):", "function_signature": "def set_default_providers_for_netcode(netcode, provider_list):"}}
{"prompt": "    def update(self, iterable):", "metadata": {"task_id": "Utilities/boltons/92", "ground_truth": "        if type(iterable) is type(self):\n            other = iterable\n            for k in other.data:\n                if k not in self.data:\n                    self.data[k] = other.data[k]\n                else:\n                    self.data[k].update(other.data[k])\n            for k in other.inv.data:\n                if k not in self.inv.data:\n                    self.inv.data[k] = other.inv.data[k]\n                else:\n                    self.inv.data[k].update(other.inv.data[k])\n        elif callable(getattr(iterable, 'keys', None)):\n            for k in iterable.keys():\n                self.add(k, iterable[k])\n        else:\n            for key, val in iterable:\n                self.add(key, val)\n        return", "fpath_tuple": ["Utilities", "boltons", "boltons", "dictutils.py"], "context_start_lineno": 944, "line_no": 946, "id": "boltons.dictutils.ManyToMany.update", "target_function_prompt": "    def update(self, iterable):", "function_signature": "    def update(self, iterable):"}}
{"prompt": "    def from_object(cls, data, headers=_MISSING, max_depth=1, metadata=None):", "metadata": {"task_id": "Utilities/boltons/93", "ground_truth": "        return cls.from_data(data=data, headers=headers,\n                             max_depth=max_depth, _data_type=ObjectInputType(),\n                             metadata=metadata)", "fpath_tuple": ["Utilities", "boltons", "boltons", "tableutils.py"], "context_start_lineno": 342, "line_no": 348, "id": "boltons.tableutils.Table.from_object", "target_function_prompt": "    def from_object(cls, data, headers=_MISSING, max_depth=1, metadata=None):", "function_signature": "    def from_object(cls, data, headers=_MISSING, max_depth=1, metadata=None):"}}
{"prompt": "    def _pick_error(self, log_interpretation, step_type):", "metadata": {"task_id": "System/mrjob/92", "ground_truth": "        from mrjob.logs.errors import _pick_error_attempt_ids\n        from mrjob.logs.errors import _pick_error\n        logs_needed = self._logs_needed_to_pick_error(step_type)\n\n        if self._read_logs() and not all(\n                log_type in log_interpretation for log_type in logs_needed):\n            log.info('Scanning logs for probable cause of failure...')\n\n            if 'step' in logs_needed:\n                self._interpret_step_logs(log_interpretation, step_type)\n\n            if 'history' in logs_needed:\n                self._interpret_history_log(log_interpretation)\n\n            if 'task' in logs_needed:\n                error_attempt_ids = _pick_error_attempt_ids(log_interpretation)\n\n                self._interpret_task_logs(\n                    log_interpretation, step_type, error_attempt_ids)\n\n        return _pick_error(log_interpretation)", "fpath_tuple": ["System", "mrjob", "mrjob", "logs", "mixin.py"], "context_start_lineno": 117, "line_no": 119, "id": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "target_function_prompt": "    def _pick_error(self, log_interpretation, step_type):", "function_signature": "    def _pick_error(self, log_interpretation, step_type):"}}
{"prompt": "def laplace_smooth_counts(\n    seq1_counts: DefaultDict[str, int],\n    seq2_counts: DefaultDict[str, DefaultDict[str, int]],\n    param_counts: DefaultDict[str, int],\n    cmd_param_counts: DefaultDict[str, DefaultDict[str, int]],\n    start_token: str,\n    end_token: str,\n    unk_token: str,\n):", "metadata": {"task_id": "Security/msticpy/17", "ground_truth": "    from ..utils.laplace_smooth import laplace_smooth_cmd_counts\n    from ..utils.laplace_smooth import laplace_smooth_param_counts\n    cmds: List[str] = list(seq1_counts.keys()) + [unk_token]\n\n    # apply laplace smoothing for cmds\n    seq1_counts_ls, seq2_counts_ls = laplace_smooth_cmd_counts(\n        seq1_counts=copy.deepcopy(seq1_counts),\n        seq2_counts=copy.deepcopy(seq2_counts),\n        start_token=start_token,\n        end_token=end_token,\n        unk_token=unk_token,\n    )\n\n    # apply laplace smoothing for params\n    param_counts_ls, cmd_param_counts_ls = laplace_smooth_param_counts(\n        cmds=cmds,\n        param_counts=copy.deepcopy(param_counts),\n        cmd_param_counts=copy.deepcopy(cmd_param_counts),\n        unk_token=unk_token,\n    )\n\n    seq1_counts_sm = StateMatrix(states=seq1_counts_ls, unk_token=unk_token)\n    seq2_counts_sm = StateMatrix(states=seq2_counts_ls, unk_token=unk_token)\n    param_counts_sm = StateMatrix(states=param_counts_ls, unk_token=unk_token)\n    cmd_param_counts_sm = StateMatrix(states=cmd_param_counts_ls, unk_token=unk_token)\n\n    return seq1_counts_sm, seq2_counts_sm, param_counts_sm, cmd_param_counts_sm", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "cmds_params_only.py"], "context_start_lineno": 91, "line_no": 135, "id": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "target_function_prompt": "def laplace_smooth_counts(\n    seq1_counts: DefaultDict[str, int],\n    seq2_counts: DefaultDict[str, DefaultDict[str, int]],\n    param_counts: DefaultDict[str, int],\n    cmd_param_counts: DefaultDict[str, DefaultDict[str, int]],\n    start_token: str,\n    end_token: str,\n    unk_token: str,\n):", "function_signature": "def laplace_smooth_counts(\n    seq1_counts: DefaultDict[str, int],\n    seq2_counts: DefaultDict[str, DefaultDict[str, int]],\n    param_counts: DefaultDict[str, int],\n    cmd_param_counts: DefaultDict[str, DefaultDict[str, int]],\n    start_token: str,\n    end_token: str,\n    unk_token: str,\n):"}}
{"prompt": "def l33t_match(password, _ranked_dictionaries=RANKED_DICTIONARIES,\n               _l33t_table=L33T_TABLE):", "metadata": {"task_id": "Security/zxcvbn-python/16", "ground_truth": "    matches = []\n\n    for sub in enumerate_l33t_subs(\n            relevant_l33t_subtable(password, _l33t_table)):\n        if not len(sub):\n            break\n\n        subbed_password = translate(password, sub)\n        for match in dictionary_match(subbed_password, _ranked_dictionaries):\n            token = password[match['i']:match['j'] + 1]\n            if token.lower() == match['matched_word']:\n                # only return the matches that contain an actual substitution\n                continue\n\n            # subset of mappings in sub that are in use for this match\n            match_sub = {}\n            for subbed_chr, chr in sub.items():\n                if subbed_chr in token:\n                    match_sub[subbed_chr] = chr\n            match['l33t'] = True\n            match['token'] = token\n            match['sub'] = match_sub\n            match['sub_display'] = ', '.join(\n                [\"%s -> %s\" % (k, v) for k, v in match_sub.items()]\n            )\n            matches.append(match)\n\n    matches = [match for match in matches if len(match['token']) > 1]\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "matching.py"], "context_start_lineno": 214, "line_no": 216, "id": "zxcvbn.matching.l33t_match", "target_function_prompt": "def l33t_match(password, _ranked_dictionaries=RANKED_DICTIONARIES,\n               _l33t_table=L33T_TABLE):", "function_signature": "def l33t_match(password, _ranked_dictionaries=RANKED_DICTIONARIES,\n               _l33t_table=L33T_TABLE):"}}
{"prompt": "    def write(self, s):", "metadata": {"task_id": "Utilities/boltons/94", "ground_truth": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                binary_type.__name__,\n                type(s).__name__\n            ))\n\n        if self.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s)", "fpath_tuple": ["Utilities", "boltons", "boltons", "ioutils.py"], "context_start_lineno": 316, "line_no": 317, "id": "boltons.ioutils.SpooledBytesIO.write", "target_function_prompt": "    def write(self, s):", "function_signature": "    def write(self, s):"}}
{"prompt": "    def where(self, *expressions):", "metadata": {"task_id": "Software-Development/peewee/11", "ground_truth": "        if self._where is not None:\n            expressions = (self._where,) + expressions\n        self._where = reduce(operator.and_, expressions)", "fpath_tuple": ["Software-Development", "peewee", "peewee.py"], "context_start_lineno": 2889, "line_no": 2890, "id": "peewee.Index.where", "target_function_prompt": "    def where(self, *expressions):", "function_signature": "    def where(self, *expressions):"}}
{"prompt": "", "metadata": {"task_id": "Database/datasette/39", "ground_truth": "    @classmethod\n    def json(cls, body, status=200, headers=None, default=None):\n        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "asgi.py"], "context_start_lineno": 401, "line_no": 402, "id": "datasette.utils.asgi.Response.json", "target_function_prompt": "", "function_signature": ""}}
{"prompt": "def obvious_conflict_checker(ctx, name_tuples, blacklist=None):", "metadata": {"task_id": "Utilities/python-for-android/29", "ground_truth": "    deps_were_added_by = dict()\n    deps = set()\n    if blacklist is None:\n        blacklist = set()\n\n    # Add dependencies for all recipes:\n    to_be_added = [(name_tuple, None) for name_tuple in name_tuples]\n    while len(to_be_added) > 0:\n        current_to_be_added = list(to_be_added)\n        to_be_added = []\n        for (added_tuple, adding_recipe) in current_to_be_added:\n            assert type(added_tuple) is tuple\n            if len(added_tuple) > 1:\n                # No obvious commitment in what to add, don't check it itself\n                # but throw it into deps for later comparing against\n                # (Remember this function only catches obvious issues)\n                deps.add(added_tuple)\n                continue\n\n            name = added_tuple[0]\n            recipe_conflicts = set()\n            recipe_dependencies = []\n            try:\n                # Get recipe to add and who's ultimately adding it:\n                recipe = Recipe.get_recipe(name, ctx)\n                recipe_conflicts = {c.lower() for c in recipe.conflicts}\n                recipe_dependencies = get_dependency_tuple_list_for_recipe(\n                    recipe, blacklist=blacklist\n                )\n            except ValueError:\n                pass\n            adder_first_recipe_name = adding_recipe or name\n\n            # Collect the conflicts:\n            triggered_conflicts = []\n            for dep_tuple_list in deps:\n                # See if the new deps conflict with things added before:\n                if set(dep_tuple_list).intersection(\n                       recipe_conflicts) == set(dep_tuple_list):\n                    triggered_conflicts.append(dep_tuple_list)\n                    continue\n\n                # See if what was added before conflicts with the new deps:\n                if len(dep_tuple_list) > 1:\n                    # Not an obvious commitment to a specific recipe/dep\n                    # to be added, so we won't check.\n                    # (remember this function only catches obvious issues)\n                    continue\n                try:\n                    dep_recipe = Recipe.get_recipe(dep_tuple_list[0], ctx)\n                except ValueError:\n                    continue\n                conflicts = [c.lower() for c in dep_recipe.conflicts]\n                if name in conflicts:\n                    triggered_conflicts.append(dep_tuple_list)\n\n            # Throw error on conflict:\n            if triggered_conflicts:\n                # Get first conflict and see who added that one:\n                adder_second_recipe_name = \"'||'\".join(triggered_conflicts[0])\n                second_recipe_original_adder = deps_were_added_by.get(\n                    (adder_second_recipe_name,), None\n                )\n                if second_recipe_original_adder:\n                    adder_second_recipe_name = second_recipe_original_adder\n\n                # Prompt error:\n                raise BuildInterruptingException(\n                    \"Conflict detected: '{}'\"\n                    \" inducing dependencies {}, and '{}'\"\n                    \" inducing conflicting dependencies {}\".format(\n                        adder_first_recipe_name,\n                        (recipe.name,),\n                        adder_second_recipe_name,\n                        triggered_conflicts[0]\n                    ))\n\n            # Actually add it to our list:\n            deps.add(added_tuple)\n            deps_were_added_by[added_tuple] = adding_recipe\n\n            # Schedule dependencies to be added\n            to_be_added += [\n                (dep, adder_first_recipe_name or name)\n                for dep in recipe_dependencies\n                if dep not in deps\n            ]\n    # If we came here, then there were no obvious conflicts.\n    return None", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "graph.py"], "context_start_lineno": 145, "line_no": 151, "id": "pythonforandroid.graph.obvious_conflict_checker", "target_function_prompt": "def obvious_conflict_checker(ctx, name_tuples, blacklist=None):", "function_signature": "def obvious_conflict_checker(ctx, name_tuples, blacklist=None):"}}
{"prompt": "    def valid_char(self, ch: str) -> bool:\n        # This method 'strips' leading space *before* entering it in the box", "metadata": {"task_id": "Communications/zulip-term/29", "ground_truth": "        if self.edit_text:\n            # Use regular validation if already have text\n            return super().valid_char(ch)\n        elif len(ch) != 1:\n            # urwid expands some unicode to strings to be useful\n            # (so we need to work around eg 'backspace')\n            return False\n        else:\n            # Skip unicode 'Control characters' and 'space Zeperators'\n            # This includes various invalid characters and complex spaces\n            return unicodedata.category(ch) not in (\"Cc\", \"Zs\")", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "boxes.py"], "context_start_lineno": 2041, "line_no": 2043, "id": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "target_function_prompt": "    def valid_char(self, ch: str) -> bool:\n        # This method 'strips' leading space *before* entering it in the box", "function_signature": "    def valid_char(self, ch: str) -> bool:\n        # This method 'strips' leading space *before* entering it in the box"}}
{"prompt": "    def include_dirs(self):", "metadata": {"task_id": "Utilities/python-for-android/30", "ground_truth": "        return [\n            \"{}/{}\".format(\n                self.ctx.include_dir,\n                d.format(arch=self))\n            for d in self.ctx.include_dirs]", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "archs.py"], "context_start_lineno": 63, "line_no": 64, "id": "pythonforandroid.archs.Arch.include_dirs", "target_function_prompt": "    def include_dirs(self):", "function_signature": "    def include_dirs(self):"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/112", "ground_truth": "    from boto.regioninfo import connect\n    from boto.cloudtrail.layer1 import CloudTrailConnection\n    return connect('cloudtrail', region_name,\n                   connection_cls=CloudTrailConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "cloudtrail", "__init__.py"], "context_start_lineno": 37, "line_no": 38, "id": "boto.cloudtrail.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def shutdown(self) -> None:", "metadata": {"task_id": "Communications/IMAPClient/31", "ground_truth": "        self._imap.shutdown()\n        logger.info(\"Connection closed\")", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "imapclient.py"], "context_start_lineno": 526, "line_no": 532, "id": "imapclient.imapclient.IMAPClient.shutdown", "target_function_prompt": "    def shutdown(self) -> None:", "function_signature": "    def shutdown(self) -> None:"}}
{"prompt": "    def action(\n        self,\n        discriminator,\n        callable=None,\n        args=(),\n        kw=None,\n        order=0,\n        includepath=(),\n        info=None,\n        introspectables=(),\n        **extra,\n    ):", "metadata": {"task_id": "Internet/pyramid/91", "ground_truth": "        if kw is None:\n            kw = {}\n        action = extra\n        action.update(\n            dict(\n                discriminator=discriminator,\n                callable=callable,\n                args=args,\n                kw=kw,\n                includepath=includepath,\n                info=info,\n                order=order,\n                introspectables=introspectables,\n            )\n        )\n        self.actions.append(action)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "actions.py"], "context_start_lineno": 177, "line_no": 191, "id": "pyramid.config.actions.ActionState.action", "target_function_prompt": "    def action(\n        self,\n        discriminator,\n        callable=None,\n        args=(),\n        kw=None,\n        order=0,\n        includepath=(),\n        info=None,\n        introspectables=(),\n        **extra,\n    ):", "function_signature": "    def action(\n        self,\n        discriminator,\n        callable=None,\n        args=(),\n        kw=None,\n        order=0,\n        includepath=(),\n        info=None,\n        introspectables=(),\n        **extra,\n    ):"}}
{"prompt": "    def check(self):", "metadata": {"task_id": "Security/oletools/5", "ground_truth": "        self.ftg = ftguess.FileTypeGuesser(filepath=self.filename, data=self.data)\n        ftype = self.ftg.ftype\n        # if it's an unrecognized OLE file, display the root CLSID in description:\n        if self.ftg.filetype == ftguess.FTYPE.GENERIC_OLE:\n            description = 'Unrecognized OLE file. Root CLSID: {} - {}'.format(\n                self.ftg.root_clsid, self.ftg.root_clsid_name)\n        else:\n            description = ''\n        ft = Indicator('ftype', value=ftype.longname, _type=str, name='File format', risk=RISK.INFO,\n                       description=description)\n        self.indicators.append(ft)\n        ct = Indicator('container', value=ftype.container, _type=str, name='Container format', risk=RISK.INFO,\n                       description='Container type')\n        self.indicators.append(ct)\n\n        # check if it is actually an OLE file:\n        if self.ftg.container == ftguess.CONTAINER.OLE:\n            # reuse olefile already opened by ftguess\n            self.ole = self.ftg.olefile\n        # oleformat = Indicator('ole_format', True, name='OLE format')\n        # self.indicators.append(oleformat)\n        # if self.ole:\n        #     oleformat.value = True\n        # elif not olefile.isOleFile(self.filename):\n        #     oleformat.value = False\n        #     return self.indicators\n        # else:\n        #     # parse file:\n        #     self.ole = olefile.OleFileIO(self.filename)\n\n        # checks:\n        # TODO: add try/except around each check\n        self.check_properties()\n        self.check_encrypted()\n        self.check_macros()\n        self.check_external_relationships()\n        self.check_object_pool()\n        self.check_flash()\n        if self.ole is not None:\n            self.ole.close()\n        return self.indicators", "fpath_tuple": ["Security", "oletools", "oletools", "oleid.py"], "context_start_lineno": 258, "line_no": 264, "id": "oletools.oleid.OleID.check", "target_function_prompt": "    def check(self):", "function_signature": "    def check(self):"}}
{"prompt": "def discover_files(targets, excluded_files, recursive=False):", "metadata": {"task_id": "Security/python-taint/4", "ground_truth": "    included_files = list()\n    excluded_list = excluded_files.split(\",\")\n    for target in targets:\n        if os.path.isdir(target):\n            for root, _, files in os.walk(target):\n                for file in files:\n                    if file.endswith('.py') and file not in excluded_list:\n                        fullpath = os.path.join(root, file)\n                        included_files.append(fullpath)\n                        log.debug('Discovered file: %s', fullpath)\n                if not recursive:\n                    break\n        else:\n            if target not in excluded_list:\n                included_files.append(target)\n                log.debug('Discovered file: %s', target)\n    return included_files", "fpath_tuple": ["Security", "python-taint", "pyt", "__main__.py"], "context_start_lineno": 32, "line_no": 33, "id": "pyt.__main__.discover_files", "target_function_prompt": "def discover_files(targets, excluded_files, recursive=False):", "function_signature": "def discover_files(targets, excluded_files, recursive=False):"}}
{"prompt": "def from_shorthand(note, interval, up=True):", "metadata": {"task_id": "Multimedia/mingus/46", "ground_truth": "    if not notes.is_valid_note(note):\n        return False\n\n    # [shorthand, interval function up, interval function down]\n    shorthand_lookup = [\n        [\"1\", major_unison, major_unison],\n        [\"2\", major_second, minor_seventh],\n        [\"3\", major_third, minor_sixth],\n        [\"4\", major_fourth, major_fifth],\n        [\"5\", major_fifth, major_fourth],\n        [\"6\", major_sixth, minor_third],\n        [\"7\", major_seventh, minor_second],\n    ]\n\n    # Looking up last character in interval in shorthand_lookup and calling that\n    # function.\n    val = False\n    for shorthand in shorthand_lookup:\n        if shorthand[0] == interval[-1]:\n            if up:\n                val = shorthand[1](note)\n            else:\n                val = shorthand[2](note)\n\n    # warning Last character in interval should be 1-7\n    if val == False:\n        return False\n\n    # Collect accidentals\n    for x in interval:\n        if x == \"#\":\n            if up:\n                val = notes.augment(val)\n            else:\n                val = notes.diminish(val)\n        elif x == \"b\":\n            if up:\n                val = notes.diminish(val)\n            else:\n                val = notes.augment(val)\n        else:\n            return val", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "intervals.py"], "context_start_lineno": 432, "line_no": 444, "id": "mingus.core.intervals.from_shorthand", "target_function_prompt": "def from_shorthand(note, interval, up=True):", "function_signature": "def from_shorthand(note, interval, up=True):"}}
{"prompt": "    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.NpNDArray:", "metadata": {"task_id": "Scientific-Engineering/bentoml/34", "ground_truth": "        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return t.cast(\"ext.NpNDArray\", pep574_loads(bs, payload.data, indices))\n\n        return pickle.loads(payload.data)", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 308, "line_no": 312, "id": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "target_function_prompt": "    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.NpNDArray:", "function_signature": "    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.NpNDArray:"}}
{"prompt": "    def transpose(self, interval, up=True):", "metadata": {"task_id": "Multimedia/mingus/47", "ground_truth": "        from mingus.core import intervals\n        (old, o_octave) = (self.name, self.octave)\n        self.name = intervals.from_shorthand(self.name, interval, up)\n        if up:\n            if self < Note(old, o_octave):\n                self.octave += 1\n        else:\n            if self > Note(old, o_octave):\n                self.octave -= 1", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "note.py"], "context_start_lineno": 179, "line_no": 191, "id": "mingus.containers.note.Note.transpose", "target_function_prompt": "    def transpose(self, interval, up=True):", "function_signature": "    def transpose(self, interval, up=True):"}}
{"prompt": "def inspect_routes(app: App) -> 'List[RouteInfo]':", "metadata": {"task_id": "Internet/falcon/46", "ground_truth": "    router = app._router\n\n    inspect_function = _supported_routers.get(type(router))\n    if inspect_function is None:\n        raise TypeError(\n            'Unsupported router class {}. Use \"register_router\" '\n            'to register a function that can inspect the router '\n            'used by the provided application'.format(type(router))\n        )\n    return inspect_function(router)", "fpath_tuple": ["Internet", "falcon", "falcon", "inspect.py"], "context_start_lineno": 48, "line_no": 58, "id": "falcon.inspect.inspect_routes", "target_function_prompt": "def inspect_routes(app: App) -> 'List[RouteInfo]':", "function_signature": "def inspect_routes(app: App) -> 'List[RouteInfo]':"}}
{"prompt": "    def get_views(self, request):", "metadata": {"task_id": "Internet/pyramid/92", "ground_truth": "        if self.accepts and hasattr(request, 'accept'):\n            views = []\n            for offer, _ in request.accept.acceptable_offers(self.accepts):\n                views.extend(self.media_views[offer])\n            views.extend(self.views)\n            return views\n        return self.views", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "views.py"], "context_start_lineno": 114, "line_no": 115, "id": "pyramid.config.views.MultiView.get_views", "target_function_prompt": "    def get_views(self, request):", "function_signature": "    def get_views(self, request):"}}
{"prompt": "    def seek(self, pos, offset=io.SEEK_SET):", "metadata": {"task_id": "Security/oletools/6", "ground_truth": "        if offset == io.SEEK_SET:\n            new_pos = pos\n        elif offset == io.SEEK_CUR:\n            new_pos = self.pos + pos\n        elif offset == io.SEEK_END:\n            new_pos = self.size + pos\n        else:\n            raise ValueError(\"invalid offset {0}, need SEEK_* constant\"\n                             .format(offset))\n\n        # now get to that position, doing reads and resets as necessary\n        if new_pos < 0:\n            # print('ZipSubFile: Error: seek to {}'.format(new_pos))\n            raise IOError('Seek beyond start of file not allowed')\n        elif new_pos == self.pos:\n            # print('ZipSubFile: nothing to do')\n            pass\n        elif new_pos == 0:\n            # print('ZipSubFile: seek to start')\n            self.reset()\n        elif new_pos < self.pos:\n            # print('ZipSubFile: seek back')\n            self.reset()\n            self._seek_skip(new_pos)             # --> read --> update self.pos\n        elif new_pos < self.size:\n            # print('ZipSubFile: seek forward')\n            self._seek_skip(new_pos - self.pos)  # --> read --> update self.pos\n        else:   # new_pos >= self.size\n            # print('ZipSubFile: seek to end')\n            self.pos = new_pos    # fake being at the end; remember pos >= size", "fpath_tuple": ["Security", "oletools", "oletools", "ooxml.py"], "context_start_lineno": 327, "line_no": 330, "id": "oletools.ooxml.ZipSubFile.seek", "target_function_prompt": "    def seek(self, pos, offset=io.SEEK_SET):", "function_signature": "    def seek(self, pos, offset=io.SEEK_SET):"}}
{"prompt": "def get_copy_folder_location():", "metadata": {"task_id": "Utilities/mackup/6", "ground_truth": "    copy_settings_path = \"Library/Application Support/Copy Agent/config.db\"\n    copy_home = None\n\n    copy_settings = os.path.join(os.environ[\"HOME\"], copy_settings_path)\n\n    if os.path.isfile(copy_settings):\n        database = sqlite3.connect(copy_settings)\n        if database:\n            cur = database.cursor()\n            query = \"SELECT value \" \"FROM config2 \" \"WHERE option = 'csmRootPath';\"\n            cur.execute(query)\n            data = cur.fetchone()\n            copy_home = str(data[0])\n            cur.close()\n\n    if not copy_home:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Copy install\"))\n\n    return copy_home", "fpath_tuple": ["Utilities", "mackup", "mackup", "utils.py"], "context_start_lineno": 255, "line_no": 262, "id": "mackup.utils.get_copy_folder_location", "target_function_prompt": "def get_copy_folder_location():", "function_signature": "def get_copy_folder_location():"}}
{"prompt": "    def forwarded(self):\n        # PERF(kgriffs): We could DRY up this memoization pattern using\n        # a decorator, but that would incur additional overhead without\n        # resorting to some trickery to rewrite the body of the method\n        # itself (vs. simply wrapping it with some memoization logic).\n        # At some point we might look into this but I don't think\n        # it's worth it right now.", "metadata": {"task_id": "Internet/falcon/47", "ground_truth": "        from falcon.forwarded import _parse_forwarded_header\n        if self._cached_forwarded is None:\n            forwarded = self.get_header('Forwarded')\n            if forwarded is None:\n                return None\n\n            self._cached_forwarded = _parse_forwarded_header(forwarded)\n\n        return self._cached_forwarded", "fpath_tuple": ["Internet", "falcon", "falcon", "request.py"], "context_start_lineno": 558, "line_no": 565, "id": "falcon.request.Request.forwarded", "target_function_prompt": "    def forwarded(self):\n        # PERF(kgriffs): We could DRY up this memoization pattern using\n        # a decorator, but that would incur additional overhead without\n        # resorting to some trickery to rewrite the body of the method\n        # itself (vs. simply wrapping it with some memoization logic).\n        # At some point we might look into this but I don't think\n        # it's worth it right now.", "function_signature": "    def forwarded(self):\n        # PERF(kgriffs): We could DRY up this memoization pattern using\n        # a decorator, but that would incur additional overhead without\n        # resorting to some trickery to rewrite the body of the method\n        # itself (vs. simply wrapping it with some memoization logic).\n        # At some point we might look into this but I don't think\n        # it's worth it right now."}}
{"prompt": "    def from_batch_payloads(  # pylint: disable=arguments-differ\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:", "metadata": {"task_id": "Scientific-Engineering/bentoml/35", "ground_truth": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 442, "line_no": 447, "id": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "target_function_prompt": "    def from_batch_payloads(  # pylint: disable=arguments-differ\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:", "function_signature": "    def from_batch_payloads(  # pylint: disable=arguments-differ\n        cls,\n        payloads: t.Sequence[Payload],\n        batch_dim: int = 0,\n    ) -> tuple[ext.PdDataFrame, list[int]]:"}}
{"prompt": "    def get_item(self, consistent=False, attributes=None, **kwargs):", "metadata": {"task_id": "Internet/boto/113", "ground_truth": "        raw_key = self._encode_keys(kwargs)\n        item_data = self.connection.get_item(\n            self.table_name,\n            raw_key,\n            attributes_to_get=attributes,\n            consistent_read=consistent\n        )\n        if 'Item' not in item_data:\n            raise exceptions.ItemNotFound(\"Item %s couldn't be found.\" % kwargs)\n        item = Item(self)\n        item.load(item_data)\n        return item", "fpath_tuple": ["Internet", "boto", "boto", "dynamodb2", "table.py"], "context_start_lineno": 653, "line_no": 699, "id": "boto.dynamodb2.table.Table.get_item", "target_function_prompt": "    def get_item(self, consistent=False, attributes=None, **kwargs):", "function_signature": "    def get_item(self, consistent=False, attributes=None, **kwargs):"}}
{"prompt": "def get_valid_service_name(user_provided_svc_name: str) -> str:", "metadata": {"task_id": "Scientific-Engineering/bentoml/36", "ground_truth": "    lower_name = user_provided_svc_name.lower()\n\n    if user_provided_svc_name != lower_name:\n        logger.warning(\n            \"Converting %s to lowercase: %s.\", user_provided_svc_name, lower_name\n        )\n\n    # Service name must be a valid Tag name; create a dummy tag to use its validation\n    Tag(lower_name)\n    return lower_name", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "service", "service.py"], "context_start_lineno": 65, "line_no": 66, "id": "bentoml._internal.service.service.get_valid_service_name", "target_function_prompt": "def get_valid_service_name(user_provided_svc_name: str) -> str:", "function_signature": "def get_valid_service_name(user_provided_svc_name: str) -> str:"}}
{"prompt": "def build_sanitiser_node_dict(\n    cfg,\n    sinks_in_file\n):", "metadata": {"task_id": "Security/python-taint/5", "ground_truth": "    from .vulnerability_helper import Sanitiser\n    sanitisers = list()\n    for sink in sinks_in_file:\n        sanitisers.extend(sink.sanitisers)\n\n    sanitisers_in_file = list()\n    for sanitiser in sanitisers:\n        for cfg_node in cfg.nodes:\n            if sanitiser in cfg_node.label:\n                sanitisers_in_file.append(Sanitiser(sanitiser, cfg_node))\n\n    sanitiser_node_dict = dict()\n    for sanitiser in sanitisers:\n        sanitiser_node_dict[sanitiser] = list(find_sanitiser_nodes(\n            sanitiser,\n            sanitisers_in_file\n        ))\n    return sanitiser_node_dict", "fpath_tuple": ["Security", "python-taint", "pyt", "vulnerabilities", "vulnerabilities.py"], "context_start_lineno": 169, "line_no": 184, "id": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "target_function_prompt": "def build_sanitiser_node_dict(\n    cfg,\n    sinks_in_file\n):", "function_signature": "def build_sanitiser_node_dict(\n    cfg,\n    sinks_in_file\n):"}}
{"prompt": "    def disable_availability_zones(self, load_balancer_name, zones_to_remove):", "metadata": {"task_id": "Internet/boto/114", "ground_truth": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "elb", "__init__.py"], "context_start_lineno": 367, "line_no": 385, "id": "boto.ec2.elb.ELBConnection.disable_availability_zones", "target_function_prompt": "    def disable_availability_zones(self, load_balancer_name, zones_to_remove):", "function_signature": "    def disable_availability_zones(self, load_balancer_name, zones_to_remove):"}}
{"prompt": "def get_version():", "metadata": {"task_id": "Communications/hl7/5", "ground_truth": "    main_version = \"%s.%s.%s\" % VERSION[0:3]\n\n    if len(VERSION) < 4:\n        return main_version\n\n    version_type = VERSION[3]\n    if not version_type or version_type == \"final\":\n        return main_version\n    elif version_type == \"dev\":\n        return \"%s.dev\" % main_version\n    else:\n        return \"%s%s\" % (main_version, version_type)", "fpath_tuple": ["Communications", "hl7", "hl7", "version.py"], "context_start_lineno": 11, "line_no": 19, "id": "hl7.version.get_version", "target_function_prompt": "def get_version():", "function_signature": "def get_version():"}}
{"prompt": "def validate_colors(theme_name: str, color_depth: int) -> None:", "metadata": {"task_id": "Communications/zulip-term/30", "ground_truth": "    theme_colors = THEMES[theme_name].Color\n    failure_text = []\n    if color_depth == 16:\n        for color in theme_colors:\n            color_16code = color.value.split()[0]\n            if color_16code not in valid_16_color_codes:\n                invalid_16_color_code = str(color.name)\n                failure_text.append(f\"- {invalid_16_color_code} = {color_16code}\")\n        if failure_text == []:\n            return\n        else:\n            text = \"\\n\".join(\n                [f\"Invalid 16-color codes in theme '{theme_name}':\"] + failure_text\n            )\n            raise InvalidThemeColorCode(text)", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "config", "themes.py"], "context_start_lineno": 166, "line_no": 173, "id": "zulipterminal.config.themes.validate_colors", "target_function_prompt": "def validate_colors(theme_name: str, color_depth: int) -> None:", "function_signature": "def validate_colors(theme_name: str, color_depth: int) -> None:"}}
{"prompt": "    def rollback(self):\n        # Rollback is a no-op when there is no uncommitted pages", "metadata": {"task_id": "Database/bplustree/16", "ground_truth": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)", "fpath_tuple": ["Database", "bplustree", "bplustree", "memory.py"], "context_start_lineno": 422, "line_no": 424, "id": "bplustree.memory.WAL.rollback", "target_function_prompt": "    def rollback(self):\n        # Rollback is a no-op when there is no uncommitted pages", "function_signature": "    def rollback(self):\n        # Rollback is a no-op when there is no uncommitted pages"}}
{"prompt": "def rebuild_quotas(storage, dry_run=False):", "metadata": {"task_id": "Internet/kinto/38", "ground_truth": "    from .listener import BUCKET_QUOTA_OBJECT_ID\n    for bucket in paginated(storage, resource_name=\"bucket\", parent_id=\"\", sorting=[OLDEST_FIRST]):\n        bucket_id = bucket[\"id\"]\n        bucket_path = f\"/buckets/{bucket['id']}\"\n        bucket_collection_count = 0\n        bucket_record_count = 0\n        bucket_storage_size = record_size(bucket)\n\n        for collection in paginated(\n            storage, resource_name=\"collection\", parent_id=bucket_path, sorting=[OLDEST_FIRST]\n        ):\n            collection_info = rebuild_quotas_collection(storage, bucket_id, collection, dry_run)\n            (collection_record_count, collection_storage_size) = collection_info\n            bucket_collection_count += 1\n            bucket_record_count += collection_record_count\n            bucket_storage_size += collection_storage_size\n\n        bucket_record = {\n            \"record_count\": bucket_record_count,\n            \"storage_size\": bucket_storage_size,\n            \"collection_count\": bucket_collection_count,\n        }\n        if not dry_run:\n            storage.update(\n                resource_name=\"quota\",\n                parent_id=bucket_path,\n                object_id=BUCKET_QUOTA_OBJECT_ID,\n                obj=bucket_record,\n            )\n\n        logger.info(\n            f\"Bucket {bucket_id}. Final size: {bucket_collection_count} collections, {bucket_record_count} records, {bucket_storage_size} bytes.\"\n        )", "fpath_tuple": ["Internet", "kinto", "kinto", "plugins", "quotas", "scripts.py"], "context_start_lineno": 16, "line_no": 17, "id": "kinto.plugins.quotas.scripts.rebuild_quotas", "target_function_prompt": "def rebuild_quotas(storage, dry_run=False):", "function_signature": "def rebuild_quotas(storage, dry_run=False):"}}
{"prompt": "def suggest_type(full_text, text_before_cursor):", "metadata": {"task_id": "Database/litecli/4", "ground_truth": "    from litecli.encodingutils import text_type\n    word_before_cursor = last_word(text_before_cursor, include=\"many_punctuations\")\n\n    identifier = None\n\n    # here should be removed once sqlparse has been fixed\n    try:\n        # If we've partially typed a word then word_before_cursor won't be an empty\n        # string. In that case we want to remove the partially typed string before\n        # sending it to the sqlparser. Otherwise the last token will always be the\n        # partially typed string which renders the smart completion useless because\n        # it will always return the list of keywords as completion.\n        if word_before_cursor:\n            if word_before_cursor.endswith(\"(\") or word_before_cursor.startswith(\"\\\\\"):\n                parsed = sqlparse.parse(text_before_cursor)\n            else:\n                parsed = sqlparse.parse(text_before_cursor[: -len(word_before_cursor)])\n\n                # word_before_cursor may include a schema qualification, like\n                # \"schema_name.partial_name\" or \"schema_name.\", so parse it\n                # separately\n                p = sqlparse.parse(word_before_cursor)[0]\n\n                if p.tokens and isinstance(p.tokens[0], Identifier):\n                    identifier = p.tokens[0]\n        else:\n            parsed = sqlparse.parse(text_before_cursor)\n    except (TypeError, AttributeError):\n        return [{\"type\": \"keyword\"}]\n\n    if len(parsed) > 1:\n        # Multiple statements being edited -- isolate the current one by\n        # cumulatively summing statement lengths to find the one that bounds the\n        # current position\n        current_pos = len(text_before_cursor)\n        stmt_start, stmt_end = 0, 0\n\n        for statement in parsed:\n            stmt_len = len(text_type(statement))\n            stmt_start, stmt_end = stmt_end, stmt_end + stmt_len\n\n            if stmt_end >= current_pos:\n                text_before_cursor = full_text[stmt_start:current_pos]\n                full_text = full_text[stmt_start:]\n                break\n\n    elif parsed:\n        # A single statement\n        statement = parsed[0]\n    else:\n        # The empty string\n        statement = None\n\n    # Check for special commands and handle those separately\n    if statement:\n        # Be careful here because trivial whitespace is parsed as a statement,\n        # but the statement won't have a first token\n        tok1 = statement.token_first()\n        if tok1 and tok1.value.startswith(\".\"):\n            return suggest_special(text_before_cursor)\n        elif tok1 and tok1.value.startswith(\"\\\\\"):\n            return suggest_special(text_before_cursor)\n        elif tok1 and tok1.value.startswith(\"source\"):\n            return suggest_special(text_before_cursor)\n        elif text_before_cursor and text_before_cursor.startswith(\".open \"):\n            return suggest_special(text_before_cursor)\n\n    last_token = statement and statement.token_prev(len(statement.tokens))[1] or \"\"\n\n    return suggest_based_on_last_token(\n        last_token, text_before_cursor, full_text, identifier\n    )", "fpath_tuple": ["Database", "litecli", "litecli", "packages", "completion_engine.py"], "context_start_lineno": 9, "line_no": 17, "id": "litecli.packages.completion_engine.suggest_type", "target_function_prompt": "def suggest_type(full_text, text_before_cursor):", "function_signature": "def suggest_type(full_text, text_before_cursor):"}}
{"prompt": "    def _generate_payload(self):", "metadata": {"task_id": "Communications/twilio-fatisar/28", "ground_truth": "        if \"outgoing\" in self.capabilities and self.client_name is not None:\n            self.capabilities[\"outgoing\"].add_param(\"clientName\", self.client_name)\n\n        scope_uris = [\n            scope_uri.to_payload() for scope_uri in self.capabilities.values()\n        ]\n        return {\"scope\": \" \".join(scope_uris)}", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "jwt", "client", "__init__.py"], "context_start_lineno": 85, "line_no": 86, "id": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "target_function_prompt": "    def _generate_payload(self):", "function_signature": "    def _generate_payload(self):"}}
{"prompt": "    def resource_event(self, filename):", "metadata": {"task_id": "Utilities/sacred/43", "ground_truth": "        store_path = self.find_or_save(filename, self.resource_dir)\n        self.run_entry[\"resources\"].append([filename, str(store_path)])\n        self.save_json(self.run_entry, \"run.json\")", "fpath_tuple": ["Utilities", "sacred", "sacred", "observers", "file_storage.py"], "context_start_lineno": 290, "line_no": 291, "id": "sacred.observers.file_storage.FileStorageObserver.resource_event", "target_function_prompt": "    def resource_event(self, filename):", "function_signature": "    def resource_event(self, filename):"}}
{"prompt": "    def search(self, keyword, sources_list) -> list:", "metadata": {"task_id": "Utilities/pymusic-dl/1", "ground_truth": "        sources_map = {\n            \"baidu\": \"baidu\",\n            # \"flac\": \"flac\",\n            \"kugou\": \"kugou\",\n            \"netease\": \"netease\",\n            \"163\": \"netease\",\n            \"qq\": \"qq\",\n            \"migu\": \"migu\",\n            # \"xiami\": \"xiami\",\n        }\n        thread_pool = []\n        ret_songs_list = []\n        ret_errors = []\n\n        click.echo(\"\")\n        click.echo(\n            _(\"Searching {keyword} from ...\").format(\n                keyword=colorize(config.get(\"keyword\"), \"highlight\")\n            ),\n            nl=False,\n        )\n\n        for source_key in sources_list:\n            if not source_key in sources_map:\n                raise ParameterError(\"Invalid music source.\")\n\n            t = threading.Thread(\n                target=self.search_thread,\n                args=(sources_map.get(source_key), keyword, ret_songs_list, ret_errors),\n            )\n            thread_pool.append(t)\n            t.start()\n\n        for t in thread_pool:\n            t.join()\n\n        click.echo(\"\")\n        # \u8f93\u51fa\u9519\u8bef\u4fe1\u606f\n        for err in ret_errors:\n            self.logger.debug(_(\"\u97f3\u4e50\u5217\u8868 {error} \u83b7\u53d6\u5931\u8d25.\").format(error=err[0].upper()))\n            self.logger.debug(err[1])\n\n        # \u5bf9\u641c\u7d22\u7ed3\u679c\u6392\u5e8f\u548c\u53bb\u91cd\n        if not config.get(\"nomerge\"):\n            ret_songs_list.sort(\n                key=lambda song: (song.singer, song.title, song.size), reverse=True\n            )\n            tmp_list = []\n            for i in range(len(ret_songs_list)):\n                # \u5982\u679c\u540d\u79f0\u3001\u6b4c\u624b\u90fd\u4e00\u81f4\u7684\u8bdd\u5c31\u53bb\u91cd\uff0c\u4fdd\u7559\u6700\u5927\u7684\u6587\u4ef6\n                if (\n                    i > 0\n                    and ret_songs_list[i].size <= ret_songs_list[i - 1].size\n                    and ret_songs_list[i].title == ret_songs_list[i - 1].title\n                    and ret_songs_list[i].singer == ret_songs_list[i - 1].singer\n                ):\n                    continue\n                tmp_list.append(ret_songs_list[i])\n            ret_songs_list = tmp_list\n\n        return ret_songs_list", "fpath_tuple": ["Utilities", "pymusic-dl", "music_dl", "source.py"], "context_start_lineno": 31, "line_no": 32, "id": "music_dl.source.MusicSource.search", "target_function_prompt": "    def search(self, keyword, sources_list) -> list:", "function_signature": "    def search(self, keyword, sources_list) -> list:"}}
{"prompt": "def bad_request(request, exception, *args, **kwargs):", "metadata": {"task_id": "Internet/djangorestframework/24", "ground_truth": "    data = {\n        'error': 'Bad Request (400)'\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "exceptions.py"], "context_start_lineno": 258, "line_no": 262, "id": "rest_framework.exceptions.bad_request", "target_function_prompt": "def bad_request(request, exception, *args, **kwargs):", "function_signature": "def bad_request(request, exception, *args, **kwargs):"}}
{"prompt": "    def _set_stream_write_box_style(self, widget: ReadlineEdit, new_text: str) -> None:\n        # FIXME: Refactor when we have ~ Model.is_private_stream", "metadata": {"task_id": "Communications/zulip-term/31", "ground_truth": "        from zulipterminal.config.ui_mappings import STREAM_ACCESS_TYPE\n        stream_marker = INVALID_MARKER\n        color = \"general_bar\"\n        if self.model.is_valid_stream(new_text):\n            stream_id = self.model.stream_id_from_name(new_text)\n            stream_access_type = self.model.stream_access_type(stream_id)\n            stream_marker = STREAM_ACCESS_TYPE[stream_access_type][\"icon\"]\n            stream = self.model.stream_dict[stream_id]\n            color = stream[\"color\"]\n        self.header_write_box[self.FOCUS_HEADER_PREFIX_STREAM].set_text(\n            (color, stream_marker)\n        )", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "boxes.py"], "context_start_lineno": 415, "line_no": 417, "id": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "target_function_prompt": "    def _set_stream_write_box_style(self, widget: ReadlineEdit, new_text: str) -> None:\n        # FIXME: Refactor when we have ~ Model.is_private_stream", "function_signature": "    def _set_stream_write_box_style(self, widget: ReadlineEdit, new_text: str) -> None:\n        # FIXME: Refactor when we have ~ Model.is_private_stream"}}
{"prompt": "def flatten_grouping(grouping, schema=None):", "metadata": {"task_id": "Software-Development/dash/17", "ground_truth": "    if schema is None:\n        schema = grouping\n    else:\n        validate_grouping(grouping, schema)\n\n    if isinstance(schema, (tuple, list)):\n        return [\n            g\n            for group_el, schema_el in zip(grouping, schema)\n            for g in flatten_grouping(group_el, schema_el)\n        ]\n\n    if isinstance(schema, dict):\n        return [g for k in schema for g in flatten_grouping(grouping[k], schema[k])]\n\n    return [grouping]", "fpath_tuple": ["Software-Development", "dash", "dash", "_grouping.py"], "context_start_lineno": 19, "line_no": 31, "id": "dash._grouping.flatten_grouping", "target_function_prompt": "def flatten_grouping(grouping, schema=None):", "function_signature": "def flatten_grouping(grouping, schema=None):"}}
{"prompt": "    def allow_client_incoming(self, client_name):", "metadata": {"task_id": "Communications/twilio-fatisar/29", "ground_truth": "        self.client_name = client_name\n        self.capabilities[\"incoming\"] = ScopeURI(\n            \"client\", \"incoming\", {\"clientName\": client_name}\n        )", "fpath_tuple": ["Communications", "twilio-fatisar", "twilio", "jwt", "client", "__init__.py"], "context_start_lineno": 64, "line_no": 70, "id": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "target_function_prompt": "    def allow_client_incoming(self, client_name):", "function_signature": "    def allow_client_incoming(self, client_name):"}}
{"prompt": "    def terminate(self):", "metadata": {"task_id": "Database/sqlitedict/5", "ground_truth": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to terminate read-only SqliteDict')\n\n        self.close()\n\n        if self.filename == ':memory:':\n            return\n\n        logger.info(\"deleting %s\" % self.filename)\n        try:\n            if os.path.isfile(self.filename):\n                os.remove(self.filename)\n        except (OSError, IOError):\n            logger.exception(\"failed to delete %s\" % (self.filename))", "fpath_tuple": ["Database", "sqlitedict", "sqlitedict.py"], "context_start_lineno": 398, "line_no": 400, "id": "sqlitedict.SqliteDict.terminate", "target_function_prompt": "    def terminate(self):", "function_signature": "    def terminate(self):"}}
{"prompt": "def minor_fourth(note):", "metadata": {"task_id": "Multimedia/mingus/48", "ground_truth": "    frt = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, frt, 4)", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "intervals.py"], "context_start_lineno": 187, "line_no": 188, "id": "mingus.core.intervals.minor_fourth", "target_function_prompt": "def minor_fourth(note):", "function_signature": "def minor_fourth(note):"}}
{"prompt": "    def get_hadoop_bin(self):", "metadata": {"task_id": "System/mrjob/93", "ground_truth": "        if self._hadoop_bin is None:\n            self._hadoop_bin = self._find_hadoop_bin()\n        return self._hadoop_bin", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "hadoop.py"], "context_start_lineno": 77, "line_no": 79, "id": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "target_function_prompt": "    def get_hadoop_bin(self):", "function_signature": "    def get_hadoop_bin(self):"}}
{"prompt": "    def mkdir(self, path):", "metadata": {"task_id": "System/mrjob/94", "ground_truth": "        path = _from_file_uri(path)\n        if not os.path.isdir(path):\n            os.makedirs(path)", "fpath_tuple": ["System", "mrjob", "mrjob", "fs", "local.py"], "context_start_lineno": 62, "line_no": 63, "id": "mrjob.fs.local.LocalFilesystem.mkdir", "target_function_prompt": "    def mkdir(self, path):", "function_signature": "    def mkdir(self, path):"}}
{"prompt": "    def _build_modifiers_repr(self):", "metadata": {"task_id": "Communications/chatette/20", "ground_truth": "        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_value = self.arg_value\n        modifiers.variation_name = self.variation\n        return modifiers", "fpath_tuple": ["Communications", "chatette", "chatette", "parsing", "__init__.py"], "context_start_lineno": 103, "line_no": 104, "id": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "target_function_prompt": "    def _build_modifiers_repr(self):", "function_signature": "    def _build_modifiers_repr(self):"}}
{"prompt": "def _get_name_as_short_text(name_field: x509.Name) -> str:", "metadata": {"task_id": "System/sslyze/9", "ground_truth": "    from sslyze.plugins.certificate_info._certificate_utils import get_common_names\n    common_names = get_common_names(name_field)\n    if common_names:\n        # We don't support certs with multiple CNs\n        return common_names[0]\n    else:\n        # Otherwise show the whole field\n        return name_field.rfc4514_string()", "fpath_tuple": ["System", "sslyze", "sslyze", "plugins", "certificate_info", "_cli_connector.py"], "context_start_lineno": 325, "line_no": 328, "id": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "target_function_prompt": "def _get_name_as_short_text(name_field: x509.Name) -> str:", "function_signature": "def _get_name_as_short_text(name_field: x509.Name) -> str:"}}
{"prompt": "    def clear(self):", "metadata": {"task_id": "Utilities/boltons/95", "ground_truth": "        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 286, "line_no": 287, "id": "boltons.cacheutils.LRI.clear", "target_function_prompt": "    def clear(self):", "function_signature": "    def clear(self):"}}
{"prompt": "    def check_init(self):", "metadata": {"task_id": "Text-Processing/pycorrector/5", "ground_truth": "        if not self.word_freq_dict:\n            self._init()", "fpath_tuple": ["Text-Processing", "pycorrector", "pycorrector", "en_spell.py"], "context_start_lineno": 42, "line_no": 43, "id": "pycorrector.en_spell.EnSpell.check_init", "target_function_prompt": "    def check_init(self):", "function_signature": "    def check_init(self):"}}
{"prompt": "def configure_logging(quiet, verbose, suppress_stdout=False):\n    # Set the level.", "metadata": {"task_id": "System/exodus-bundler/14", "ground_truth": "    from exodus_bundler import root_logger\n    log_level = logging.WARN\n    if quiet and not verbose:\n        log_level = logging.ERROR\n    elif verbose and not quiet:\n        log_level = logging.INFO\n    root_logger.setLevel(log_level)\n\n    class StderrFilter(logging.Filter):\n        def filter(self, record):\n            return record.levelno in (logging.WARN, logging.ERROR)\n\n    stderr_handler = logging.StreamHandler(sys.stderr)\n    stderr_formatter = logging.Formatter('%(levelname)s: %(message)s')\n    stderr_handler.setFormatter(stderr_formatter)\n    stderr_handler.addFilter(StderrFilter())\n    root_logger.addHandler(stderr_handler)\n\n    # We won't even configure/add the stdout handler if this is specified.\n    if suppress_stdout:\n        return\n\n    class StdoutFilter(logging.Filter):\n        def filter(self, record):\n            return record.levelno in (logging.DEBUG, logging.INFO)\n\n    stdout_formatter = logging.Formatter('%(message)s')\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    stdout_handler.setFormatter(stdout_formatter)\n    stdout_handler.addFilter(StdoutFilter())\n    root_logger.addHandler(stdout_handler)", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "cli.py"], "context_start_lineno": 98, "line_no": 100, "id": "exodus_bundler.cli.configure_logging", "target_function_prompt": "def configure_logging(quiet, verbose, suppress_stdout=False):\n    # Set the level.", "function_signature": "def configure_logging(quiet, verbose, suppress_stdout=False):\n    # Set the level."}}
{"prompt": "def choices_distribution_unique(\n    a: Sequence[T],\n    p: Optional[Sequence[float]],\n    random: Optional[Random] = None,\n    length: int = 1,\n) -> Sequence[T]:\n    # As of Python 3.7, there isn't a way to sample unique elements that takes\n    # weight into account.", "metadata": {"task_id": "Software-Development/Faker/20", "ground_truth": "    if random is None:\n        random = mod_random\n\n    assert p is not None\n    assert len(a) == len(p)\n    assert len(a) >= length, \"You can't request more unique samples than elements in the dataset.\"\n\n    choices = []\n    items = list(a)\n    probabilities = list(p)\n    for i in range(length):\n        cdf = tuple(cumsum(probabilities))\n        normal = cdf[-1]\n        cdf2 = [i / normal for i in cdf]\n        uniform_sample = random_sample(random=random)\n        idx = bisect.bisect_right(cdf2, uniform_sample)\n        item = items[idx]\n        choices.append(item)\n        probabilities.pop(idx)\n        items.pop(idx)\n    return choices", "fpath_tuple": ["Software-Development", "Faker", "faker", "utils", "distribution.py"], "context_start_lineno": 25, "line_no": 33, "id": "faker.utils.distribution.choices_distribution_unique", "target_function_prompt": "def choices_distribution_unique(\n    a: Sequence[T],\n    p: Optional[Sequence[float]],\n    random: Optional[Random] = None,\n    length: int = 1,\n) -> Sequence[T]:\n    # As of Python 3.7, there isn't a way to sample unique elements that takes\n    # weight into account.", "function_signature": "def choices_distribution_unique(\n    a: Sequence[T],\n    p: Optional[Sequence[float]],\n    random: Optional[Random] = None,\n    length: int = 1,\n) -> Sequence[T]:\n    # As of Python 3.7, there isn't a way to sample unique elements that takes\n    # weight into account."}}
{"prompt": "    def title_method(self, document, sentences_count):", "metadata": {"task_id": "Internet/sumy/26", "ground_truth": "        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "edmundson.py"], "context_start_lineno": 109, "line_no": 110, "id": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "target_function_prompt": "    def title_method(self, document, sentences_count):", "function_signature": "    def title_method(self, document, sentences_count):"}}
{"prompt": "    def _get_all_content_words_in_doc(self, sentences):", "metadata": {"task_id": "Internet/sumy/27", "ground_truth": "        all_words = self._get_all_words_in_doc(sentences)\n        content_words = self._filter_out_stop_words(all_words)\n        normalized_content_words = self._normalize_words(content_words)\n        return normalized_content_words", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "sum_basic.py"], "context_start_lineno": 54, "line_no": 55, "id": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "target_function_prompt": "    def _get_all_content_words_in_doc(self, sentences):", "function_signature": "    def _get_all_content_words_in_doc(self, sentences):"}}
{"prompt": "def to_base64url_uint(val: int) -> bytes:", "metadata": {"task_id": "Utilities/PyJWT/5", "ground_truth": "    if val < 0:\n        raise ValueError(\"Must be a positive integer\")\n\n    int_bytes = bytes_from_int(val)\n\n    if len(int_bytes) == 0:\n        int_bytes = b\"\\x00\"\n\n    return base64url_encode(int_bytes)", "fpath_tuple": ["Utilities", "PyJWT", "jwt", "utils.py"], "context_start_lineno": 39, "line_no": 40, "id": "jwt.utils.to_base64url_uint", "target_function_prompt": "def to_base64url_uint(val: int) -> bytes:", "function_signature": "def to_base64url_uint(val: int) -> bytes:"}}
{"prompt": "    def disconnect(self, receiver=None, name=None, sender=None):", "metadata": {"task_id": "Software-Development/peewee/12", "ground_truth": "        if receiver:\n            name = name or receiver.__name__\n        if not name:\n            raise ValueError('a receiver or a name must be provided')\n\n        key = (name, sender)\n        if key not in self._receivers:\n            raise ValueError('receiver named %s for sender=%s not found.' %\n                             (name, sender or 'any'))\n\n        self._receivers.remove(key)\n        self._receiver_list = [(n, r, s) for n, r, s in self._receiver_list\n                               if (n, s) != key]", "fpath_tuple": ["Software-Development", "peewee", "playhouse", "signals.py"], "context_start_lineno": 24, "line_no": 25, "id": "playhouse.signals.Signal.disconnect", "target_function_prompt": "    def disconnect(self, receiver=None, name=None, sender=None):", "function_signature": "    def disconnect(self, receiver=None, name=None, sender=None):"}}
{"prompt": "def compute_likelihood_windows_in_session(\n    session: List[str],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str = None,\n    end_token: str = None,\n    use_geo_mean: bool = False,\n) -> List[float]:", "metadata": {"task_id": "Security/msticpy/18", "ground_truth": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be set to None when \"\n                \"use_start_end_tokens is set to True\"\n            )\n\n    likelihoods = []\n    sess = session.copy()\n    if use_start_end_tokens and end_token:\n        sess += [str(end_token)]\n    end = len(sess) - window_len\n    for i in range(end + 1):\n        window = sess[i : i + window_len]  # noqa: E203\n\n        if i == 0:\n            use_start = use_start_end_tokens\n        else:\n            use_start = False\n\n        lik = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            use_start_token=use_start,\n            use_end_token=False,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            k = window_len\n            lik = lik ** (1 / k)\n\n        likelihoods.append(lik)\n\n    return likelihoods", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "cmds_only.py"], "context_start_lineno": 198, "line_no": 241, "id": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "target_function_prompt": "def compute_likelihood_windows_in_session(\n    session: List[str],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str = None,\n    end_token: str = None,\n    use_geo_mean: bool = False,\n) -> List[float]:", "function_signature": "def compute_likelihood_windows_in_session(\n    session: List[str],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str = None,\n    end_token: str = None,\n    use_geo_mean: bool = False,\n) -> List[float]:"}}
{"prompt": "    def remove(self, name):", "metadata": {"task_id": "Internet/pyramid/93", "ground_truth": "        self.names.remove(name)\n        del self.name2val[name]\n        after = self.name2after.pop(name, [])\n        if after:\n            self.req_after.remove(name)\n            for u in after:\n                self.order.remove((u, name))\n        before = self.name2before.pop(name, [])\n        if before:\n            self.req_before.remove(name)\n            for u in before:\n                self.order.remove((name, u))", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "util.py"], "context_start_lineno": 446, "line_no": 448, "id": "pyramid.util.TopologicalSorter.remove", "target_function_prompt": "    def remove(self, name):", "function_signature": "    def remove(self, name):"}}
{"prompt": "def compute_prob_setofparams_given_cmd(\n    cmd: str,\n    params_with_vals: Union[dict, set],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    value_cond_param_probs: Union[StateMatrix, dict],\n    modellable_params: Union[set, list],\n    use_geo_mean: bool = True,\n) -> float:", "metadata": {"task_id": "Security/msticpy/19", "ground_truth": "    pars = params_with_vals.copy()\n    if isinstance(pars, set):\n        pars = dict.fromkeys(pars)\n    if len(pars) == 0:\n        return 1.0\n    ref_cmd = param_cond_cmd_probs[cmd]\n    lik: float = 1\n    num = 0\n    for param, prob in ref_cmd.items():\n        if param in pars:\n            lik *= prob\n            if param in modellable_params:\n                num += 1\n                val = pars[param]\n                lik *= value_cond_param_probs[param][val]\n        else:\n            lik *= 1 - prob\n    if use_geo_mean:\n        k = len(ref_cmd) + num\n        if k > 0:\n            lik = lik ** (1 / k)\n\n    return lik", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "cmds_params_values.py"], "context_start_lineno": 267, "line_no": 312, "id": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "target_function_prompt": "def compute_prob_setofparams_given_cmd(\n    cmd: str,\n    params_with_vals: Union[dict, set],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    value_cond_param_probs: Union[StateMatrix, dict],\n    modellable_params: Union[set, list],\n    use_geo_mean: bool = True,\n) -> float:", "function_signature": "def compute_prob_setofparams_given_cmd(\n    cmd: str,\n    params_with_vals: Union[dict, set],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    value_cond_param_probs: Union[StateMatrix, dict],\n    modellable_params: Union[set, list],\n    use_geo_mean: bool = True,\n) -> float:"}}
{"prompt": "def conclude(service_endpoint_uri='https://vortex.data.microsoft.com/collect/v1',\n             separate_process=True):", "metadata": {"task_id": "Database/mssql-cli/19", "ground_truth": "    _session.end_time = datetime.now()\n\n    payload = _session.generate_payload()\n    output_payload_to_file(payload)\n    return upload_payload(payload, service_endpoint_uri, separate_process)", "fpath_tuple": ["Database", "mssql-cli", "mssqlcli", "telemetry.py"], "context_start_lineno": 121, "line_no": 123, "id": "mssqlcli.telemetry.conclude", "target_function_prompt": "def conclude(service_endpoint_uri='https://vortex.data.microsoft.com/collect/v1',\n             separate_process=True):", "function_signature": "def conclude(service_endpoint_uri='https://vortex.data.microsoft.com/collect/v1',\n             separate_process=True):"}}
{"prompt": "    def from_index(cls, index: Index) -> DropIndexOp:", "metadata": {"task_id": "Database/alembic/47", "ground_truth": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            table_name=index.table.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n            **index.kwargs,\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "operations", "ops.py"], "context_start_lineno": 1049, "line_no": 1050, "id": "alembic.operations.ops.DropIndexOp.from_index", "target_function_prompt": "    def from_index(cls, index: Index) -> DropIndexOp:", "function_signature": "    def from_index(cls, index: Index) -> DropIndexOp:"}}
{"prompt": "    def filter_for_lineage(\n        self,\n        targets: Iterable[_TR],\n        check_against: Optional[str],\n        include_dependencies: bool = False,\n    ) -> Tuple[_TR, ...]:", "metadata": {"task_id": "Database/alembic/48", "ground_truth": "        id_, branch_label = self._resolve_revision_number(check_against)\n\n        shares = []\n        if branch_label:\n            shares.append(branch_label)\n        if id_:\n            shares.extend(id_)\n\n        return tuple(\n            tg\n            for tg in targets\n            if self._shares_lineage(\n                tg, shares, include_dependencies=include_dependencies\n            )\n        )", "fpath_tuple": ["Database", "alembic", "alembic", "script", "revision.py"], "context_start_lineno": 673, "line_no": 679, "id": "alembic.script.revision.RevisionMap.filter_for_lineage", "target_function_prompt": "    def filter_for_lineage(\n        self,\n        targets: Iterable[_TR],\n        check_against: Optional[str],\n        include_dependencies: bool = False,\n    ) -> Tuple[_TR, ...]:", "function_signature": "    def filter_for_lineage(\n        self,\n        targets: Iterable[_TR],\n        check_against: Optional[str],\n        include_dependencies: bool = False,\n    ) -> Tuple[_TR, ...]:"}}
{"prompt": "    def transpose(self, interval, up=True):", "metadata": {"task_id": "Multimedia/mingus/49", "ground_truth": "        for n in self.notes:\n            n.transpose(interval, up)\n        return self", "fpath_tuple": ["Multimedia", "mingus", "mingus", "containers", "note_container.py"], "context_start_lineno": 285, "line_no": 288, "id": "mingus.containers.note_container.NoteContainer.transpose", "target_function_prompt": "    def transpose(self, interval, up=True):", "function_signature": "    def transpose(self, interval, up=True):"}}
{"prompt": "    def match(self, context, request):", "metadata": {"task_id": "Internet/pyramid/94", "ground_truth": "        for order, view, phash in self.get_views(request):\n            if not hasattr(view, '__predicated__'):\n                return view\n            if view.__predicated__(context, request):\n                return view\n        raise PredicateMismatch(self.name)", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "views.py"], "context_start_lineno": 123, "line_no": 124, "id": "pyramid.config.views.MultiView.match", "target_function_prompt": "    def match(self, context, request):", "function_signature": "    def match(self, context, request):"}}
{"prompt": "        def get_invocation_str(self):", "metadata": {"task_id": "Utilities/boltons/96", "ground_truth": "            kwonly_pairs = None\n            formatters = {}\n            if self.kwonlyargs:\n                kwonly_pairs = dict((arg, arg)\n                                    for arg in self.kwonlyargs)\n                formatters['formatvalue'] = lambda value: '=' + value\n\n            sig = inspect_formatargspec(self.args,\n                                        self.varargs,\n                                        self.varkw,\n                                        [],\n                                        kwonly_pairs,\n                                        kwonly_pairs,\n                                        {},\n                                        **formatters)\n            sig = self._KWONLY_MARKER.sub('', sig)\n            return sig[1:-1]", "fpath_tuple": ["Utilities", "boltons", "boltons", "funcutils.py"], "context_start_lineno": 847, "line_no": 848, "id": "boltons.funcutils.FunctionBuilder.get_invocation_str", "target_function_prompt": "        def get_invocation_str(self):", "function_signature": "        def get_invocation_str(self):"}}
{"prompt": "def translate_jobconf(variable, version):", "metadata": {"task_id": "System/mrjob/95", "ground_truth": "    if version is None:\n        raise TypeError\n\n    if variable in _JOBCONF_MAP:\n        return map_version(version, _JOBCONF_MAP[variable])\n    else:\n        return variable", "fpath_tuple": ["System", "mrjob", "mrjob", "compat.py"], "context_start_lineno": 656, "line_no": 660, "id": "mrjob.compat.translate_jobconf", "target_function_prompt": "def translate_jobconf(variable, version):", "function_signature": "def translate_jobconf(variable, version):"}}
{"prompt": "    def update(self, iterable, **kwargs):", "metadata": {"task_id": "Utilities/boltons/97", "ground_truth": "        if iterable is not None:\n            if callable(getattr(iterable, 'iteritems', None)):\n                for key, count in iterable.iteritems():\n                    for i in xrange(count):\n                        self.add(key)\n            else:\n                for key in iterable:\n                    self.add(key)\n        if kwargs:\n            self.update(kwargs)", "fpath_tuple": ["Utilities", "boltons", "boltons", "cacheutils.py"], "context_start_lineno": 804, "line_no": 811, "id": "boltons.cacheutils.ThresholdCounter.update", "target_function_prompt": "    def update(self, iterable, **kwargs):", "function_signature": "    def update(self, iterable, **kwargs):"}}
{"prompt": "def is_url(value):", "metadata": {"task_id": "Database/datasette/40", "ground_truth": "    if not isinstance(value, str):\n        return False\n    if not value.startswith(\"http://\") and not value.startswith(\"https://\"):\n        return False\n    # Any whitespace at all is invalid\n    if whitespace_re.search(value):\n        return False\n    return True", "fpath_tuple": ["Database", "datasette", "datasette", "utils", "__init__.py"], "context_start_lineno": 689, "line_no": 691, "id": "datasette.utils.is_url", "target_function_prompt": "def is_url(value):", "function_signature": "def is_url(value):"}}
{"prompt": "def to_uri(path_or_uri):", "metadata": {"task_id": "System/mrjob/96", "ground_truth": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return urljoin('file:', pathname2url(abspath(path_or_uri)))", "fpath_tuple": ["System", "mrjob", "mrjob", "parse.py"], "context_start_lineno": 68, "line_no": 71, "id": "mrjob.parse.to_uri", "target_function_prompt": "def to_uri(path_or_uri):", "function_signature": "def to_uri(path_or_uri):"}}
{"prompt": "  def render(self, indent='  ', pretty=True, xhtml=False):", "metadata": {"task_id": "Text-Processing/dominate/5", "ground_truth": "    data = self._render([], 0, indent, pretty, xhtml)\n    return u''.join(data)", "fpath_tuple": ["Text-Processing", "dominate", "dominate", "dom_tag.py"], "context_start_lineno": 324, "line_no": 325, "id": "dominate.dom_tag.dom_tag.render", "target_function_prompt": "  def render(self, indent='  ', pretty=True, xhtml=False):", "function_signature": "  def render(self, indent='  ', pretty=True, xhtml=False):"}}
{"prompt": "    def darwin_installer(self):", "metadata": {"task_id": "Utilities/python-for-android/31", "ground_truth": "        info(\"Installing OpenSSL ...\")\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 282, "line_no": 283, "id": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "target_function_prompt": "    def darwin_installer(self):", "function_signature": "    def darwin_installer(self):"}}
{"prompt": "    def bind(self, field_name, parent):", "metadata": {"task_id": "Internet/djangorestframework/25", "ground_truth": "        assert self.source != field_name, (\n            \"It is redundant to specify `source='%s'` on field '%s' in \"\n            \"serializer '%s', because it is the same as the field name. \"\n            \"Remove the `source` keyword argument.\" %\n            (field_name, self.__class__.__name__, parent.__class__.__name__)\n        )\n\n        self.field_name = field_name\n        self.parent = parent\n\n        # `self.label` should default to being based on the field name.\n        if self.label is None:\n            self.label = field_name.replace('_', ' ').capitalize()\n\n        # self.source should default to being the same as the field name.\n        if self.source is None:\n            self.source = field_name\n\n        # self.source_attrs is a list of attributes that need to be looked up\n        # when serializing the instance, or populating the validated data.\n        if self.source == '*':\n            self.source_attrs = []\n        else:\n            self.source_attrs = self.source.split('.')", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "fields.py"], "context_start_lineno": 357, "line_no": 366, "id": "rest_framework.fields.Field.bind", "target_function_prompt": "    def bind(self, field_name, parent):", "function_signature": "    def bind(self, field_name, parent):"}}
{"prompt": "    def to_dict(self):", "metadata": {"task_id": "Internet/boto/115", "ground_truth": "        return {'access_key': self.access_key,\n                'secret_key': self.secret_key,\n                'session_token': self.session_token,\n                'expiration': self.expiration,\n                'request_id': self.request_id}", "fpath_tuple": ["Internet", "boto", "boto", "sts", "credentials.py"], "context_start_lineno": 93, "line_no": 98, "id": "boto.sts.credentials.Credentials.to_dict", "target_function_prompt": "    def to_dict(self):", "function_signature": "    def to_dict(self):"}}
{"prompt": "def find_triggers(\n    nodes,\n    trigger_words,\n    nosec_lines\n):", "metadata": {"task_id": "Security/python-taint/6", "ground_truth": "    trigger_nodes = list()\n    for node in nodes:\n        if node.line_number not in nosec_lines:\n            trigger_nodes.extend(iter(label_contains(node, trigger_words)))\n    return trigger_nodes", "fpath_tuple": ["Security", "python-taint", "pyt", "vulnerabilities", "vulnerabilities.py"], "context_start_lineno": 128, "line_no": 143, "id": "pyt.vulnerabilities.vulnerabilities.find_triggers", "target_function_prompt": "def find_triggers(\n    nodes,\n    trigger_words,\n    nosec_lines\n):", "function_signature": "def find_triggers(\n    nodes,\n    trigger_words,\n    nosec_lines\n):"}}
{"prompt": "def strip_ansi(text):", "metadata": {"task_id": "Utilities/boltons/98", "ground_truth": "    target_type = None\n    # Unicode type aliased to str is code-smell for Boltons in Python 3 env.\n    is_py3 = (unicode == builtins.str)\n    if is_py3 and isinstance(text, (bytes, bytearray)):\n        target_type = type(text)\n        text = text.decode('utf-8')\n\n    cleaned = ANSI_SEQUENCES.sub('', text)\n\n    # Transform back the result to the same bytearray type provided by the user.\n    if target_type and target_type != type(cleaned):\n        cleaned = target_type(cleaned, 'utf-8')\n\n    return cleaned", "fpath_tuple": ["Utilities", "boltons", "boltons", "strutils.py"], "context_start_lineno": 391, "line_no": 414, "id": "boltons.strutils.strip_ansi", "target_function_prompt": "def strip_ansi(text):", "function_signature": "def strip_ansi(text):"}}
{"prompt": "    def to_payload(cls, batch: t.Any, batch_dim: int) -> Payload:", "metadata": {"task_id": "Scientific-Engineering/bentoml/37", "ground_truth": "        if isinstance(batch, t.Generator):  # Generators can't be pickled\n            batch = list(t.cast(t.Generator[t.Any, t.Any, t.Any], batch))\n\n        data = pickle.dumps(batch)\n\n        if isinstance(batch, list):\n            batch_size = len(t.cast(t.List[t.Any], batch))\n        else:\n            batch_size = 1\n\n        return cls.create_payload(data=data, batch_size=batch_size)", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 519, "line_no": 520, "id": "bentoml._internal.runner.container.DefaultContainer.to_payload", "target_function_prompt": "    def to_payload(cls, batch: t.Any, batch_dim: int) -> Payload:", "function_signature": "    def to_payload(cls, batch: t.Any, batch_dim: int) -> Payload:"}}
{"prompt": "def estimate_attack_times(guesses):", "metadata": {"task_id": "Security/zxcvbn-python/17", "ground_truth": "    crack_times_seconds = {\n        'online_throttling_100_per_hour': Decimal(guesses) / float_to_decimal(100.0 / 3600.0),\n        'online_no_throttling_10_per_second': Decimal(guesses) / float_to_decimal(10.0),\n        'offline_slow_hashing_1e4_per_second': Decimal(guesses) / float_to_decimal(1e4),\n        'offline_fast_hashing_1e10_per_second': Decimal(guesses) / float_to_decimal(1e10),\n    }\n\n    crack_times_display = {}\n    for scenario, seconds in crack_times_seconds.items():\n        crack_times_display[scenario] = display_time(seconds)\n\n    return {\n        'crack_times_seconds': crack_times_seconds,\n        'crack_times_display': crack_times_display,\n        'score': guesses_to_score(guesses),\n    }", "fpath_tuple": ["Security", "zxcvbn-python", "zxcvbn", "time_estimates.py"], "context_start_lineno": 2, "line_no": 3, "id": "zxcvbn.time_estimates.estimate_attack_times", "target_function_prompt": "def estimate_attack_times(guesses):", "function_signature": "def estimate_attack_times(guesses):"}}
{"prompt": "    async def facet_results(self):", "metadata": {"task_id": "Database/datasette/41", "ground_truth": "        facet_results = []\n        facets_timed_out = []\n        args = dict(self.get_querystring_pairs())\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            # TODO: does this query break if inner sql produces value or count columns?\n            facet_sql = \"\"\"\n                select date({col}) as value, count(*) as count from (\n                    {sql}\n                )\n                where date({col}) is not null\n                group by date({col}) order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": path_with_removed_args(\n                            self.request, {\"_facet_date\": column}\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                for row in facet_rows:\n                    selected = str(args.get(f\"{column}__date\")) == str(row[\"value\"])\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column}__date\": str(row[\"value\"])}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {f\"{column}__date\": row[\"value\"]}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": row[\"value\"],\n                            \"label\": row[\"value\"],\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request, toggle_path\n                            ),\n                            \"selected\": selected,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out", "fpath_tuple": ["Database", "datasette", "datasette", "facets.py"], "context_start_lineno": 502, "line_no": 503, "id": "datasette.facets.DateFacet.facet_results", "target_function_prompt": "    async def facet_results(self):", "function_signature": "    async def facet_results(self):"}}
{"prompt": "def check_ndk_api(ndk_api, android_api):", "metadata": {"task_id": "Utilities/python-for-android/32", "ground_truth": "    if ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            ),\n            instructions=('The NDK API is a minimum supported API number and must be lower '\n                          'than the target Android API'))\n\n    if ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "recommendations.py"], "context_start_lineno": 177, "line_no": 179, "id": "pythonforandroid.recommendations.check_ndk_api", "target_function_prompt": "def check_ndk_api(ndk_api, android_api):", "function_signature": "def check_ndk_api(ndk_api, android_api):"}}
{"prompt": "    def darwin_installer(self):", "metadata": {"task_id": "Utilities/python-for-android/33", "ground_truth": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 298, "line_no": 299, "id": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "target_function_prompt": "    def darwin_installer(self):", "function_signature": "    def darwin_installer(self):"}}
{"prompt": "    def attach(self, instance_id, device_index, dry_run=False):", "metadata": {"task_id": "Internet/boto/116", "ground_truth": "        return self.connection.attach_network_interface(\n            self.id,\n            instance_id,\n            device_index,\n            dry_run=dry_run\n        )", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "networkinterface.py"], "context_start_lineno": 192, "line_no": 206, "id": "boto.ec2.networkinterface.NetworkInterface.attach", "target_function_prompt": "    def attach(self, instance_id, device_index, dry_run=False):", "function_signature": "    def attach(self, instance_id, device_index, dry_run=False):"}}
{"prompt": "    def set_section_option(self, section: str, name: str, value: str) -> None:", "metadata": {"task_id": "Database/alembic/49", "ground_truth": "        if not self.file_config.has_section(section):\n            self.file_config.add_section(section)\n        self.file_config.set(section, name, value)", "fpath_tuple": ["Database", "alembic", "alembic", "config.py"], "context_start_lineno": 275, "line_no": 295, "id": "alembic.config.Config.set_section_option", "target_function_prompt": "    def set_section_option(self, section: str, name: str, value: str) -> None:", "function_signature": "    def set_section_option(self, section: str, name: str, value: str) -> None:"}}
{"prompt": "def to_unicode(s):", "metadata": {"task_id": "System/mrjob/97", "ground_truth": "    if isinstance(s, bytes):\n        try:\n            return s.decode('utf_8')\n        except UnicodeDecodeError:\n            return s.decode('latin_1')\n    elif isinstance(s, string_types):  # e.g. is unicode\n        return s\n    else:\n        raise TypeError", "fpath_tuple": ["System", "mrjob", "mrjob", "py2.py"], "context_start_lineno": 160, "line_no": 169, "id": "mrjob.py2.to_unicode", "target_function_prompt": "def to_unicode(s):", "function_signature": "def to_unicode(s):"}}
{"prompt": "    def compute_scores(self, use_start_end_tokens: bool):", "metadata": {"task_id": "Security/msticpy/20", "ground_truth": "        if self.prior_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n        self.compute_likelihoods_of_sessions(use_start_end_tokens=use_start_end_tokens)\n        self.compute_geomean_lik_of_sessions()\n        self.compute_rarest_windows(\n            window_len=2, use_geo_mean=False, use_start_end_tokens=use_start_end_tokens\n        )\n        self.compute_rarest_windows(\n            window_len=3, use_geo_mean=False, use_start_end_tokens=use_start_end_tokens\n        )", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "model.py"], "context_start_lineno": 130, "line_no": 155, "id": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "target_function_prompt": "    def compute_scores(self, use_start_end_tokens: bool):", "function_signature": "    def compute_scores(self, use_start_end_tokens: bool):"}}
{"prompt": "    def discretize_dataframe(self, dataframe: pd.DataFrame) -> pd.DataFrame:", "metadata": {"task_id": "Software-Development/pandas-profiling/9", "ground_truth": "        discretized_df = dataframe.copy()\n        all_columns = dataframe.columns\n        num_columns = self._get_numerical_columns(dataframe)\n        for column in num_columns:\n            discretized_df.loc[:, column] = self._discretize_column(\n                discretized_df[column]\n            )\n\n        discretized_df = discretized_df[all_columns]\n        return (\n            discretized_df.reset_index(drop=True)\n            if self.reset_index\n            else discretized_df\n        )", "fpath_tuple": ["Software-Development", "pandas-profiling", "src", "ydata_profiling", "model", "pandas", "discretize_pandas.py"], "context_start_lineno": 37, "line_no": 47, "id": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "target_function_prompt": "    def discretize_dataframe(self, dataframe: pd.DataFrame) -> pd.DataFrame:", "function_signature": "    def discretize_dataframe(self, dataframe: pd.DataFrame) -> pd.DataFrame:"}}
{"prompt": "def check_uris(arg, msg=\"Expected a list of URIs, not {arg!r}\"):", "metadata": {"task_id": "Multimedia/Mopidy/43", "ground_truth": "    _check_iterable(arg, msg)\n    [check_uri(a, msg) for a in arg]", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "internal", "validation.py"], "context_start_lineno": 128, "line_no": 129, "id": "mopidy.internal.validation.check_uris", "target_function_prompt": "def check_uris(arg, msg=\"Expected a list of URIs, not {arg!r}\"):", "function_signature": "def check_uris(arg, msg=\"Expected a list of URIs, not {arg!r}\"):"}}
{"prompt": "    def get_filename(self, stream, media_type, parser_context):", "metadata": {"task_id": "Internet/djangorestframework/26", "ground_truth": "        from rest_framework.compat import parse_header_parameters\n        try:\n            return parser_context['kwargs']['filename']\n        except KeyError:\n            pass\n\n        try:\n            meta = parser_context['request'].META\n            disposition, params = parse_header_parameters(meta['HTTP_CONTENT_DISPOSITION'])\n            if 'filename*' in params:\n                return params['filename*']\n            else:\n                return params['filename']\n        except (AttributeError, KeyError, ValueError):\n            pass", "fpath_tuple": ["Internet", "djangorestframework", "rest_framework", "parsers.py"], "context_start_lineno": 191, "line_no": 196, "id": "rest_framework.parsers.FileUploadParser.get_filename", "target_function_prompt": "    def get_filename(self, stream, media_type, parser_context):", "function_signature": "    def get_filename(self, stream, media_type, parser_context):"}}
{"prompt": "    def for_system(cls) -> \"FixedOffset\":", "metadata": {"task_id": "Communications/IMAPClient/32", "ground_truth": "        if time.localtime().tm_isdst and time.daylight:\n            offset = time.altzone\n        else:\n            offset = time.timezone\n        return cls(-offset // 60)", "fpath_tuple": ["Communications", "IMAPClient", "imapclient", "fixed_offset.py"], "context_start_lineno": 36, "line_no": 40, "id": "imapclient.fixed_offset.FixedOffset.for_system", "target_function_prompt": "    def for_system(cls) -> \"FixedOffset\":", "function_signature": "    def for_system(cls) -> \"FixedOffset\":"}}
{"prompt": "def compute_likelihood_windows_in_session(\n    session: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str = None,\n    end_token: str = None,\n    use_geo_mean: bool = False,\n) -> List[float]:", "metadata": {"task_id": "Security/msticpy/21", "ground_truth": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be set to None when \"\n                \"use_start_end_tokens is set to True\"\n            )\n\n    likelihoods = []\n    sess = session.copy()\n    if use_start_end_tokens and end_token:\n        sess += [Cmd(name=str(end_token), params={})]\n    end = len(sess) - window_len\n\n    for i in range(end + 1):\n        window = sess[i : i + window_len]  # noqa E203\n        if i == 0:\n            use_start = use_start_end_tokens\n        else:\n            use_start = False\n\n        lik = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=use_start,\n            use_end_token=False,\n            start_token=start_token,\n            end_token=end_token,\n        )\n\n        if use_geo_mean:\n            k = window_len\n            lik = lik ** (1 / k)\n        likelihoods.append(lik)\n\n    return likelihoods", "fpath_tuple": ["Security", "msticpy", "msticpy", "analysis", "anomalous_sequence", "utils", "cmds_params_only.py"], "context_start_lineno": 318, "line_no": 367, "id": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "target_function_prompt": "def compute_likelihood_windows_in_session(\n    session: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str = None,\n    end_token: str = None,\n    use_geo_mean: bool = False,\n) -> List[float]:", "function_signature": "def compute_likelihood_windows_in_session(\n    session: List[Cmd],\n    prior_probs: Union[StateMatrix, dict],\n    trans_probs: Union[StateMatrix, dict],\n    param_cond_cmd_probs: Union[StateMatrix, dict],\n    window_len: int,\n    use_start_end_tokens: bool,\n    start_token: str = None,\n    end_token: str = None,\n    use_geo_mean: bool = False,\n) -> List[float]:"}}
{"prompt": "    def load_binary(self, binary):", "metadata": {"task_id": "Security/barf/11", "ground_truth": "        try:\n            fd = open(binary.filename, 'rb')\n            signature = fd.read(4)\n            fd.close()\n        except:\n            raise Exception(\"Error loading file.\")\n\n        if signature[:4] == b\"\\x7f\\x45\\x4c\\x46\":\n            self._load_binary_elf(binary.filename)\n        elif signature[:2] == b\"\\x4d\\x5a\":\n            self._load_binary_pe(binary.filename)\n        else:\n            raise Exception(\"Unknown file format.\")", "fpath_tuple": ["Security", "barf", "barf", "arch", "emulator.py"], "context_start_lineno": 368, "line_no": 369, "id": "barf.arch.emulator.Emulator.load_binary", "target_function_prompt": "    def load_binary(self, binary):", "function_signature": "    def load_binary(self, binary):"}}
{"prompt": "    def path(self, path_text):", "metadata": {"task_id": "Utilities/boltons/99", "ground_truth": "        self.path_parts = tuple([unquote(p) if '%' in p else p\n                                 for p in to_unicode(path_text).split(u'/')])\n        return", "fpath_tuple": ["Utilities", "boltons", "boltons", "urlutils.py"], "context_start_lineno": 585, "line_no": 586, "id": "boltons.urlutils.URL.path", "target_function_prompt": "    def path(self, path_text):", "function_signature": "    def path(self, path_text):"}}
{"prompt": "    def sandbox(self, stdin=None, stdout=None, stderr=None):", "metadata": {"task_id": "System/mrjob/98", "ground_truth": "        self._stdin = stdin or BytesIO()\n        self._stdout = stdout or BytesIO()\n        self._stderr = stderr or BytesIO()\n\n        return self", "fpath_tuple": ["System", "mrjob", "mrjob", "job.py"], "context_start_lineno": 1602, "line_no": 1657, "id": "mrjob.job.MRJob.sandbox", "target_function_prompt": "    def sandbox(self, stdin=None, stdout=None, stderr=None):", "function_signature": "    def sandbox(self, stdin=None, stdout=None, stderr=None):"}}
{"prompt": "    def _bootstrap_mrjob(self):", "metadata": {"task_id": "System/mrjob/99", "ground_truth": "        if self._opts['bootstrap_mrjob'] is None:\n            return True\n        else:\n            return bool(self._opts['bootstrap_mrjob'])", "fpath_tuple": ["System", "mrjob", "mrjob", "runner.py"], "context_start_lineno": 1061, "line_no": 1063, "id": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "target_function_prompt": "    def _bootstrap_mrjob(self):", "function_signature": "    def _bootstrap_mrjob(self):"}}
{"prompt": "def construct_bash_launcher(linker, library_path, executable, full_linker=True):", "metadata": {"task_id": "System/exodus-bundler/15", "ground_truth": "    linker_dirname, linker_basename = os.path.split(linker)\n    full_linker = 'true' if full_linker else 'false'\n    return render_template_file('launcher.sh', linker_basename=linker_basename,\n                                linker_dirname=linker_dirname, library_path=library_path,\n                                executable=executable, full_linker=full_linker)", "fpath_tuple": ["System", "exodus-bundler", "src", "exodus_bundler", "launchers.py"], "context_start_lineno": 92, "line_no": 93, "id": "exodus_bundler.launchers.construct_bash_launcher", "target_function_prompt": "def construct_bash_launcher(linker, library_path, executable, full_linker=True):", "function_signature": "def construct_bash_launcher(linker, library_path, executable, full_linker=True):"}}
{"prompt": "def set_item(d, keys, value):", "metadata": {"task_id": "Text-Processing/python-benedict/3", "ground_truth": "    item = d\n    i = 0\n    j = len(keys)\n    while i < j:\n        key = keys[i]\n        if i < (j - 1):\n            item = _get_or_new_item_value(item, key, keys[i + 1])\n            i += 1\n            continue\n        _set_item_value(item, key, value)\n        break", "fpath_tuple": ["Text-Processing", "python-benedict", "benedict", "dicts", "keylist", "keylist_util.py"], "context_start_lineno": 68, "line_no": 69, "id": "benedict.dicts.keylist.keylist_util.set_item", "target_function_prompt": "def set_item(d, keys, value):", "function_signature": "def set_item(d, keys, value):"}}
{"prompt": "def get_recipe_order_and_bootstrap(ctx, names, bs=None, blacklist=None):\n    # Get set of recipe/dependency names, clean up and add bootstrap deps:", "metadata": {"task_id": "Utilities/python-for-android/34", "ground_truth": "    from pythonforandroid.bootstrap import Bootstrap\n    from pythonforandroid.logger import info\n    names = set(names)\n    if bs is not None and bs.recipe_depends:\n        names = names.union(set(bs.recipe_depends))\n    names = fix_deplist([\n        ([name] if not isinstance(name, (list, tuple)) else name)\n        for name in names\n    ])\n    if blacklist is None:\n        blacklist = set()\n    blacklist = {bitem.lower() for bitem in blacklist}\n\n    # Remove all values that are in the blacklist:\n    names_before_blacklist = list(names)\n    names = []\n    for name in names_before_blacklist:\n        cleaned_up_tuple = tuple([\n            item for item in name if item not in blacklist\n        ])\n        if cleaned_up_tuple:\n            names.append(cleaned_up_tuple)\n\n    # Do check for obvious conflicts (that would trigger in any order, and\n    # without comitting to any specific choice in a multi-choice tuple of\n    # dependencies):\n    obvious_conflict_checker(ctx, names, blacklist=blacklist)\n    # If we get here, no obvious conflicts!\n\n    # get all possible order graphs, as names may include tuples/lists\n    # of alternative dependencies\n    possible_orders = []\n    for name_set in product(*names):\n        new_possible_orders = [RecipeOrder(ctx)]\n        for name in name_set:\n            new_possible_orders = recursively_collect_orders(\n                name, ctx, name_set, orders=new_possible_orders,\n                blacklist=blacklist\n            )\n        possible_orders.extend(new_possible_orders)\n\n    # turn each order graph into a linear list if possible\n    orders = []\n    for possible_order in possible_orders:\n        try:\n            order = find_order(possible_order)\n        except ValueError:  # a circular dependency was found\n            info('Circular dependency found in graph {}, skipping it.'.format(\n                possible_order))\n            continue\n        orders.append(list(order))\n\n    # prefer python3 and SDL2 if available\n    orders = sorted(orders,\n                    key=lambda order: -('python3' in order) - ('sdl2' in order))\n\n    if not orders:\n        raise BuildInterruptingException(\n            'Didn\\'t find any valid dependency graphs. '\n            'This means that some of your '\n            'requirements pull in conflicting dependencies.')\n\n    # It would be better to check against possible orders other\n    # than the first one, but in practice clashes will be rare,\n    # and can be resolved by specifying more parameters\n    chosen_order = orders[0]\n    if len(orders) > 1:\n        info('Found multiple valid dependency orders:')\n        for order in orders:\n            info('    {}'.format(order))\n        info('Using the first of these: {}'.format(chosen_order))\n    else:\n        info('Found a single valid recipe set: {}'.format(chosen_order))\n\n    if bs is None:\n        bs = Bootstrap.get_bootstrap_from_recipes(chosen_order, ctx)\n        if bs is None:\n            # Note: don't remove this without thought, causes infinite loop\n            raise BuildInterruptingException(\n                \"Could not find any compatible bootstrap!\"\n            )\n        recipes, python_modules, bs = get_recipe_order_and_bootstrap(\n            ctx, chosen_order, bs=bs, blacklist=blacklist\n        )\n    else:\n        # check if each requirement has a recipe\n        recipes = []\n        python_modules = []\n        for name in chosen_order:\n            try:\n                recipe = Recipe.get_recipe(name, ctx)\n                python_modules += recipe.python_depends\n            except ValueError:\n                python_modules.append(name)\n            else:\n                recipes.append(name)\n\n    python_modules = list(set(python_modules))\n    return recipes, python_modules, bs", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "graph.py"], "context_start_lineno": 242, "line_no": 244, "id": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "target_function_prompt": "def get_recipe_order_and_bootstrap(ctx, names, bs=None, blacklist=None):\n    # Get set of recipe/dependency names, clean up and add bootstrap deps:", "function_signature": "def get_recipe_order_and_bootstrap(ctx, names, bs=None, blacklist=None):\n    # Get set of recipe/dependency names, clean up and add bootstrap deps:"}}
{"prompt": "def splitext(path):\n    # type: (Text) -> Tuple[Text, Text]", "metadata": {"task_id": "System/fs/28", "ground_truth": "    parent_path, pathname = split(path)\n    if pathname.startswith(\".\") and pathname.count(\".\") == 1:\n        return path, \"\"\n    if \".\" not in pathname:\n        return path, \"\"\n    pathname, ext = pathname.rsplit(\".\", 1)\n    path = join(parent_path, pathname)\n    return path, \".\" + ext", "fpath_tuple": ["System", "fs", "fs", "path.py"], "context_start_lineno": 320, "line_no": 339, "id": "fs.path.splitext", "target_function_prompt": "def splitext(path):\n    # type: (Text) -> Tuple[Text, Text]", "function_signature": "def splitext(path):\n    # type: (Text) -> Tuple[Text, Text]"}}
{"prompt": "    def _find_entry_index(self, key) -> int:", "metadata": {"task_id": "Database/bplustree/17", "ground_truth": "        entry = self._entry_class(\n            self._tree_conf,\n            key=key  # Hack to compare and order\n        )\n        i = bisect.bisect_left(self.entries, entry)\n        if i != len(self.entries) and self.entries[i] == entry:\n            return i\n        raise ValueError('No entry for key {}'.format(key))", "fpath_tuple": ["Database", "bplustree", "bplustree", "node.py"], "context_start_lineno": 122, "line_no": 123, "id": "bplustree.node.Node._find_entry_index", "target_function_prompt": "    def _find_entry_index(self, key) -> int:", "function_signature": "    def _find_entry_index(self, key) -> int:"}}
{"prompt": "    def get_default_config(self):", "metadata": {"task_id": "Multimedia/Mopidy/44", "ground_truth": "        conf_file = os.path.join(os.path.dirname(__file__), \"ext.conf\")\n        return config_lib.read(conf_file)", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "http", "__init__.py"], "context_start_lineno": 15, "line_no": 16, "id": "mopidy.http.Extension.get_default_config", "target_function_prompt": "    def get_default_config(self):", "function_signature": "    def get_default_config(self):"}}
{"prompt": "    def update_count(self, count: int, text_color: Optional[str] = None) -> None:", "metadata": {"task_id": "Communications/zulip-term/32", "ground_truth": "        new_color = self.original_color if text_color is None else text_color\n\n        self.count = count\n        if count == 0:\n            count_text = \"\"\n        else:\n            count_text = str(count)\n\n        self.update_widget((self.count_style, count_text), new_color)", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "ui_tools", "buttons.py"], "context_start_lineno": 57, "line_no": 58, "id": "zulipterminal.ui_tools.buttons.TopButton.update_count", "target_function_prompt": "    def update_count(self, count: int, text_color: Optional[str] = None) -> None:", "function_signature": "    def update_count(self, count: int, text_color: Optional[str] = None) -> None:"}}
{"prompt": "def resolve_invite(invite: Union[Invite, str]) -> ResolvedInvite:", "metadata": {"task_id": "Software-Development/discord-py/5", "ground_truth": "    from .invite import Invite  # circular import\n\n    if isinstance(invite, Invite):\n        return ResolvedInvite(invite.code, invite.scheduled_event_id)\n    else:\n        rx = r'(?:https?\\:\\/\\/)?discord(?:\\.gg|(?:app)?\\.com\\/invite)\\/[^/]+'\n        m = re.match(rx, invite)\n\n        if m:\n            url = yarl.URL(invite)\n            code = url.parts[-1]\n            event_id = url.query.get('event')\n\n            return ResolvedInvite(code, int(event_id) if event_id else None)\n    return ResolvedInvite(invite, None)", "fpath_tuple": ["Software-Development", "discord-py", "discord", "utils.py"], "context_start_lineno": 837, "line_no": 854, "id": "discord.utils.resolve_invite", "target_function_prompt": "def resolve_invite(invite: Union[Invite, str]) -> ResolvedInvite:", "function_signature": "def resolve_invite(invite: Union[Invite, str]) -> ResolvedInvite:"}}
{"prompt": "def connect_to_region(region_name, **kw_params):", "metadata": {"task_id": "Internet/boto/117", "ground_truth": "    from boto.regioninfo import connect\n    return connect('autoscaling', region_name,\n                   connection_cls=AutoScaleConnection, **kw_params)", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "autoscale", "__init__.py"], "context_start_lineno": 63, "line_no": 74, "id": "boto.ec2.autoscale.connect_to_region", "target_function_prompt": "def connect_to_region(region_name, **kw_params):", "function_signature": "def connect_to_region(region_name, **kw_params):"}}
{"prompt": "    def _stream_task_log_dirs(self, application_id=None, output_dir=None):", "metadata": {"task_id": "System/mrjob/100", "ground_truth": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if application_id:\n                path = self.fs.join(log_dir, 'userlogs', application_id)\n            else:\n                path = self.fs.join(log_dir, 'userlogs')\n\n            if _logs_exist(self.fs, path):\n                log.info('Looking for task syslogs in %s...' % path)\n                yield [path]", "fpath_tuple": ["System", "mrjob", "mrjob", "hadoop.py"], "context_start_lineno": 556, "line_no": 561, "id": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "target_function_prompt": "    def _stream_task_log_dirs(self, application_id=None, output_dir=None):", "function_signature": "    def _stream_task_log_dirs(self, application_id=None, output_dir=None):"}}
{"prompt": "def get_path(root, path, default=_UNSET):", "metadata": {"task_id": "Utilities/boltons/100", "ground_truth": "    if isinstance(path, basestring):\n        path = path.split('.')\n    cur = root\n    try:\n        for seg in path:\n            try:\n                cur = cur[seg]\n            except (KeyError, IndexError) as exc:\n                raise PathAccessError(exc, seg, path)\n            except TypeError as exc:\n                # either string index in a list, or a parent that\n                # doesn't support indexing\n                try:\n                    seg = int(seg)\n                    cur = cur[seg]\n                except (ValueError, KeyError, IndexError, TypeError):\n                    if not is_iterable(cur):\n                        exc = TypeError('%r object is not indexable'\n                                        % type(cur).__name__)\n                    raise PathAccessError(exc, seg, path)\n    except PathAccessError:\n        if default is _UNSET:\n            raise\n        return default\n    return cur", "fpath_tuple": ["Utilities", "boltons", "boltons", "iterutils.py"], "context_start_lineno": 1220, "line_no": 1253, "id": "boltons.iterutils.get_path", "target_function_prompt": "def get_path(root, path, default=_UNSET):", "function_signature": "def get_path(root, path, default=_UNSET):"}}
{"prompt": "    def deserialize(self, value):", "metadata": {"task_id": "Multimedia/Mopidy/45", "ground_truth": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "config", "types.py"], "context_start_lineno": 174, "line_no": 175, "id": "mopidy.config.types.Float.deserialize", "target_function_prompt": "    def deserialize(self, value):", "function_signature": "    def deserialize(self, value):"}}
{"prompt": "    def container(self, data):", "metadata": {"task_id": "Communications/hl7/6", "ground_truth": "        return self.containers[0](\n            sequence=data,\n            esc=self.esc,\n            separators=self.separators,\n            factory=self.factory,\n        )", "fpath_tuple": ["Communications", "hl7", "hl7", "parser.py"], "context_start_lineno": 400, "line_no": 404, "id": "hl7.parser._ParsePlan.container", "target_function_prompt": "    def container(self, data):", "function_signature": "    def container(self, data):"}}
{"prompt": "    def to_xml(self):", "metadata": {"task_id": "Internet/boto/118", "ground_truth": "        inner_text = []\n        for rule in self:\n            inner_text.append(rule.to_xml())\n        return tag('RoutingRules', '\\n'.join(inner_text))", "fpath_tuple": ["Internet", "boto", "boto", "s3", "website.py"], "context_start_lineno": 166, "line_no": 167, "id": "boto.s3.website.RoutingRules.to_xml", "target_function_prompt": "    def to_xml(self):", "function_signature": "    def to_xml(self):"}}
{"prompt": "def get_key(accidentals=0):", "metadata": {"task_id": "Multimedia/mingus/50", "ground_truth": "    from mingus.core.mt_exceptions import RangeError\n    if accidentals not in range(-7, 8):\n        raise RangeError(\"integer not in range (-7)-(+7).\")\n    return keys[accidentals + 7]", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "keys.py"], "context_start_lineno": 65, "line_no": 72, "id": "mingus.core.keys.get_key", "target_function_prompt": "def get_key(accidentals=0):", "function_signature": "def get_key(accidentals=0):"}}
{"prompt": "def create_adapter(adapter_name, base_filepath=None):", "metadata": {"task_id": "Communications/chatette/21", "ground_truth": "    from chatette.adapters.jsonl import JsonListAdapter\n    from chatette.adapters.rasa import RasaAdapter\n    from chatette.adapters.rasa_md import RasaMdAdapter\n    if adapter_name is None:\n        return None\n    adapter_name = adapter_name.lower()\n    if adapter_name == 'rasa':\n        return RasaAdapter(base_filepath)\n    elif adapter_name in ('rasa-md', 'rasamd'):\n        return RasaMdAdapter(base_filepath)\n    elif adapter_name == 'jsonl':\n        return JsonListAdapter(base_filepath)\n    raise ValueError(\"Unknown adapter was selected.\")", "fpath_tuple": ["Communications", "chatette", "chatette", "adapters", "factory.py"], "context_start_lineno": 10, "line_no": 18, "id": "chatette.adapters.factory.create_adapter", "target_function_prompt": "def create_adapter(adapter_name, base_filepath=None):", "function_signature": "def create_adapter(adapter_name, base_filepath=None):"}}
{"prompt": "    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.PdDataFrame:", "metadata": {"task_id": "Scientific-Engineering/bentoml/38", "ground_truth": "        if payload.meta[\"with_buffer\"]:\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return pep574_loads(bs, payload.data, indices)\n        else:\n            return pep574_loads(payload.data, b\"\", [])", "fpath_tuple": ["Scientific-Engineering", "bentoml", "src", "bentoml", "_internal", "runner", "container.py"], "context_start_lineno": 417, "line_no": 421, "id": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "target_function_prompt": "    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.PdDataFrame:", "function_signature": "    def from_payload(\n        cls,\n        payload: Payload,\n    ) -> ext.PdDataFrame:"}}
{"prompt": "def is_rtf(arg, treat_str_as_data=False):", "metadata": {"task_id": "Security/oletools/7", "ground_truth": "    magic_len = len(RTF_MAGIC)\n    if isinstance(arg, UNICODE_TYPE):\n        with open(arg, 'rb') as reader:\n            return reader.read(len(RTF_MAGIC)) == RTF_MAGIC\n    if isinstance(arg, bytes) and not isinstance(arg, str):  # only in PY3\n        return arg[:magic_len] == RTF_MAGIC\n    if isinstance(arg, bytearray):\n        return arg[:magic_len] == RTF_MAGIC\n    if isinstance(arg, str):      # could be bytes, but we assume file name\n        if treat_str_as_data:\n            try:\n                return arg[:magic_len].encode('ascii', errors='strict')\\\n                    == RTF_MAGIC\n            except UnicodeError:\n                return False\n        else:\n            with open(arg, 'rb') as reader:\n                return reader.read(len(RTF_MAGIC)) == RTF_MAGIC\n    if hasattr(arg, 'read'):      # a stream (i.e. file-like object)\n        return arg.read(len(RTF_MAGIC)) == RTF_MAGIC\n    if isinstance(arg, (list, tuple)):\n        iter_arg = iter(arg)\n    else:\n        iter_arg = arg\n\n    # check iterable\n    for magic_byte in zip(RTF_MAGIC):\n        try:\n            if next(iter_arg) not in magic_byte:\n                return False\n        except StopIteration:\n            return False\n\n    return True  # checked the complete magic without returning False --> match", "fpath_tuple": ["Security", "oletools", "oletools", "rtfobj.py"], "context_start_lineno": 784, "line_no": 794, "id": "oletools.rtfobj.is_rtf", "target_function_prompt": "def is_rtf(arg, treat_str_as_data=False):", "function_signature": "def is_rtf(arg, treat_str_as_data=False):"}}
{"prompt": "def pythonize_name(name):", "metadata": {"task_id": "Internet/boto/119", "ground_truth": "    s1 = _first_cap_regex.sub(r'\\1_\\2', name)\n    s2 = _number_cap_regex.sub(r'\\1_\\2', s1)\n    return _end_cap_regex.sub(r'\\1_\\2', s2).lower()", "fpath_tuple": ["Internet", "boto", "boto", "utils.py"], "context_start_lineno": 883, "line_no": 896, "id": "boto.utils.pythonize_name", "target_function_prompt": "def pythonize_name(name):", "function_signature": "def pythonize_name(name):"}}
{"prompt": "    def add(self, path):", "metadata": {"task_id": "System/mrjob/101", "ground_truth": "        if is_uri(path):\n            return path\n\n        if path not in self._path_to_name:\n            # use unhide so that input files won't be hidden from Hadoop,\n            # see #1200\n            name = name_uniquely(\n                path, names_taken=self._names_taken, unhide=True)\n            self._names_taken.add(name)\n            self._path_to_name[path] = name\n\n        return self.uri(path)", "fpath_tuple": ["System", "mrjob", "mrjob", "setup.py"], "context_start_lineno": 313, "line_no": 318, "id": "mrjob.setup.UploadDirManager.add", "target_function_prompt": "    def add(self, path):", "function_signature": "    def add(self, path):"}}
{"prompt": "    def text(self):", "metadata": {"task_id": "Internet/pyramid/95", "ground_truth": "        from pyramid.util import object_description\n        return getattr(\n            self.func,\n            '__text__',\n            'custom predicate: %s' % object_description(self.func),\n        )", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "predicates.py"], "context_start_lineno": 209, "line_no": 210, "id": "pyramid.predicates.CustomPredicate.text", "target_function_prompt": "    def text(self):", "function_signature": "    def text(self):"}}
{"prompt": "    def suffix(self):\n        # type: () -> Text", "metadata": {"task_id": "System/fs/29", "ground_truth": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return \"\"\n        basename, dot, ext = name.rpartition(\".\")\n        return \".\" + ext if dot else \"\"", "fpath_tuple": ["System", "fs", "fs", "info.py"], "context_start_lineno": 206, "line_no": 221, "id": "fs.info.Info.suffix", "target_function_prompt": "    def suffix(self):\n        # type: () -> Text", "function_signature": "    def suffix(self):\n        # type: () -> Text"}}
{"prompt": "    def save(self, output_file: Union[str, TextIO] = \"result.html\", file_info: bool = True) -> None:", "metadata": {"task_id": "System/viztracer/4", "ground_truth": "        if isinstance(output_file, str):\n            file_type = output_file.split(\".\")[-1]\n\n            if file_type == \"html\":\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, output_format=\"html\", file_info=file_info)\n            elif file_type == \"json\":\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, output_format=\"json\", file_info=file_info)\n            elif file_type == \"gz\":\n                with gzip.open(output_file, \"wt\") as f:\n                    self.generate_report(f, output_format=\"json\", file_info=file_info)\n            else:\n                raise Exception(\"Only html, json and gz are supported\")\n        else:\n            self.generate_report(output_file, output_format=\"json\", file_info=file_info)\n\n        if isinstance(output_file, str):\n            self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file)}))\n\n        self.print_messages()", "fpath_tuple": ["System", "viztracer", "src", "viztracer", "report_builder.py"], "context_start_lineno": 171, "line_no": 172, "id": "viztracer.report_builder.ReportBuilder.save", "target_function_prompt": "    def save(self, output_file: Union[str, TextIO] = \"result.html\", file_info: bool = True) -> None:", "function_signature": "    def save(self, output_file: Union[str, TextIO] = \"result.html\", file_info: bool = True) -> None:"}}
{"prompt": "    def _root_node(self) -> Union['LonelyRootNode', 'RootNode']:", "metadata": {"task_id": "Database/bplustree/18", "ground_truth": "        root_node = self._mem.get_node(self._root_node_page)\n        assert isinstance(root_node, (LonelyRootNode, RootNode))\n        return root_node", "fpath_tuple": ["Database", "bplustree", "bplustree", "tree.py"], "context_start_lineno": 271, "line_no": 272, "id": "bplustree.tree.BPlusTree._root_node", "target_function_prompt": "    def _root_node(self) -> Union['LonelyRootNode', 'RootNode']:", "function_signature": "    def _root_node(self) -> Union['LonelyRootNode', 'RootNode']:"}}
{"prompt": "def add_op(state: State, op_func, *args, **kwargs):", "metadata": {"task_id": "System/pyinfra/15", "ground_truth": "    from pyinfra.context import ctx_host\n    from pyinfra.context import ctx_state\n    if pyinfra.is_cli:\n        raise PyinfraError(\n            (\"`add_op` should not be called when pyinfra is executing in CLI mode! ({0})\").format(\n                get_call_location(),\n            ),\n        )\n\n    hosts = kwargs.pop(\"host\", state.inventory.iter_active_hosts())\n    if isinstance(hosts, Host):\n        hosts = [hosts]\n\n    with ctx_state.use(state):\n        results = {}\n        for op_host in hosts:\n            with ctx_host.use(op_host):\n                results[op_host] = op_func(*args, **kwargs)\n\n    return results", "fpath_tuple": ["System", "pyinfra", "pyinfra", "api", "operation.py"], "context_start_lineno": 84, "line_no": 95, "id": "pyinfra.api.operation.add_op", "target_function_prompt": "def add_op(state: State, op_func, *args, **kwargs):", "function_signature": "def add_op(state: State, op_func, *args, **kwargs):"}}
{"prompt": "    def begin(self, request=_marker):", "metadata": {"task_id": "Internet/pyramid/96", "ground_truth": "        if request is _marker:\n            current = self.manager.get()\n            if current['registry'] == self.registry:\n                request = current['request']\n            else:\n                request = None\n        self.manager.push({'registry': self.registry, 'request': request})", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "__init__.py"], "context_start_lineno": 753, "line_no": 771, "id": "pyramid.config.Configurator.begin", "target_function_prompt": "    def begin(self, request=_marker):", "function_signature": "    def begin(self, request=_marker):"}}
{"prompt": "def resolve_annotation(\n    annotation: Any,\n    globalns: Dict[str, Any],\n    localns: Optional[Dict[str, Any]],\n    cache: Optional[Dict[str, Any]],\n) -> Any:", "metadata": {"task_id": "Software-Development/discord-py/6", "ground_truth": "    if annotation is None:\n        return type(None)\n    if isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n\n    locals = globalns if localns is None else localns\n    if cache is None:\n        cache = {}\n    return evaluate_annotation(annotation, globalns, locals, cache)", "fpath_tuple": ["Software-Development", "discord-py", "discord", "utils.py"], "context_start_lineno": 1160, "line_no": 1166, "id": "discord.utils.resolve_annotation", "target_function_prompt": "def resolve_annotation(\n    annotation: Any,\n    globalns: Dict[str, Any],\n    localns: Optional[Dict[str, Any]],\n    cache: Optional[Dict[str, Any]],\n) -> Any:", "function_signature": "def resolve_annotation(\n    annotation: Any,\n    globalns: Dict[str, Any],\n    localns: Optional[Dict[str, Any]],\n    cache: Optional[Dict[str, Any]],\n) -> Any:"}}
{"prompt": "    def __repr__(self):", "metadata": {"task_id": "Internet/pyramid/97", "ground_truth": "        self._assert_resolved()\n        return '<%s category %r, discriminator %r>' % (\n            self.__class__.__name__,\n            self.category_name,\n            self.discriminator,\n        )", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "registry.py"], "context_start_lineno": 239, "line_no": 240, "id": "pyramid.registry.Introspectable.__repr__", "target_function_prompt": "    def __repr__(self):", "function_signature": "    def __repr__(self):"}}
{"prompt": "def _match_history_log_path(path, job_id=None):", "metadata": {"task_id": "System/mrjob/102", "ground_truth": "    m = _HISTORY_LOG_PATH_RE.match(path)\n    if not m:\n        return None\n\n    if not (job_id is None or m.group('job_id') == job_id):\n        return None\n\n    # TODO: couldn't manage to include .jhist in regex; an optional\n    # group has less priority than a non-greedy match, apparently\n    return dict(job_id=m.group('job_id'), yarn='.jhist' in m.group('suffix'))", "fpath_tuple": ["System", "mrjob", "mrjob", "logs", "history.py"], "context_start_lineno": 94, "line_no": 98, "id": "mrjob.logs.history._match_history_log_path", "target_function_prompt": "def _match_history_log_path(path, job_id=None):", "function_signature": "def _match_history_log_path(path, job_id=None):"}}
{"prompt": "    def replace(self, **kwargs):", "metadata": {"task_id": "Multimedia/Mopidy/46", "ground_truth": "        if not kwargs:\n            return self\n        other = super().replace(**kwargs)\n        if hasattr(self, \"_hash\"):\n            object.__delattr__(other, \"_hash\")\n        return self._instances.setdefault(weakref.ref(other), other)", "fpath_tuple": ["Multimedia", "Mopidy", "mopidy", "models", "immutable.py"], "context_start_lineno": 194, "line_no": 213, "id": "mopidy.models.immutable.ValidatedImmutableObject.replace", "target_function_prompt": "    def replace(self, **kwargs):", "function_signature": "    def replace(self, **kwargs):"}}
{"prompt": "def choose_encoder(accept_header: str) -> Tuple[Callable[[CollectorRegistry], bytes], str]:", "metadata": {"task_id": "System/prometheus-client/4", "ground_truth": "    from .openmetrics import exposition as openmetrics\n    accept_header = accept_header or ''\n    for accepted in accept_header.split(','):\n        if accepted.split(';')[0].strip() == 'application/openmetrics-text':\n            return (openmetrics.generate_latest,\n                    openmetrics.CONTENT_TYPE_LATEST)\n    return generate_latest, CONTENT_TYPE_LATEST", "fpath_tuple": ["System", "prometheus-client", "prometheus_client", "exposition.py"], "context_start_lineno": 240, "line_no": 241, "id": "prometheus_client.exposition.choose_encoder", "target_function_prompt": "def choose_encoder(accept_header: str) -> Tuple[Callable[[CollectorRegistry], bytes], str]:", "function_signature": "def choose_encoder(accept_header: str) -> Tuple[Callable[[CollectorRegistry], bytes], str]:"}}
{"prompt": "    def created(self):\n        # type: () -> Optional[datetime]", "metadata": {"task_id": "System/fs/30", "ground_truth": "        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time", "fpath_tuple": ["System", "fs", "fs", "info.py"], "context_start_lineno": 327, "line_no": 338, "id": "fs.info.Info.created", "target_function_prompt": "    def created(self):\n        # type: () -> Optional[datetime]", "function_signature": "    def created(self):\n        # type: () -> Optional[datetime]"}}
{"prompt": "    def add(self, spec, renderer):", "metadata": {"task_id": "Internet/pyramid/98", "ground_truth": "        self.renderers[spec] = renderer\n        if ':' in spec:\n            package, relative = spec.split(':', 1)\n            self.renderers[relative] = renderer", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "testing.py"], "context_start_lineno": 570, "line_no": 571, "id": "pyramid.testing.DummyRendererFactory.add", "target_function_prompt": "    def add(self, spec, renderer):", "function_signature": "    def add(self, spec, renderer):"}}
{"prompt": "def _parse_progress_from_job_tracker(html_bytes):", "metadata": {"task_id": "System/mrjob/103", "ground_truth": "    start = html_bytes.rfind(b'Running Jobs')\n    if start == -1:\n        return None, None\n    end = html_bytes.find(b'Jobs', start + len(b'Running Jobs'))\n    if end == -1:\n        end = None\n\n    html_bytes = html_bytes[start:end]\n\n    # search it for percents\n    matches = _JOB_TRACKER_HTML_RE.findall(html_bytes)\n    if len(matches) >= 2:\n        return float(matches[0]), float(matches[1])\n    else:\n        return None, None", "fpath_tuple": ["System", "mrjob", "mrjob", "parse.py"], "context_start_lineno": 172, "line_no": 179, "id": "mrjob.parse._parse_progress_from_job_tracker", "target_function_prompt": "def _parse_progress_from_job_tracker(html_bytes):", "function_signature": "def _parse_progress_from_job_tracker(html_bytes):"}}
{"prompt": "    def delete_header(self, name):", "metadata": {"task_id": "Internet/falcon/48", "ground_truth": "        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to remove cookies')\n\n        self._headers.pop(name, None)", "fpath_tuple": ["Internet", "falcon", "falcon", "response.py"], "context_start_lineno": 650, "line_no": 677, "id": "falcon.response.Response.delete_header", "target_function_prompt": "    def delete_header(self, name):", "function_signature": "    def delete_header(self, name):"}}
{"prompt": "    def key_method(self, document, sentences_count, weight=0.5):", "metadata": {"task_id": "Internet/sumy/28", "ground_truth": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)", "fpath_tuple": ["Internet", "sumy", "sumy", "summarizers", "edmundson.py"], "context_start_lineno": 100, "line_no": 101, "id": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "target_function_prompt": "    def key_method(self, document, sentences_count, weight=0.5):", "function_signature": "    def key_method(self, document, sentences_count, weight=0.5):"}}
{"prompt": "def cache_file(file_name: str, url: str) -> Path:", "metadata": {"task_id": "Software-Development/pandas-profiling/10", "ground_truth": "    data_path = get_data_path()\n    data_path.mkdir(exist_ok=True)\n\n    file_path = data_path / file_name\n\n    # If not exists, download and create file\n    if not file_path.exists():\n        response = request.urlopen(url)\n        file_path.write_bytes(response.read())\n\n    return file_path", "fpath_tuple": ["Software-Development", "pandas-profiling", "src", "ydata_profiling", "utils", "cache.py"], "context_start_lineno": 8, "line_no": 19, "id": "ydata_profiling.utils.cache.cache_file", "target_function_prompt": "def cache_file(file_name: str, url: str) -> Path:", "function_signature": "def cache_file(file_name: str, url: str) -> Path:"}}
{"prompt": "def var(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n        **unused_args):", "metadata": {"task_id": "Security/diffprivlib/29", "ground_truth": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)", "fpath_tuple": ["Security", "diffprivlib", "diffprivlib", "tools", "utils.py"], "context_start_lineno": 303, "line_no": 357, "id": "diffprivlib.tools.utils.var", "target_function_prompt": "def var(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n        **unused_args):", "function_signature": "def var(array, epsilon=1.0, bounds=None, axis=None, dtype=None, keepdims=False, random_state=None, accountant=None,\n        **unused_args):"}}
{"prompt": "    def seek(self, pos, mode=0):", "metadata": {"task_id": "Utilities/boltons/101", "ground_truth": "        self._checkClosed()\n        return self.buffer.seek(pos, mode)", "fpath_tuple": ["Utilities", "boltons", "boltons", "ioutils.py"], "context_start_lineno": 328, "line_no": 329, "id": "boltons.ioutils.SpooledBytesIO.seek", "target_function_prompt": "    def seek(self, pos, mode=0):", "function_signature": "    def seek(self, pos, mode=0):"}}
{"prompt": "def successful_GUI_return_code() -> int:", "metadata": {"task_id": "Communications/zulip-term/33", "ground_truth": "    if PLATFORM == \"WSL\":\n        return 1\n\n    return 0", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "platform_code.py"], "context_start_lineno": 56, "line_no": 64, "id": "zulipterminal.platform_code.successful_GUI_return_code", "target_function_prompt": "def successful_GUI_return_code() -> int:", "function_signature": "def successful_GUI_return_code() -> int:"}}
{"prompt": "    def from_file(cls, file_path, url, tokenizer):", "metadata": {"task_id": "Internet/sumy/29", "ground_truth": "        with open(file_path, \"rb\") as file:\n            return cls(file.read(), tokenizer, url)", "fpath_tuple": ["Internet", "sumy", "sumy", "parsers", "html.py"], "context_start_lineno": 29, "line_no": 30, "id": "sumy.parsers.html.HtmlParser.from_file", "target_function_prompt": "    def from_file(cls, file_path, url, tokenizer):", "function_signature": "    def from_file(cls, file_path, url, tokenizer):"}}
{"prompt": "    def darwin_installer(self):", "metadata": {"task_id": "Utilities/python-for-android/35", "ground_truth": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])", "fpath_tuple": ["Utilities", "python-for-android", "pythonforandroid", "prerequisites.py"], "context_start_lineno": 314, "line_no": 315, "id": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "target_function_prompt": "    def darwin_installer(self):", "function_signature": "    def darwin_installer(self):"}}
{"prompt": "    def processSpec(self, spec):", "metadata": {"task_id": "Internet/pyramid/99", "ground_truth": "        if spec in self._seen_files:\n            return False\n        self._seen_files.add(spec)\n        return True", "fpath_tuple": ["Internet", "pyramid", "src", "pyramid", "config", "actions.py"], "context_start_lineno": 163, "line_no": 172, "id": "pyramid.config.actions.ActionState.processSpec", "target_function_prompt": "    def processSpec(self, spec):", "function_signature": "    def processSpec(self, spec):"}}
{"prompt": "    def heads(self) -> Tuple[str, ...]:", "metadata": {"task_id": "Database/alembic/50", "ground_truth": "        self._revision_map\n        return self.heads", "fpath_tuple": ["Database", "alembic", "alembic", "script", "revision.py"], "context_start_lineno": 133, "line_no": 142, "id": "alembic.script.revision.RevisionMap.heads", "target_function_prompt": "    def heads(self) -> Tuple[str, ...]:", "function_signature": "    def heads(self) -> Tuple[str, ...]:"}}
{"prompt": "def get_key_signature_accidentals(key=\"C\"):", "metadata": {"task_id": "Multimedia/mingus/51", "ground_truth": "    from mingus.core import notes\n    accidentals = get_key_signature(key)\n    res = []\n\n    if accidentals < 0:\n        for i in range(-accidentals):\n            res.append(\"{0}{1}\".format(list(reversed(notes.fifths))[i], \"b\"))\n    elif accidentals > 0:\n        for i in range(accidentals):\n            res.append(\"{0}{1}\".format(notes.fifths[i], \"#\"))\n    return res", "fpath_tuple": ["Multimedia", "mingus", "mingus", "core", "keys.py"], "context_start_lineno": 93, "line_no": 95, "id": "mingus.core.keys.get_key_signature_accidentals", "target_function_prompt": "def get_key_signature_accidentals(key=\"C\"):", "function_signature": "def get_key_signature_accidentals(key=\"C\"):"}}
{"prompt": "def is_command_key(command: str, key: str) -> bool:", "metadata": {"task_id": "Communications/zulip-term/34", "ground_truth": "    try:\n        return key in KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "config", "keys.py"], "context_start_lineno": 423, "line_no": 428, "id": "zulipterminal.config.keys.is_command_key", "target_function_prompt": "def is_command_key(command: str, key: str) -> bool:", "function_signature": "def is_command_key(command: str, key: str) -> bool:"}}
{"prompt": "def keys_for_command(command: str) -> List[str]:", "metadata": {"task_id": "Communications/zulip-term/35", "ground_truth": "    try:\n        return list(KEY_BINDINGS[command][\"keys\"])\n    except KeyError as exception:\n        raise InvalidCommand(command)", "fpath_tuple": ["Communications", "zulip-term", "zulipterminal", "config", "keys.py"], "context_start_lineno": 434, "line_no": 438, "id": "zulipterminal.config.keys.keys_for_command", "target_function_prompt": "def keys_for_command(command: str) -> List[str]:", "function_signature": "def keys_for_command(command: str) -> List[str]:"}}
{"prompt": "    def attach(self, instance_id, device, dry_run=False):", "metadata": {"task_id": "Internet/boto/120", "ground_truth": "        return self.connection.attach_volume(\n            self.id,\n            instance_id,\n            device,\n            dry_run=dry_run\n        )", "fpath_tuple": ["Internet", "boto", "boto", "ec2", "volume.py"], "context_start_lineno": 136, "line_no": 151, "id": "boto.ec2.volume.Volume.attach", "target_function_prompt": "    def attach(self, instance_id, device, dry_run=False):", "function_signature": "    def attach(self, instance_id, device, dry_run=False):"}}
{"prompt": "def fully_qualify_hdfs_path(path):", "metadata": {"task_id": "System/mrjob/104", "ground_truth": "    from mrjob.parse import is_uri\n    if is_uri(path):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        return 'hdfs:///user/%s/%s' % (getpass.getuser(), path)", "fpath_tuple": ["System", "mrjob", "mrjob", "hadoop.py"], "context_start_lineno": 107, "line_no": 109, "id": "mrjob.hadoop.fully_qualify_hdfs_path", "target_function_prompt": "def fully_qualify_hdfs_path(path):", "function_signature": "def fully_qualify_hdfs_path(path):"}}
{"prompt": "    def from_traceback(cls, tb=None, limit=None):", "metadata": {"task_id": "Utilities/boltons/102", "ground_truth": "        ret = []\n        if tb is None:\n            tb = sys.exc_info()[2]\n            if tb is None:\n                raise ValueError('no tb set and no exception being handled')\n        if limit is None:\n            limit = getattr(sys, 'tracebacklimit', 1000)\n        n = 0\n        while tb is not None and n < limit:\n            item = cls.callpoint_type.from_tb(tb)\n            ret.append(item)\n            tb = tb.tb_next\n            n += 1\n        return cls(ret)", "fpath_tuple": ["Utilities", "boltons", "boltons", "tbutils.py"], "context_start_lineno": 291, "line_no": 308, "id": "boltons.tbutils.TracebackInfo.from_traceback", "target_function_prompt": "    def from_traceback(cls, tb=None, limit=None):", "function_signature": "    def from_traceback(cls, tb=None, limit=None):"}}
{"prompt": "    def items(self) -> t.Iterable[t.Tuple[t.Any, t.Any]]:", "metadata": {"task_id": "Internet/Jinja2/16", "ground_truth": "        result = [(key, self._mapping[key]) for key in list(self._queue)]\n        result.reverse()\n        return result", "fpath_tuple": ["Internet", "Jinja2", "src", "jinja2", "utils.py"], "context_start_lineno": 543, "line_no": 545, "id": "jinja2.utils.LRUCache.items", "target_function_prompt": "    def items(self) -> t.Iterable[t.Tuple[t.Any, t.Any]]:", "function_signature": "    def items(self) -> t.Iterable[t.Tuple[t.Any, t.Any]]:"}}
{"prompt": "    def register(self, refresh_token):", "metadata": {"task_id": "Utilities/praw/13", "ground_truth": "        cursor = self._connection.execute(\n            \"INSERT OR IGNORE INTO tokens VALUES (?, ?, datetime('now'))\",\n            (self.key, refresh_token),\n        )\n        self._connection.commit()\n        return cursor.rowcount == 1", "fpath_tuple": ["Utilities", "praw", "praw", "util", "token_manager.py"], "context_start_lineno": 180, "line_no": 188, "id": "praw.util.token_manager.SQLiteTokenManager.register", "target_function_prompt": "    def register(self, refresh_token):", "function_signature": "    def register(self, refresh_token):"}}
